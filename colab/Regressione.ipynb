{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regressione.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visiont3lab/deep-learning-course/blob/main/colab/Regressione.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nESzafMjgIou"
      },
      "source": [
        "## Importa Libreria"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N35_WMlfOgf"
      },
      "source": [
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "#!pip install torchsummary\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset,Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "# Loss function pytorch: https://neptune.ai/blog/pytorch-loss-functions\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLFs2Z3Wfew4"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kajs9lR3fRK7",
        "outputId": "366c1dad-0598-4fb8-a0ca-6672d4d1a087"
      },
      "source": [
        "# Dati Numpy\n",
        "X = np.linspace(-2,8,1000)\n",
        "Y = np.exp(0.2*X)*np.sin(3*X) - 10*np.cos(X)\n",
        "\n",
        "# Normalization\n",
        "R_mean = np.mean(X)\n",
        "R_std = np.std(X)\n",
        "\n",
        "# Dati Pytorch Tensor\n",
        "Xt = torch.from_numpy(X).type(torch.float32).reshape(-1,1)#.unsqueeze(1)\n",
        "Yt = torch.from_numpy(Y).type(torch.float32).unsqueeze(1)\n",
        "print(f\"X Tensor data shape: \", Xt.shape)\n",
        "print(f\"Y Tensor data shape: \", Yt.shape)\n",
        "\n",
        "# Training and Test Set\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3,shuffle=True,random_state=4)\n",
        "print(f\"X Train shape: {X_train.shape} , X Test shape: {X_test.shape}\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X Tensor data shape:  torch.Size([1000, 1])\n",
            "Y Tensor data shape:  torch.Size([1000, 1])\n",
            "X Train shape: (700,) , X Test shape: (300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "TGgqiGibfWNw",
        "outputId": "c857027f-3e02-4ff9-e1be-f915f7c29c76"
      },
      "source": [
        "# Visualization\n",
        "fig = go.Figure()\n",
        "fig.add_traces( go.Scatter(x=X, y=Y,hovertemplate='x: %{x} <br>y: %{y}',mode=\"markers\", name=\"Real data\") )\n",
        "fig.add_traces( go.Scatter(x=X_train, y=Y_train,hovertemplate='x: %{x} <br>y: %{y}',mode=\"markers\", name=\"Train data\") )\n",
        "fig.add_traces( go.Scatter(x=X_test, y=Y_test,hovertemplate='x: %{x} <br>y: %{y}',mode=\"markers\", name=\"Test data\") )\n",
        "\n",
        "fig.update_layout(title=\"Funzione di stimare\")\n",
        "fig.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"7eaf77b7-0086-44cc-b0f6-3e617df4ffca\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"7eaf77b7-0086-44cc-b0f6-3e617df4ffca\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '7eaf77b7-0086-44cc-b0f6-3e617df4ffca',\n",
              "                        [{\"hovertemplate\": \"x: %{x} <br>y: %{y}\", \"mode\": \"markers\", \"name\": \"Real data\", \"type\": \"scatter\", \"x\": [-2.0, -1.98998998998999, -1.97997997997998, -1.96996996996997, -1.95995995995996, -1.94994994994995, -1.93993993993994, -1.92992992992993, -1.91991991991992, -1.90990990990991, -1.8998998998999, -1.88988988988989, -1.87987987987988, -1.86986986986987, -1.85985985985986, -1.84984984984985, -1.83983983983984, -1.82982982982983, -1.8198198198198199, -1.8098098098098099, -1.7997997997997999, -1.7897897897897899, -1.7797797797797799, -1.7697697697697699, -1.7597597597597598, -1.7497497497497498, -1.7397397397397398, -1.7297297297297298, -1.7197197197197198, -1.7097097097097098, -1.6996996996996998, -1.6896896896896898, -1.6796796796796798, -1.6696696696696698, -1.6596596596596598, -1.6496496496496498, -1.6396396396396398, -1.6296296296296298, -1.6196196196196198, -1.6096096096096097, -1.5995995995995997, -1.5895895895895895, -1.5795795795795795, -1.5695695695695695, -1.5595595595595595, -1.5495495495495495, -1.5395395395395395, -1.5295295295295295, -1.5195195195195195, -1.5095095095095095, -1.4994994994994995, -1.4894894894894894, -1.4794794794794794, -1.4694694694694694, -1.4594594594594594, -1.4494494494494494, -1.4394394394394394, -1.4294294294294294, -1.4194194194194194, -1.4094094094094094, -1.3993993993993994, -1.3893893893893894, -1.3793793793793794, -1.3693693693693694, -1.3593593593593594, -1.3493493493493494, -1.3393393393393394, -1.3293293293293293, -1.3193193193193193, -1.3093093093093093, -1.2992992992992993, -1.2892892892892893, -1.2792792792792793, -1.2692692692692693, -1.2592592592592593, -1.2492492492492493, -1.2392392392392393, -1.2292292292292293, -1.2192192192192193, -1.2092092092092093, -1.1991991991991993, -1.189189189189189, -1.179179179179179, -1.169169169169169, -1.159159159159159, -1.149149149149149, -1.139139139139139, -1.129129129129129, -1.119119119119119, -1.109109109109109, -1.099099099099099, -1.089089089089089, -1.079079079079079, -1.069069069069069, -1.059059059059059, -1.049049049049049, -1.039039039039039, -1.029029029029029, -1.019019019019019, -1.009009009009009, -0.9989989989989989, -0.9889889889889889, -0.9789789789789789, -0.9689689689689689, -0.9589589589589589, -0.9489489489489489, -0.9389389389389389, -0.9289289289289289, -0.9189189189189189, -0.9089089089089089, -0.8988988988988988, -0.8888888888888888, -0.8788788788788788, -0.8688688688688688, -0.8588588588588588, -0.8488488488488488, -0.8388388388388388, -0.8288288288288288, -0.8188188188188188, -0.8088088088088088, -0.7987987987987988, -0.7887887887887888, -0.7787787787787788, -0.7687687687687688, -0.7587587587587588, -0.7487487487487487, -0.7387387387387387, -0.7287287287287287, -0.7187187187187187, -0.7087087087087087, -0.6986986986986987, -0.6886886886886887, -0.6786786786786787, -0.6686686686686687, -0.6586586586586587, -0.6486486486486487, -0.6386386386386387, -0.6286286286286287, -0.6186186186186187, -0.6086086086086087, -0.5985985985985987, -0.5885885885885886, -0.5785785785785786, -0.5685685685685686, -0.5585585585585586, -0.5485485485485486, -0.5385385385385386, -0.5285285285285286, -0.5185185185185186, -0.5085085085085086, -0.4984984984984986, -0.4884884884884886, -0.4784784784784786, -0.46846846846846857, -0.45845845845845856, -0.44844844844844856, -0.43843843843843855, -0.42842842842842854, -0.41841841841841854, -0.40840840840840853, -0.3983983983983985, -0.3883883883883883, -0.3783783783783783, -0.3683683683683683, -0.3583583583583583, -0.34834834834834827, -0.33833833833833826, -0.32832832832832826, -0.31831831831831825, -0.30830830830830824, -0.29829829829829824, -0.28828828828828823, -0.2782782782782782, -0.2682682682682682, -0.2582582582582582, -0.2482482482482482, -0.2382382382382382, -0.2282282282282282, -0.21821821821821819, -0.20820820820820818, -0.19819819819819817, -0.18818818818818817, -0.17817817817817816, -0.16816816816816815, -0.15815815815815815, -0.14814814814814814, -0.13813813813813813, -0.12812812812812813, -0.11811811811811812, -0.10810810810810811, -0.09809809809809811, -0.0880880880880881, -0.0780780780780781, -0.06806806806806809, -0.05805805805805808, -0.048048048048048075, -0.03803803803803807, -0.028028028028028062, -0.018018018018018056, -0.00800800800800805, 0.002002002002002179, 0.012012012012012185, 0.022022022022022192, 0.0320320320320322, 0.042042042042042205, 0.05205205205205221, 0.06206206206206222, 0.07207207207207222, 0.08208208208208223, 0.09209209209209224, 0.10210210210210224, 0.11211211211211225, 0.12212212212212226, 0.13213213213213226, 0.14214214214214227, 0.15215215215215228, 0.16216216216216228, 0.1721721721721723, 0.1821821821821823, 0.1921921921921923, 0.2022022022022023, 0.21221221221221231, 0.22222222222222232, 0.23223223223223233, 0.24224224224224233, 0.25225225225225234, 0.26226226226226235, 0.27227227227227235, 0.28228228228228236, 0.29229229229229237, 0.3023023023023024, 0.3123123123123124, 0.3223223223223224, 0.3323323323323324, 0.3423423423423424, 0.3523523523523524, 0.3623623623623624, 0.3723723723723724, 0.3823823823823824, 0.39239239239239243, 0.40240240240240244, 0.41241241241241244, 0.42242242242242245, 0.43243243243243246, 0.44244244244244246, 0.45245245245245247, 0.4624624624624625, 0.4724724724724725, 0.4824824824824825, 0.4924924924924925, 0.5025025025025025, 0.5125125125125125, 0.5225225225225225, 0.5325325325325325, 0.5425425425425425, 0.5525525525525525, 0.5625625625625625, 0.5725725725725725, 0.5825825825825826, 0.5925925925925926, 0.6026026026026026, 0.6126126126126126, 0.6226226226226226, 0.6326326326326326, 0.6426426426426426, 0.6526526526526526, 0.6626626626626626, 0.6726726726726726, 0.6826826826826826, 0.6926926926926926, 0.7027027027027026, 0.7127127127127126, 0.7227227227227226, 0.7327327327327327, 0.7427427427427427, 0.7527527527527527, 0.7627627627627627, 0.7727727727727727, 0.7827827827827827, 0.7927927927927927, 0.8028028028028027, 0.8128128128128127, 0.8228228228228227, 0.8328328328328327, 0.8428428428428427, 0.8528528528528527, 0.8628628628628627, 0.8728728728728727, 0.8828828828828827, 0.8928928928928928, 0.9029029029029028, 0.9129129129129128, 0.9229229229229228, 0.9329329329329328, 0.9429429429429428, 0.9529529529529528, 0.9629629629629628, 0.9729729729729728, 0.9829829829829828, 0.9929929929929928, 1.0030030030030028, 1.0130130130130128, 1.0230230230230228, 1.0330330330330328, 1.0430430430430429, 1.0530530530530529, 1.0630630630630629, 1.0730730730730729, 1.0830830830830829, 1.0930930930930929, 1.1031031031031029, 1.113113113113113, 1.123123123123123, 1.133133133133133, 1.143143143143143, 1.153153153153153, 1.163163163163163, 1.173173173173173, 1.183183183183183, 1.193193193193193, 1.203203203203203, 1.2132132132132134, 1.2232232232232234, 1.2332332332332334, 1.2432432432432434, 1.2532532532532534, 1.2632632632632634, 1.2732732732732734, 1.2832832832832834, 1.2932932932932935, 1.3033033033033035, 1.3133133133133135, 1.3233233233233235, 1.3333333333333335, 1.3433433433433435, 1.3533533533533535, 1.3633633633633635, 1.3733733733733735, 1.3833833833833835, 1.3933933933933935, 1.4034034034034035, 1.4134134134134135, 1.4234234234234235, 1.4334334334334335, 1.4434434434434436, 1.4534534534534536, 1.4634634634634636, 1.4734734734734736, 1.4834834834834836, 1.4934934934934936, 1.5035035035035036, 1.5135135135135136, 1.5235235235235236, 1.5335335335335336, 1.5435435435435436, 1.5535535535535536, 1.5635635635635636, 1.5735735735735736, 1.5835835835835836, 1.5935935935935936, 1.6036036036036037, 1.6136136136136137, 1.6236236236236237, 1.6336336336336337, 1.6436436436436437, 1.6536536536536537, 1.6636636636636637, 1.6736736736736737, 1.6836836836836837, 1.6936936936936937, 1.7037037037037037, 1.7137137137137137, 1.7237237237237237, 1.7337337337337337, 1.7437437437437437, 1.7537537537537538, 1.7637637637637638, 1.7737737737737738, 1.7837837837837838, 1.7937937937937938, 1.8038038038038038, 1.8138138138138138, 1.8238238238238238, 1.8338338338338338, 1.8438438438438438, 1.8538538538538538, 1.8638638638638638, 1.8738738738738738, 1.8838838838838838, 1.8938938938938938, 1.9039039039039038, 1.9139139139139139, 1.9239239239239239, 1.9339339339339339, 1.9439439439439439, 1.9539539539539539, 1.9639639639639639, 1.973973973973974, 1.983983983983984, 1.993993993993994, 2.0040040040040044, 2.0140140140140144, 2.0240240240240244, 2.0340340340340344, 2.0440440440440444, 2.0540540540540544, 2.0640640640640644, 2.0740740740740744, 2.0840840840840844, 2.0940940940940944, 2.1041041041041044, 2.1141141141141144, 2.1241241241241244, 2.1341341341341344, 2.1441441441441444, 2.1541541541541545, 2.1641641641641645, 2.1741741741741745, 2.1841841841841845, 2.1941941941941945, 2.2042042042042045, 2.2142142142142145, 2.2242242242242245, 2.2342342342342345, 2.2442442442442445, 2.2542542542542545, 2.2642642642642645, 2.2742742742742745, 2.2842842842842845, 2.2942942942942945, 2.3043043043043046, 2.3143143143143146, 2.3243243243243246, 2.3343343343343346, 2.3443443443443446, 2.3543543543543546, 2.3643643643643646, 2.3743743743743746, 2.3843843843843846, 2.3943943943943946, 2.4044044044044046, 2.4144144144144146, 2.4244244244244246, 2.4344344344344346, 2.4444444444444446, 2.4544544544544546, 2.4644644644644647, 2.4744744744744747, 2.4844844844844847, 2.4944944944944947, 2.5045045045045047, 2.5145145145145147, 2.5245245245245247, 2.5345345345345347, 2.5445445445445447, 2.5545545545545547, 2.5645645645645647, 2.5745745745745747, 2.5845845845845847, 2.5945945945945947, 2.6046046046046047, 2.6146146146146148, 2.6246246246246248, 2.6346346346346348, 2.6446446446446448, 2.6546546546546548, 2.664664664664665, 2.674674674674675, 2.684684684684685, 2.694694694694695, 2.704704704704705, 2.714714714714715, 2.724724724724725, 2.734734734734735, 2.744744744744745, 2.754754754754755, 2.764764764764765, 2.774774774774775, 2.784784784784785, 2.794794794794795, 2.804804804804805, 2.814814814814815, 2.824824824824825, 2.834834834834835, 2.844844844844845, 2.854854854854855, 2.864864864864865, 2.874874874874875, 2.884884884884885, 2.894894894894895, 2.904904904904905, 2.914914914914915, 2.924924924924925, 2.934934934934935, 2.944944944944945, 2.954954954954955, 2.964964964964965, 2.974974974974975, 2.984984984984985, 2.994994994994995, 3.005005005005005, 3.015015015015015, 3.025025025025025, 3.035035035035035, 3.045045045045045, 3.055055055055055, 3.065065065065065, 3.075075075075075, 3.085085085085085, 3.095095095095095, 3.105105105105105, 3.115115115115115, 3.125125125125125, 3.135135135135135, 3.145145145145145, 3.155155155155155, 3.165165165165165, 3.175175175175175, 3.185185185185185, 3.195195195195195, 3.205205205205205, 3.215215215215215, 3.225225225225225, 3.235235235235235, 3.245245245245245, 3.255255255255255, 3.265265265265265, 3.275275275275275, 3.285285285285285, 3.295295295295295, 3.305305305305305, 3.315315315315315, 3.325325325325325, 3.335335335335335, 3.3453453453453452, 3.3553553553553552, 3.3653653653653652, 3.3753753753753752, 3.3853853853853852, 3.3953953953953953, 3.4054054054054053, 3.4154154154154153, 3.4254254254254253, 3.4354354354354353, 3.4454454454454453, 3.4554554554554553, 3.4654654654654653, 3.4754754754754753, 3.4854854854854853, 3.4954954954954953, 3.5055055055055053, 3.5155155155155153, 3.5255255255255253, 3.5355355355355353, 3.5455455455455454, 3.5555555555555554, 3.5655655655655654, 3.5755755755755754, 3.5855855855855854, 3.5955955955955954, 3.6056056056056054, 3.6156156156156154, 3.6256256256256254, 3.6356356356356354, 3.6456456456456454, 3.6556556556556554, 3.6656656656656654, 3.6756756756756754, 3.6856856856856854, 3.6956956956956954, 3.7057057057057055, 3.7157157157157155, 3.7257257257257255, 3.7357357357357355, 3.7457457457457455, 3.7557557557557555, 3.7657657657657655, 3.7757757757757755, 3.7857857857857855, 3.7957957957957955, 3.8058058058058055, 3.8158158158158155, 3.8258258258258255, 3.8358358358358355, 3.8458458458458455, 3.8558558558558556, 3.8658658658658656, 3.8758758758758756, 3.8858858858858856, 3.8958958958958956, 3.9059059059059056, 3.9159159159159156, 3.9259259259259256, 3.9359359359359356, 3.9459459459459456, 3.9559559559559556, 3.9659659659659656, 3.9759759759759756, 3.9859859859859856, 3.9959959959959956, 4.006006006006006, 4.016016016016016, 4.026026026026026, 4.036036036036036, 4.046046046046046, 4.056056056056056, 4.066066066066066, 4.076076076076076, 4.086086086086086, 4.096096096096096, 4.106106106106106, 4.116116116116116, 4.126126126126126, 4.136136136136136, 4.146146146146146, 4.156156156156156, 4.166166166166166, 4.176176176176176, 4.186186186186186, 4.196196196196196, 4.206206206206206, 4.216216216216216, 4.226226226226226, 4.236236236236236, 4.246246246246246, 4.256256256256256, 4.266266266266266, 4.276276276276276, 4.286286286286286, 4.296296296296296, 4.306306306306306, 4.316316316316316, 4.326326326326326, 4.336336336336336, 4.346346346346346, 4.356356356356356, 4.366366366366366, 4.376376376376376, 4.386386386386386, 4.396396396396396, 4.406406406406406, 4.416416416416417, 4.426426426426427, 4.436436436436437, 4.446446446446447, 4.456456456456457, 4.466466466466467, 4.476476476476477, 4.486486486486487, 4.496496496496497, 4.506506506506507, 4.516516516516517, 4.526526526526527, 4.536536536536537, 4.546546546546547, 4.556556556556557, 4.566566566566567, 4.576576576576577, 4.586586586586587, 4.596596596596597, 4.606606606606607, 4.616616616616617, 4.626626626626627, 4.636636636636637, 4.646646646646647, 4.656656656656657, 4.666666666666667, 4.676676676676677, 4.686686686686687, 4.696696696696697, 4.706706706706707, 4.716716716716717, 4.726726726726727, 4.736736736736737, 4.746746746746747, 4.756756756756757, 4.766766766766767, 4.776776776776777, 4.786786786786787, 4.796796796796797, 4.806806806806807, 4.816816816816817, 4.826826826826827, 4.836836836836837, 4.846846846846847, 4.856856856856857, 4.866866866866867, 4.876876876876877, 4.886886886886887, 4.896896896896897, 4.906906906906907, 4.916916916916917, 4.926926926926927, 4.936936936936937, 4.946946946946947, 4.956956956956957, 4.966966966966967, 4.976976976976977, 4.986986986986987, 4.996996996996997, 5.007007007007007, 5.017017017017017, 5.027027027027027, 5.037037037037037, 5.047047047047047, 5.057057057057057, 5.067067067067067, 5.077077077077077, 5.087087087087087, 5.097097097097097, 5.107107107107107, 5.117117117117117, 5.127127127127127, 5.137137137137137, 5.147147147147147, 5.157157157157157, 5.167167167167167, 5.177177177177177, 5.187187187187187, 5.197197197197197, 5.207207207207207, 5.217217217217217, 5.227227227227227, 5.237237237237237, 5.247247247247247, 5.257257257257257, 5.267267267267267, 5.277277277277277, 5.287287287287287, 5.297297297297297, 5.307307307307307, 5.317317317317317, 5.327327327327327, 5.337337337337337, 5.347347347347347, 5.357357357357357, 5.367367367367367, 5.377377377377377, 5.387387387387387, 5.397397397397397, 5.407407407407407, 5.4174174174174174, 5.4274274274274275, 5.4374374374374375, 5.4474474474474475, 5.4574574574574575, 5.4674674674674675, 5.4774774774774775, 5.4874874874874875, 5.4974974974974975, 5.5075075075075075, 5.5175175175175175, 5.5275275275275275, 5.5375375375375375, 5.5475475475475475, 5.5575575575575575, 5.5675675675675675, 5.5775775775775776, 5.587587587587588, 5.597597597597598, 5.607607607607608, 5.617617617617618, 5.627627627627628, 5.637637637637638, 5.647647647647648, 5.657657657657658, 5.667667667667668, 5.677677677677678, 5.687687687687688, 5.697697697697698, 5.707707707707708, 5.717717717717718, 5.727727727727728, 5.737737737737738, 5.747747747747748, 5.757757757757758, 5.767767767767768, 5.777777777777778, 5.787787787787788, 5.797797797797798, 5.807807807807808, 5.817817817817818, 5.827827827827828, 5.837837837837838, 5.847847847847848, 5.857857857857858, 5.867867867867868, 5.877877877877878, 5.887887887887888, 5.897897897897898, 5.907907907907908, 5.917917917917918, 5.927927927927928, 5.937937937937938, 5.947947947947948, 5.957957957957958, 5.967967967967968, 5.977977977977978, 5.987987987987988, 5.997997997997998, 6.008008008008009, 6.018018018018019, 6.028028028028029, 6.038038038038039, 6.048048048048049, 6.058058058058059, 6.068068068068069, 6.078078078078079, 6.088088088088089, 6.098098098098099, 6.108108108108109, 6.118118118118119, 6.128128128128129, 6.138138138138139, 6.148148148148149, 6.158158158158159, 6.168168168168169, 6.178178178178179, 6.188188188188189, 6.198198198198199, 6.208208208208209, 6.218218218218219, 6.228228228228229, 6.238238238238239, 6.248248248248249, 6.258258258258259, 6.268268268268269, 6.278278278278279, 6.288288288288289, 6.298298298298299, 6.308308308308309, 6.318318318318319, 6.328328328328329, 6.338338338338339, 6.348348348348349, 6.358358358358359, 6.368368368368369, 6.378378378378379, 6.388388388388389, 6.398398398398399, 6.408408408408409, 6.418418418418419, 6.428428428428429, 6.438438438438439, 6.448448448448449, 6.458458458458459, 6.468468468468469, 6.478478478478479, 6.488488488488489, 6.498498498498499, 6.508508508508509, 6.518518518518519, 6.528528528528529, 6.538538538538539, 6.548548548548549, 6.558558558558559, 6.568568568568569, 6.578578578578579, 6.588588588588589, 6.598598598598599, 6.608608608608609, 6.618618618618619, 6.628628628628629, 6.638638638638639, 6.648648648648649, 6.658658658658659, 6.668668668668669, 6.678678678678679, 6.688688688688689, 6.698698698698699, 6.708708708708709, 6.718718718718719, 6.728728728728729, 6.738738738738739, 6.748748748748749, 6.758758758758759, 6.768768768768769, 6.778778778778779, 6.788788788788789, 6.798798798798799, 6.808808808808809, 6.818818818818819, 6.828828828828829, 6.838838838838839, 6.848848848848849, 6.858858858858859, 6.868868868868869, 6.878878878878879, 6.888888888888889, 6.898898898898899, 6.908908908908909, 6.918918918918919, 6.928928928928929, 6.938938938938939, 6.948948948948949, 6.958958958958959, 6.968968968968969, 6.978978978978979, 6.988988988988989, 6.998998998998999, 7.009009009009009, 7.019019019019019, 7.029029029029029, 7.039039039039039, 7.049049049049049, 7.059059059059059, 7.069069069069069, 7.079079079079079, 7.089089089089089, 7.099099099099099, 7.109109109109109, 7.119119119119119, 7.129129129129129, 7.1391391391391394, 7.1491491491491495, 7.1591591591591595, 7.1691691691691695, 7.1791791791791795, 7.1891891891891895, 7.1991991991991995, 7.2092092092092095, 7.2192192192192195, 7.2292292292292295, 7.2392392392392395, 7.2492492492492495, 7.2592592592592595, 7.2692692692692695, 7.2792792792792795, 7.2892892892892895, 7.2992992992992995, 7.3093093093093096, 7.31931931931932, 7.32932932932933, 7.33933933933934, 7.34934934934935, 7.35935935935936, 7.36936936936937, 7.37937937937938, 7.38938938938939, 7.3993993993994, 7.40940940940941, 7.41941941941942, 7.42942942942943, 7.43943943943944, 7.44944944944945, 7.45945945945946, 7.46946946946947, 7.47947947947948, 7.48948948948949, 7.4994994994995, 7.50950950950951, 7.51951951951952, 7.52952952952953, 7.53953953953954, 7.54954954954955, 7.55955955955956, 7.56956956956957, 7.57957957957958, 7.58958958958959, 7.5995995995996, 7.60960960960961, 7.61961961961962, 7.62962962962963, 7.63963963963964, 7.64964964964965, 7.65965965965966, 7.66966966966967, 7.67967967967968, 7.68968968968969, 7.6996996996997, 7.70970970970971, 7.71971971971972, 7.72972972972973, 7.73973973973974, 7.74974974974975, 7.75975975975976, 7.76976976976977, 7.77977977977978, 7.78978978978979, 7.7997997997998, 7.80980980980981, 7.81981981981982, 7.82982982982983, 7.83983983983984, 7.84984984984985, 7.85985985985986, 7.86986986986987, 7.87987987987988, 7.88988988988989, 7.8998998998999, 7.90990990990991, 7.91991991991992, 7.92992992992993, 7.93993993993994, 7.94994994994995, 7.95995995995996, 7.96996996996997, 7.97997997997998, 7.98998998998999, 8.0], \"y\": [4.348766175087199, 4.277192966093969, 4.205102954463674, 4.132487146126188, 4.059336629423321, 3.9856425908123354, 3.911396330578623, 3.8365892785425917, 3.7612130097457555, 3.6852592601009544, 3.6087199419916254, 3.5315871598050066, 3.4538532253841696, 3.375510673383772, 3.296552276514456, 3.2169710606608466, 3.1367603198581673, 3.055913631112543, 2.974424869050148, 2.892288220380445, 2.8094981981588774, 2.7260496558344807, 2.641937801068032, 2.5571582093064924, 2.4717068370996693, 2.385580035145178, 2.2987745610480106, 2.211287591781181, 2.123116735834155, 2.0342600450359956, 1.9447160260403917, 1.8544836514599983, 1.763562370637775, 1.6719521200432987, 1.579653333282303, 1.4866669507080115, 1.392994428623144, 1.2986377480617897, 1.2035994231406981, 1.1078825089698654, 1.0114906091126674, 0.9144278825861474, 0.8166990503924698, 0.7183094015728864, 0.6192647987760354, 0.5195716833327357, 0.41923707982988656, 0.31826860017649905, 0.21667444715532413, 0.11446341745397193, 0.011644904169876624, -0.09177110121609311, -0.19577400740211237, -0.3003526223682337, -0.40549515302337114, -0.5111892053997501, -0.6174217853976534, -0.7241793000828051, -0.8314475595382479, -0.9392117792720592, -1.0474565831817677, -1.1561660070758175, -1.2653235027519267, -1.3749119426316847, -1.484913624950209, -1.595310279499198, -1.706083073921177, -1.8172126205522456, -1.9286789838101188, -2.0404616881237443, -2.15253972640026, -2.2648915690245763, -2.377495173386338, -2.4903279939285246, -2.6033669927114618, -2.716588650485514, -2.8299689782652266, -2.9434835293972257, -3.0571074121136768, -3.1708153025626418, -3.284581458306205, -3.3983797322767635, -3.5121835871814295, -3.6259661103440575, -3.7397000289739224, -3.8533577258496927, -3.9669112554068846, -4.080332360216582, -4.193592487842789, -4.3066628080653935, -4.419514230455329, -4.532117422288148, -4.644442826781849, -4.756460681644461, -4.868141037916503, -4.9794537790931885, -5.0903686405108175, -5.200855228981597, -5.310883042660767, -5.420421491129657, -5.529439915678046, -5.637907609768911, -5.745793839668446, -5.8530678652239985, -5.959698960772354, -6.065656436160612, -6.170909657861748, -6.275428070166713, -6.379181216434895, -6.482138760384516, -6.584270507404513, -6.685546425869305, -6.7859366684377775, -6.88541159331777, -6.983941785477266, -7.081498077783479, -7.178051572051032, -7.2735736599803715, -7.368036043967623, -7.461410757767144, -7.553670186988011, -7.644787089405842, -7.734734615071362, -7.823486326197288, -7.9110162168051685, -7.997298732114024, -8.082308787652714, -8.166021788078188, -8.24841364568196, -8.329460798567293, -8.409140228479904, -8.487429478275137, -8.564306669004896, -8.639750516607812, -8.71374034818652, -8.78625611785608, -8.857278422148063, -8.926788514954989, -8.994768322000299, -9.061200454819298, -9.126068224236976, -9.189355653328905, -9.251047489851947, -9.311129218131782, -9.369587070394845, -9.426408037532582, -9.4815798792865, -9.535091133842874, -9.586931126826506, -9.637089979683441, -9.68555861744299, -9.732328775849988, -9.777393007858722, -9.820744689480492, -9.862378024977327, -9.902288051394937, -9.940470642428538, -9.976922511615763, -10.011641214851448, -10.044625152219691, -10.075873569139151, -10.105386556818152, -10.133165052016768, -10.159210836113736, -10.183526533476522, -10.206115609133608, -10.226982365748656, -10.2461319398968, -10.263570297643966, -10.27930422943074, -10.293341344262984, -10.305690063211918, -10.31635961222714, -10.325360014266613, -10.332702080748282, -10.338397402328647, -10.342458339014197, -10.34489800961226, -10.34573028052846, -10.344969753918493, -10.342631755202705, -10.338732319952378, -10.333288180157382, -10.326316749885335, -10.317836110343055, -10.307864994351664, -10.296422770247228, -10.283529425219443, -10.26920554810139, -10.253472311623915, -10.236351454148753, -10.217865260895032, -10.198036544674254, -10.17688862614946, -10.15444531363461, -10.130730882450868, -10.105770053856764, -10.079587973569812, -10.052210189897455, -10.02366263149575, -9.993971584774501, -9.963163670968031, -9.93126582289107, -9.898305261399686, -9.86430947157741, -9.829306178667146, -9.793323323769677, -9.756389039329914, -9.718531624432263, -9.679779519926786, -9.640161283408018, -9.599705564068548, -9.558441077449636, -9.516396580111353, -9.473600844244842, -9.430082632249466, -9.385870671297702, -9.340993627910766, -9.29548008256797, -9.24935850437294, -9.202657225799813, -9.155404417542508, -9.107628063490283, -9.059355935852572, -9.010615570456242, -8.961434242238193, -8.911838940956192, -8.86185634714073, -8.811512808310493, -8.760834315473982, -8.709846479939516, -8.658574510455768, -8.607043190704692, -8.555276857168487, -8.503299377391981, -8.451134128661536, -8.398803977121258, -8.346331257347012, -8.293737752398362, -8.241044674368224, -8.188272645449604, -8.13544167953846, -8.082571164391203, -8.029679844355053, -7.9767858036888795, -7.923906450491755, -7.871058501255976, -7.818257966060714, -7.7655201344220455, -7.712859561814483, -7.660290056878594, -7.6078246693287745, -7.555475678574574, -7.503254583068439, -7.4511720903920695, -7.399238108093019, -7.347461735282465, -7.295851255004445, -7.244414127386212, -7.193156983578656, -7.142085620495041, -7.091204996355662, -7.040519227045238, -6.990031583289241, -6.939744488654514, -6.889659518378907, -6.8397773990338875, -6.790098009023257, -6.740620379920504, -6.6913426986464, -6.642262310487838, -6.593375722957994, -6.544678610497276, -6.496165820013618, -6.447831377259997, -6.39966849404622, -6.351669576281285, -6.303826232841837, -6.2561292852614505, -6.208568778234713, -6.161133990929321, -6.113813449098615, -6.0665949379862205, -6.019465516013725, -5.972411529241536, -5.925418626592335, -5.878471775825825, -5.831555280252663, -5.7846527961748455, -5.737747351039019, -5.6908213622884976, -5.643856656899113, -5.596834491583287, -5.549735573646058, -5.502540082476148, -5.455227691654493, -5.40777759166201, -5.360168513167784, -5.312378750878208, -5.264386187927086, -5.2161683207860285, -5.167702284674023, -5.1189648794444444, -5.069932595927254, -5.020581642703672, -4.970887973290062, -4.920827313707345, -4.8703751904117745, -4.819506958562505, -4.768197830600948, -4.716422905116556, -4.6641571959732735, -4.61137566167058, -4.558053234912709, -4.50416485235933, -4.449685484530726, -4.394590165840208, -4.3388540247263165, -4.28245231385714, -4.225360440378891, -4.16755399618074, -4.109008788147777, -4.0497008683738525, -3.9896065643059693, -3.9287025087918632, -3.866965670002368, -3.804373381200133, -3.740903370326329, -3.6765337893770025, -3.611243243540793, -3.545010820069881, -3.4778161168561006, -3.40963927068435, -3.3404609851355844, -3.270262558111895, -3.199025908956392, -3.1267336051408874, -3.053368888494635, -2.978915700947703, -2.9033587097628804, -2.8266833322303846, -2.7488757598000113, -2.669922981625776, -2.5898128074985363, -2.5085338901425227, -2.4260757468522085, -2.342428780446422, -2.2575842995171604, -2.171534537951091, -2.0842726737023045, -1.9957928467954744, -1.906090176539199, -1.8151607779299195, -1.7230017772274842, -1.6296113266840813, -1.5349886184089783, -1.4391338973522037, -1.3420484733910423, -1.24373473250397, -1.144196147017411, -1.0434372849114877, -0.9414638181717343, -0.838282530174549, -0.7339013220950003, -0.6283292183264301, -0.5215763709021658, -0.41365406291050655, -0.3045747108950356, -0.1943518662331991, -0.08300021548698666, 0.029464420279539327, 0.14302508722019924, 0.25766370146755135, 0.3733610529371285, 0.4900968101765919, 0.6078495262604091, 0.7265966457297051, 0.8463145125759866, 0.9669783792664914, 1.0885624168079528, 1.2110397258446128, 1.3343823487853694, 1.4585612829539754, 1.583546494755262, 1.7093069348494034, 1.835810554325292, 1.9630243218631453, 2.090914241875538, 2.219445373615101, 2.34858185123623, 2.4782869047971854, 2.6085228821881135, 2.7392512719695548, 2.8704327271051713, 3.0020270895715004, 3.133993415826708, 3.2662900031194466, 3.3988744166180713, 3.53170351733967, 3.66473349085752, 3.7979198767648183, 3.931217598871703, 4.064580996111949, 4.197963854134772, 4.331319437556655, 4.464600522847288, 4.597759431823088, 4.730748065721046, 4.86351793982508, 4.996020218616403, 5.128205751418833, 5.260025108509433, 5.3914286176642685, 5.522366401108609, 5.65278841284036, 5.782644476295081, 5.911884322320461, 6.040457627427765, 6.168314052287326, 6.295403280434818, 6.421675057154756, 6.547079228507284, 6.671565780464135, 6.795084878119327, 6.9175869049400145, 7.039022502022695, 7.159342607319804, 7.278498494801697, 7.396441813518818, 7.513124626528885, 7.628499449653879, 7.742519290031576, 7.8551376844265075, 7.966308737265197, 8.075987158360707, 8.184128300291611, 8.290688195400712, 8.395623592379014, 8.498891992400681, 8.600451684774983, 8.700261782081544, 8.798282254755511, 8.894473965089635, 8.988798700620645, 9.081219206867713, 9.171699219391266, 9.260203495140903, 9.34669784306162, 9.431149153928208, 9.513525429378124, 9.593795810113859, 9.671930603246393, 9.747901308751963, 9.821680645015165, 9.893242573431987, 9.962562322047196, 10.029616408201301, 10.094382660162976, 10.156840237723788, 10.216969651732825, 10.274752782549744, 10.33017289739556, 10.383214666581562, 10.433864178597473, 10.48210895404111, 10.527937958372647, 10.57134161347766, 10.612311808024073, 10.650841906599188, 10.686926757614064, 10.720562699963477, 10.751747568430844, 10.78048069782859, 10.806762925865455, 10.830596594733443, 10.851985551408205, 10.870935146657773, 10.887452232755718, 10.901545159895951, 10.91322377130758, 10.922499397069318, 10.929384846624243, 10.933894399996719, 10.936043797714618, 10.935850229441066, 10.933332321321153, 10.928510122050232, 10.921405087671591, 10.912040065112459, 10.900439274468523, 10.886628290048241, 10.870634020189433, 10.852484685861787, 10.832209798070078, 10.809840134073971, 10.785407712441529, 10.758945766954538, 10.73048871938497, 10.700072151162914, 10.66773277395747, 10.63350839919309, 10.597437906524956, 10.559561211297957, 10.519919231014917, 10.478553850840655, 10.435507888169434, 10.390825056284386, 10.344549927138306, 10.29672789328623, 10.247405129001036, 10.196628550604173, 10.144445776044455, 10.090905083758708, 10.036055370848718, 9.979946110609887, 9.922627309447439, 9.864149463217032, 9.804563513026999, 9.743920800540291, 9.682273022814663, 9.61967218672025, 9.556170562974263, 9.491820639832925, 9.42667507648131, 9.360786656162208, 9.294208239085407, 9.22699271515926, 9.159192956586699, 9.090861770368068, 9.022051850753524, 8.95281573168783, 8.883205739290625, 8.81327394441538, 8.743072115330262, 8.672651670564306, 8.602063631962231, 8.531358577991192, 8.460586597342793, 8.389797242873492, 8.31903948592642, 8.248361671077463, 8.177811471348246, 8.107435843928323, 8.037280986448712, 7.9673922938484045, 7.897814315875275, 7.828590715262258, 7.759764226619268, 7.691376616080868, 7.623468641749072, 7.5560800149701866, 7.489249362483946, 7.423014189482531, 7.357410843616417, 7.292474479983277, 7.228239027135354, 7.1647371541400116, 7.102000238727275, 7.0400583365573715, 6.978940151640332, 6.918673007938847, 6.859282822184577, 6.800794077937142, 6.743229800913996, 6.68661153561834, 6.630959323291161, 6.576291681212377, 6.522625583374912, 6.469976442554433, 6.418358093796188, 6.367782779339304, 6.31826113499759, 6.269802178014638, 6.222413296409805, 6.17610023983028, 6.130867111923216, 6.086716364240486, 6.043648791687345, 6.0016635295249, 5.960758051934876, 5.920928172153827, 5.882168044182504, 5.844470166074709, 5.807825384808474, 5.772222902741094, 5.737650285647962, 5.704093472343827, 5.671536785883562, 5.6399629463381755, 5.609353085140203, 5.579686760991327, 5.550941977323491, 5.523095201303375, 5.496121384368668, 5.469993984283102, 5.4446849886957525, 5.420164940188773, 5.396402962796191, 5.37336678997507, 5.3510227940089115, 5.329336016821781, 5.30827020218028, 5.287787829259118, 5.26785014754472, 5.2484172130499385, 5.229447925811702, 5.210900068642074, 5.192730347101995, 5.17489443066572, 5.157346995042734, 5.140041765622765, 5.122931562008336, 5.10596834359817, 5.089103256183654, 5.072286679519485, 5.055468275828595, 5.038597039200407, 5.021621345840529, 5.0044890051290425, 4.987147311443584, 4.969543096702615, 4.9516227835833995, 4.933332439368401, 4.914617830373074, 4.895424476907289, 4.875697708721963, 4.855382720891809, 4.834424630084545, 4.812768531166325, 4.790359554092671, 4.767142921033679, 4.743064003681909, 4.718068380690924, 4.692101895192162, 4.66511071233753, 4.637041376814851, 4.607840870283129, 4.577456668674436, 4.545836799309146, 4.5129298977711745, 4.478685264489892, 4.4430529209754335, 4.405983665654244, 4.36742912925174, 4.327341829669365, 4.285675226303332, 4.24238377375282, 4.197422974865617, 4.150749433069659, 4.102320903939326, 4.05209634594584, 4.000035970341659, 3.946101290129342, 3.8902551680659654, 3.832461863654893, 3.772687079077378, 3.71089800401729, 3.6470633593330235, 3.5811534395315645, 3.513140154000544, 3.4429970669550864, 3.370699436057243, 3.29622424966683, 3.219550262683585, 3.1406580309416343, 3.059529944118472, 2.9761502571217964, 2.890505119918813, 2.8025826057738885, 2.7123727378617035, 2.6198675142244667, 2.5250609310430416, 2.4279490041933367, 2.3285297890606613, 2.2268033985863003, 2.1227720195220074, 2.016439926869658, 1.9078134964848836, 1.7969012158250526, 1.6837136928235923, 1.568263662874276, 1.450565993910725, 1.3306376895680794, 1.2084978904154515, 1.084167873249493, 0.957671048441135, 0.8290329553292759, 0.6982812556569626, 0.5654457250473572, 0.43055824251854546, 0.29365277803803824, 0.15476537811958613, 0.013934149466708678, -0.12880075933083823, -0.27339717803967556, -0.4198109369547032, -0.5679958909887011, -0.7179039456662692, -0.8694850850069475, -1.0226874012805889, -1.177457126616284, -1.3337386664443802, -1.4914746347493921, -1.6506058911098629, -1.8110715794995116, -1.972809168822312, -2.1357544951524496, -2.2998418056484415, -2.4650038041090614, -2.631171698137088, -2.798275247875295, -2.9662428162775343, -3.135001420876215, -3.3044767870059584, -3.4745934024417426, -3.6452745734083765, -3.81644248191673, -3.9880182443807684, -4.159921971468075, -4.332072829135237, -4.504389100798229, -4.676788250586616, -4.849186987629303, -5.021501331318337, -5.193646677496183, -5.36553786551087, -5.5370892460822985, -5.708214749922152, -5.878827957048844, -6.048842166738094, -6.218170468048917, -6.386725810864032, -6.554421077382977, -6.721169154005559, -6.886883003542663, -7.051475737690883, -7.214860689706944, -7.376951487217433, -7.53766212509899, -7.696907038363763, -7.854601174984654, -8.010660068594701, -8.164999910994766, -8.317537624403581, -8.468190933384241, -8.616878436381157, -8.763519676801652, -8.908035213576497, -9.050346691133855, -9.190376908721426, -9.328049889011861, -9.463290945926932, -9.596026751616355, -9.726185402527692, -9.85369648450435, -9.978491136849224, -10.100502115292329, -10.219663853801432, -10.33591252517552, -10.449186100361784, -10.55942440643775, -10.666569183201101, -10.770564138310826, -10.871355000924336, -10.968889573776442, -11.06311778364712, -11.1539917301664, -11.241465732905873, -11.325496376707742, -11.40604255520373, -11.483065512477527, -11.556528882826072, -11.626398728576397, -11.692643575916412, -11.755234448699612, -11.81414490018536, -11.869351042678115, -11.920831575030743, -11.968567807978783, -12.012543687274487, -12.052745814591157, -12.089163466170362, -12.1217886091864, -12.15061591580448, -12.175642774910964, -12.196869301496102, -12.21429834367169, -12.227935487308233, -12.23778905827816, -12.243870122293862, -12.246192482331399, -12.244772673632877, -12.239629956282675, -12.230786305354844, -12.218266398631224, -12.202097601892001, -12.182309951782615, -12.15893613626318, -12.132011472648724, -12.101573883250854, -12.067663868633568, -12.030324478498246, -11.98960128021492, -11.945542325019304, -11.898198111897067, -11.847621549179163, -11.793867913874058, -11.736994808764988, -11.677062117302384, -11.614131956323805, -11.548268626635767, -11.479538561493962, -11.408010273020334, -11.333754296597625, -11.25684313328384, -11.177351190291192, -11.095354719575866, -11.01093175458699, -10.92416204522489, -10.835126991060706, -10.74390957287104, -10.65059428254319, -10.555267051408146, -10.45801517706018, -10.358927248723473, -10.258093071227792, -10.155603587656678, -10.05155080073312, -9.946027693009059, -9.839128145926402, -9.730946857818525, -9.621579260922486, -9.511121437473317, -9.399670034952871, -9.287322180566802, -9.174175395024173, -9.06032750569515, -8.945876559223114, -8.830920733668261, -8.715558250260548, -8.599887284840468, -8.48400587906672, -8.36801185147039, -8.252002708435683, -8.136075555187645, -8.020327006867607, -7.904853099777321, -7.789749202872942, -7.675109929590075, -7.561029050081119, -7.447599403946139, -7.3349128135382715, -7.223059997924567, -7.11213048758283, -7.002212539914661, -6.893393055654519, -6.785757496254121, -6.679389802320847, -6.574372313188264, -6.470785687696118, -6.368708826256327, -6.268218794280683, -6.1693907470450045, -6.07229785606347, -5.977011237045797, -5.88359987950876, -5.792130578112314, -5.702667865789349, -5.615273948736656, -5.530008643333332, -5.446929315051358, -5.366090819421446, -5.287545445115754, -5.21134285920728, -5.13753005466408, -5.066151300134576, -4.997248092078453, -4.930859109295625, -4.867020169903874, -4.805764190813655, -4.747121149746523, -4.691118049841552, -4.637778886891797, -4.587124619250819, -4.539173140446831, -4.493939254539883, -4.451434654254989, -4.411667901921836, -4.374644413249209, -4.340366443959809, -4.308833079308677, -4.2800402265058475, -4.2539806100613795, -4.2306437700682, -4.210016063435735, -4.192080668084552, -4.176817590109626, -4.164203673917198, -4.154212615337467, -4.146814977712665, -4.141978210957384, -4.139666673585298, -4.1398416576936405, -4.1424614168942036, -4.147481197176742, -4.154853270688072, -4.164526972407357, -4.1764487396953935, -4.190562154693, -4.206807989540907, -4.225124254390895, -4.245446248175236, -4.267706612098888, -4.2918353858162295, -4.31776006625158, -4.345405669020107, -4.374694792403274, -4.405547683830409, -4.4378823088154915, -4.471614422295899, -4.506657642317348, -4.542923526006987, -4.580321647774252, -4.618759679676819, -4.658143473886788, -4.69837714719004, -4.739363167449615, -4.781002441961843, -4.823194407632048, -4.865837122894565, -4.908827361300027, -4.952060706691028, -4.995431649885444, -5.038833686785085, -5.082159417825674, -5.125300648682573, -5.168148492145254, -5.210593471072026, -5.25252562233524, -5.2938346016658935, -5.334409789305427, -5.374140396371308, -5.412915571842073, -5.45062451006647, -5.487156558700541, -5.522401326975663, -5.5562487941999015, -5.588589418394416, -5.619314244966119, -5.648315015317388, -5.675484275293268, -5.700715483366334, -5.723903118459248, -5.744942787304947, -5.763731331244422, -5.780166932362148, -5.794149218859424, -5.805579369566189, -5.814360217492217, -5.820396352319115, -5.823594221735092, -5.823862231515073, -5.821110844249599, -5.815252676626628, -5.806202595171439, -5.793877810350731, -5.778197968948188, -5.759085244619953, -5.736464426539731, -5.710263006044626, -5.680411261194252, -5.64684233915722, -5.6094923363407165, -5.5683003761805985, -5.5232086845112285, -5.474162662436128, -5.42111095662249, -5.364005526944633, -5.302801711403535, -5.237458288251825, -5.16793753525583, -5.094205286028574, -5.016230983370076, -4.933987729553711, -4.847452333499936, -4.756605354781256, -4.661431144405003, -4.561917882323138, -4.458057611621113, -4.349846269340639, -4.237283713894041, -4.1203737490308585, -3.999124144320276, -3.8735466521160222, -3.743657020973415, -3.6094750054913023, -3.471024372554849, -3.3283329039582323, -3.18143239538952, -3.03035865176329]}, {\"hovertemplate\": \"x: %{x} <br>y: %{y}\", \"mode\": \"markers\", \"name\": \"Train data\", \"type\": \"scatter\", \"x\": [5.157157157157157, 7.2092092092092095, 0.9529529529529528, -1.169169169169169, 7.42942942942943, 4.786786786786787, 2.0440440440440444, 6.398398398398399, 3.7057057057057055, 7.029029029029029, 0.7827827827827827, -0.32832832832832826, -0.6086086086086087, 4.856856856856857, -0.15815815815815815, 7.2892892892892895, 1.3633633633633635, 6.768768768768769, 2.6346346346346348, 0.1921921921921923, 6.628628628628629, 7.1591591591591595, 7.57957957957958, 6.108108108108109, 2.0940940940940944, 7.80980980980981, 2.1341341341341344, 1.1031031031031029, 6.608608608608609, 5.027027027027027, 7.67967967967968, 5.4474474474474475, 3.6856856856856854, 4.736736736736737, 1.2632632632632634, 0.7727727727727727, 4.826826826826827, 6.988988988988989, 0.13213213213213226, 4.926926926926927, 3.4554554554554553, -1.6896896896896898, 1.4534534534534536, 7.84984984984985, 2.904904904904905, 6.378378378378379, 5.827827827827828, 0.0320320320320322, 4.816816816816817, 6.328328328328329, 6.798798798798799, 3.5055055055055053, -0.6486486486486487, 6.198198198198199, 6.418418418418419, 7.1891891891891895, 5.397397397397397, -0.8288288288288288, 3.035035035035035, 7.78978978978979, -0.9889889889889889, 0.3123123123123124, 4.836836836836837, 7.77977977977978, 6.668668668668669, 5.657657657657658, 4.616616616616617, 1.9539539539539539, 6.358358358358359, 0.12212212212212226, -1.91991991991992, 2.3143143143143146, -0.9189189189189189, 6.638638638638639, 3.9059059059059056, 1.5935935935935936, -0.2382382382382382, 4.166166166166166, 1.6436436436436437, 4.206206206206206, 5.037037037037037, 0.4824824824824825, -1.5495495495495495, -0.40840840840840853, 0.6626626626626626, 1.203203203203203, 5.697697697697698, -1.1991991991991993, 2.774774774774775, 1.4234234234234235, 6.788788788788789, 4.306306306306306, 0.6826826826826826, 4.006006006006006, 2.1141141141141144, 4.516516516516517, 5.5275275275275275, 0.6726726726726726, -1.029029029029029, 5.287287287287287, 3.075075075075075, 0.4924924924924925, -1.8198198198198199, 2.3643643643643646, 3.8258258258258255, -1.3893893893893894, -0.33833833833833826, -1.7497497497497498, 6.738738738738739, 3.305305305305305, 4.666666666666667, -1.2892892892892893, 0.7527527527527527, 7.2992992992992995, 4.716716716716717, 5.4574574574574575, -0.30830830830830824, -1.2492492492492493, 3.5855855855855854, 5.247247247247247, 1.0530530530530529, 4.056056056056056, 1.7637637637637638, 1.8338338338338338, 1.4834834834834836, 4.346346346346346, 7.86986986986987, 1.5035035035035036, 2.954954954954955, 4.016016016016016, 5.187187187187187, 6.578578578578579, -1.3193193193193193, 4.596596596596597, 0.7627627627627627, 2.3443443443443446, 1.7837837837837838, 4.876876876876877, -0.2482482482482482, 0.16216216216216228, 5.767767767767768, 7.53953953953954, 7.74974974974975, 4.756756756756757, 3.5955955955955954, 6.898898898898899, 7.009009009009009, 7.55955955955956, -0.2282282282282282, 7.93993993993994, 0.022022022022022192, 2.3043043043043046, 6.958958958958959, 6.468468468468469, -1.96996996996997, 0.6326326326326326, 2.3243243243243246, 6.888888888888889, 4.586586586586587, 4.176176176176176, 0.6126126126126126, 0.5125125125125125, 1.973973973973974, 2.4744744744744747, -1.069069069069069, 3.8458458458458455, 0.26226226226226235, 2.4444444444444446, -1.7597597597597598, 7.91991991991992, 2.3343343343343346, 3.5755755755755754, 5.747747747747748, 0.002002002002002179, 5.227227227227227, -1.84984984984985, 0.5725725725725725, 5.817817817817818, -1.85985985985986, 1.3333333333333335, 0.29229229229229237, 4.976976976976977, 2.764764764764765, 6.598598598598599, 7.76976976976977, 3.085085085085085, 7.059059059059059, 3.3753753753753752, 7.44944944944945, 4.116116116116116, 7.33933933933934, -1.009009009009009, 2.5945945945945947, 6.948948948948949, 6.878878878878879, 7.37937937937938, 6.128128128128129, 5.777777777777778, 4.406406406406406, 2.714714714714715, 1.163163163163163, 6.138138138138139, 0.7127127127127126, -1.4294294294294294, 6.748748748748749, 6.528528528528529, -0.7687687687687688, 3.9459459459459456, -0.5585585585585586, 7.099099099099099, 6.038038038038039, 6.278278278278279, -0.41841841841841854, 3.245245245245245, 1.173173173173173, -0.31831831831831825, 4.446446446446447, 1.8238238238238238, 0.7427427427427427, -1.93993993993994, -1.95995995995996, 2.1741741741741745, 0.10210210210210224, 8.0, -1.98998998998999, 2.974974974974975, 3.6756756756756754, 7.40940940940941, -1.3593593593593594, 2.6146146146146148, 1.113113113113113, 5.327327327327327, -0.9989989989989989, 7.63963963963964, 5.897897897897898, 6.368368368368369, 1.3833833833833835, 5.4174174174174174, 1.8938938938938938, -0.9689689689689689, -0.03803803803803807, 1.3433433433433435, 6.318318318318319, -0.6586586586586587, 1.4134134134134135, 2.924924924924925, -0.0780780780780781, -0.5785785785785786, 2.1041041041041044, 0.9829829829829828, 4.546546546546547, 1.0130130130130128, 2.6446446446446448, 3.5155155155155153, -0.7187187187187187, -0.3983983983983985, 1.5635635635635636, 5.407407407407407, 5.107107107107107, 3.9559559559559556, -0.9489489489489489, 5.867867867867868, 4.726726726726727, 7.34934934934935, 0.3523523523523524, 4.146146146146146, -1.149149149149149, 1.6536536536536537, 6.588588588588589, 4.506506506506507, 7.43943943943944, 4.256256256256256, -1.83983983983984, 6.068068068068069, -1.5395395395395395, 4.046046046046046, 3.7257257257257255, 1.4434434434434436, -0.7087087087087087, 0.6926926926926926, 2.4044044044044046, 6.208208208208209, -1.059059059059059, 2.684684684684685, 4.696696696696697, 6.758758758758759, 3.135135135135135, 7.81981981981982, -0.048048048048048075, 6.448448448448449, 3.7357357357357355, 1.3233233233233235, 0.42242242242242245, -1.2092092092092093, 5.677677677677678, 4.576576576576577, -1.5695695695695695, -1.7997997997997999, 0.05205205205205221, 7.2192192192192195, 0.06206206206206222, 6.538538538538539, -1.089089089089089, 0.9229229229229228, -0.4984984984984986, 5.967967967967968, -1.179179179179179, -1.129129129129129, 6.218218218218219, -0.5885885885885886, 4.956956956956957, 0.5625625625625625, 4.656656656656657, 1.3933933933933935, 7.019019019019019, 7.38938938938939, 7.1791791791791795, -1.6096096096096097, 4.466466466466467, 7.039039039039039, -1.159159159159159, 0.3023023023023024, 3.4654654654654653, 5.217217217217217, 7.68968968968969, 5.337337337337337, 1.143143143143143, 1.9239239239239239, 6.118118118118119, 2.5145145145145147, 4.196196196196196, 3.5555555555555554, 2.834834834834835, -0.2782782782782782, -0.028028028028028062, 5.017017017017017, 6.148148148148149, 2.4944944944944947, 1.6136136136136137, 0.012012012012012185, -0.5985985985985987, 6.508508508508509, 1.123123123123123, -1.5195195195195195, 1.6836836836836837, 6.658658658658659, -1.039039039039039, 5.077077077077077, 0.41241241241241244, -1.7197197197197198, 0.3223223223223224, 2.4244244244244246, 6.858858858858859, 6.438438438438439, -1.3993993993993994, -1.87987987987988, 6.308308308308309, 7.71971971971972, -1.7897897897897899, 4.686686686686687, 4.376376376376376, -0.2682682682682682, -1.6196196196196198, 5.5775775775775776, 2.694694694694695, 0.7227227227227226, 1.4934934934934936, 5.347347347347347, -1.3093093093093093, 5.607607607607608, 0.45245245245245247, 7.4994994994995, 1.2932932932932935, 7.85985985985986, 1.8638638638638638, 7.5995995995996, 1.0230230230230228, 6.648648648648649, 5.837837837837838, 5.887887887887888, -0.6986986986986987, 3.8358358358358355, 3.065065065065065, 1.4634634634634636, -0.13813813813813813, 1.5535535535535536, 5.5675675675675675, 2.3843843843843846, 4.796796796796797, 5.237237237237237, 4.226226226226226, -0.6286286286286287, 3.285285285285285, 1.2332332332332334, 5.5175175175175175, 4.036036036036036, 2.5345345345345347, 6.688688688688689, 2.1541541541541545, 6.388388388388389, 4.136136136136136, 0.042042042042042205, 7.32932932932933, 3.295295295295295, 5.797797797797798, 1.2732732732732734, 3.325325325325325, 1.6736736736736737, 2.0040040040040044, 7.82982982982983, -1.5895895895895895, 1.5135135135135136, 7.60960960960961, -0.00800800800800805, 6.098098098098099, 6.498498498498499, 3.8158158158158155, 0.23223223223223233, 2.804804804804805, 5.997997997997998, 3.145145145145145, 6.028028028028029, 5.937937937937938, 1.6936936936936937, 5.257257257257257, -0.17817817817817816, 3.4254254254254253, -0.14814814814814814, 3.315315315315315, -0.6786786786786787, 3.4954954954954953, 4.896896896896897, 0.3423423423423424, 2.854854854854855, 5.947947947947948, -1.8098098098098099, 2.0740740740740744, 7.62962962962963, 3.6056056056056054, 6.348348348348349, 4.076076076076076, 2.0340340340340344, 7.1691691691691695, 2.5045045045045047, 6.778778778778779, -0.8388388388388388, -0.5185185185185186, 5.4774774774774775, 3.5655655655655654, 6.908908908908909, 0.4624624624624625, 0.1721721721721723, -0.8888888888888888, 7.6996996996997, 5.917917917917918, -1.6296296296296298, 1.6636636636636637, -0.8188188188188188, 0.3323323323323324, 5.927927927927928, -1.7697697697697699, 0.5825825825825826, 1.0930930930930929, 4.026026026026026, 3.4054054054054053, 1.7737737737737738, 2.1841841841841845, 3.4754754754754753, 7.3993993993994, 0.07207207207207222, -0.5685685685685686, 4.906906906906907, -1.189189189189189, 3.6956956956956954, 7.129129129129129, -0.7887887887887888, 2.5445445445445447, 2.944944944944945, -1.7397397397397398, 3.005005005005005, 5.5475475475475475, 1.9639639639639639, 5.717717717717718, 6.568568568568569, 3.165165165165165, 3.8958958958958956, 7.35935935935936, 7.1391391391391394, 4.476476476476477, 1.183183183183183, 2.3543543543543546, 0.1821821821821823, 1.3533533533533535, 6.618618618618619, 0.9629629629629628, 4.676676676676677, 5.977977977977978, 7.31931931931932, 5.617617617617618, 0.8428428428428427, -1.7797797797797799, 0.27227227227227235, 5.207207207207207, -1.97997997997998, 2.4544544544544546, 3.4854854854854853, -1.7097097097097098, 2.984984984984985, 2.5745745745745747, -1.94994994994995, 6.298298298298299, 6.058058058058059, 0.8128128128128127, -1.6796796796796798, 2.2142142142142145, 6.458458458458459, 4.276276276276276, -0.43843843843843855, 7.75975975975976, 3.195195195195195, 7.3093093093093096, 7.83983983983984, 3.8658658658658656, 0.5925925925925926, 0.9729729729729728, 4.556556556556557, 3.025025025025025, 2.704704704704705, 3.5355355355355353, 6.968968968968969, -1.8998998998999, 4.916916916916917, -1.92992992992993, 0.08208208208208223, 4.606606606606607, 1.7537537537537538, 3.9859859859859856, 6.338338338338339, 2.1441441441441444, 0.9129129129129128, 7.88988988988989, -1.5095095095095095, 4.866866866866867, 4.096096096096096, 5.277277277277277, 2.4844844844844847, -1.4794794794794794, 1.0830830830830829, 1.153153153153153, 2.884884884884885, 3.9159159159159156, 5.4274274274274275, 1.9139139139139139, 7.48948948948949, 3.015015015015015, -0.8088088088088088, 7.94994994994995, 1.9439439439439439, 7.73973973973974, 4.536536536536537, 3.275275275275275, 4.396396396396396, -1.139139139139139, -0.28828828828828823, 6.288288288288289, -1.7297297297297298, 5.857857857857858, 5.4674674674674675, 3.105105105105105, 5.4874874874874875, 4.626626626626627, 4.936936936936937, 2.914914914914915, 3.7957957957957955, 5.367367367367367, 0.5225225225225225, 3.5255255255255253, 6.928928928928929, 5.727727727727728, -0.6886886886886887, 4.996996996996997, 4.266266266266266, 5.087087087087087, 3.6556556556556554, -0.7487487487487487, -1.90990990990991, -1.2192192192192193, 6.088088088088089, 3.4354354354354353, 2.844844844844845, 7.049049049049049, 3.8758758758758756, -1.4594594594594594, 1.8838838838838838, 4.706706706706707, 1.5235235235235236, 2.874874874874875, 3.6656656656656654, 6.938938938938939, 0.5025025025025025, 2.4344344344344346, 6.408408408408409, -0.34834834834834827, 3.055055055055055, 0.6426426426426426, -1.4894894894894894, -0.9289289289289289, -1.4094094094094094, 6.048048048048049, 6.478478478478479, 2.784784784784785, 3.9759759759759756, 7.36936936936937, 2.994994994994995, -0.18818818818818817, 7.97997997997998, 2.664664664664665, 5.117117117117117, 5.4974974974974975, 5.4374374374374375, -0.018018018018018056, 6.828828828828829, 7.65965965965966, 0.7927927927927927, 1.3133133133133135, 7.2692692692692695, -0.09809809809809811, 3.3453453453453452, 0.22222222222222232, -0.8588588588588588, -0.4884884884884886, 2.6046046046046047, 4.886886886886887, -0.12812812812812813, -1.2692692692692693, 5.297297297297297, 0.40240240240240244, -0.5485485485485486, 3.6256256256256254, 0.5525525525525525, -1.049049049049049, 4.106106106106106, 5.197197197197197, 1.7137137137137137, 1.7037037037037037, 3.155155155155155, 2.1241241241241244, 2.934934934934935, 4.386386386386386, 6.008008008008009, -0.3883883883883883, 7.41941941941942, 2.2642642642642645, 0.43243243243243246, 4.806806806806807, 2.1641641641641645, 1.4334334334334335, 7.54954954954955, 1.2232232232232234, 3.6156156156156154, 4.486486486486487, -1.6996996996996998, 1.7337337337337337, 7.98998998998999, 5.067067067067067, -1.4394394394394394, 2.864864864864865, 3.095095095095095, 2.2242242242242245, 5.137137137137137, -0.5085085085085086, 3.335335335335335, -0.16816816816816815, -2.0, 1.8738738738738738, 3.6456456456456454, -0.7387387387387387, 0.9429429429429428, -1.5595595595595595, 7.46946946946947, 4.766766766766767, 6.728728728728729, 1.133133133133133, 7.51951951951952, 6.238238238238239, 4.066066066066066, -0.9089089089089089, 6.718718718718719, 5.647647647647648, -1.4194194194194194, 1.9339339339339339, 6.188188188188189, 2.5645645645645647, 3.9959959959959956, 1.6036036036036037, 6.978978978978979, 5.097097097097097, 2.3943943943943946, -0.2582582582582582, -0.7787787787787788], \"y\": [-3.6452745734083765, -4.506657642317348, -5.455227691654493, -3.6259661103440575, -5.412915571842073, 1.7969012158250526, 4.331319437556655, -8.715558250260548, 6.367782779339304, -4.1398416576936405, -6.2561292852614505, -10.2461319398968, -9.061200454819298, 0.957671048441135, -10.317836110343055, -4.823194407632048, -3.1267336051408874, -4.997248092078453, 10.433864178597473, -9.24935850437294, -6.1693907470450045, -4.345405669020107, -5.805579369566189, -11.548268626635767, 4.996020218616403, -5.16793753525583, 5.522366401108609, -4.716422905116556, -6.368708826256327, -1.4914746347493921, -5.759085244619953, -8.468190933384241, 6.469976442554433, 2.3285297890606613, -3.804373381200133, -6.303826232841837, 1.3306376895680794, -4.154212615337467, -9.516396580111353, 0.013934149466708678, 7.897814315875275, 1.8544836514599983, -2.4260757468522085, -4.847452333499936, 10.886628290048241, -8.945876559223114, -12.1217886091864, -9.898305261399686, 1.450565993910725, -9.511121437473317, -4.805764190813655, 7.5560800149701866, -8.78625611785608, -10.835126991060706, -8.48400587906672, -4.4378823088154915, -7.696907038363763, -7.2735736599803715, 10.519919231014917, -5.302801711403535, -5.637907609768911, -8.658574510455768, 1.2084978904154515, -5.364005526944633, -5.792130578112314, -11.06311778364712, 3.370699436057243, 3.133993415826708, -9.174175395024173, -9.558441077449636, 3.7612130097457555, 7.742519290031576, -6.379181216434895, -6.07229785606347, 5.579686760991327, -1.144196147017411, -10.342458339014197, 5.038597039200407, -0.6283292183264301, 4.969543096702615, -1.6506058911098629, -7.7655201344220455, 0.5195716833327357, -10.044625152219691, -6.8397773990338875, -4.16755399618074, -11.40604255520373, -3.284581458306205, 10.887452232755718, -2.669922981625776, -4.867020169903874, 4.767142921033679, -6.740620379920504, 5.329336016821781, 5.260025108509433, 4.000035970341659, -9.596026751616355, -6.790098009023257, -5.200855228981597, -5.878827957048844, 10.344549927138306, -7.712859561814483, 2.974424869050148, 8.290688195400712, 5.844470166074709, -1.1561660070758175, -10.226982365748656, 2.385580035145178, -5.21134285920728, 8.95281573168783, 2.9761502571217964, -2.2648915690245763, -6.39966849404622, -4.865837122894565, 2.5250609310430416, -8.616878436381157, -10.27930422943074, -2.716588650485514, 7.0400583365573715, -5.193646677496183, -4.970887973290062, 5.229447925811702, 0.7265966457297051, 1.583546494755262, -2.171534537951091, 4.66511071233753, -4.661431144405003, -1.9957928467954744, 10.785407712441529, 5.30827020218028, -4.159921971468075, -6.679389802320847, -1.9286789838101188, 3.513140154000544, -6.351669576281285, 8.075987158360707, 0.9669783792664914, 0.6982812556569626, -10.338397402328647, -9.385870671297702, -11.869351042678115, -5.744942787304947, -5.5232086845112285, 2.1227720195220074, 6.978940151640332, -4.340366443959809, -4.141978210957384, -5.780166932362148, -10.34489800961226, -3.8735466521160222, -9.93126582289107, 7.628499449653879, -4.192080668084552, -7.904853099777321, 4.132487146126188, -6.990031583289241, 7.8551376844265075, -4.374644413249209, 3.5811534395315645, 5.021621345840529, -7.091204996355662, -7.6078246693287745, 3.3988744166180713, 9.34669784306162, -4.756460681644461, 5.772222902741094, -8.911838940956192, 9.081219206867713, 2.4717068370996693, -4.1203737490308585, 7.966308737265197, 7.102000238727275, -11.755234448699612, -9.993971584774501, -4.849186987629303, 3.2169710606608466, -7.295851255004445, -12.089163466170362, 3.296552276514456, -3.3404609851355844, -8.760834315473982, -0.7179039456662692, 10.870935146657773, -6.470785687696118, -5.42111095662249, 10.29672789328623, -4.154853270688072, 8.460586597342793, -5.487156558700541, 5.122931562008336, -5.038833686785085, -5.420421491129657, 10.216969651732825, -4.210016063435735, -4.411667901921836, -5.210593471072026, -11.408010273020334, -11.920831575030743, 4.478685264489892, 10.751747568430844, -4.394590165840208, -11.333754296597625, -6.593375722957994, -0.7241793000828051, -5.13753005466408, -7.223059997924567, -7.823486326197288, 5.469993984283102, -9.369587070394845, -4.206807989540907, -11.945542325019304, -10.05155080073312, -10.011641214851448, 9.360786656162208, -4.3388540247263165, -10.263570297643966, 4.327341829669365, 1.4585612829539754, -6.447831377259997, 3.911396330578623, 4.059336629423321, 6.040457627427765, -9.640161283408018, -3.03035865176329, 4.277192966093969, 10.73048871938497, 6.522625583374912, -5.334409789305427, -1.484913624950209, 10.33017289739556, -4.6641571959732735, -6.554421077382977, -5.529439915678046, -5.815252676626628, -12.243870122293862, -9.06032750569515, -2.978915700947703, -8.010660068594701, 2.34858185123623, -5.8530678652239985, -10.105770053856764, -3.270262558111895, -9.621579260922486, -8.71374034818652, -2.7488757598000113, 10.852484685861787, -10.198036544674254, -9.251047489851947, 5.128205751418833, -5.312378750878208, 3.832461863654893, -5.167702284674023, 10.48210895404111, 7.489249362483946, -8.24841364568196, -10.075873569139151, -1.4391338973522037, -7.854601174984654, -2.798275247875295, 5.4446849886957525, -6.065656436160612, -12.21429834367169, 2.4279490041933367, -5.082159417825674, -8.451134128661536, 5.072286679519485, -3.8533577258496927, -0.5215763709021658, -6.574372313188264, 4.05209634594584, -5.45062451006647, 4.875697708721963, 3.1367603198581673, -11.793867913874058, 0.41923707982988656, 5.2484172130499385, 6.269802178014638, -2.5085338901425227, -8.329460798567293, -6.6913426986464, 8.700261782081544, -10.74390957287104, -4.868141037916503, 10.650841906599188, 2.7123727378617035, -5.066151300134576, 10.036055370848718, -5.094205286028574, -10.130730882450868, -8.136075555187645, 6.222413296409805, -3.40963927068435, -8.082571164391203, -3.1708153025626418, -11.241465732905873, 3.6470633593330235, 0.7183094015728864, 2.8094981981588774, -9.829306178667146, -4.542923526006987, -9.793323323769677, -7.11213048758283, -4.532117422288148, -5.596834491583287, -9.68555861744299, -12.182309951782615, -3.5121835871814295, -4.080332360216582, -10.65059428254319, -9.189355653328905, -0.4198109369547032, -7.347461735282465, 3.059529944118472, -2.9033587097628804, -4.139666673585298, -5.25252562233524, -4.405547683830409, 1.1078825089698654, 4.24238377375282, -4.1424614168942036, -3.7397000289739224, -8.709846479939516, 7.828590715262258, -4.676788250586616, -5.736464426539731, -6.721169154005559, -4.50416485235933, 2.7392512719695548, -11.479538561493962, 9.671930603246393, 4.987147311443584, 7.228239027135354, 10.936043797714618, -10.31635961222714, -10.079587973569812, -1.3337386664443802, -11.25684313328384, 9.513525429378124, -0.9414638181717343, -9.963163670968031, -9.126068224236976, -7.447599403946139, -4.61137566167058, 0.21667444715532413, -0.1943518662331991, -5.88359987950876, -5.0903686405108175, -2.2998418056484415, -8.13544167953846, 2.123116735834155, -8.607043190704692, 8.894473965089635, -4.493939254539883, -8.252002708435683, -1.0474565831817677, 3.4538532253841696, -9.730946857818525, -5.64684233915722, 2.7260496558344807, 2.8025826057738885, 4.577456668674436, -10.325360014266613, 1.2035994231406981, -10.219663853801432, 10.686926757614064, -6.544678610497276, -2.0842726737023045, -6.886883003542663, -2.0404616881237443, -10.55942440643775, -7.923906450491755, -5.648315015317388, -3.611243243540793, -4.756605354781256, 1.9630243218631453, -5.820396352319115, -5.1189648794444444, -5.977011237045797, -12.15061591580448, -12.23778905827816, -8.409140228479904, 5.807825384808474, 10.390825056284386, -2.342428780446422, -10.296422770247228, -1.5349886184089783, -10.100502115292329, 8.498891992400681, 1.6837136928235923, -5.021501331318337, 4.933332439368401, -8.926788514954989, 9.090861770368068, -3.9896065643059693, -9.463290945926932, 5.26785014754472, 9.821680645015165, -5.615273948736656, 5.782644476295081, -8.830920733668261, 5.089103256183654, -9.86430947157741, -4.995431649885444, 9.022051850753524, -12.012543687274487, -3.740903370326329, 8.81327394441538, -0.3045747108950356, 3.7979198767648183, -5.016230983370076, 0.9144278825861474, -1.906090176539199, -5.823594221735092, -10.02366263149575, -11.614131956323805, -7.561029050081119, 5.882168044182504, -9.059355935852572, 10.922499397069318, -12.101573883250854, 9.979946110609887, -11.98960128021492, -12.230786305354844, -0.08300021548698666, -5.36553786551087, -10.333288180157382, 8.107435843928323, -10.307864994351664, 8.883205739290625, -8.564306669004896, 7.623468641749072, 0.43055824251854546, -8.503299377391981, 10.933332321321153, -12.218266398631224, 2.892288220380445, 4.730748065721046, -5.821110844249599, 6.918673007938847, -9.287322180566802, 5.192730347101995, 4.197963854134772, -4.374694792403274, 9.593795810113859, -4.930859109295625, -7.178051572051032, -9.586931126826506, -8.908035213576497, 7.1647371541400116, -4.308833079308677, -7.871058501255976, -9.340993627910766, -6.685546425869305, -5.710263006044626, -12.244772673632877, 1.2986377480617897, -0.41365406291050655, -7.368036043967623, -8.555276857168487, -12.239629956282675, 2.5571582093064924, -7.244414127386212, -4.768197830600948, 5.287787829259118, 8.248361671077463, 0.8463145125759866, 6.168314052287326, 7.759764226619268, -5.2938346016658935, -9.756389039329914, -9.311129218131782, 0.29365277803803824, -3.3983797322767635, 6.418358093796188, -4.267706612098888, -7.644787089405842, 9.893242573431987, 10.809840134073971, 2.2987745610480106, 10.63350839919309, -9.85369648450435, 3.2662900031194466, -11.556528882826072, -6.785757496254121, 9.864149463217032, 5.609353085140203, -5.125300648682573, -4.2918353858162295, 4.197422974865617, -4.28245231385714, 8.184128300291611, -9.29548008256797, -3.199025908956392, -6.268218794280683, -5.40777759166201, 2.890505119918813, -12.15893613626318, -4.952060706691028, -10.666569183201101, -5.972411529241536, 2.641937801068032, -8.86185634714073, -4.504389100798229, 4.205102954463674, 9.171699219391266, 7.691376616080868, 2.0342600450359956, 10.700072151162914, 10.094382660162976, 3.9856425908123354, -9.839128145926402, -11.847621549179163, -6.113813449098615, 1.763562370637775, 6.547079228507284, -8.020327006867607, 4.834424630084545, -9.940470642428538, -5.474162662436128, 9.682273022814663, -4.908827361300027, -4.933987729553711, 5.704093472343827, -7.193156983578656, -5.360168513167784, 3.772687079077378, 10.559561211297957, 10.720562699963477, 7.357410843616417, -4.176817590109626, 3.6087199419916254, 0.15476537811958613, 3.8365892785425917, -9.718531624432263, 3.4429970669550864, 0.6078495262604091, 5.37336678997507, -9.399670034952871, 5.65278841284036, -5.643856656899113, -4.458057611621113, 0.11446341745397193, 0.8290329553292759, 5.157346995042734, -5.708214749922152, 9.431149153928208, -0.19577400740211237, -4.819506958562505, -4.449685484530726, 10.912040065112459, 5.550941977323491, -8.164999910994766, 2.6085228821881135, -5.619314244966119, 10.597437906524956, -7.461410757767144, -3.743657020973415, 3.0020270895715004, -5.5683003761805985, 3.8902551680659654, 9.159192956586699, 4.5129298977711745, -3.9669112554068846, -10.305690063211918, -9.946027693009059, 2.211287591781181, -12.196869301496102, -8.763519676801652, 10.196628550604173, -9.050346691133855, 3.29622424966683, -0.12880075933083823, 10.870634020189433, 5.960758051934876, -7.214860689706944, -7.555475678574574, 7.423014189482531, -4.2539806100613795, -11.626398728576397, -8.487429478275137, -1.0226874012805889, 4.855382720891809, -2.4650038041090614, 6.630959323291161, -7.997298732114024, 3.6852592601009544, -3.0571074121136768, -11.677062117302384, 8.037280986448712, 10.935850229441066, -4.147481197176742, 5.671536785883562, -0.40549515302337114, 2.219445373615101, 2.6198675142244667, -1.8151607779299195, 10.921405087671591, 6.576291681212377, -4.2306437700682, -7.660290056878594, 8.988798700620645, -8.599887284840468, -10.206115609133608, 10.435507888169434, -6.939744488654514, -0.09177110121609311, -6.275428070166713, -0.9392117792720592, -11.898198111897067, -7.789749202872942, 10.901545159895951, 5.396402962796191, -5.168148492145254, 10.66773277395747, -10.338732319952378, -3.3283329039582323, 10.57134161347766, -2.9662428162775343, -9.190376908721426, -8.317537624403581, -10.052210189897455, -4.637778886891797, -5.793877810350731, -6.208568778234713, -3.4778161168561006, -4.739363167449615, -10.236351454148753, 8.672651670564306, -9.107628063490283, -6.983941785477266, -9.732328775849988, 10.274752782549744, 0.5654457250473572, -10.283529425219443, -2.4903279939285246, -6.048842166738094, -8.188272645449604, -9.426408037532582, 6.800794077937142, -7.399238108093019, -4.9794537790931885, 5.140041765622765, -4.332072829135237, 0.14302508722019924, 0.029464420279539327, 9.922627309447439, 5.3914286176642685, 10.832209798070078, 4.545836799309146, -12.067663868633568, -10.105386556818152, -5.374140396371308, 7.159342607319804, -8.029679844355053, 1.568263662874276, 5.911884322320461, -2.5898128074985363, -5.763731331244422, -4.0497008683738525, 6.859282822184577, 4.150749433069659, 1.9447160260403917, 0.3733610529371285, -3.18143239538952, -2.1357544951524496, -0.6174217853976534, 10.928510122050232, 10.247405129001036, 6.671565780464135, -3.3044767870059584, -9.637089979683441, 8.743072115330262, -10.326316749885335, 4.348766175087199, 2.090914241875538, 6.68661153561834, -8.082308787652714, -5.502540082476148, 0.6192647987760354, -5.5562487941999015, 2.016439926869658, -5.287545445115754, -4.558053234912709, -5.700715483366334, -10.45801517706018, 5.210900068642074, -6.482138760384516, -5.366090819421446, -10.968889573776442, -0.8314475595382479, 2.8704327271051713, -10.92416204522489, 10.029616408201301, 5.3510227940089115, -1.0434372849114877, -4.164203673917198, -2.631171698137088, 8.600451684774983, -10.332702080748282, -7.734734615071362]}, {\"hovertemplate\": \"x: %{x} <br>y: %{y}\", \"mode\": \"markers\", \"name\": \"Test data\", \"type\": \"scatter\", \"x\": [4.986986986986987, 3.7757757757757755, 5.637637637637638, 5.907907907907908, 3.205205205205205, 6.258258258258259, 7.58958958958959, 7.96996996996997, 6.178178178178179, -0.4784784784784786, 3.175175175175175, 1.8438438438438438, 6.818818818818819, 0.4724724724724725, -0.29829829829829824, 7.089089089089089, 3.3653653653653652, 4.366366366366366, 1.0030030030030028, 6.078078078078079, 5.847847847847848, 5.5575575575575575, 0.24224224224224233, 2.2842842842842845, -1.119119119119119, -0.46846846846846857, 4.246246246246246, -0.5285285285285286, 6.708708708708709, 3.9659659659659656, 3.8858858858858856, -1.2792792792792793, 5.787787787787788, 1.5835835835835836, 1.2832832832832834, 2.6546546546546548, 3.265265265265265, 6.558558558558559, 0.3623623623623624, 3.3953953953953953, 5.597597597597598, -1.5295295295295295, 0.7327327327327327, 7.50950950950951, 7.079079079079079, 4.636636636636637, 7.8998998998999, -1.5995995995995997, -1.079079079079079, 5.007007007007007, 2.2442442442442445, 6.518518518518519, 4.746746746746747, -1.2392392392392393, 1.8538538538538538, -0.0880880880880881, -1.3693693693693694, 6.158158158158159, 5.307307307307307, 7.87987987987988, 3.7557557557557555, -0.8988988988988988, -1.109109109109109, 4.526526526526527, 7.2392392392392395, 4.216216216216216, -0.45845845845845856, 7.52952952952953, -0.6686686686686687, 7.119119119119119, -1.82982982982983, 2.794794794794795, 3.7157157157157155, 7.7997997997998, 2.814814814814815, -0.21821821821821819, 1.8138138138138138, 5.5075075075075075, 5.5375375375375375, 2.2542542542542545, 0.44244244244244246, 0.7027027027027026, 1.9039039039039038, 5.047047047047047, -0.8488488488488488, 0.8328328328328327, 1.2532532532532534, 0.8928928928928928, 3.5455455455455454, -0.6186186186186187, 4.946946946946947, 5.377377377377377, -0.3683683683683683, 7.61961961961962, -1.3393393393393394, 2.674674674674675, 1.6236236236236237, 5.317317317317317, 1.8038038038038038, 2.894894894894895, 0.39239239239239243, 1.0430430430430429, 7.64964964964965, -0.9789789789789789, 6.678678678678679, 0.3723723723723724, 0.5325325325325325, 0.15215215215215228, 0.6226226226226226, 4.336336336336336, 6.228228228228229, -1.2992992992992993, 5.357357357357357, -1.6496496496496498, 7.56956956956957, 3.215215215215215, -1.019019019019019, 7.2792792792792795, 3.4154154154154153, 2.1941941941941945, -0.20820820820820818, 4.156156156156156, -0.3583583583583583, 1.2132132132132134, 5.737737737737738, 6.998998998998999, 5.057057057057057, 7.1991991991991995, 3.8058058058058055, 1.4034034034034035, -0.9589589589589589, -0.42842842842842854, -0.44844844844844856, -1.4494494494494494, 3.225225225225225, -1.2592592592592593, 6.918918918918919, 5.707707707707708, 0.3823823823823824, -1.3293293293293293, 1.6336336336336337, 7.47947947947948, 0.14214214214214227, 2.6246246246246248, 4.436436436436437, 5.387387387387387, 5.177177177177177, 2.754754754754755, 4.456456456456457, 3.4454454454454453, 0.21221221221221231, -0.5385385385385386, 5.667667667667668, 4.236236236236236, 7.2292292292292295, 5.957957957957958, -1.3493493493493494, 3.255255255255255, 0.9029029029029028, 2.0540540540540544, 0.8528528528528527, 4.316316316316316, 3.7457457457457455, 6.168168168168169, 7.069069069069069, 0.8028028028028027, 0.5425425425425425, 4.296296296296296, 2.5845845845845847, 1.5735735735735736, 3.9359359359359356, 2.724724724724725, 6.808808808808809, 1.7237237237237237, 2.0640640640640644, 2.2942942942942945, 0.9929929929929928, 0.9329329329329328, 3.7657657657657655, 0.11211211211211225, -1.2292292292292293, 3.125125125125125, 2.4144144144144146, 3.045045045045045, 5.127127127127127, -0.3783783783783783, 5.167167167167167, 1.3033033033033035, 4.846846846846847, 6.488488488488489, 1.0330330330330328, 4.776776776776777, 1.193193193193193, 2.0240240240240244, 5.877877877877878, -0.7287287287287287, 7.2492492492492495, 4.326326326326326, 6.698698698698699, -1.88988988988989, 4.086086086086086, 3.115115115115115, 4.416416416416417, -0.8788788788788788, 3.3553553553553552, 1.7437437437437437, 5.587587587587588, 2.5545545545545547, 5.807807807807808, -0.11811811811811812, 3.9259259259259256, 6.248248248248249, 2.964964964964965, 4.646646646646647, -1.6396396396396398, 7.92992992992993, 4.286286286286286, -0.7587587587587588, 1.5335335335335336, 0.6526526526526526, 6.018018018018019, 0.6026026026026026, 0.25225225225225234, 1.5435435435435436, 1.2432432432432434, 1.7937937937937938, -1.3793793793793794, 3.6356356356356354, 7.90990990990991, 6.848848848848849, -1.6696696696696698, 1.993993993993994, 1.0630630630630629, 6.868868868868869, -1.86986986986987, 5.987987987987988, 0.28228228228228236, 2.3743743743743746, -0.06806806806806809, 4.356356356356356, 1.983983983983984, 5.147147147147147, 4.126126126126126, 5.687687687687688, -0.10810810810810811, 4.426426426426427, 6.838838838838839, 2.2042042042042045, 2.4644644644644647, -1.6596596596596598, 0.09209209209209224, 2.5245245245245247, 3.185185185185185, 7.66966966966967, 7.2592592592592595, 4.966966966966967, 2.0840840840840844, -0.19819819819819817, -0.6386386386386387, 4.186186186186186, 4.496496496496497, 7.109109109109109, 0.8628628628628627, 1.4734734734734736, 6.548548548548549, 2.0140140140140144, 7.70970970970971, -1.5795795795795795, 2.824824824824825, -0.05805805805805808, 7.72972972972973, 5.627627627627628, 5.757757757757758, -0.7987987987987988, 3.235235235235235, -0.9389389389389389, -0.8688688688688688, 2.2742742742742745, 3.8558558558558556, 1.0730730730730729, 5.267267267267267, 7.45945945945946, 0.8728728728728727, 2.734734734734735, 6.268268268268269, -1.099099099099099, 6.428428428428429, 0.2022022022022023, 2.744744744744745, 3.3853853853853852, 0.8828828828828827, 2.2342342342342345, 7.1491491491491495, 3.7857857857857855, 1.3733733733733735, -1.4694694694694694, 0.8228228228228227, 4.566566566566567, 7.95995995995996, -1.4994994994994995], \"y\": [-0.8694850850069475, 6.043648791687345, -10.871355000924336, -12.246192482331399, 9.61967218672025, -10.258093071227792, -5.814360217492217, -3.471024372554849, -11.01093175458699, -9.777393007858722, 9.804563513026999, 1.7093069348494034, -4.691118049841552, -7.818257966060714, -10.293341344262984, -4.190562154693, 8.531358577991192, 4.607840870283129, -5.2161683207860285, -11.736994808764988, -12.175642774910964, -9.978491136849224, -9.010615570456242, 7.396441813518818, -4.193592487842789, -9.820744689480492, 4.895424476907289, -9.535091133842874, -5.446929315051358, 5.420164940188773, 5.6399629463381755, -2.377495173386338, -11.968567807978783, -1.24373473250397, -3.6765337893770025, 10.527937958372647, 9.22699271515926, -6.893393055654519, -8.398803977121258, 8.31903948592642, -10.449186100361784, 0.31826860017649905, -6.496165820013618, -5.675484275293268, -4.1764487396953935, 3.219550262683585, -4.349846269340639, 1.0114906091126674, -4.644442826781849, -1.177457126616284, 6.9175869049400145, -7.3349128135382715, 2.2268033985863003, -2.8299689782652266, 1.835810554325292, -10.217865260895032, -1.3749119426316847, -11.177351190291192, -6.218170468048917, -4.561917882323138, 6.130867111923216, -6.584270507404513, -4.3066628080653935, 3.946101290129342, -4.618759679676819, 4.9516227835833995, -9.862378024977327, -5.723903118459248, -8.639750516607812, -4.245446248175236, 3.055913631112543, 10.91322377130758, 6.31826113499759, -5.237458288251825, 10.929384846624243, -10.34573028052846, 1.3343823487853694, -9.328049889011861, -9.726185402527692, 7.039022502022695, -7.9767858036888795, -6.642262310487838, 2.4782869047971854, -1.8110715794995116, -7.081498077783479, -6.019465516013725, -3.866965670002368, -5.737747351039019, 7.292474479983277, -8.994768322000299, -0.27339717803967556, -7.376951487217433, -10.159210836113736, -5.823862231515073, -1.706083073921177, 10.612311808024073, -0.838282530174549, -6.386725810864032, 1.2110397258446128, 10.900439274468523, -8.241044674368224, -5.020581642703672, -5.806202595171439, -5.745793839668446, -5.702667865789349, -8.346331257347012, -7.503254583068439, -9.430082632249466, -7.040519227045238, 4.692101895192162, -10.555267051408146, -2.15253972640026, -7.051475737690883, 1.4866669507080115, -5.794149218859424, 9.556170562974263, -5.310883042660767, -4.781002441961843, 8.177811471348246, 6.295403280434818, -10.344969753918493, 5.055468275828595, -10.183526533476522, -4.109008788147777, -11.692643575916412, -4.146814977712665, -1.972809168822312, -4.471614422295899, 5.920928172153827, -2.8266833322303846, -5.959698960772354, -9.976922511615763, -9.902288051394937, -0.5111892053997501, 9.491820639832925, -2.6033669927114618, -4.2800402265058475, -11.483065512477527, -8.293737752398362, -1.8172126205522456, -0.7339013220950003, -5.588589418394416, -9.473600844244842, 10.383214666581562, 4.36742912925174, -7.53766212509899, -3.9880182443807684, 10.851985551408205, 4.285675226303332, 7.9673922938484045, -9.155404417542508, -9.4815798792865, -11.1539917301664, 4.914617830373074, -4.580321647774252, -12.202097601892001, -1.595310279499198, 9.294208239085407, -5.6908213622884976, 4.464600522847288, -5.925418626592335, 4.743064003681909, 6.17610023983028, -11.095354719575866, -4.164526972407357, -6.161133990929321, -7.4511720903920695, 4.790359554092671, 10.156840237723788, -1.3420484733910423, 5.496121384368668, 10.78048069782859, -4.747121149746523, 0.25766370146755135, 4.597759431823088, 7.513124626528885, -5.264386187927086, -5.549735573646058, 6.086716364240486, -9.599705564068548, -2.9434835293972257, 10.090905083758708, 8.798282254755511, 10.478553850840655, -3.135001420876215, -10.133165052016768, -3.81644248191673, -3.545010820069881, 1.084167873249493, -7.675109929590075, -5.069932595927254, 1.9078134964848836, -4.225360440378891, 4.064580996111949, -12.227935487308233, -8.166021788078188, -4.658143473886788, 4.718068380690924, -5.530008643333332, 3.5315871598050066, 5.17489443066572, 10.144445776044455, 4.4430529209754335, -6.7859366684377775, 8.602063631962231, 0.4900968101765919, -10.33591252517552, 9.962562322047196, -12.052745814591157, -10.26920554810139, 5.523095201303375, -10.358927248723473, 10.758945766954538, 3.1406580309416343, 1.392994428623144, -3.999124144320276, 4.812768531166325, -7.9110162168051685, -1.7230017772274842, -6.889659518378907, -12.030324478498246, -7.142085620495041, -8.961434242238193, -1.6296113266840813, -3.9287025087918632, 1.0885624168079528, -1.2653235027519267, 6.743229800913996, -4.237283713894041, -4.539173140446831, 1.6719521200432987, 3.66473349085752, -4.920827313707345, -4.451434654254989, 3.375510673383772, -12.132011472648724, -8.811512808310493, 8.395623592379014, -10.17688862614946, 4.637041376814851, 3.53170351733967, -3.4745934024417426, 5.10596834359817, -11.325496376707742, -10.253472311623915, 4.405983665654244, -4.587124619250819, 6.421675057154756, 9.260203495140903, 1.579653333282303, -9.679779519926786, 9.747901308751963, 9.743920800540291, -5.778197968948188, -4.69837714719004, -0.5679958909887011, 4.86351793982508, -10.342631755202705, -8.857278422148063, 5.0044890051290425, 4.102320903939326, -4.225124254390895, -5.878471775825825, -2.2575842995171604, -7.002212539914661, 3.931217598871703, -5.680411261194252, 0.8166990503924698, 10.933894399996719, -10.15444531363461, -5.6094923363407165, -10.770564138310826, -11.81414490018536, -7.553670186988011, 9.42667507648131, -6.170909657861748, -6.88541159331777, 7.278498494801697, 5.737650285647962, -4.8703751904117745, -5.5370892460822985, -5.522401326975663, -5.831555280252663, 10.806762925865455, -10.155603587656678, -4.419514230455329, -8.36801185147039, -9.202657225799813, 10.830596594733443, 8.389797242873492, -5.7846527961748455, 6.795084878119327, -4.31776006625158, 6.0016635295249, -3.053368888494635, -0.3003526223682337, -6.0665949379862205, 3.71089800401729, -3.6094750054913023, 0.011644904169876624]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Funzione di stimare\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7eaf77b7-0086-44cc-b0f6-3e617df4ffca');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qZnYsQIfmCm"
      },
      "source": [
        "# Tensor Dataset Che converte i dati da numpy a Pytorch\n",
        "class CustomTensorDataset(Dataset):\n",
        "    def __init__(self, x,y,mean,std):\n",
        "        x = (x - mean)/std\n",
        "        self.x = torch.from_numpy(x).type(torch.float32).unsqueeze(1)\n",
        "        self.y = torch.from_numpy(y).type(torch.float32).unsqueeze(1)\n",
        "    def __getitem__(self, index):\n",
        "        x = self.x[index]\n",
        "        y = self.y[index]\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]\n",
        "\n",
        "# Dataset generator creation\n",
        "train_ds = CustomTensorDataset(X_train,Y_train,R_mean,R_std)\n",
        "test_ds = CustomTensorDataset(X_test,Y_test,R_mean,R_std)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DOE5WolPLJl"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqCouO7OPOh5",
        "outputId": "b4023101-2a3b-4246-e251-aa2b699df055",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class RegressionNet(nn.Module):\n",
        "    def __init__(self,num_inputs):\n",
        "        super(RegressionNet,self).__init__()\n",
        "        self.fc1 = nn.Linear(num_inputs,100)\n",
        "        self.fc2 = nn.Linear(100,50)\n",
        "        self.fc3 = nn.Linear(50,1)\n",
        "    def forward(self,x):\n",
        "        # torch.sigmoid, torch.tanh, torch.relu\n",
        "        x = torch.tanh(self.fc1(x)) \n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = RegressionNet(num_inputs=1)\n",
        "summary(model, (1,1), batch_size=-1, device='cpu')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1               [-1, 1, 100]             200\n",
            "            Linear-2                [-1, 1, 50]           5,050\n",
            "            Linear-3                 [-1, 1, 1]              51\n",
            "================================================================\n",
            "Total params: 5,301\n",
            "Trainable params: 5,301\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.02\n",
            "Estimated Total Size (MB): 0.02\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIi1CUhQfnzh"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HQp0vM5fYf-"
      },
      "source": [
        "# validation: metric regression\n",
        "def metrics_func_regression(target, output):\n",
        "    # Comptue mean squaer error (Migliora quanto piu' ci avviciniamo a zero)\n",
        "    mse = torch.sum((output - target) ** 2)\n",
        "    return mse\n",
        "\n",
        "# validation metric classification\n",
        "def metrics_func_classification(target, output):\n",
        "    # Compute number of correct prediction\n",
        "    pred = output.argmax(dim=-1,keepdim=True)\n",
        "    corrects =pred.eq(target.reshape(pred.shape)).sum().item()\n",
        "    return -corrects # minus for coeherence with best result is the most negative one\n",
        "\n",
        "# training: loss calculation and backward step\n",
        "def loss_batch(loss_func,metric_func, xb,yb,yb_h, opt=None):\n",
        "    # obtain loss\n",
        "    loss = loss_func(yb_h, yb)\n",
        "    # obtain performance metric \n",
        "    with torch.no_grad():\n",
        "        metric_b = metric_func(yb,yb_h)\n",
        "    if opt is not None:\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    return loss.item(), metric_b\n",
        "\n",
        "# one epoch training\n",
        "def loss_epoch(model, loss_func,metric_func, dataset_dl, sanity_check,opt, device):\n",
        "    loss = 0.0\n",
        "    metric = 0.0\n",
        "    len_data = float(len(dataset_dl.dataset))\n",
        "    # get batch data\n",
        "    for xb,yb in dataset_dl:    \n",
        "        # send to cuda the data (batch size)\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        # obtain model output \n",
        "        yb_h = model.forward(xb)\n",
        "        # loss and metric Calculation\n",
        "        loss_b, metric_b = loss_batch(loss_func,metric_func, xb,yb,yb_h,opt)\n",
        "        # update loss\n",
        "        loss += loss_b\n",
        "        # update metric\n",
        "        if metric_b is not None:\n",
        "            metric+=metric_b \n",
        "        if sanity_check is True:\n",
        "            break\n",
        "    # average loss\n",
        "    loss /=len_data\n",
        "    # average metric\n",
        "    metric /=len_data\n",
        "    return loss, metric\n",
        "\n",
        "# get learning rate from optimizer\n",
        "def get_lr(opt):\n",
        "    # opt.param_groups[0]['lr']\n",
        "    for param_group in opt.param_groups:\n",
        "        return param_group[\"lr\"]\n",
        "\n",
        "# trainig - test loop\n",
        "def train_test(params):\n",
        "    # --> extract params\n",
        "    model = params[\"model\"]\n",
        "    loss_func=params[\"loss_func\"]\n",
        "    metric_func=params[\"metric_func\"]\n",
        "    num_epochs=params[\"num_epochs\"]\n",
        "    opt=params[\"optimizer\"]\n",
        "    lr_scheduler=params[\"lr_scheduler\"]\n",
        "    train_dl=params[\"train_dl\"]\n",
        "    test_dl=params[\"test_dl\"]\n",
        "    device=params[\"device\"]\n",
        "    continue_training=params[\"continue_training\"]\n",
        "    sanity_check=params[\"sanity_check\"]\n",
        "    path2weigths=params[\"path2weigths\"]\n",
        "    # --> send model to device and print device\n",
        "    model = model.to(device)\n",
        "    print(\"--> training device %s\" % (device))\n",
        "    # --> if continue_training=True load path2weigths\n",
        "    if continue_training==True and os.path.isfile(path2weigths):\n",
        "        print(\"--> continue training  from last best weights\")\n",
        "        weights = torch.load(path2weigths)\n",
        "        model.load_state_dict(weights)\n",
        "    # --> history of loss values in each epoch\n",
        "    loss_history={\"train\": [],\"test\":[]}\n",
        "    # --> history of metric values in each epoch\n",
        "    metric_history={\"train\": [],\"test\":[]}\n",
        "    # --> a deep copy of weights for the best performing model\n",
        "    best_model_weights = copy.deepcopy(model.state_dict())\n",
        "    # --> initialiaze best loss to large value\n",
        "    best_loss=float(\"inf\")\n",
        "    # --> main loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # --> get learning rate\n",
        "        lr = get_lr(opt)\n",
        "        print(\"----\\nEpoch %s/%s, lr=%.6f\" % (epoch+1,num_epochs,lr))\n",
        "        # --> train model on training dataset\n",
        "        # we tell to the model to enter in train state. it is important because\n",
        "        # there are somelayers like dropout, batchnorm that behaves \n",
        "        # differently between train and test\n",
        "        model.train()\n",
        "        train_loss,train_metric = loss_epoch(model, loss_func, metric_func,train_dl,sanity_check, opt,device)\n",
        "        # --> collect loss and metric for training dataset\n",
        "        loss_history[\"train\"].append(train_loss)\n",
        "        metric_history[\"train\"].append(train_metric)\n",
        "        # --> tell the model to be in test (validation) mode\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_loss, test_metric = loss_epoch(model, loss_func, metric_func, test_dl,sanity_check,opt=None,device=device)\n",
        "        # --> collect loss and metric for test dataset\n",
        "        loss_history[\"test\"].append(test_loss)\n",
        "        metric_history[\"test\"].append(test_metric)\n",
        "        # --> store best model\n",
        "        if test_loss < best_loss:\n",
        "            print(\"--> model improved! --> saved to %s\" %(path2weigths))\n",
        "            best_loss = test_loss\n",
        "            best_model_weights = copy.deepcopy(model.state_dict())\n",
        "            # --> store weights into local file\n",
        "            torch.save(model.state_dict(),path2weigths)\n",
        "        # --> learning rate scheduler\n",
        "        lr_scheduler.step()\n",
        "        print(\"--> train_loss: %.6f, test_loss: %.6f, train_metric: %.3f, test_metric: %.3f\" % (train_loss,test_loss,train_metric,test_metric))\n",
        "    # --> load best weights\n",
        "    model.load_state_dict(best_model_weights)\n",
        "    return model, loss_history,metric_history\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QakBtSSMfaNd",
        "outputId": "7e617240-7b17-43e5-a6db-98bea65c0cee"
      },
      "source": [
        "\n",
        "# Setup GPU Device\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "\n",
        "# Regression\n",
        "model = RegressionNet(num_inputs=1).to(device)\n",
        "loss_func = nn.MSELoss(reduction=\"sum\")  \n",
        "opt = optim.Adam(model.parameters(),lr=0.005)\n",
        "train_dl = DataLoader(train_ds,batch_size=100,shuffle=True)\n",
        "test_dl = DataLoader(test_ds,batch_size=50,shuffle=True)\n",
        "\n",
        "# Setup GPU Device\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.999)  #  lr = lr * gamma ** last_epoch\n",
        "params = {\n",
        "    \"model\":                 model,\n",
        "    \"loss_func\":             loss_func, \n",
        "    \"metric_func\":           metrics_func_regression,\n",
        "    \"num_epochs\":            2500,\n",
        "    \"optimizer\":             opt,\n",
        "    \"lr_scheduler\":          lr_scheduler,\n",
        "    \"train_dl\":              train_dl,\n",
        "    \"test_dl\":               test_dl,\n",
        "    \"device\":                device,  \n",
        "    \"continue_training\" :    False,  # continue training from last save weights\n",
        "    \"sanity_check\":          False, # if true we only do one batch per epoch\n",
        "    \"path2weigths\":          \"./weights_regression.pt\"  \n",
        "} \n",
        "model, loss_history,metric_history = train_test(params)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "--> train_loss: 0.041124, test_loss: 0.045541, train_metric: 0.041, test_metric: 0.046\n",
            "----\n",
            "Epoch 934/2500, lr=0.001966\n",
            "--> train_loss: 0.041467, test_loss: 0.046051, train_metric: 0.041, test_metric: 0.046\n",
            "----\n",
            "Epoch 935/2500, lr=0.001964\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.041172, test_loss: 0.045117, train_metric: 0.041, test_metric: 0.045\n",
            "----\n",
            "Epoch 936/2500, lr=0.001962\n",
            "--> train_loss: 0.041392, test_loss: 0.045664, train_metric: 0.041, test_metric: 0.046\n",
            "----\n",
            "Epoch 937/2500, lr=0.001960\n",
            "--> train_loss: 0.040724, test_loss: 0.045230, train_metric: 0.041, test_metric: 0.045\n",
            "----\n",
            "Epoch 938/2500, lr=0.001958\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.040592, test_loss: 0.045010, train_metric: 0.041, test_metric: 0.045\n",
            "----\n",
            "Epoch 939/2500, lr=0.001956\n",
            "--> train_loss: 0.040624, test_loss: 0.046151, train_metric: 0.041, test_metric: 0.046\n",
            "----\n",
            "Epoch 940/2500, lr=0.001954\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.041822, test_loss: 0.044738, train_metric: 0.042, test_metric: 0.045\n",
            "----\n",
            "Epoch 941/2500, lr=0.001952\n",
            "--> train_loss: 0.041208, test_loss: 0.046648, train_metric: 0.041, test_metric: 0.047\n",
            "----\n",
            "Epoch 942/2500, lr=0.001950\n",
            "--> train_loss: 0.040809, test_loss: 0.045901, train_metric: 0.041, test_metric: 0.046\n",
            "----\n",
            "Epoch 943/2500, lr=0.001948\n",
            "--> train_loss: 0.040955, test_loss: 0.045804, train_metric: 0.041, test_metric: 0.046\n",
            "----\n",
            "Epoch 944/2500, lr=0.001946\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.040728, test_loss: 0.044527, train_metric: 0.041, test_metric: 0.045\n",
            "----\n",
            "Epoch 945/2500, lr=0.001944\n",
            "--> train_loss: 0.040559, test_loss: 0.045772, train_metric: 0.041, test_metric: 0.046\n",
            "----\n",
            "Epoch 946/2500, lr=0.001942\n",
            "--> train_loss: 0.040454, test_loss: 0.044857, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 947/2500, lr=0.001941\n",
            "--> train_loss: 0.040295, test_loss: 0.045252, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 948/2500, lr=0.001939\n",
            "--> train_loss: 0.040366, test_loss: 0.045475, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 949/2500, lr=0.001937\n",
            "--> train_loss: 0.040066, test_loss: 0.045238, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 950/2500, lr=0.001935\n",
            "--> train_loss: 0.040930, test_loss: 0.045963, train_metric: 0.041, test_metric: 0.046\n",
            "----\n",
            "Epoch 951/2500, lr=0.001933\n",
            "--> train_loss: 0.040685, test_loss: 0.045175, train_metric: 0.041, test_metric: 0.045\n",
            "----\n",
            "Epoch 952/2500, lr=0.001931\n",
            "--> train_loss: 0.040112, test_loss: 0.044962, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 953/2500, lr=0.001929\n",
            "--> train_loss: 0.040068, test_loss: 0.044900, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 954/2500, lr=0.001927\n",
            "--> train_loss: 0.039892, test_loss: 0.044719, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 955/2500, lr=0.001925\n",
            "--> train_loss: 0.040224, test_loss: 0.045241, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 956/2500, lr=0.001923\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.039598, test_loss: 0.044376, train_metric: 0.040, test_metric: 0.044\n",
            "----\n",
            "Epoch 957/2500, lr=0.001921\n",
            "--> train_loss: 0.039911, test_loss: 0.044588, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 958/2500, lr=0.001919\n",
            "--> train_loss: 0.039927, test_loss: 0.045396, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 959/2500, lr=0.001917\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.040126, test_loss: 0.044325, train_metric: 0.040, test_metric: 0.044\n",
            "----\n",
            "Epoch 960/2500, lr=0.001915\n",
            "--> train_loss: 0.039783, test_loss: 0.045074, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 961/2500, lr=0.001914\n",
            "--> train_loss: 0.039801, test_loss: 0.044713, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 962/2500, lr=0.001912\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.040691, test_loss: 0.043931, train_metric: 0.041, test_metric: 0.044\n",
            "----\n",
            "Epoch 963/2500, lr=0.001910\n",
            "--> train_loss: 0.040997, test_loss: 0.046044, train_metric: 0.041, test_metric: 0.046\n",
            "----\n",
            "Epoch 964/2500, lr=0.001908\n",
            "--> train_loss: 0.039599, test_loss: 0.044265, train_metric: 0.040, test_metric: 0.044\n",
            "----\n",
            "Epoch 965/2500, lr=0.001906\n",
            "--> train_loss: 0.040009, test_loss: 0.044482, train_metric: 0.040, test_metric: 0.044\n",
            "----\n",
            "Epoch 966/2500, lr=0.001904\n",
            "--> train_loss: 0.040337, test_loss: 0.045273, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 967/2500, lr=0.001902\n",
            "--> train_loss: 0.039823, test_loss: 0.044120, train_metric: 0.040, test_metric: 0.044\n",
            "----\n",
            "Epoch 968/2500, lr=0.001900\n",
            "--> train_loss: 0.040020, test_loss: 0.045140, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 969/2500, lr=0.001898\n",
            "--> train_loss: 0.039661, test_loss: 0.044372, train_metric: 0.040, test_metric: 0.044\n",
            "----\n",
            "Epoch 970/2500, lr=0.001896\n",
            "--> train_loss: 0.040301, test_loss: 0.045144, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 971/2500, lr=0.001894\n",
            "--> train_loss: 0.040096, test_loss: 0.045974, train_metric: 0.040, test_metric: 0.046\n",
            "----\n",
            "Epoch 972/2500, lr=0.001893\n",
            "--> train_loss: 0.040121, test_loss: 0.044649, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 973/2500, lr=0.001891\n",
            "--> train_loss: 0.039836, test_loss: 0.044208, train_metric: 0.040, test_metric: 0.044\n",
            "----\n",
            "Epoch 974/2500, lr=0.001889\n",
            "--> train_loss: 0.040223, test_loss: 0.045610, train_metric: 0.040, test_metric: 0.046\n",
            "----\n",
            "Epoch 975/2500, lr=0.001887\n",
            "--> train_loss: 0.039565, test_loss: 0.044006, train_metric: 0.040, test_metric: 0.044\n",
            "----\n",
            "Epoch 976/2500, lr=0.001885\n",
            "--> train_loss: 0.039462, test_loss: 0.044254, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 977/2500, lr=0.001883\n",
            "--> train_loss: 0.039280, test_loss: 0.044222, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 978/2500, lr=0.001881\n",
            "--> train_loss: 0.039191, test_loss: 0.044573, train_metric: 0.039, test_metric: 0.045\n",
            "----\n",
            "Epoch 979/2500, lr=0.001879\n",
            "--> train_loss: 0.039364, test_loss: 0.044405, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 980/2500, lr=0.001878\n",
            "--> train_loss: 0.039277, test_loss: 0.044125, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 981/2500, lr=0.001876\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.039102, test_loss: 0.043883, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 982/2500, lr=0.001874\n",
            "--> train_loss: 0.038838, test_loss: 0.043909, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 983/2500, lr=0.001872\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.038896, test_loss: 0.043853, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 984/2500, lr=0.001870\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.038971, test_loss: 0.043464, train_metric: 0.039, test_metric: 0.043\n",
            "----\n",
            "Epoch 985/2500, lr=0.001868\n",
            "--> train_loss: 0.038839, test_loss: 0.044217, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 986/2500, lr=0.001866\n",
            "--> train_loss: 0.039345, test_loss: 0.044403, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 987/2500, lr=0.001864\n",
            "--> train_loss: 0.038817, test_loss: 0.043542, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 988/2500, lr=0.001863\n",
            "--> train_loss: 0.038859, test_loss: 0.044164, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 989/2500, lr=0.001861\n",
            "--> train_loss: 0.038790, test_loss: 0.043621, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 990/2500, lr=0.001859\n",
            "--> train_loss: 0.038822, test_loss: 0.043994, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 991/2500, lr=0.001857\n",
            "--> train_loss: 0.038844, test_loss: 0.044106, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 992/2500, lr=0.001855\n",
            "--> train_loss: 0.038966, test_loss: 0.043501, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 993/2500, lr=0.001853\n",
            "--> train_loss: 0.038594, test_loss: 0.044379, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 994/2500, lr=0.001851\n",
            "--> train_loss: 0.038887, test_loss: 0.043933, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 995/2500, lr=0.001850\n",
            "--> train_loss: 0.039474, test_loss: 0.043667, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 996/2500, lr=0.001848\n",
            "--> train_loss: 0.039189, test_loss: 0.044366, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 997/2500, lr=0.001846\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.038960, test_loss: 0.043320, train_metric: 0.039, test_metric: 0.043\n",
            "----\n",
            "Epoch 998/2500, lr=0.001844\n",
            "--> train_loss: 0.040066, test_loss: 0.045712, train_metric: 0.040, test_metric: 0.046\n",
            "----\n",
            "Epoch 999/2500, lr=0.001842\n",
            "--> train_loss: 0.039955, test_loss: 0.043882, train_metric: 0.040, test_metric: 0.044\n",
            "----\n",
            "Epoch 1000/2500, lr=0.001840\n",
            "--> train_loss: 0.039042, test_loss: 0.045349, train_metric: 0.039, test_metric: 0.045\n",
            "----\n",
            "Epoch 1001/2500, lr=0.001838\n",
            "--> train_loss: 0.040025, test_loss: 0.044622, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 1002/2500, lr=0.001837\n",
            "--> train_loss: 0.039603, test_loss: 0.044060, train_metric: 0.040, test_metric: 0.044\n",
            "----\n",
            "Epoch 1003/2500, lr=0.001835\n",
            "--> train_loss: 0.039133, test_loss: 0.044207, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1004/2500, lr=0.001833\n",
            "--> train_loss: 0.039490, test_loss: 0.043835, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1005/2500, lr=0.001831\n",
            "--> train_loss: 0.039022, test_loss: 0.043696, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1006/2500, lr=0.001829\n",
            "--> train_loss: 0.039502, test_loss: 0.043796, train_metric: 0.040, test_metric: 0.044\n",
            "----\n",
            "Epoch 1007/2500, lr=0.001827\n",
            "--> train_loss: 0.038727, test_loss: 0.043601, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1008/2500, lr=0.001826\n",
            "--> train_loss: 0.039029, test_loss: 0.044437, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1009/2500, lr=0.001824\n",
            "--> train_loss: 0.039316, test_loss: 0.043911, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1010/2500, lr=0.001822\n",
            "--> train_loss: 0.039096, test_loss: 0.045339, train_metric: 0.039, test_metric: 0.045\n",
            "----\n",
            "Epoch 1011/2500, lr=0.001820\n",
            "--> train_loss: 0.039743, test_loss: 0.043491, train_metric: 0.040, test_metric: 0.043\n",
            "----\n",
            "Epoch 1012/2500, lr=0.001818\n",
            "--> train_loss: 0.039479, test_loss: 0.045430, train_metric: 0.039, test_metric: 0.045\n",
            "----\n",
            "Epoch 1013/2500, lr=0.001817\n",
            "--> train_loss: 0.039184, test_loss: 0.044028, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1014/2500, lr=0.001815\n",
            "--> train_loss: 0.039331, test_loss: 0.043738, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1015/2500, lr=0.001813\n",
            "--> train_loss: 0.039488, test_loss: 0.044631, train_metric: 0.039, test_metric: 0.045\n",
            "----\n",
            "Epoch 1016/2500, lr=0.001811\n",
            "--> train_loss: 0.040874, test_loss: 0.045441, train_metric: 0.041, test_metric: 0.045\n",
            "----\n",
            "Epoch 1017/2500, lr=0.001809\n",
            "--> train_loss: 0.041607, test_loss: 0.045809, train_metric: 0.042, test_metric: 0.046\n",
            "----\n",
            "Epoch 1018/2500, lr=0.001807\n",
            "--> train_loss: 0.040977, test_loss: 0.045373, train_metric: 0.041, test_metric: 0.045\n",
            "----\n",
            "Epoch 1019/2500, lr=0.001806\n",
            "--> train_loss: 0.040919, test_loss: 0.045980, train_metric: 0.041, test_metric: 0.046\n",
            "----\n",
            "Epoch 1020/2500, lr=0.001804\n",
            "--> train_loss: 0.040522, test_loss: 0.045264, train_metric: 0.041, test_metric: 0.045\n",
            "----\n",
            "Epoch 1021/2500, lr=0.001802\n",
            "--> train_loss: 0.040207, test_loss: 0.045357, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 1022/2500, lr=0.001800\n",
            "--> train_loss: 0.039702, test_loss: 0.044353, train_metric: 0.040, test_metric: 0.044\n",
            "----\n",
            "Epoch 1023/2500, lr=0.001798\n",
            "--> train_loss: 0.040238, test_loss: 0.044974, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 1024/2500, lr=0.001797\n",
            "--> train_loss: 0.039754, test_loss: 0.044713, train_metric: 0.040, test_metric: 0.045\n",
            "----\n",
            "Epoch 1025/2500, lr=0.001795\n",
            "--> train_loss: 0.038748, test_loss: 0.044043, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1026/2500, lr=0.001793\n",
            "--> train_loss: 0.039133, test_loss: 0.043413, train_metric: 0.039, test_metric: 0.043\n",
            "----\n",
            "Epoch 1027/2500, lr=0.001791\n",
            "--> train_loss: 0.039088, test_loss: 0.044593, train_metric: 0.039, test_metric: 0.045\n",
            "----\n",
            "Epoch 1028/2500, lr=0.001789\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.038260, test_loss: 0.043247, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1029/2500, lr=0.001788\n",
            "--> train_loss: 0.039213, test_loss: 0.043560, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1030/2500, lr=0.001786\n",
            "--> train_loss: 0.038454, test_loss: 0.044542, train_metric: 0.038, test_metric: 0.045\n",
            "----\n",
            "Epoch 1031/2500, lr=0.001784\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.038183, test_loss: 0.043152, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1032/2500, lr=0.001782\n",
            "--> train_loss: 0.038383, test_loss: 0.043329, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1033/2500, lr=0.001781\n",
            "--> train_loss: 0.038172, test_loss: 0.043788, train_metric: 0.038, test_metric: 0.044\n",
            "----\n",
            "Epoch 1034/2500, lr=0.001779\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.038179, test_loss: 0.042648, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1035/2500, lr=0.001777\n",
            "--> train_loss: 0.037817, test_loss: 0.043224, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1036/2500, lr=0.001775\n",
            "--> train_loss: 0.038092, test_loss: 0.043690, train_metric: 0.038, test_metric: 0.044\n",
            "----\n",
            "Epoch 1037/2500, lr=0.001773\n",
            "--> train_loss: 0.038163, test_loss: 0.042972, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1038/2500, lr=0.001772\n",
            "--> train_loss: 0.038168, test_loss: 0.043274, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1039/2500, lr=0.001770\n",
            "--> train_loss: 0.038786, test_loss: 0.043333, train_metric: 0.039, test_metric: 0.043\n",
            "----\n",
            "Epoch 1040/2500, lr=0.001768\n",
            "--> train_loss: 0.038708, test_loss: 0.043100, train_metric: 0.039, test_metric: 0.043\n",
            "----\n",
            "Epoch 1041/2500, lr=0.001766\n",
            "--> train_loss: 0.038651, test_loss: 0.044203, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1042/2500, lr=0.001765\n",
            "--> train_loss: 0.038819, test_loss: 0.043131, train_metric: 0.039, test_metric: 0.043\n",
            "----\n",
            "Epoch 1043/2500, lr=0.001763\n",
            "--> train_loss: 0.038610, test_loss: 0.044111, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1044/2500, lr=0.001761\n",
            "--> train_loss: 0.038892, test_loss: 0.044139, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1045/2500, lr=0.001759\n",
            "--> train_loss: 0.038264, test_loss: 0.043034, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1046/2500, lr=0.001758\n",
            "--> train_loss: 0.038385, test_loss: 0.043309, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1047/2500, lr=0.001756\n",
            "--> train_loss: 0.038194, test_loss: 0.043578, train_metric: 0.038, test_metric: 0.044\n",
            "----\n",
            "Epoch 1048/2500, lr=0.001754\n",
            "--> train_loss: 0.038775, test_loss: 0.043640, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1049/2500, lr=0.001752\n",
            "--> train_loss: 0.038320, test_loss: 0.042895, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1050/2500, lr=0.001751\n",
            "--> train_loss: 0.038064, test_loss: 0.043116, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1051/2500, lr=0.001749\n",
            "--> train_loss: 0.038054, test_loss: 0.043313, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1052/2500, lr=0.001747\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.037845, test_loss: 0.042642, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1053/2500, lr=0.001745\n",
            "--> train_loss: 0.038061, test_loss: 0.043022, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1054/2500, lr=0.001744\n",
            "--> train_loss: 0.038575, test_loss: 0.043935, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1055/2500, lr=0.001742\n",
            "--> train_loss: 0.038516, test_loss: 0.043061, train_metric: 0.039, test_metric: 0.043\n",
            "----\n",
            "Epoch 1056/2500, lr=0.001740\n",
            "--> train_loss: 0.038917, test_loss: 0.043992, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1057/2500, lr=0.001738\n",
            "--> train_loss: 0.038817, test_loss: 0.043618, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1058/2500, lr=0.001737\n",
            "--> train_loss: 0.038585, test_loss: 0.042934, train_metric: 0.039, test_metric: 0.043\n",
            "----\n",
            "Epoch 1059/2500, lr=0.001735\n",
            "--> train_loss: 0.038445, test_loss: 0.043154, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1060/2500, lr=0.001733\n",
            "--> train_loss: 0.038931, test_loss: 0.043117, train_metric: 0.039, test_metric: 0.043\n",
            "----\n",
            "Epoch 1061/2500, lr=0.001731\n",
            "--> train_loss: 0.038842, test_loss: 0.043837, train_metric: 0.039, test_metric: 0.044\n",
            "----\n",
            "Epoch 1062/2500, lr=0.001730\n",
            "--> train_loss: 0.038378, test_loss: 0.042803, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1063/2500, lr=0.001728\n",
            "--> train_loss: 0.037962, test_loss: 0.043184, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1064/2500, lr=0.001726\n",
            "--> train_loss: 0.039178, test_loss: 0.043378, train_metric: 0.039, test_metric: 0.043\n",
            "----\n",
            "Epoch 1065/2500, lr=0.001724\n",
            "--> train_loss: 0.039449, test_loss: 0.042667, train_metric: 0.039, test_metric: 0.043\n",
            "----\n",
            "Epoch 1066/2500, lr=0.001723\n",
            "--> train_loss: 0.038011, test_loss: 0.044238, train_metric: 0.038, test_metric: 0.044\n",
            "----\n",
            "Epoch 1067/2500, lr=0.001721\n",
            "--> train_loss: 0.038148, test_loss: 0.042775, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1068/2500, lr=0.001719\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.038239, test_loss: 0.042573, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1069/2500, lr=0.001718\n",
            "--> train_loss: 0.038241, test_loss: 0.043364, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1070/2500, lr=0.001716\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.038897, test_loss: 0.042438, train_metric: 0.039, test_metric: 0.042\n",
            "----\n",
            "Epoch 1071/2500, lr=0.001714\n",
            "--> train_loss: 0.038155, test_loss: 0.043408, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1072/2500, lr=0.001712\n",
            "--> train_loss: 0.038154, test_loss: 0.042462, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1073/2500, lr=0.001711\n",
            "--> train_loss: 0.037709, test_loss: 0.042890, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1074/2500, lr=0.001709\n",
            "--> train_loss: 0.037904, test_loss: 0.042646, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1075/2500, lr=0.001707\n",
            "--> train_loss: 0.037793, test_loss: 0.042777, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1076/2500, lr=0.001706\n",
            "--> train_loss: 0.037887, test_loss: 0.043009, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1077/2500, lr=0.001704\n",
            "--> train_loss: 0.037782, test_loss: 0.043389, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1078/2500, lr=0.001702\n",
            "--> train_loss: 0.037955, test_loss: 0.043162, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1079/2500, lr=0.001700\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.037970, test_loss: 0.042082, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1080/2500, lr=0.001699\n",
            "--> train_loss: 0.037944, test_loss: 0.043014, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1081/2500, lr=0.001697\n",
            "--> train_loss: 0.037554, test_loss: 0.042541, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1082/2500, lr=0.001695\n",
            "--> train_loss: 0.037551, test_loss: 0.042400, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1083/2500, lr=0.001694\n",
            "--> train_loss: 0.037428, test_loss: 0.042382, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1084/2500, lr=0.001692\n",
            "--> train_loss: 0.037305, test_loss: 0.042169, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1085/2500, lr=0.001690\n",
            "--> train_loss: 0.037338, test_loss: 0.042117, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1086/2500, lr=0.001689\n",
            "--> train_loss: 0.038265, test_loss: 0.043148, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1087/2500, lr=0.001687\n",
            "--> train_loss: 0.038050, test_loss: 0.042350, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1088/2500, lr=0.001685\n",
            "--> train_loss: 0.037677, test_loss: 0.043546, train_metric: 0.038, test_metric: 0.044\n",
            "----\n",
            "Epoch 1089/2500, lr=0.001684\n",
            "--> train_loss: 0.038110, test_loss: 0.043437, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1090/2500, lr=0.001682\n",
            "--> train_loss: 0.037656, test_loss: 0.042719, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1091/2500, lr=0.001680\n",
            "--> train_loss: 0.037522, test_loss: 0.042832, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1092/2500, lr=0.001678\n",
            "--> train_loss: 0.037331, test_loss: 0.042197, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1093/2500, lr=0.001677\n",
            "--> train_loss: 0.037769, test_loss: 0.042395, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1094/2500, lr=0.001675\n",
            "--> train_loss: 0.037606, test_loss: 0.042867, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1095/2500, lr=0.001673\n",
            "--> train_loss: 0.037394, test_loss: 0.042389, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1096/2500, lr=0.001672\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.037751, test_loss: 0.041915, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1097/2500, lr=0.001670\n",
            "--> train_loss: 0.037204, test_loss: 0.043715, train_metric: 0.037, test_metric: 0.044\n",
            "----\n",
            "Epoch 1098/2500, lr=0.001668\n",
            "--> train_loss: 0.037731, test_loss: 0.042612, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1099/2500, lr=0.001667\n",
            "--> train_loss: 0.037212, test_loss: 0.042161, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1100/2500, lr=0.001665\n",
            "--> train_loss: 0.037064, test_loss: 0.042411, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1101/2500, lr=0.001663\n",
            "--> train_loss: 0.037028, test_loss: 0.041993, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1102/2500, lr=0.001662\n",
            "--> train_loss: 0.037132, test_loss: 0.041986, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1103/2500, lr=0.001660\n",
            "--> train_loss: 0.037169, test_loss: 0.042236, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1104/2500, lr=0.001658\n",
            "--> train_loss: 0.037257, test_loss: 0.042047, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1105/2500, lr=0.001657\n",
            "--> train_loss: 0.037230, test_loss: 0.042381, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1106/2500, lr=0.001655\n",
            "--> train_loss: 0.038000, test_loss: 0.042815, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1107/2500, lr=0.001653\n",
            "--> train_loss: 0.037728, test_loss: 0.043561, train_metric: 0.038, test_metric: 0.044\n",
            "----\n",
            "Epoch 1108/2500, lr=0.001652\n",
            "--> train_loss: 0.038202, test_loss: 0.043613, train_metric: 0.038, test_metric: 0.044\n",
            "----\n",
            "Epoch 1109/2500, lr=0.001650\n",
            "--> train_loss: 0.038101, test_loss: 0.042411, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1110/2500, lr=0.001649\n",
            "--> train_loss: 0.038108, test_loss: 0.042051, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1111/2500, lr=0.001647\n",
            "--> train_loss: 0.037865, test_loss: 0.043880, train_metric: 0.038, test_metric: 0.044\n",
            "----\n",
            "Epoch 1112/2500, lr=0.001645\n",
            "--> train_loss: 0.037998, test_loss: 0.042510, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1113/2500, lr=0.001644\n",
            "--> train_loss: 0.037553, test_loss: 0.042302, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1114/2500, lr=0.001642\n",
            "--> train_loss: 0.037056, test_loss: 0.042216, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1115/2500, lr=0.001640\n",
            "--> train_loss: 0.036863, test_loss: 0.041949, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1116/2500, lr=0.001639\n",
            "--> train_loss: 0.037098, test_loss: 0.042415, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1117/2500, lr=0.001637\n",
            "--> train_loss: 0.037278, test_loss: 0.042382, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1118/2500, lr=0.001635\n",
            "--> train_loss: 0.037499, test_loss: 0.042564, train_metric: 0.037, test_metric: 0.043\n",
            "----\n",
            "Epoch 1119/2500, lr=0.001634\n",
            "--> train_loss: 0.037546, test_loss: 0.042779, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1120/2500, lr=0.001632\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.037739, test_loss: 0.041849, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1121/2500, lr=0.001630\n",
            "--> train_loss: 0.037588, test_loss: 0.042552, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1122/2500, lr=0.001629\n",
            "--> train_loss: 0.037241, test_loss: 0.042138, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1123/2500, lr=0.001627\n",
            "--> train_loss: 0.037041, test_loss: 0.042111, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1124/2500, lr=0.001626\n",
            "--> train_loss: 0.036974, test_loss: 0.041973, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1125/2500, lr=0.001624\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.037055, test_loss: 0.041832, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1126/2500, lr=0.001622\n",
            "--> train_loss: 0.037188, test_loss: 0.042565, train_metric: 0.037, test_metric: 0.043\n",
            "----\n",
            "Epoch 1127/2500, lr=0.001621\n",
            "--> train_loss: 0.037474, test_loss: 0.042077, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1128/2500, lr=0.001619\n",
            "--> train_loss: 0.037441, test_loss: 0.042278, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1129/2500, lr=0.001617\n",
            "--> train_loss: 0.037414, test_loss: 0.042934, train_metric: 0.037, test_metric: 0.043\n",
            "----\n",
            "Epoch 1130/2500, lr=0.001616\n",
            "--> train_loss: 0.037284, test_loss: 0.042196, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1131/2500, lr=0.001614\n",
            "--> train_loss: 0.037743, test_loss: 0.042249, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1132/2500, lr=0.001613\n",
            "--> train_loss: 0.037651, test_loss: 0.042846, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1133/2500, lr=0.001611\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.037024, test_loss: 0.041758, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1134/2500, lr=0.001609\n",
            "--> train_loss: 0.036924, test_loss: 0.041954, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1135/2500, lr=0.001608\n",
            "--> train_loss: 0.036820, test_loss: 0.041792, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1136/2500, lr=0.001606\n",
            "--> train_loss: 0.036940, test_loss: 0.041858, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1137/2500, lr=0.001605\n",
            "--> train_loss: 0.036762, test_loss: 0.042034, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1138/2500, lr=0.001603\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.036968, test_loss: 0.041597, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1139/2500, lr=0.001601\n",
            "--> train_loss: 0.037001, test_loss: 0.042643, train_metric: 0.037, test_metric: 0.043\n",
            "----\n",
            "Epoch 1140/2500, lr=0.001600\n",
            "--> train_loss: 0.036956, test_loss: 0.042290, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1141/2500, lr=0.001598\n",
            "--> train_loss: 0.036955, test_loss: 0.041862, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1142/2500, lr=0.001597\n",
            "--> train_loss: 0.036905, test_loss: 0.041704, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1143/2500, lr=0.001595\n",
            "--> train_loss: 0.037078, test_loss: 0.041931, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1144/2500, lr=0.001593\n",
            "--> train_loss: 0.036954, test_loss: 0.041800, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1145/2500, lr=0.001592\n",
            "--> train_loss: 0.036982, test_loss: 0.042048, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1146/2500, lr=0.001590\n",
            "--> train_loss: 0.036912, test_loss: 0.041889, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1147/2500, lr=0.001589\n",
            "--> train_loss: 0.036741, test_loss: 0.041626, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1148/2500, lr=0.001587\n",
            "--> train_loss: 0.037347, test_loss: 0.041864, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1149/2500, lr=0.001585\n",
            "--> train_loss: 0.037341, test_loss: 0.043092, train_metric: 0.037, test_metric: 0.043\n",
            "----\n",
            "Epoch 1150/2500, lr=0.001584\n",
            "--> train_loss: 0.037086, test_loss: 0.042019, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1151/2500, lr=0.001582\n",
            "--> train_loss: 0.037377, test_loss: 0.042206, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1152/2500, lr=0.001581\n",
            "--> train_loss: 0.037212, test_loss: 0.042553, train_metric: 0.037, test_metric: 0.043\n",
            "----\n",
            "Epoch 1153/2500, lr=0.001579\n",
            "--> train_loss: 0.036982, test_loss: 0.041634, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1154/2500, lr=0.001578\n",
            "--> train_loss: 0.037038, test_loss: 0.042224, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1155/2500, lr=0.001576\n",
            "--> train_loss: 0.037242, test_loss: 0.042250, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1156/2500, lr=0.001574\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.037036, test_loss: 0.041575, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1157/2500, lr=0.001573\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.037205, test_loss: 0.041569, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1158/2500, lr=0.001571\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.037260, test_loss: 0.041539, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1159/2500, lr=0.001570\n",
            "--> train_loss: 0.036767, test_loss: 0.042143, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1160/2500, lr=0.001568\n",
            "--> train_loss: 0.037327, test_loss: 0.043912, train_metric: 0.037, test_metric: 0.044\n",
            "----\n",
            "Epoch 1161/2500, lr=0.001567\n",
            "--> train_loss: 0.039527, test_loss: 0.045760, train_metric: 0.040, test_metric: 0.046\n",
            "----\n",
            "Epoch 1162/2500, lr=0.001565\n",
            "--> train_loss: 0.039069, test_loss: 0.042963, train_metric: 0.039, test_metric: 0.043\n",
            "----\n",
            "Epoch 1163/2500, lr=0.001563\n",
            "--> train_loss: 0.038157, test_loss: 0.042046, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1164/2500, lr=0.001562\n",
            "--> train_loss: 0.038668, test_loss: 0.043287, train_metric: 0.039, test_metric: 0.043\n",
            "----\n",
            "Epoch 1165/2500, lr=0.001560\n",
            "--> train_loss: 0.039916, test_loss: 0.044389, train_metric: 0.040, test_metric: 0.044\n",
            "----\n",
            "Epoch 1166/2500, lr=0.001559\n",
            "--> train_loss: 0.040789, test_loss: 0.043450, train_metric: 0.041, test_metric: 0.043\n",
            "----\n",
            "Epoch 1167/2500, lr=0.001557\n",
            "--> train_loss: 0.039900, test_loss: 0.043886, train_metric: 0.040, test_metric: 0.044\n",
            "----\n",
            "Epoch 1168/2500, lr=0.001556\n",
            "--> train_loss: 0.038966, test_loss: 0.042124, train_metric: 0.039, test_metric: 0.042\n",
            "----\n",
            "Epoch 1169/2500, lr=0.001554\n",
            "--> train_loss: 0.037852, test_loss: 0.042133, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1170/2500, lr=0.001552\n",
            "--> train_loss: 0.038446, test_loss: 0.042962, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1171/2500, lr=0.001551\n",
            "--> train_loss: 0.038278, test_loss: 0.042714, train_metric: 0.038, test_metric: 0.043\n",
            "----\n",
            "Epoch 1172/2500, lr=0.001549\n",
            "--> train_loss: 0.037630, test_loss: 0.041915, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1173/2500, lr=0.001548\n",
            "--> train_loss: 0.036931, test_loss: 0.042246, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1174/2500, lr=0.001546\n",
            "--> train_loss: 0.036857, test_loss: 0.042125, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1175/2500, lr=0.001545\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.037053, test_loss: 0.041463, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1176/2500, lr=0.001543\n",
            "--> train_loss: 0.036956, test_loss: 0.042098, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1177/2500, lr=0.001542\n",
            "--> train_loss: 0.036919, test_loss: 0.041540, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1178/2500, lr=0.001540\n",
            "--> train_loss: 0.036591, test_loss: 0.041708, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1179/2500, lr=0.001539\n",
            "--> train_loss: 0.036739, test_loss: 0.042408, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1180/2500, lr=0.001537\n",
            "--> train_loss: 0.036821, test_loss: 0.041670, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1181/2500, lr=0.001535\n",
            "--> train_loss: 0.037084, test_loss: 0.041536, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1182/2500, lr=0.001534\n",
            "--> train_loss: 0.037032, test_loss: 0.041527, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1183/2500, lr=0.001532\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.036464, test_loss: 0.041175, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1184/2500, lr=0.001531\n",
            "--> train_loss: 0.036706, test_loss: 0.041676, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1185/2500, lr=0.001529\n",
            "--> train_loss: 0.036386, test_loss: 0.041359, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1186/2500, lr=0.001528\n",
            "--> train_loss: 0.036670, test_loss: 0.041270, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1187/2500, lr=0.001526\n",
            "--> train_loss: 0.036578, test_loss: 0.041596, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1188/2500, lr=0.001525\n",
            "--> train_loss: 0.036685, test_loss: 0.041442, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1189/2500, lr=0.001523\n",
            "--> train_loss: 0.036785, test_loss: 0.041220, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1190/2500, lr=0.001522\n",
            "--> train_loss: 0.036415, test_loss: 0.041834, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1191/2500, lr=0.001520\n",
            "--> train_loss: 0.036694, test_loss: 0.041519, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1192/2500, lr=0.001519\n",
            "--> train_loss: 0.036363, test_loss: 0.041353, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1193/2500, lr=0.001517\n",
            "--> train_loss: 0.036464, test_loss: 0.041258, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1194/2500, lr=0.001516\n",
            "--> train_loss: 0.036562, test_loss: 0.041754, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1195/2500, lr=0.001514\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.036526, test_loss: 0.041140, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1196/2500, lr=0.001513\n",
            "--> train_loss: 0.036198, test_loss: 0.041221, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1197/2500, lr=0.001511\n",
            "--> train_loss: 0.036173, test_loss: 0.041406, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1198/2500, lr=0.001510\n",
            "--> train_loss: 0.036285, test_loss: 0.041347, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1199/2500, lr=0.001508\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.036604, test_loss: 0.041009, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1200/2500, lr=0.001507\n",
            "--> train_loss: 0.036677, test_loss: 0.041685, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1201/2500, lr=0.001505\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.036871, test_loss: 0.040965, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1202/2500, lr=0.001504\n",
            "--> train_loss: 0.036253, test_loss: 0.041539, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1203/2500, lr=0.001502\n",
            "--> train_loss: 0.036431, test_loss: 0.041395, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1204/2500, lr=0.001501\n",
            "--> train_loss: 0.036392, test_loss: 0.041631, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1205/2500, lr=0.001499\n",
            "--> train_loss: 0.036345, test_loss: 0.041263, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1206/2500, lr=0.001498\n",
            "--> train_loss: 0.036396, test_loss: 0.041021, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1207/2500, lr=0.001496\n",
            "--> train_loss: 0.036588, test_loss: 0.041145, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1208/2500, lr=0.001495\n",
            "--> train_loss: 0.036833, test_loss: 0.041429, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1209/2500, lr=0.001493\n",
            "--> train_loss: 0.037522, test_loss: 0.042303, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1210/2500, lr=0.001492\n",
            "--> train_loss: 0.037262, test_loss: 0.041968, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1211/2500, lr=0.001490\n",
            "--> train_loss: 0.037205, test_loss: 0.041299, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1212/2500, lr=0.001489\n",
            "--> train_loss: 0.036868, test_loss: 0.041631, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1213/2500, lr=0.001487\n",
            "--> train_loss: 0.037297, test_loss: 0.041434, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1214/2500, lr=0.001486\n",
            "--> train_loss: 0.036424, test_loss: 0.041478, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1215/2500, lr=0.001484\n",
            "--> train_loss: 0.036268, test_loss: 0.041677, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1216/2500, lr=0.001483\n",
            "--> train_loss: 0.036612, test_loss: 0.041965, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1217/2500, lr=0.001481\n",
            "--> train_loss: 0.036732, test_loss: 0.041640, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1218/2500, lr=0.001480\n",
            "--> train_loss: 0.036462, test_loss: 0.041334, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1219/2500, lr=0.001478\n",
            "--> train_loss: 0.036464, test_loss: 0.041529, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1220/2500, lr=0.001477\n",
            "--> train_loss: 0.036340, test_loss: 0.041112, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1221/2500, lr=0.001475\n",
            "--> train_loss: 0.036230, test_loss: 0.041818, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1222/2500, lr=0.001474\n",
            "--> train_loss: 0.036594, test_loss: 0.041296, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1223/2500, lr=0.001472\n",
            "--> train_loss: 0.036740, test_loss: 0.041404, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1224/2500, lr=0.001471\n",
            "--> train_loss: 0.036362, test_loss: 0.041538, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1225/2500, lr=0.001469\n",
            "--> train_loss: 0.036626, test_loss: 0.041214, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1226/2500, lr=0.001468\n",
            "--> train_loss: 0.036392, test_loss: 0.041042, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1227/2500, lr=0.001466\n",
            "--> train_loss: 0.036532, test_loss: 0.041252, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1228/2500, lr=0.001465\n",
            "--> train_loss: 0.036257, test_loss: 0.041465, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1229/2500, lr=0.001463\n",
            "--> train_loss: 0.036394, test_loss: 0.041081, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1230/2500, lr=0.001462\n",
            "--> train_loss: 0.036279, test_loss: 0.041662, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1231/2500, lr=0.001461\n",
            "--> train_loss: 0.036526, test_loss: 0.041437, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1232/2500, lr=0.001459\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.036347, test_loss: 0.040958, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1233/2500, lr=0.001458\n",
            "--> train_loss: 0.037177, test_loss: 0.043002, train_metric: 0.037, test_metric: 0.043\n",
            "----\n",
            "Epoch 1234/2500, lr=0.001456\n",
            "--> train_loss: 0.036988, test_loss: 0.041011, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1235/2500, lr=0.001455\n",
            "--> train_loss: 0.037239, test_loss: 0.041414, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1236/2500, lr=0.001453\n",
            "--> train_loss: 0.037672, test_loss: 0.041936, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1237/2500, lr=0.001452\n",
            "--> train_loss: 0.036451, test_loss: 0.041089, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1238/2500, lr=0.001450\n",
            "--> train_loss: 0.037006, test_loss: 0.041960, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1239/2500, lr=0.001449\n",
            "--> train_loss: 0.036411, test_loss: 0.041239, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1240/2500, lr=0.001447\n",
            "--> train_loss: 0.036243, test_loss: 0.041401, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1241/2500, lr=0.001446\n",
            "--> train_loss: 0.037742, test_loss: 0.042057, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1242/2500, lr=0.001445\n",
            "--> train_loss: 0.037530, test_loss: 0.041990, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1243/2500, lr=0.001443\n",
            "--> train_loss: 0.036928, test_loss: 0.041686, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1244/2500, lr=0.001442\n",
            "--> train_loss: 0.037314, test_loss: 0.041362, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1245/2500, lr=0.001440\n",
            "--> train_loss: 0.036550, test_loss: 0.041766, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1246/2500, lr=0.001439\n",
            "--> train_loss: 0.036569, test_loss: 0.041430, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1247/2500, lr=0.001437\n",
            "--> train_loss: 0.036451, test_loss: 0.041565, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1248/2500, lr=0.001436\n",
            "--> train_loss: 0.036683, test_loss: 0.041400, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1249/2500, lr=0.001434\n",
            "--> train_loss: 0.036992, test_loss: 0.042366, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1250/2500, lr=0.001433\n",
            "--> train_loss: 0.037858, test_loss: 0.042168, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1251/2500, lr=0.001432\n",
            "--> train_loss: 0.036968, test_loss: 0.042064, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1252/2500, lr=0.001430\n",
            "--> train_loss: 0.036595, test_loss: 0.041366, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1253/2500, lr=0.001429\n",
            "--> train_loss: 0.036456, test_loss: 0.041118, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1254/2500, lr=0.001427\n",
            "--> train_loss: 0.036513, test_loss: 0.042026, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1255/2500, lr=0.001426\n",
            "--> train_loss: 0.036417, test_loss: 0.041087, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1256/2500, lr=0.001424\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.036645, test_loss: 0.040943, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1257/2500, lr=0.001423\n",
            "--> train_loss: 0.036544, test_loss: 0.041730, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1258/2500, lr=0.001422\n",
            "--> train_loss: 0.036327, test_loss: 0.041025, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1259/2500, lr=0.001420\n",
            "--> train_loss: 0.036315, test_loss: 0.041822, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1260/2500, lr=0.001419\n",
            "--> train_loss: 0.036744, test_loss: 0.041680, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1261/2500, lr=0.001417\n",
            "--> train_loss: 0.036612, test_loss: 0.041413, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1262/2500, lr=0.001416\n",
            "--> train_loss: 0.036571, test_loss: 0.041225, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1263/2500, lr=0.001415\n",
            "--> train_loss: 0.036455, test_loss: 0.041388, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1264/2500, lr=0.001413\n",
            "--> train_loss: 0.036458, test_loss: 0.041839, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1265/2500, lr=0.001412\n",
            "--> train_loss: 0.036437, test_loss: 0.041698, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1266/2500, lr=0.001410\n",
            "--> train_loss: 0.037522, test_loss: 0.041993, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1267/2500, lr=0.001409\n",
            "--> train_loss: 0.037489, test_loss: 0.041312, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1268/2500, lr=0.001407\n",
            "--> train_loss: 0.036613, test_loss: 0.041285, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1269/2500, lr=0.001406\n",
            "--> train_loss: 0.037314, test_loss: 0.041826, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1270/2500, lr=0.001405\n",
            "--> train_loss: 0.037502, test_loss: 0.041065, train_metric: 0.038, test_metric: 0.041\n",
            "----\n",
            "Epoch 1271/2500, lr=0.001403\n",
            "--> train_loss: 0.037146, test_loss: 0.042789, train_metric: 0.037, test_metric: 0.043\n",
            "----\n",
            "Epoch 1272/2500, lr=0.001402\n",
            "--> train_loss: 0.037288, test_loss: 0.041829, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1273/2500, lr=0.001400\n",
            "--> train_loss: 0.037116, test_loss: 0.041881, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1274/2500, lr=0.001399\n",
            "--> train_loss: 0.036790, test_loss: 0.041834, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1275/2500, lr=0.001398\n",
            "--> train_loss: 0.037719, test_loss: 0.041630, train_metric: 0.038, test_metric: 0.042\n",
            "----\n",
            "Epoch 1276/2500, lr=0.001396\n",
            "--> train_loss: 0.036967, test_loss: 0.041360, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1277/2500, lr=0.001395\n",
            "--> train_loss: 0.036494, test_loss: 0.040967, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1278/2500, lr=0.001393\n",
            "--> train_loss: 0.036903, test_loss: 0.041159, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1279/2500, lr=0.001392\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.036378, test_loss: 0.040906, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1280/2500, lr=0.001391\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.036217, test_loss: 0.040847, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1281/2500, lr=0.001389\n",
            "--> train_loss: 0.036268, test_loss: 0.041199, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1282/2500, lr=0.001388\n",
            "--> train_loss: 0.036491, test_loss: 0.041190, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1283/2500, lr=0.001387\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.036560, test_loss: 0.040517, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1284/2500, lr=0.001385\n",
            "--> train_loss: 0.036568, test_loss: 0.041319, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1285/2500, lr=0.001384\n",
            "--> train_loss: 0.036885, test_loss: 0.040961, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1286/2500, lr=0.001382\n",
            "--> train_loss: 0.036258, test_loss: 0.041173, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1287/2500, lr=0.001381\n",
            "--> train_loss: 0.036713, test_loss: 0.041517, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1288/2500, lr=0.001380\n",
            "--> train_loss: 0.035997, test_loss: 0.040742, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1289/2500, lr=0.001378\n",
            "--> train_loss: 0.036007, test_loss: 0.040873, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1290/2500, lr=0.001377\n",
            "--> train_loss: 0.035928, test_loss: 0.040608, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1291/2500, lr=0.001375\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.036102, test_loss: 0.040413, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1292/2500, lr=0.001374\n",
            "--> train_loss: 0.036128, test_loss: 0.040836, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1293/2500, lr=0.001373\n",
            "--> train_loss: 0.035783, test_loss: 0.040614, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1294/2500, lr=0.001371\n",
            "--> train_loss: 0.035934, test_loss: 0.040676, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1295/2500, lr=0.001370\n",
            "--> train_loss: 0.035759, test_loss: 0.040934, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1296/2500, lr=0.001369\n",
            "--> train_loss: 0.035762, test_loss: 0.040648, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1297/2500, lr=0.001367\n",
            "--> train_loss: 0.035734, test_loss: 0.040483, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1298/2500, lr=0.001366\n",
            "--> train_loss: 0.035750, test_loss: 0.040828, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1299/2500, lr=0.001365\n",
            "--> train_loss: 0.035857, test_loss: 0.040634, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1300/2500, lr=0.001363\n",
            "--> train_loss: 0.036029, test_loss: 0.040729, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1301/2500, lr=0.001362\n",
            "--> train_loss: 0.035716, test_loss: 0.040738, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1302/2500, lr=0.001360\n",
            "--> train_loss: 0.035671, test_loss: 0.040724, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1303/2500, lr=0.001359\n",
            "--> train_loss: 0.036376, test_loss: 0.040634, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1304/2500, lr=0.001358\n",
            "--> train_loss: 0.035869, test_loss: 0.041322, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1305/2500, lr=0.001356\n",
            "--> train_loss: 0.035738, test_loss: 0.040566, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1306/2500, lr=0.001355\n",
            "--> train_loss: 0.036102, test_loss: 0.040556, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1307/2500, lr=0.001354\n",
            "--> train_loss: 0.036056, test_loss: 0.041919, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1308/2500, lr=0.001352\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.035889, test_loss: 0.040408, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1309/2500, lr=0.001351\n",
            "--> train_loss: 0.035803, test_loss: 0.040506, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1310/2500, lr=0.001350\n",
            "--> train_loss: 0.035655, test_loss: 0.040805, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1311/2500, lr=0.001348\n",
            "--> train_loss: 0.035725, test_loss: 0.040636, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1312/2500, lr=0.001347\n",
            "--> train_loss: 0.035865, test_loss: 0.040708, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1313/2500, lr=0.001346\n",
            "--> train_loss: 0.035850, test_loss: 0.041171, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1314/2500, lr=0.001344\n",
            "--> train_loss: 0.036124, test_loss: 0.041747, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1315/2500, lr=0.001343\n",
            "--> train_loss: 0.036355, test_loss: 0.041089, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1316/2500, lr=0.001341\n",
            "--> train_loss: 0.036786, test_loss: 0.041576, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1317/2500, lr=0.001340\n",
            "--> train_loss: 0.036416, test_loss: 0.041758, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1318/2500, lr=0.001339\n",
            "--> train_loss: 0.036451, test_loss: 0.040649, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1319/2500, lr=0.001337\n",
            "--> train_loss: 0.036649, test_loss: 0.041072, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1320/2500, lr=0.001336\n",
            "--> train_loss: 0.036320, test_loss: 0.041637, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1321/2500, lr=0.001335\n",
            "--> train_loss: 0.035920, test_loss: 0.040691, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1322/2500, lr=0.001333\n",
            "--> train_loss: 0.036638, test_loss: 0.041222, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1323/2500, lr=0.001332\n",
            "--> train_loss: 0.036892, test_loss: 0.041584, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1324/2500, lr=0.001331\n",
            "--> train_loss: 0.036755, test_loss: 0.040516, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1325/2500, lr=0.001329\n",
            "--> train_loss: 0.036408, test_loss: 0.040621, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1326/2500, lr=0.001328\n",
            "--> train_loss: 0.036606, test_loss: 0.041800, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1327/2500, lr=0.001327\n",
            "--> train_loss: 0.036524, test_loss: 0.040863, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1328/2500, lr=0.001325\n",
            "--> train_loss: 0.036194, test_loss: 0.041134, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1329/2500, lr=0.001324\n",
            "--> train_loss: 0.036036, test_loss: 0.041288, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1330/2500, lr=0.001323\n",
            "--> train_loss: 0.035898, test_loss: 0.040857, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1331/2500, lr=0.001322\n",
            "--> train_loss: 0.036196, test_loss: 0.040929, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1332/2500, lr=0.001320\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.035837, test_loss: 0.040397, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1333/2500, lr=0.001319\n",
            "--> train_loss: 0.035958, test_loss: 0.040855, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1334/2500, lr=0.001318\n",
            "--> train_loss: 0.035948, test_loss: 0.041392, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1335/2500, lr=0.001316\n",
            "--> train_loss: 0.035803, test_loss: 0.040542, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1336/2500, lr=0.001315\n",
            "--> train_loss: 0.035864, test_loss: 0.040609, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1337/2500, lr=0.001314\n",
            "--> train_loss: 0.035709, test_loss: 0.040743, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1338/2500, lr=0.001312\n",
            "--> train_loss: 0.035656, test_loss: 0.040429, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1339/2500, lr=0.001311\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.035801, test_loss: 0.040273, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1340/2500, lr=0.001310\n",
            "--> train_loss: 0.035727, test_loss: 0.040671, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1341/2500, lr=0.001308\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.035670, test_loss: 0.040255, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1342/2500, lr=0.001307\n",
            "--> train_loss: 0.035493, test_loss: 0.040901, train_metric: 0.035, test_metric: 0.041\n",
            "----\n",
            "Epoch 1343/2500, lr=0.001306\n",
            "--> train_loss: 0.035973, test_loss: 0.041033, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1344/2500, lr=0.001304\n",
            "--> train_loss: 0.036174, test_loss: 0.040926, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1345/2500, lr=0.001303\n",
            "--> train_loss: 0.035828, test_loss: 0.041103, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1346/2500, lr=0.001302\n",
            "--> train_loss: 0.036223, test_loss: 0.041120, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1347/2500, lr=0.001301\n",
            "--> train_loss: 0.036044, test_loss: 0.040476, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1348/2500, lr=0.001299\n",
            "--> train_loss: 0.035774, test_loss: 0.040668, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1349/2500, lr=0.001298\n",
            "--> train_loss: 0.035847, test_loss: 0.040468, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1350/2500, lr=0.001297\n",
            "--> train_loss: 0.035820, test_loss: 0.040313, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1351/2500, lr=0.001295\n",
            "--> train_loss: 0.035672, test_loss: 0.040416, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1352/2500, lr=0.001294\n",
            "--> train_loss: 0.035683, test_loss: 0.040454, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1353/2500, lr=0.001293\n",
            "--> train_loss: 0.036006, test_loss: 0.040755, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1354/2500, lr=0.001291\n",
            "--> train_loss: 0.036571, test_loss: 0.041335, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1355/2500, lr=0.001290\n",
            "--> train_loss: 0.036613, test_loss: 0.040838, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1356/2500, lr=0.001289\n",
            "--> train_loss: 0.036086, test_loss: 0.041149, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1357/2500, lr=0.001288\n",
            "--> train_loss: 0.036838, test_loss: 0.040658, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1358/2500, lr=0.001286\n",
            "--> train_loss: 0.036023, test_loss: 0.040909, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1359/2500, lr=0.001285\n",
            "--> train_loss: 0.035732, test_loss: 0.040445, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1360/2500, lr=0.001284\n",
            "--> train_loss: 0.035764, test_loss: 0.040788, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1361/2500, lr=0.001282\n",
            "--> train_loss: 0.035668, test_loss: 0.040922, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1362/2500, lr=0.001281\n",
            "--> train_loss: 0.035875, test_loss: 0.040788, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1363/2500, lr=0.001280\n",
            "--> train_loss: 0.035767, test_loss: 0.040715, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1364/2500, lr=0.001279\n",
            "--> train_loss: 0.035947, test_loss: 0.040472, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1365/2500, lr=0.001277\n",
            "--> train_loss: 0.036040, test_loss: 0.041250, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1366/2500, lr=0.001276\n",
            "--> train_loss: 0.036210, test_loss: 0.041080, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1367/2500, lr=0.001275\n",
            "--> train_loss: 0.035942, test_loss: 0.040421, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1368/2500, lr=0.001273\n",
            "--> train_loss: 0.035946, test_loss: 0.040582, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1369/2500, lr=0.001272\n",
            "--> train_loss: 0.036210, test_loss: 0.040836, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1370/2500, lr=0.001271\n",
            "--> train_loss: 0.035943, test_loss: 0.040667, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1371/2500, lr=0.001270\n",
            "--> train_loss: 0.035778, test_loss: 0.040650, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1372/2500, lr=0.001268\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.035736, test_loss: 0.039975, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1373/2500, lr=0.001267\n",
            "--> train_loss: 0.036658, test_loss: 0.040604, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1374/2500, lr=0.001266\n",
            "--> train_loss: 0.036110, test_loss: 0.041747, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1375/2500, lr=0.001265\n",
            "--> train_loss: 0.036481, test_loss: 0.042857, train_metric: 0.036, test_metric: 0.043\n",
            "----\n",
            "Epoch 1376/2500, lr=0.001263\n",
            "--> train_loss: 0.037068, test_loss: 0.041524, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1377/2500, lr=0.001262\n",
            "--> train_loss: 0.037055, test_loss: 0.041774, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1378/2500, lr=0.001261\n",
            "--> train_loss: 0.036389, test_loss: 0.041140, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1379/2500, lr=0.001260\n",
            "--> train_loss: 0.036081, test_loss: 0.040284, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1380/2500, lr=0.001258\n",
            "--> train_loss: 0.036052, test_loss: 0.040631, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1381/2500, lr=0.001257\n",
            "--> train_loss: 0.035629, test_loss: 0.040519, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1382/2500, lr=0.001256\n",
            "--> train_loss: 0.036252, test_loss: 0.041280, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1383/2500, lr=0.001255\n",
            "--> train_loss: 0.036047, test_loss: 0.040974, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1384/2500, lr=0.001253\n",
            "--> train_loss: 0.035905, test_loss: 0.040506, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1385/2500, lr=0.001252\n",
            "--> train_loss: 0.036120, test_loss: 0.040532, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1386/2500, lr=0.001251\n",
            "--> train_loss: 0.036119, test_loss: 0.040579, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1387/2500, lr=0.001250\n",
            "--> train_loss: 0.035517, test_loss: 0.040279, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1388/2500, lr=0.001248\n",
            "--> train_loss: 0.035486, test_loss: 0.040129, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1389/2500, lr=0.001247\n",
            "--> train_loss: 0.035650, test_loss: 0.040524, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1390/2500, lr=0.001246\n",
            "--> train_loss: 0.035389, test_loss: 0.040119, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1391/2500, lr=0.001245\n",
            "--> train_loss: 0.035424, test_loss: 0.040252, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1392/2500, lr=0.001243\n",
            "--> train_loss: 0.035425, test_loss: 0.040228, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1393/2500, lr=0.001242\n",
            "--> train_loss: 0.035432, test_loss: 0.040433, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1394/2500, lr=0.001241\n",
            "--> train_loss: 0.035337, test_loss: 0.040114, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1395/2500, lr=0.001240\n",
            "--> train_loss: 0.035973, test_loss: 0.040478, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1396/2500, lr=0.001238\n",
            "--> train_loss: 0.036614, test_loss: 0.041145, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1397/2500, lr=0.001237\n",
            "--> train_loss: 0.036437, test_loss: 0.040718, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1398/2500, lr=0.001236\n",
            "--> train_loss: 0.035807, test_loss: 0.040644, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1399/2500, lr=0.001235\n",
            "--> train_loss: 0.035999, test_loss: 0.040344, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1400/2500, lr=0.001233\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.035485, test_loss: 0.039901, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1401/2500, lr=0.001232\n",
            "--> train_loss: 0.035262, test_loss: 0.040338, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1402/2500, lr=0.001231\n",
            "--> train_loss: 0.035364, test_loss: 0.040218, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1403/2500, lr=0.001230\n",
            "--> train_loss: 0.035372, test_loss: 0.040410, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1404/2500, lr=0.001228\n",
            "--> train_loss: 0.035319, test_loss: 0.040303, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1405/2500, lr=0.001227\n",
            "--> train_loss: 0.035456, test_loss: 0.040007, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1406/2500, lr=0.001226\n",
            "--> train_loss: 0.035193, test_loss: 0.040284, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1407/2500, lr=0.001225\n",
            "--> train_loss: 0.035290, test_loss: 0.040084, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1408/2500, lr=0.001224\n",
            "--> train_loss: 0.035784, test_loss: 0.040507, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1409/2500, lr=0.001222\n",
            "--> train_loss: 0.035995, test_loss: 0.040961, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1410/2500, lr=0.001221\n",
            "--> train_loss: 0.036283, test_loss: 0.040299, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1411/2500, lr=0.001220\n",
            "--> train_loss: 0.035865, test_loss: 0.040670, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1412/2500, lr=0.001219\n",
            "--> train_loss: 0.035947, test_loss: 0.040804, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1413/2500, lr=0.001217\n",
            "--> train_loss: 0.036321, test_loss: 0.041285, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1414/2500, lr=0.001216\n",
            "--> train_loss: 0.035715, test_loss: 0.040649, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1415/2500, lr=0.001215\n",
            "--> train_loss: 0.035548, test_loss: 0.040268, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1416/2500, lr=0.001214\n",
            "--> train_loss: 0.035378, test_loss: 0.040097, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1417/2500, lr=0.001213\n",
            "--> train_loss: 0.035364, test_loss: 0.040009, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1418/2500, lr=0.001211\n",
            "--> train_loss: 0.035267, test_loss: 0.039999, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1419/2500, lr=0.001210\n",
            "--> train_loss: 0.036157, test_loss: 0.040398, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1420/2500, lr=0.001209\n",
            "--> train_loss: 0.036205, test_loss: 0.040060, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1421/2500, lr=0.001208\n",
            "--> train_loss: 0.035753, test_loss: 0.041588, train_metric: 0.036, test_metric: 0.042\n",
            "----\n",
            "Epoch 1422/2500, lr=0.001207\n",
            "--> train_loss: 0.035790, test_loss: 0.040580, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1423/2500, lr=0.001205\n",
            "--> train_loss: 0.035532, test_loss: 0.040186, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1424/2500, lr=0.001204\n",
            "--> train_loss: 0.035331, test_loss: 0.040526, train_metric: 0.035, test_metric: 0.041\n",
            "----\n",
            "Epoch 1425/2500, lr=0.001203\n",
            "--> train_loss: 0.035508, test_loss: 0.040909, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1426/2500, lr=0.001202\n",
            "--> train_loss: 0.035581, test_loss: 0.040233, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1427/2500, lr=0.001200\n",
            "--> train_loss: 0.035571, test_loss: 0.040136, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1428/2500, lr=0.001199\n",
            "--> train_loss: 0.035511, test_loss: 0.040140, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1429/2500, lr=0.001198\n",
            "--> train_loss: 0.035624, test_loss: 0.040634, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1430/2500, lr=0.001197\n",
            "--> train_loss: 0.035524, test_loss: 0.039975, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1431/2500, lr=0.001196\n",
            "--> train_loss: 0.035661, test_loss: 0.040579, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1432/2500, lr=0.001194\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.035722, test_loss: 0.039887, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1433/2500, lr=0.001193\n",
            "--> train_loss: 0.035387, test_loss: 0.040081, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1434/2500, lr=0.001192\n",
            "--> train_loss: 0.035564, test_loss: 0.040133, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1435/2500, lr=0.001191\n",
            "--> train_loss: 0.035423, test_loss: 0.040474, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1436/2500, lr=0.001190\n",
            "--> train_loss: 0.035626, test_loss: 0.040178, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1437/2500, lr=0.001189\n",
            "--> train_loss: 0.035890, test_loss: 0.040012, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1438/2500, lr=0.001187\n",
            "--> train_loss: 0.035585, test_loss: 0.040252, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1439/2500, lr=0.001186\n",
            "--> train_loss: 0.035706, test_loss: 0.040546, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1440/2500, lr=0.001185\n",
            "--> train_loss: 0.035194, test_loss: 0.040043, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1441/2500, lr=0.001184\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.035521, test_loss: 0.039786, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1442/2500, lr=0.001183\n",
            "--> train_loss: 0.035488, test_loss: 0.040034, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1443/2500, lr=0.001181\n",
            "--> train_loss: 0.035704, test_loss: 0.041384, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1444/2500, lr=0.001180\n",
            "--> train_loss: 0.036572, test_loss: 0.041417, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1445/2500, lr=0.001179\n",
            "--> train_loss: 0.036843, test_loss: 0.041586, train_metric: 0.037, test_metric: 0.042\n",
            "----\n",
            "Epoch 1446/2500, lr=0.001178\n",
            "--> train_loss: 0.036751, test_loss: 0.040965, train_metric: 0.037, test_metric: 0.041\n",
            "----\n",
            "Epoch 1447/2500, lr=0.001177\n",
            "--> train_loss: 0.036685, test_loss: 0.039936, train_metric: 0.037, test_metric: 0.040\n",
            "----\n",
            "Epoch 1448/2500, lr=0.001176\n",
            "--> train_loss: 0.036373, test_loss: 0.040765, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1449/2500, lr=0.001174\n",
            "--> train_loss: 0.035580, test_loss: 0.040611, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1450/2500, lr=0.001173\n",
            "--> train_loss: 0.035858, test_loss: 0.040731, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1451/2500, lr=0.001172\n",
            "--> train_loss: 0.035934, test_loss: 0.040237, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1452/2500, lr=0.001171\n",
            "--> train_loss: 0.035620, test_loss: 0.040580, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1453/2500, lr=0.001170\n",
            "--> train_loss: 0.035742, test_loss: 0.040432, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1454/2500, lr=0.001168\n",
            "--> train_loss: 0.035398, test_loss: 0.040287, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1455/2500, lr=0.001167\n",
            "--> train_loss: 0.035878, test_loss: 0.040418, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1456/2500, lr=0.001166\n",
            "--> train_loss: 0.035591, test_loss: 0.040356, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1457/2500, lr=0.001165\n",
            "--> train_loss: 0.035883, test_loss: 0.040333, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1458/2500, lr=0.001164\n",
            "--> train_loss: 0.035496, test_loss: 0.039973, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1459/2500, lr=0.001163\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.035389, test_loss: 0.039753, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1460/2500, lr=0.001161\n",
            "--> train_loss: 0.035303, test_loss: 0.039944, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1461/2500, lr=0.001160\n",
            "--> train_loss: 0.035272, test_loss: 0.040133, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1462/2500, lr=0.001159\n",
            "--> train_loss: 0.036211, test_loss: 0.040609, train_metric: 0.036, test_metric: 0.041\n",
            "----\n",
            "Epoch 1463/2500, lr=0.001158\n",
            "--> train_loss: 0.035249, test_loss: 0.040099, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1464/2500, lr=0.001157\n",
            "--> train_loss: 0.035217, test_loss: 0.039779, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1465/2500, lr=0.001156\n",
            "--> train_loss: 0.035222, test_loss: 0.039882, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1466/2500, lr=0.001155\n",
            "--> train_loss: 0.035503, test_loss: 0.039988, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1467/2500, lr=0.001153\n",
            "--> train_loss: 0.035292, test_loss: 0.040401, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1468/2500, lr=0.001152\n",
            "--> train_loss: 0.035437, test_loss: 0.040577, train_metric: 0.035, test_metric: 0.041\n",
            "----\n",
            "Epoch 1469/2500, lr=0.001151\n",
            "--> train_loss: 0.035517, test_loss: 0.040379, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1470/2500, lr=0.001150\n",
            "--> train_loss: 0.035181, test_loss: 0.040113, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1471/2500, lr=0.001149\n",
            "--> train_loss: 0.035277, test_loss: 0.040179, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1472/2500, lr=0.001148\n",
            "--> train_loss: 0.035232, test_loss: 0.039973, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1473/2500, lr=0.001146\n",
            "--> train_loss: 0.035403, test_loss: 0.039823, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1474/2500, lr=0.001145\n",
            "--> train_loss: 0.035549, test_loss: 0.039891, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1475/2500, lr=0.001144\n",
            "--> train_loss: 0.035270, test_loss: 0.040123, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1476/2500, lr=0.001143\n",
            "--> train_loss: 0.035414, test_loss: 0.040063, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1477/2500, lr=0.001142\n",
            "--> train_loss: 0.035466, test_loss: 0.040704, train_metric: 0.035, test_metric: 0.041\n",
            "----\n",
            "Epoch 1478/2500, lr=0.001141\n",
            "--> train_loss: 0.036000, test_loss: 0.040257, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1479/2500, lr=0.001140\n",
            "--> train_loss: 0.035606, test_loss: 0.039939, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1480/2500, lr=0.001138\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.035658, test_loss: 0.039674, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1481/2500, lr=0.001137\n",
            "--> train_loss: 0.035249, test_loss: 0.040304, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1482/2500, lr=0.001136\n",
            "--> train_loss: 0.035319, test_loss: 0.039878, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1483/2500, lr=0.001135\n",
            "--> train_loss: 0.035285, test_loss: 0.040157, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1484/2500, lr=0.001134\n",
            "--> train_loss: 0.035741, test_loss: 0.040146, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1485/2500, lr=0.001133\n",
            "--> train_loss: 0.035784, test_loss: 0.040316, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1486/2500, lr=0.001132\n",
            "--> train_loss: 0.035772, test_loss: 0.040492, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1487/2500, lr=0.001131\n",
            "--> train_loss: 0.035999, test_loss: 0.039778, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1488/2500, lr=0.001129\n",
            "--> train_loss: 0.035501, test_loss: 0.040242, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1489/2500, lr=0.001128\n",
            "--> train_loss: 0.035291, test_loss: 0.039675, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1490/2500, lr=0.001127\n",
            "--> train_loss: 0.035116, test_loss: 0.040037, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1491/2500, lr=0.001126\n",
            "--> train_loss: 0.035424, test_loss: 0.040129, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1492/2500, lr=0.001125\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.035136, test_loss: 0.039593, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1493/2500, lr=0.001124\n",
            "--> train_loss: 0.035432, test_loss: 0.039884, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1494/2500, lr=0.001123\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.034989, test_loss: 0.039469, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1495/2500, lr=0.001122\n",
            "--> train_loss: 0.035457, test_loss: 0.039964, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1496/2500, lr=0.001120\n",
            "--> train_loss: 0.035254, test_loss: 0.040264, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1497/2500, lr=0.001119\n",
            "--> train_loss: 0.035978, test_loss: 0.039990, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1498/2500, lr=0.001118\n",
            "--> train_loss: 0.035254, test_loss: 0.039951, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1499/2500, lr=0.001117\n",
            "--> train_loss: 0.035676, test_loss: 0.039822, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1500/2500, lr=0.001116\n",
            "--> train_loss: 0.035372, test_loss: 0.039726, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1501/2500, lr=0.001115\n",
            "--> train_loss: 0.034850, test_loss: 0.039800, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1502/2500, lr=0.001114\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.035096, test_loss: 0.039436, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1503/2500, lr=0.001113\n",
            "--> train_loss: 0.034815, test_loss: 0.039459, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1504/2500, lr=0.001111\n",
            "--> train_loss: 0.034710, test_loss: 0.039819, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1505/2500, lr=0.001110\n",
            "--> train_loss: 0.034908, test_loss: 0.039686, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1506/2500, lr=0.001109\n",
            "--> train_loss: 0.034901, test_loss: 0.039619, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1507/2500, lr=0.001108\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.034735, test_loss: 0.039298, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1508/2500, lr=0.001107\n",
            "--> train_loss: 0.035190, test_loss: 0.039403, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1509/2500, lr=0.001106\n",
            "--> train_loss: 0.035110, test_loss: 0.039678, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1510/2500, lr=0.001105\n",
            "--> train_loss: 0.035597, test_loss: 0.039391, train_metric: 0.036, test_metric: 0.039\n",
            "----\n",
            "Epoch 1511/2500, lr=0.001104\n",
            "--> train_loss: 0.035231, test_loss: 0.040600, train_metric: 0.035, test_metric: 0.041\n",
            "----\n",
            "Epoch 1512/2500, lr=0.001103\n",
            "--> train_loss: 0.035492, test_loss: 0.040207, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1513/2500, lr=0.001102\n",
            "--> train_loss: 0.035127, test_loss: 0.040551, train_metric: 0.035, test_metric: 0.041\n",
            "----\n",
            "Epoch 1514/2500, lr=0.001100\n",
            "--> train_loss: 0.035524, test_loss: 0.039914, train_metric: 0.036, test_metric: 0.040\n",
            "----\n",
            "Epoch 1515/2500, lr=0.001099\n",
            "--> train_loss: 0.035159, test_loss: 0.039725, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1516/2500, lr=0.001098\n",
            "--> train_loss: 0.035207, test_loss: 0.039677, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1517/2500, lr=0.001097\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.034939, test_loss: 0.039243, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1518/2500, lr=0.001096\n",
            "--> train_loss: 0.034641, test_loss: 0.039317, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1519/2500, lr=0.001095\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.034800, test_loss: 0.039212, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1520/2500, lr=0.001094\n",
            "--> train_loss: 0.034770, test_loss: 0.039536, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1521/2500, lr=0.001093\n",
            "--> train_loss: 0.034817, test_loss: 0.039257, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1522/2500, lr=0.001092\n",
            "--> train_loss: 0.034862, test_loss: 0.039634, train_metric: 0.035, test_metric: 0.040\n",
            "----\n",
            "Epoch 1523/2500, lr=0.001091\n",
            "--> train_loss: 0.034971, test_loss: 0.039467, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1524/2500, lr=0.001089\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.034607, test_loss: 0.039110, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1525/2500, lr=0.001088\n",
            "--> train_loss: 0.034697, test_loss: 0.039314, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1526/2500, lr=0.001087\n",
            "--> train_loss: 0.034642, test_loss: 0.039190, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1527/2500, lr=0.001086\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.034899, test_loss: 0.039082, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1528/2500, lr=0.001085\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.034670, test_loss: 0.039073, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1529/2500, lr=0.001084\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.034607, test_loss: 0.038919, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1530/2500, lr=0.001083\n",
            "--> train_loss: 0.034374, test_loss: 0.039171, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1531/2500, lr=0.001082\n",
            "--> train_loss: 0.034817, test_loss: 0.039240, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1532/2500, lr=0.001081\n",
            "--> train_loss: 0.034786, test_loss: 0.039143, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1533/2500, lr=0.001080\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.035063, test_loss: 0.038852, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1534/2500, lr=0.001079\n",
            "--> train_loss: 0.034733, test_loss: 0.039201, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1535/2500, lr=0.001078\n",
            "--> train_loss: 0.034684, test_loss: 0.038930, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1536/2500, lr=0.001076\n",
            "--> train_loss: 0.034537, test_loss: 0.038924, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1537/2500, lr=0.001075\n",
            "--> train_loss: 0.034438, test_loss: 0.038861, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1538/2500, lr=0.001074\n",
            "--> train_loss: 0.034368, test_loss: 0.038985, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1539/2500, lr=0.001073\n",
            "--> train_loss: 0.034902, test_loss: 0.038907, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1540/2500, lr=0.001072\n",
            "--> train_loss: 0.034517, test_loss: 0.038893, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1541/2500, lr=0.001071\n",
            "--> train_loss: 0.034307, test_loss: 0.038974, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1542/2500, lr=0.001070\n",
            "--> train_loss: 0.034305, test_loss: 0.038900, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1543/2500, lr=0.001069\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.034465, test_loss: 0.038805, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1544/2500, lr=0.001068\n",
            "--> train_loss: 0.035193, test_loss: 0.038912, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1545/2500, lr=0.001067\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.034868, test_loss: 0.038563, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1546/2500, lr=0.001066\n",
            "--> train_loss: 0.034948, test_loss: 0.038784, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1547/2500, lr=0.001065\n",
            "--> train_loss: 0.034665, test_loss: 0.039141, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1548/2500, lr=0.001064\n",
            "--> train_loss: 0.034464, test_loss: 0.039424, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1549/2500, lr=0.001063\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.034382, test_loss: 0.038504, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1550/2500, lr=0.001061\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.034120, test_loss: 0.038436, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1551/2500, lr=0.001060\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.034116, test_loss: 0.038215, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1552/2500, lr=0.001059\n",
            "--> train_loss: 0.033782, test_loss: 0.038697, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1553/2500, lr=0.001058\n",
            "--> train_loss: 0.034210, test_loss: 0.039013, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1554/2500, lr=0.001057\n",
            "--> train_loss: 0.034351, test_loss: 0.038646, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1555/2500, lr=0.001056\n",
            "--> train_loss: 0.034281, test_loss: 0.038476, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1556/2500, lr=0.001055\n",
            "--> train_loss: 0.034017, test_loss: 0.038638, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1557/2500, lr=0.001054\n",
            "--> train_loss: 0.034141, test_loss: 0.038277, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1558/2500, lr=0.001053\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.033964, test_loss: 0.038108, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1559/2500, lr=0.001052\n",
            "--> train_loss: 0.034212, test_loss: 0.038616, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1560/2500, lr=0.001051\n",
            "--> train_loss: 0.033968, test_loss: 0.038315, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1561/2500, lr=0.001050\n",
            "--> train_loss: 0.034017, test_loss: 0.038286, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1562/2500, lr=0.001049\n",
            "--> train_loss: 0.034277, test_loss: 0.038256, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1563/2500, lr=0.001048\n",
            "--> train_loss: 0.034265, test_loss: 0.038118, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1564/2500, lr=0.001047\n",
            "--> train_loss: 0.034900, test_loss: 0.038452, train_metric: 0.035, test_metric: 0.038\n",
            "----\n",
            "Epoch 1565/2500, lr=0.001046\n",
            "--> train_loss: 0.033778, test_loss: 0.038127, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1566/2500, lr=0.001045\n",
            "--> train_loss: 0.034293, test_loss: 0.038405, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1567/2500, lr=0.001044\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.034316, test_loss: 0.038005, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1568/2500, lr=0.001043\n",
            "--> train_loss: 0.033597, test_loss: 0.038598, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1569/2500, lr=0.001041\n",
            "--> train_loss: 0.034611, test_loss: 0.039096, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1570/2500, lr=0.001040\n",
            "--> train_loss: 0.035094, test_loss: 0.038935, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1571/2500, lr=0.001039\n",
            "--> train_loss: 0.034549, test_loss: 0.038583, train_metric: 0.035, test_metric: 0.039\n",
            "----\n",
            "Epoch 1572/2500, lr=0.001038\n",
            "--> train_loss: 0.034778, test_loss: 0.038322, train_metric: 0.035, test_metric: 0.038\n",
            "----\n",
            "Epoch 1573/2500, lr=0.001037\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.034239, test_loss: 0.037826, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1574/2500, lr=0.001036\n",
            "--> train_loss: 0.033749, test_loss: 0.038131, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1575/2500, lr=0.001035\n",
            "--> train_loss: 0.034108, test_loss: 0.038464, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1576/2500, lr=0.001034\n",
            "--> train_loss: 0.034465, test_loss: 0.038556, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1577/2500, lr=0.001033\n",
            "--> train_loss: 0.034299, test_loss: 0.038616, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1578/2500, lr=0.001032\n",
            "--> train_loss: 0.033834, test_loss: 0.038030, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1579/2500, lr=0.001031\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.033860, test_loss: 0.037679, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1580/2500, lr=0.001030\n",
            "--> train_loss: 0.033670, test_loss: 0.038246, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1581/2500, lr=0.001029\n",
            "--> train_loss: 0.033437, test_loss: 0.038285, train_metric: 0.033, test_metric: 0.038\n",
            "----\n",
            "Epoch 1582/2500, lr=0.001028\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.033635, test_loss: 0.037407, train_metric: 0.034, test_metric: 0.037\n",
            "----\n",
            "Epoch 1583/2500, lr=0.001027\n",
            "--> train_loss: 0.034298, test_loss: 0.037731, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1584/2500, lr=0.001026\n",
            "--> train_loss: 0.034081, test_loss: 0.037664, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1585/2500, lr=0.001025\n",
            "--> train_loss: 0.033622, test_loss: 0.037649, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1586/2500, lr=0.001024\n",
            "--> train_loss: 0.034288, test_loss: 0.038635, train_metric: 0.034, test_metric: 0.039\n",
            "----\n",
            "Epoch 1587/2500, lr=0.001023\n",
            "--> train_loss: 0.034424, test_loss: 0.039567, train_metric: 0.034, test_metric: 0.040\n",
            "----\n",
            "Epoch 1588/2500, lr=0.001022\n",
            "--> train_loss: 0.034453, test_loss: 0.037576, train_metric: 0.034, test_metric: 0.038\n",
            "----\n",
            "Epoch 1589/2500, lr=0.001021\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.033394, test_loss: 0.036874, train_metric: 0.033, test_metric: 0.037\n",
            "----\n",
            "Epoch 1590/2500, lr=0.001020\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.032864, test_loss: 0.036598, train_metric: 0.033, test_metric: 0.037\n",
            "----\n",
            "Epoch 1591/2500, lr=0.001019\n",
            "--> train_loss: 0.033014, test_loss: 0.036822, train_metric: 0.033, test_metric: 0.037\n",
            "----\n",
            "Epoch 1592/2500, lr=0.001018\n",
            "--> train_loss: 0.032782, test_loss: 0.036896, train_metric: 0.033, test_metric: 0.037\n",
            "----\n",
            "Epoch 1593/2500, lr=0.001017\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.032412, test_loss: 0.036334, train_metric: 0.032, test_metric: 0.036\n",
            "----\n",
            "Epoch 1594/2500, lr=0.001016\n",
            "--> train_loss: 0.032502, test_loss: 0.036559, train_metric: 0.033, test_metric: 0.037\n",
            "----\n",
            "Epoch 1595/2500, lr=0.001015\n",
            "--> train_loss: 0.032808, test_loss: 0.036680, train_metric: 0.033, test_metric: 0.037\n",
            "----\n",
            "Epoch 1596/2500, lr=0.001014\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.032316, test_loss: 0.036218, train_metric: 0.032, test_metric: 0.036\n",
            "----\n",
            "Epoch 1597/2500, lr=0.001013\n",
            "--> train_loss: 0.032399, test_loss: 0.036394, train_metric: 0.032, test_metric: 0.036\n",
            "----\n",
            "Epoch 1598/2500, lr=0.001012\n",
            "--> train_loss: 0.032875, test_loss: 0.036795, train_metric: 0.033, test_metric: 0.037\n",
            "----\n",
            "Epoch 1599/2500, lr=0.001011\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.032679, test_loss: 0.036046, train_metric: 0.033, test_metric: 0.036\n",
            "----\n",
            "Epoch 1600/2500, lr=0.001010\n",
            "--> train_loss: 0.032445, test_loss: 0.036713, train_metric: 0.032, test_metric: 0.037\n",
            "----\n",
            "Epoch 1601/2500, lr=0.001009\n",
            "--> train_loss: 0.032390, test_loss: 0.036558, train_metric: 0.032, test_metric: 0.037\n",
            "----\n",
            "Epoch 1602/2500, lr=0.001008\n",
            "--> train_loss: 0.032871, test_loss: 0.036680, train_metric: 0.033, test_metric: 0.037\n",
            "----\n",
            "Epoch 1603/2500, lr=0.001007\n",
            "--> train_loss: 0.033287, test_loss: 0.036273, train_metric: 0.033, test_metric: 0.036\n",
            "----\n",
            "Epoch 1604/2500, lr=0.001006\n",
            "--> train_loss: 0.032974, test_loss: 0.036434, train_metric: 0.033, test_metric: 0.036\n",
            "----\n",
            "Epoch 1605/2500, lr=0.001005\n",
            "--> train_loss: 0.033045, test_loss: 0.036197, train_metric: 0.033, test_metric: 0.036\n",
            "----\n",
            "Epoch 1606/2500, lr=0.001004\n",
            "--> train_loss: 0.032524, test_loss: 0.036638, train_metric: 0.033, test_metric: 0.037\n",
            "----\n",
            "Epoch 1607/2500, lr=0.001003\n",
            "--> train_loss: 0.032929, test_loss: 0.036416, train_metric: 0.033, test_metric: 0.036\n",
            "----\n",
            "Epoch 1608/2500, lr=0.001002\n",
            "--> train_loss: 0.032656, test_loss: 0.036454, train_metric: 0.033, test_metric: 0.036\n",
            "----\n",
            "Epoch 1609/2500, lr=0.001001\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.031973, test_loss: 0.035681, train_metric: 0.032, test_metric: 0.036\n",
            "----\n",
            "Epoch 1610/2500, lr=0.001000\n",
            "--> train_loss: 0.032235, test_loss: 0.035803, train_metric: 0.032, test_metric: 0.036\n",
            "----\n",
            "Epoch 1611/2500, lr=0.000999\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.031836, test_loss: 0.035525, train_metric: 0.032, test_metric: 0.036\n",
            "----\n",
            "Epoch 1612/2500, lr=0.000998\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.031712, test_loss: 0.035399, train_metric: 0.032, test_metric: 0.035\n",
            "----\n",
            "Epoch 1613/2500, lr=0.000997\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.031894, test_loss: 0.035256, train_metric: 0.032, test_metric: 0.035\n",
            "----\n",
            "Epoch 1614/2500, lr=0.000996\n",
            "--> train_loss: 0.031640, test_loss: 0.035753, train_metric: 0.032, test_metric: 0.036\n",
            "----\n",
            "Epoch 1615/2500, lr=0.000995\n",
            "--> train_loss: 0.031622, test_loss: 0.035430, train_metric: 0.032, test_metric: 0.035\n",
            "----\n",
            "Epoch 1616/2500, lr=0.000994\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.031820, test_loss: 0.035154, train_metric: 0.032, test_metric: 0.035\n",
            "----\n",
            "Epoch 1617/2500, lr=0.000993\n",
            "--> train_loss: 0.032129, test_loss: 0.035613, train_metric: 0.032, test_metric: 0.036\n",
            "----\n",
            "Epoch 1618/2500, lr=0.000992\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.031326, test_loss: 0.034944, train_metric: 0.031, test_metric: 0.035\n",
            "----\n",
            "Epoch 1619/2500, lr=0.000991\n",
            "--> train_loss: 0.031431, test_loss: 0.035134, train_metric: 0.031, test_metric: 0.035\n",
            "----\n",
            "Epoch 1620/2500, lr=0.000990\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.031316, test_loss: 0.034807, train_metric: 0.031, test_metric: 0.035\n",
            "----\n",
            "Epoch 1621/2500, lr=0.000989\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.031024, test_loss: 0.034609, train_metric: 0.031, test_metric: 0.035\n",
            "----\n",
            "Epoch 1622/2500, lr=0.000988\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.031173, test_loss: 0.034522, train_metric: 0.031, test_metric: 0.035\n",
            "----\n",
            "Epoch 1623/2500, lr=0.000987\n",
            "--> train_loss: 0.031109, test_loss: 0.034563, train_metric: 0.031, test_metric: 0.035\n",
            "----\n",
            "Epoch 1624/2500, lr=0.000986\n",
            "--> train_loss: 0.031262, test_loss: 0.035045, train_metric: 0.031, test_metric: 0.035\n",
            "----\n",
            "Epoch 1625/2500, lr=0.000985\n",
            "--> train_loss: 0.030891, test_loss: 0.034665, train_metric: 0.031, test_metric: 0.035\n",
            "----\n",
            "Epoch 1626/2500, lr=0.000984\n",
            "--> train_loss: 0.031243, test_loss: 0.034605, train_metric: 0.031, test_metric: 0.035\n",
            "----\n",
            "Epoch 1627/2500, lr=0.000983\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.030743, test_loss: 0.034447, train_metric: 0.031, test_metric: 0.034\n",
            "----\n",
            "Epoch 1628/2500, lr=0.000982\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.030706, test_loss: 0.034313, train_metric: 0.031, test_metric: 0.034\n",
            "----\n",
            "Epoch 1629/2500, lr=0.000981\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.030892, test_loss: 0.034204, train_metric: 0.031, test_metric: 0.034\n",
            "----\n",
            "Epoch 1630/2500, lr=0.000980\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.030851, test_loss: 0.034085, train_metric: 0.031, test_metric: 0.034\n",
            "----\n",
            "Epoch 1631/2500, lr=0.000979\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.030531, test_loss: 0.033975, train_metric: 0.031, test_metric: 0.034\n",
            "----\n",
            "Epoch 1632/2500, lr=0.000978\n",
            "--> train_loss: 0.030348, test_loss: 0.034165, train_metric: 0.030, test_metric: 0.034\n",
            "----\n",
            "Epoch 1633/2500, lr=0.000977\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.030633, test_loss: 0.033793, train_metric: 0.031, test_metric: 0.034\n",
            "----\n",
            "Epoch 1634/2500, lr=0.000976\n",
            "--> train_loss: 0.030675, test_loss: 0.033892, train_metric: 0.031, test_metric: 0.034\n",
            "----\n",
            "Epoch 1635/2500, lr=0.000975\n",
            "--> train_loss: 0.030520, test_loss: 0.033915, train_metric: 0.031, test_metric: 0.034\n",
            "----\n",
            "Epoch 1636/2500, lr=0.000974\n",
            "--> train_loss: 0.030624, test_loss: 0.033909, train_metric: 0.031, test_metric: 0.034\n",
            "----\n",
            "Epoch 1637/2500, lr=0.000973\n",
            "--> train_loss: 0.030373, test_loss: 0.033871, train_metric: 0.030, test_metric: 0.034\n",
            "----\n",
            "Epoch 1638/2500, lr=0.000972\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.030365, test_loss: 0.033405, train_metric: 0.030, test_metric: 0.033\n",
            "----\n",
            "Epoch 1639/2500, lr=0.000971\n",
            "--> train_loss: 0.030348, test_loss: 0.034105, train_metric: 0.030, test_metric: 0.034\n",
            "----\n",
            "Epoch 1640/2500, lr=0.000970\n",
            "--> train_loss: 0.030966, test_loss: 0.033435, train_metric: 0.031, test_metric: 0.033\n",
            "----\n",
            "Epoch 1641/2500, lr=0.000969\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.029969, test_loss: 0.033363, train_metric: 0.030, test_metric: 0.033\n",
            "----\n",
            "Epoch 1642/2500, lr=0.000968\n",
            "--> train_loss: 0.030063, test_loss: 0.033617, train_metric: 0.030, test_metric: 0.034\n",
            "----\n",
            "Epoch 1643/2500, lr=0.000967\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.029904, test_loss: 0.032819, train_metric: 0.030, test_metric: 0.033\n",
            "----\n",
            "Epoch 1644/2500, lr=0.000966\n",
            "--> train_loss: 0.029779, test_loss: 0.033213, train_metric: 0.030, test_metric: 0.033\n",
            "----\n",
            "Epoch 1645/2500, lr=0.000965\n",
            "--> train_loss: 0.029569, test_loss: 0.033109, train_metric: 0.030, test_metric: 0.033\n",
            "----\n",
            "Epoch 1646/2500, lr=0.000964\n",
            "--> train_loss: 0.029723, test_loss: 0.033002, train_metric: 0.030, test_metric: 0.033\n",
            "----\n",
            "Epoch 1647/2500, lr=0.000963\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.029428, test_loss: 0.032664, train_metric: 0.029, test_metric: 0.033\n",
            "----\n",
            "Epoch 1648/2500, lr=0.000962\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.029463, test_loss: 0.032596, train_metric: 0.029, test_metric: 0.033\n",
            "----\n",
            "Epoch 1649/2500, lr=0.000961\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.029487, test_loss: 0.032526, train_metric: 0.029, test_metric: 0.033\n",
            "----\n",
            "Epoch 1650/2500, lr=0.000960\n",
            "--> train_loss: 0.029648, test_loss: 0.032909, train_metric: 0.030, test_metric: 0.033\n",
            "----\n",
            "Epoch 1651/2500, lr=0.000959\n",
            "--> train_loss: 0.029521, test_loss: 0.032610, train_metric: 0.030, test_metric: 0.033\n",
            "----\n",
            "Epoch 1652/2500, lr=0.000958\n",
            "--> train_loss: 0.030158, test_loss: 0.032601, train_metric: 0.030, test_metric: 0.033\n",
            "----\n",
            "Epoch 1653/2500, lr=0.000958\n",
            "--> train_loss: 0.030508, test_loss: 0.032856, train_metric: 0.031, test_metric: 0.033\n",
            "----\n",
            "Epoch 1654/2500, lr=0.000957\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.029656, test_loss: 0.032500, train_metric: 0.030, test_metric: 0.032\n",
            "----\n",
            "Epoch 1655/2500, lr=0.000956\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.029371, test_loss: 0.032158, train_metric: 0.029, test_metric: 0.032\n",
            "----\n",
            "Epoch 1656/2500, lr=0.000955\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.029485, test_loss: 0.032108, train_metric: 0.029, test_metric: 0.032\n",
            "----\n",
            "Epoch 1657/2500, lr=0.000954\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.029343, test_loss: 0.031906, train_metric: 0.029, test_metric: 0.032\n",
            "----\n",
            "Epoch 1658/2500, lr=0.000953\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.029070, test_loss: 0.031680, train_metric: 0.029, test_metric: 0.032\n",
            "----\n",
            "Epoch 1659/2500, lr=0.000952\n",
            "--> train_loss: 0.029216, test_loss: 0.031872, train_metric: 0.029, test_metric: 0.032\n",
            "----\n",
            "Epoch 1660/2500, lr=0.000951\n",
            "--> train_loss: 0.028667, test_loss: 0.031967, train_metric: 0.029, test_metric: 0.032\n",
            "----\n",
            "Epoch 1661/2500, lr=0.000950\n",
            "--> train_loss: 0.029035, test_loss: 0.032088, train_metric: 0.029, test_metric: 0.032\n",
            "----\n",
            "Epoch 1662/2500, lr=0.000949\n",
            "--> train_loss: 0.028454, test_loss: 0.031733, train_metric: 0.028, test_metric: 0.032\n",
            "----\n",
            "Epoch 1663/2500, lr=0.000948\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.028639, test_loss: 0.031325, train_metric: 0.029, test_metric: 0.031\n",
            "----\n",
            "Epoch 1664/2500, lr=0.000947\n",
            "--> train_loss: 0.028890, test_loss: 0.031364, train_metric: 0.029, test_metric: 0.031\n",
            "----\n",
            "Epoch 1665/2500, lr=0.000946\n",
            "--> train_loss: 0.028720, test_loss: 0.031350, train_metric: 0.029, test_metric: 0.031\n",
            "----\n",
            "Epoch 1666/2500, lr=0.000945\n",
            "--> train_loss: 0.029278, test_loss: 0.031713, train_metric: 0.029, test_metric: 0.032\n",
            "----\n",
            "Epoch 1667/2500, lr=0.000944\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.028453, test_loss: 0.030985, train_metric: 0.028, test_metric: 0.031\n",
            "----\n",
            "Epoch 1668/2500, lr=0.000943\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.028186, test_loss: 0.030858, train_metric: 0.028, test_metric: 0.031\n",
            "----\n",
            "Epoch 1669/2500, lr=0.000942\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.027959, test_loss: 0.030759, train_metric: 0.028, test_metric: 0.031\n",
            "----\n",
            "Epoch 1670/2500, lr=0.000941\n",
            "--> train_loss: 0.028001, test_loss: 0.030879, train_metric: 0.028, test_metric: 0.031\n",
            "----\n",
            "Epoch 1671/2500, lr=0.000940\n",
            "--> train_loss: 0.028417, test_loss: 0.031175, train_metric: 0.028, test_metric: 0.031\n",
            "----\n",
            "Epoch 1672/2500, lr=0.000940\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.028017, test_loss: 0.030587, train_metric: 0.028, test_metric: 0.031\n",
            "----\n",
            "Epoch 1673/2500, lr=0.000939\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.027793, test_loss: 0.030554, train_metric: 0.028, test_metric: 0.031\n",
            "----\n",
            "Epoch 1674/2500, lr=0.000938\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.027759, test_loss: 0.030345, train_metric: 0.028, test_metric: 0.030\n",
            "----\n",
            "Epoch 1675/2500, lr=0.000937\n",
            "--> train_loss: 0.028529, test_loss: 0.031055, train_metric: 0.029, test_metric: 0.031\n",
            "----\n",
            "Epoch 1676/2500, lr=0.000936\n",
            "--> train_loss: 0.027535, test_loss: 0.030374, train_metric: 0.028, test_metric: 0.030\n",
            "----\n",
            "Epoch 1677/2500, lr=0.000935\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.027979, test_loss: 0.030069, train_metric: 0.028, test_metric: 0.030\n",
            "----\n",
            "Epoch 1678/2500, lr=0.000934\n",
            "--> train_loss: 0.028247, test_loss: 0.030387, train_metric: 0.028, test_metric: 0.030\n",
            "----\n",
            "Epoch 1679/2500, lr=0.000933\n",
            "--> train_loss: 0.027204, test_loss: 0.030231, train_metric: 0.027, test_metric: 0.030\n",
            "----\n",
            "Epoch 1680/2500, lr=0.000932\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.027087, test_loss: 0.029740, train_metric: 0.027, test_metric: 0.030\n",
            "----\n",
            "Epoch 1681/2500, lr=0.000931\n",
            "--> train_loss: 0.027159, test_loss: 0.029782, train_metric: 0.027, test_metric: 0.030\n",
            "----\n",
            "Epoch 1682/2500, lr=0.000930\n",
            "--> train_loss: 0.027108, test_loss: 0.029871, train_metric: 0.027, test_metric: 0.030\n",
            "----\n",
            "Epoch 1683/2500, lr=0.000929\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.027068, test_loss: 0.029603, train_metric: 0.027, test_metric: 0.030\n",
            "----\n",
            "Epoch 1684/2500, lr=0.000928\n",
            "--> train_loss: 0.027265, test_loss: 0.029692, train_metric: 0.027, test_metric: 0.030\n",
            "----\n",
            "Epoch 1685/2500, lr=0.000927\n",
            "--> train_loss: 0.026967, test_loss: 0.029776, train_metric: 0.027, test_metric: 0.030\n",
            "----\n",
            "Epoch 1686/2500, lr=0.000926\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.027132, test_loss: 0.029203, train_metric: 0.027, test_metric: 0.029\n",
            "----\n",
            "Epoch 1687/2500, lr=0.000926\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.026721, test_loss: 0.029007, train_metric: 0.027, test_metric: 0.029\n",
            "----\n",
            "Epoch 1688/2500, lr=0.000925\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.026886, test_loss: 0.028924, train_metric: 0.027, test_metric: 0.029\n",
            "----\n",
            "Epoch 1689/2500, lr=0.000924\n",
            "--> train_loss: 0.026475, test_loss: 0.029192, train_metric: 0.026, test_metric: 0.029\n",
            "----\n",
            "Epoch 1690/2500, lr=0.000923\n",
            "--> train_loss: 0.026705, test_loss: 0.029130, train_metric: 0.027, test_metric: 0.029\n",
            "----\n",
            "Epoch 1691/2500, lr=0.000922\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.026436, test_loss: 0.028769, train_metric: 0.026, test_metric: 0.029\n",
            "----\n",
            "Epoch 1692/2500, lr=0.000921\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.026497, test_loss: 0.028715, train_metric: 0.026, test_metric: 0.029\n",
            "----\n",
            "Epoch 1693/2500, lr=0.000920\n",
            "--> train_loss: 0.026270, test_loss: 0.028872, train_metric: 0.026, test_metric: 0.029\n",
            "----\n",
            "Epoch 1694/2500, lr=0.000919\n",
            "--> train_loss: 0.026319, test_loss: 0.028977, train_metric: 0.026, test_metric: 0.029\n",
            "----\n",
            "Epoch 1695/2500, lr=0.000918\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.026406, test_loss: 0.028591, train_metric: 0.026, test_metric: 0.029\n",
            "----\n",
            "Epoch 1696/2500, lr=0.000917\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.026692, test_loss: 0.028309, train_metric: 0.027, test_metric: 0.028\n",
            "----\n",
            "Epoch 1697/2500, lr=0.000916\n",
            "--> train_loss: 0.026804, test_loss: 0.028672, train_metric: 0.027, test_metric: 0.029\n",
            "----\n",
            "Epoch 1698/2500, lr=0.000915\n",
            "--> train_loss: 0.026851, test_loss: 0.029740, train_metric: 0.027, test_metric: 0.030\n",
            "----\n",
            "Epoch 1699/2500, lr=0.000914\n",
            "--> train_loss: 0.026918, test_loss: 0.028355, train_metric: 0.027, test_metric: 0.028\n",
            "----\n",
            "Epoch 1700/2500, lr=0.000914\n",
            "--> train_loss: 0.026496, test_loss: 0.028661, train_metric: 0.026, test_metric: 0.029\n",
            "----\n",
            "Epoch 1701/2500, lr=0.000913\n",
            "--> train_loss: 0.026211, test_loss: 0.028939, train_metric: 0.026, test_metric: 0.029\n",
            "----\n",
            "Epoch 1702/2500, lr=0.000912\n",
            "--> train_loss: 0.026874, test_loss: 0.028737, train_metric: 0.027, test_metric: 0.029\n",
            "----\n",
            "Epoch 1703/2500, lr=0.000911\n",
            "--> train_loss: 0.026265, test_loss: 0.028640, train_metric: 0.026, test_metric: 0.029\n",
            "----\n",
            "Epoch 1704/2500, lr=0.000910\n",
            "--> train_loss: 0.026761, test_loss: 0.029413, train_metric: 0.027, test_metric: 0.029\n",
            "----\n",
            "Epoch 1705/2500, lr=0.000909\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.026229, test_loss: 0.027938, train_metric: 0.026, test_metric: 0.028\n",
            "----\n",
            "Epoch 1706/2500, lr=0.000908\n",
            "--> train_loss: 0.026428, test_loss: 0.028523, train_metric: 0.026, test_metric: 0.029\n",
            "----\n",
            "Epoch 1707/2500, lr=0.000907\n",
            "--> train_loss: 0.026166, test_loss: 0.028281, train_metric: 0.026, test_metric: 0.028\n",
            "----\n",
            "Epoch 1708/2500, lr=0.000906\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.025031, test_loss: 0.027461, train_metric: 0.025, test_metric: 0.027\n",
            "----\n",
            "Epoch 1709/2500, lr=0.000905\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.025529, test_loss: 0.027374, train_metric: 0.026, test_metric: 0.027\n",
            "----\n",
            "Epoch 1710/2500, lr=0.000904\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.025517, test_loss: 0.027161, train_metric: 0.026, test_metric: 0.027\n",
            "----\n",
            "Epoch 1711/2500, lr=0.000904\n",
            "--> train_loss: 0.025576, test_loss: 0.027697, train_metric: 0.026, test_metric: 0.028\n",
            "----\n",
            "Epoch 1712/2500, lr=0.000903\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.025169, test_loss: 0.026752, train_metric: 0.025, test_metric: 0.027\n",
            "----\n",
            "Epoch 1713/2500, lr=0.000902\n",
            "--> train_loss: 0.025008, test_loss: 0.026787, train_metric: 0.025, test_metric: 0.027\n",
            "----\n",
            "Epoch 1714/2500, lr=0.000901\n",
            "--> train_loss: 0.024713, test_loss: 0.027130, train_metric: 0.025, test_metric: 0.027\n",
            "----\n",
            "Epoch 1715/2500, lr=0.000900\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.025001, test_loss: 0.026552, train_metric: 0.025, test_metric: 0.027\n",
            "----\n",
            "Epoch 1716/2500, lr=0.000899\n",
            "--> train_loss: 0.024920, test_loss: 0.026775, train_metric: 0.025, test_metric: 0.027\n",
            "----\n",
            "Epoch 1717/2500, lr=0.000898\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.025173, test_loss: 0.026493, train_metric: 0.025, test_metric: 0.026\n",
            "----\n",
            "Epoch 1718/2500, lr=0.000897\n",
            "--> train_loss: 0.025649, test_loss: 0.026988, train_metric: 0.026, test_metric: 0.027\n",
            "----\n",
            "Epoch 1719/2500, lr=0.000896\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.024875, test_loss: 0.026241, train_metric: 0.025, test_metric: 0.026\n",
            "----\n",
            "Epoch 1720/2500, lr=0.000895\n",
            "--> train_loss: 0.024579, test_loss: 0.026620, train_metric: 0.025, test_metric: 0.027\n",
            "----\n",
            "Epoch 1721/2500, lr=0.000895\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.024534, test_loss: 0.026021, train_metric: 0.025, test_metric: 0.026\n",
            "----\n",
            "Epoch 1722/2500, lr=0.000894\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.024303, test_loss: 0.025891, train_metric: 0.024, test_metric: 0.026\n",
            "----\n",
            "Epoch 1723/2500, lr=0.000893\n",
            "--> train_loss: 0.024241, test_loss: 0.026236, train_metric: 0.024, test_metric: 0.026\n",
            "----\n",
            "Epoch 1724/2500, lr=0.000892\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.024507, test_loss: 0.025653, train_metric: 0.025, test_metric: 0.026\n",
            "----\n",
            "Epoch 1725/2500, lr=0.000891\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.024127, test_loss: 0.025579, train_metric: 0.024, test_metric: 0.026\n",
            "----\n",
            "Epoch 1726/2500, lr=0.000890\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.023907, test_loss: 0.025567, train_metric: 0.024, test_metric: 0.026\n",
            "----\n",
            "Epoch 1727/2500, lr=0.000889\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.024038, test_loss: 0.025401, train_metric: 0.024, test_metric: 0.025\n",
            "----\n",
            "Epoch 1728/2500, lr=0.000888\n",
            "--> train_loss: 0.024183, test_loss: 0.025506, train_metric: 0.024, test_metric: 0.026\n",
            "----\n",
            "Epoch 1729/2500, lr=0.000887\n",
            "--> train_loss: 0.024389, test_loss: 0.025896, train_metric: 0.024, test_metric: 0.026\n",
            "----\n",
            "Epoch 1730/2500, lr=0.000887\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.024290, test_loss: 0.025387, train_metric: 0.024, test_metric: 0.025\n",
            "----\n",
            "Epoch 1731/2500, lr=0.000886\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.023917, test_loss: 0.025115, train_metric: 0.024, test_metric: 0.025\n",
            "----\n",
            "Epoch 1732/2500, lr=0.000885\n",
            "--> train_loss: 0.023923, test_loss: 0.025504, train_metric: 0.024, test_metric: 0.026\n",
            "----\n",
            "Epoch 1733/2500, lr=0.000884\n",
            "--> train_loss: 0.024652, test_loss: 0.026303, train_metric: 0.025, test_metric: 0.026\n",
            "----\n",
            "Epoch 1734/2500, lr=0.000883\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.023844, test_loss: 0.024918, train_metric: 0.024, test_metric: 0.025\n",
            "----\n",
            "Epoch 1735/2500, lr=0.000882\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.023605, test_loss: 0.024468, train_metric: 0.024, test_metric: 0.024\n",
            "----\n",
            "Epoch 1736/2500, lr=0.000881\n",
            "--> train_loss: 0.023188, test_loss: 0.024769, train_metric: 0.023, test_metric: 0.025\n",
            "----\n",
            "Epoch 1737/2500, lr=0.000880\n",
            "--> train_loss: 0.023615, test_loss: 0.024732, train_metric: 0.024, test_metric: 0.025\n",
            "----\n",
            "Epoch 1738/2500, lr=0.000879\n",
            "--> train_loss: 0.023332, test_loss: 0.024545, train_metric: 0.023, test_metric: 0.025\n",
            "----\n",
            "Epoch 1739/2500, lr=0.000879\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.023391, test_loss: 0.024301, train_metric: 0.023, test_metric: 0.024\n",
            "----\n",
            "Epoch 1740/2500, lr=0.000878\n",
            "--> train_loss: 0.024095, test_loss: 0.024976, train_metric: 0.024, test_metric: 0.025\n",
            "----\n",
            "Epoch 1741/2500, lr=0.000877\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.022871, test_loss: 0.024290, train_metric: 0.023, test_metric: 0.024\n",
            "----\n",
            "Epoch 1742/2500, lr=0.000876\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.022691, test_loss: 0.023900, train_metric: 0.023, test_metric: 0.024\n",
            "----\n",
            "Epoch 1743/2500, lr=0.000875\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.022906, test_loss: 0.023726, train_metric: 0.023, test_metric: 0.024\n",
            "----\n",
            "Epoch 1744/2500, lr=0.000874\n",
            "--> train_loss: 0.022432, test_loss: 0.023868, train_metric: 0.022, test_metric: 0.024\n",
            "----\n",
            "Epoch 1745/2500, lr=0.000873\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.022244, test_loss: 0.023656, train_metric: 0.022, test_metric: 0.024\n",
            "----\n",
            "Epoch 1746/2500, lr=0.000872\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.022479, test_loss: 0.023543, train_metric: 0.022, test_metric: 0.024\n",
            "----\n",
            "Epoch 1747/2500, lr=0.000872\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.023167, test_loss: 0.023513, train_metric: 0.023, test_metric: 0.024\n",
            "----\n",
            "Epoch 1748/2500, lr=0.000871\n",
            "--> train_loss: 0.022456, test_loss: 0.023982, train_metric: 0.022, test_metric: 0.024\n",
            "----\n",
            "Epoch 1749/2500, lr=0.000870\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.022365, test_loss: 0.023371, train_metric: 0.022, test_metric: 0.023\n",
            "----\n",
            "Epoch 1750/2500, lr=0.000869\n",
            "--> train_loss: 0.021945, test_loss: 0.023376, train_metric: 0.022, test_metric: 0.023\n",
            "----\n",
            "Epoch 1751/2500, lr=0.000868\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.021911, test_loss: 0.023103, train_metric: 0.022, test_metric: 0.023\n",
            "----\n",
            "Epoch 1752/2500, lr=0.000867\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.021804, test_loss: 0.022955, train_metric: 0.022, test_metric: 0.023\n",
            "----\n",
            "Epoch 1753/2500, lr=0.000866\n",
            "--> train_loss: 0.021893, test_loss: 0.023192, train_metric: 0.022, test_metric: 0.023\n",
            "----\n",
            "Epoch 1754/2500, lr=0.000866\n",
            "--> train_loss: 0.021764, test_loss: 0.023083, train_metric: 0.022, test_metric: 0.023\n",
            "----\n",
            "Epoch 1755/2500, lr=0.000865\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.021583, test_loss: 0.022498, train_metric: 0.022, test_metric: 0.022\n",
            "----\n",
            "Epoch 1756/2500, lr=0.000864\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.021617, test_loss: 0.022478, train_metric: 0.022, test_metric: 0.022\n",
            "----\n",
            "Epoch 1757/2500, lr=0.000863\n",
            "--> train_loss: 0.021468, test_loss: 0.022781, train_metric: 0.021, test_metric: 0.023\n",
            "----\n",
            "Epoch 1758/2500, lr=0.000862\n",
            "--> train_loss: 0.021620, test_loss: 0.022724, train_metric: 0.022, test_metric: 0.023\n",
            "----\n",
            "Epoch 1759/2500, lr=0.000861\n",
            "--> train_loss: 0.021520, test_loss: 0.022685, train_metric: 0.022, test_metric: 0.023\n",
            "----\n",
            "Epoch 1760/2500, lr=0.000860\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.021614, test_loss: 0.022076, train_metric: 0.022, test_metric: 0.022\n",
            "----\n",
            "Epoch 1761/2500, lr=0.000859\n",
            "--> train_loss: 0.021553, test_loss: 0.022717, train_metric: 0.022, test_metric: 0.023\n",
            "----\n",
            "Epoch 1762/2500, lr=0.000859\n",
            "--> train_loss: 0.021895, test_loss: 0.024053, train_metric: 0.022, test_metric: 0.024\n",
            "----\n",
            "Epoch 1763/2500, lr=0.000858\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.022055, test_loss: 0.022035, train_metric: 0.022, test_metric: 0.022\n",
            "----\n",
            "Epoch 1764/2500, lr=0.000857\n",
            "--> train_loss: 0.021839, test_loss: 0.022142, train_metric: 0.022, test_metric: 0.022\n",
            "----\n",
            "Epoch 1765/2500, lr=0.000856\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.021199, test_loss: 0.021861, train_metric: 0.021, test_metric: 0.022\n",
            "----\n",
            "Epoch 1766/2500, lr=0.000855\n",
            "--> train_loss: 0.020891, test_loss: 0.022018, train_metric: 0.021, test_metric: 0.022\n",
            "----\n",
            "Epoch 1767/2500, lr=0.000854\n",
            "--> train_loss: 0.021342, test_loss: 0.022460, train_metric: 0.021, test_metric: 0.022\n",
            "----\n",
            "Epoch 1768/2500, lr=0.000853\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.020999, test_loss: 0.021375, train_metric: 0.021, test_metric: 0.021\n",
            "----\n",
            "Epoch 1769/2500, lr=0.000853\n",
            "--> train_loss: 0.020874, test_loss: 0.021827, train_metric: 0.021, test_metric: 0.022\n",
            "----\n",
            "Epoch 1770/2500, lr=0.000852\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.021031, test_loss: 0.021117, train_metric: 0.021, test_metric: 0.021\n",
            "----\n",
            "Epoch 1771/2500, lr=0.000851\n",
            "--> train_loss: 0.020529, test_loss: 0.022155, train_metric: 0.021, test_metric: 0.022\n",
            "----\n",
            "Epoch 1772/2500, lr=0.000850\n",
            "--> train_loss: 0.020766, test_loss: 0.021354, train_metric: 0.021, test_metric: 0.021\n",
            "----\n",
            "Epoch 1773/2500, lr=0.000849\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.021064, test_loss: 0.021104, train_metric: 0.021, test_metric: 0.021\n",
            "----\n",
            "Epoch 1774/2500, lr=0.000848\n",
            "--> train_loss: 0.020896, test_loss: 0.021758, train_metric: 0.021, test_metric: 0.022\n",
            "----\n",
            "Epoch 1775/2500, lr=0.000848\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.020595, test_loss: 0.021091, train_metric: 0.021, test_metric: 0.021\n",
            "----\n",
            "Epoch 1776/2500, lr=0.000847\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.020344, test_loss: 0.020833, train_metric: 0.020, test_metric: 0.021\n",
            "----\n",
            "Epoch 1777/2500, lr=0.000846\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.020325, test_loss: 0.020794, train_metric: 0.020, test_metric: 0.021\n",
            "----\n",
            "Epoch 1778/2500, lr=0.000845\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.020152, test_loss: 0.020650, train_metric: 0.020, test_metric: 0.021\n",
            "----\n",
            "Epoch 1779/2500, lr=0.000844\n",
            "--> train_loss: 0.020239, test_loss: 0.020770, train_metric: 0.020, test_metric: 0.021\n",
            "----\n",
            "Epoch 1780/2500, lr=0.000843\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.020036, test_loss: 0.020377, train_metric: 0.020, test_metric: 0.020\n",
            "----\n",
            "Epoch 1781/2500, lr=0.000842\n",
            "--> train_loss: 0.020812, test_loss: 0.020988, train_metric: 0.021, test_metric: 0.021\n",
            "----\n",
            "Epoch 1782/2500, lr=0.000842\n",
            "--> train_loss: 0.020310, test_loss: 0.020864, train_metric: 0.020, test_metric: 0.021\n",
            "----\n",
            "Epoch 1783/2500, lr=0.000841\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.020183, test_loss: 0.019993, train_metric: 0.020, test_metric: 0.020\n",
            "----\n",
            "Epoch 1784/2500, lr=0.000840\n",
            "--> train_loss: 0.019583, test_loss: 0.020565, train_metric: 0.020, test_metric: 0.021\n",
            "----\n",
            "Epoch 1785/2500, lr=0.000839\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.019509, test_loss: 0.019953, train_metric: 0.020, test_metric: 0.020\n",
            "----\n",
            "Epoch 1786/2500, lr=0.000838\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.019274, test_loss: 0.019861, train_metric: 0.019, test_metric: 0.020\n",
            "----\n",
            "Epoch 1787/2500, lr=0.000837\n",
            "--> train_loss: 0.019534, test_loss: 0.020104, train_metric: 0.020, test_metric: 0.020\n",
            "----\n",
            "Epoch 1788/2500, lr=0.000837\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.019373, test_loss: 0.019770, train_metric: 0.019, test_metric: 0.020\n",
            "----\n",
            "Epoch 1789/2500, lr=0.000836\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.019591, test_loss: 0.019626, train_metric: 0.020, test_metric: 0.020\n",
            "----\n",
            "Epoch 1790/2500, lr=0.000835\n",
            "--> train_loss: 0.019587, test_loss: 0.020327, train_metric: 0.020, test_metric: 0.020\n",
            "----\n",
            "Epoch 1791/2500, lr=0.000834\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.019381, test_loss: 0.019621, train_metric: 0.019, test_metric: 0.020\n",
            "----\n",
            "Epoch 1792/2500, lr=0.000833\n",
            "--> train_loss: 0.019420, test_loss: 0.019884, train_metric: 0.019, test_metric: 0.020\n",
            "----\n",
            "Epoch 1793/2500, lr=0.000832\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.019661, test_loss: 0.019402, train_metric: 0.020, test_metric: 0.019\n",
            "----\n",
            "Epoch 1794/2500, lr=0.000832\n",
            "--> train_loss: 0.019151, test_loss: 0.019547, train_metric: 0.019, test_metric: 0.020\n",
            "----\n",
            "Epoch 1795/2500, lr=0.000831\n",
            "--> train_loss: 0.019290, test_loss: 0.019586, train_metric: 0.019, test_metric: 0.020\n",
            "----\n",
            "Epoch 1796/2500, lr=0.000830\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.018895, test_loss: 0.019008, train_metric: 0.019, test_metric: 0.019\n",
            "----\n",
            "Epoch 1797/2500, lr=0.000829\n",
            "--> train_loss: 0.018965, test_loss: 0.019100, train_metric: 0.019, test_metric: 0.019\n",
            "----\n",
            "Epoch 1798/2500, lr=0.000828\n",
            "--> train_loss: 0.018468, test_loss: 0.019156, train_metric: 0.018, test_metric: 0.019\n",
            "----\n",
            "Epoch 1799/2500, lr=0.000827\n",
            "--> train_loss: 0.018675, test_loss: 0.019417, train_metric: 0.019, test_metric: 0.019\n",
            "----\n",
            "Epoch 1800/2500, lr=0.000827\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.018477, test_loss: 0.018994, train_metric: 0.018, test_metric: 0.019\n",
            "----\n",
            "Epoch 1801/2500, lr=0.000826\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.018433, test_loss: 0.018466, train_metric: 0.018, test_metric: 0.018\n",
            "----\n",
            "Epoch 1802/2500, lr=0.000825\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.018405, test_loss: 0.018435, train_metric: 0.018, test_metric: 0.018\n",
            "----\n",
            "Epoch 1803/2500, lr=0.000824\n",
            "--> train_loss: 0.018915, test_loss: 0.018754, train_metric: 0.019, test_metric: 0.019\n",
            "----\n",
            "Epoch 1804/2500, lr=0.000823\n",
            "--> train_loss: 0.018774, test_loss: 0.019557, train_metric: 0.019, test_metric: 0.020\n",
            "----\n",
            "Epoch 1805/2500, lr=0.000822\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.018422, test_loss: 0.018341, train_metric: 0.018, test_metric: 0.018\n",
            "----\n",
            "Epoch 1806/2500, lr=0.000822\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.018115, test_loss: 0.018079, train_metric: 0.018, test_metric: 0.018\n",
            "----\n",
            "Epoch 1807/2500, lr=0.000821\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.018238, test_loss: 0.018057, train_metric: 0.018, test_metric: 0.018\n",
            "----\n",
            "Epoch 1808/2500, lr=0.000820\n",
            "--> train_loss: 0.018269, test_loss: 0.018563, train_metric: 0.018, test_metric: 0.019\n",
            "----\n",
            "Epoch 1809/2500, lr=0.000819\n",
            "--> train_loss: 0.018050, test_loss: 0.018291, train_metric: 0.018, test_metric: 0.018\n",
            "----\n",
            "Epoch 1810/2500, lr=0.000818\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.019126, test_loss: 0.017814, train_metric: 0.019, test_metric: 0.018\n",
            "----\n",
            "Epoch 1811/2500, lr=0.000818\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.017666, test_loss: 0.017701, train_metric: 0.018, test_metric: 0.018\n",
            "----\n",
            "Epoch 1812/2500, lr=0.000817\n",
            "--> train_loss: 0.018115, test_loss: 0.019597, train_metric: 0.018, test_metric: 0.020\n",
            "----\n",
            "Epoch 1813/2500, lr=0.000816\n",
            "--> train_loss: 0.018115, test_loss: 0.017731, train_metric: 0.018, test_metric: 0.018\n",
            "----\n",
            "Epoch 1814/2500, lr=0.000815\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.017582, test_loss: 0.017237, train_metric: 0.018, test_metric: 0.017\n",
            "----\n",
            "Epoch 1815/2500, lr=0.000814\n",
            "--> train_loss: 0.017712, test_loss: 0.017573, train_metric: 0.018, test_metric: 0.018\n",
            "----\n",
            "Epoch 1816/2500, lr=0.000813\n",
            "--> train_loss: 0.017562, test_loss: 0.017959, train_metric: 0.018, test_metric: 0.018\n",
            "----\n",
            "Epoch 1817/2500, lr=0.000813\n",
            "--> train_loss: 0.017478, test_loss: 0.017567, train_metric: 0.017, test_metric: 0.018\n",
            "----\n",
            "Epoch 1818/2500, lr=0.000812\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.017273, test_loss: 0.017178, train_metric: 0.017, test_metric: 0.017\n",
            "----\n",
            "Epoch 1819/2500, lr=0.000811\n",
            "--> train_loss: 0.017773, test_loss: 0.017492, train_metric: 0.018, test_metric: 0.017\n",
            "----\n",
            "Epoch 1820/2500, lr=0.000810\n",
            "--> train_loss: 0.017794, test_loss: 0.017190, train_metric: 0.018, test_metric: 0.017\n",
            "----\n",
            "Epoch 1821/2500, lr=0.000809\n",
            "--> train_loss: 0.017503, test_loss: 0.017257, train_metric: 0.018, test_metric: 0.017\n",
            "----\n",
            "Epoch 1822/2500, lr=0.000809\n",
            "--> train_loss: 0.017020, test_loss: 0.017364, train_metric: 0.017, test_metric: 0.017\n",
            "----\n",
            "Epoch 1823/2500, lr=0.000808\n",
            "--> train_loss: 0.017160, test_loss: 0.017358, train_metric: 0.017, test_metric: 0.017\n",
            "----\n",
            "Epoch 1824/2500, lr=0.000807\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.017217, test_loss: 0.016885, train_metric: 0.017, test_metric: 0.017\n",
            "----\n",
            "Epoch 1825/2500, lr=0.000806\n",
            "--> train_loss: 0.017599, test_loss: 0.017783, train_metric: 0.018, test_metric: 0.018\n",
            "----\n",
            "Epoch 1826/2500, lr=0.000805\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.017478, test_loss: 0.016726, train_metric: 0.017, test_metric: 0.017\n",
            "----\n",
            "Epoch 1827/2500, lr=0.000805\n",
            "--> train_loss: 0.017051, test_loss: 0.016920, train_metric: 0.017, test_metric: 0.017\n",
            "----\n",
            "Epoch 1828/2500, lr=0.000804\n",
            "--> train_loss: 0.016716, test_loss: 0.016967, train_metric: 0.017, test_metric: 0.017\n",
            "----\n",
            "Epoch 1829/2500, lr=0.000803\n",
            "--> train_loss: 0.016656, test_loss: 0.016817, train_metric: 0.017, test_metric: 0.017\n",
            "----\n",
            "Epoch 1830/2500, lr=0.000802\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.016477, test_loss: 0.016596, train_metric: 0.016, test_metric: 0.017\n",
            "----\n",
            "Epoch 1831/2500, lr=0.000801\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.016496, test_loss: 0.016425, train_metric: 0.016, test_metric: 0.016\n",
            "----\n",
            "Epoch 1832/2500, lr=0.000801\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.016627, test_loss: 0.016220, train_metric: 0.017, test_metric: 0.016\n",
            "----\n",
            "Epoch 1833/2500, lr=0.000800\n",
            "--> train_loss: 0.017175, test_loss: 0.017067, train_metric: 0.017, test_metric: 0.017\n",
            "----\n",
            "Epoch 1834/2500, lr=0.000799\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.017050, test_loss: 0.015887, train_metric: 0.017, test_metric: 0.016\n",
            "----\n",
            "Epoch 1835/2500, lr=0.000798\n",
            "--> train_loss: 0.016636, test_loss: 0.015897, train_metric: 0.017, test_metric: 0.016\n",
            "----\n",
            "Epoch 1836/2500, lr=0.000797\n",
            "--> train_loss: 0.017006, test_loss: 0.017020, train_metric: 0.017, test_metric: 0.017\n",
            "----\n",
            "Epoch 1837/2500, lr=0.000797\n",
            "--> train_loss: 0.016237, test_loss: 0.015990, train_metric: 0.016, test_metric: 0.016\n",
            "----\n",
            "Epoch 1838/2500, lr=0.000796\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.016856, test_loss: 0.015323, train_metric: 0.017, test_metric: 0.015\n",
            "----\n",
            "Epoch 1839/2500, lr=0.000795\n",
            "--> train_loss: 0.016266, test_loss: 0.016221, train_metric: 0.016, test_metric: 0.016\n",
            "----\n",
            "Epoch 1840/2500, lr=0.000794\n",
            "--> train_loss: 0.016833, test_loss: 0.017568, train_metric: 0.017, test_metric: 0.018\n",
            "----\n",
            "Epoch 1841/2500, lr=0.000793\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.015865, test_loss: 0.015095, train_metric: 0.016, test_metric: 0.015\n",
            "----\n",
            "Epoch 1842/2500, lr=0.000793\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.016296, test_loss: 0.015001, train_metric: 0.016, test_metric: 0.015\n",
            "----\n",
            "Epoch 1843/2500, lr=0.000792\n",
            "--> train_loss: 0.015760, test_loss: 0.015969, train_metric: 0.016, test_metric: 0.016\n",
            "----\n",
            "Epoch 1844/2500, lr=0.000791\n",
            "--> train_loss: 0.015890, test_loss: 0.015699, train_metric: 0.016, test_metric: 0.016\n",
            "----\n",
            "Epoch 1845/2500, lr=0.000790\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.016202, test_loss: 0.014965, train_metric: 0.016, test_metric: 0.015\n",
            "----\n",
            "Epoch 1846/2500, lr=0.000789\n",
            "--> train_loss: 0.015348, test_loss: 0.015459, train_metric: 0.015, test_metric: 0.015\n",
            "----\n",
            "Epoch 1847/2500, lr=0.000789\n",
            "--> train_loss: 0.015480, test_loss: 0.015573, train_metric: 0.015, test_metric: 0.016\n",
            "----\n",
            "Epoch 1848/2500, lr=0.000788\n",
            "--> train_loss: 0.015975, test_loss: 0.015271, train_metric: 0.016, test_metric: 0.015\n",
            "----\n",
            "Epoch 1849/2500, lr=0.000787\n",
            "--> train_loss: 0.016257, test_loss: 0.015074, train_metric: 0.016, test_metric: 0.015\n",
            "----\n",
            "Epoch 1850/2500, lr=0.000786\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.015604, test_loss: 0.014592, train_metric: 0.016, test_metric: 0.015\n",
            "----\n",
            "Epoch 1851/2500, lr=0.000785\n",
            "--> train_loss: 0.015934, test_loss: 0.016341, train_metric: 0.016, test_metric: 0.016\n",
            "----\n",
            "Epoch 1852/2500, lr=0.000785\n",
            "--> train_loss: 0.015466, test_loss: 0.014984, train_metric: 0.015, test_metric: 0.015\n",
            "----\n",
            "Epoch 1853/2500, lr=0.000784\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.015410, test_loss: 0.014587, train_metric: 0.015, test_metric: 0.015\n",
            "----\n",
            "Epoch 1854/2500, lr=0.000783\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.015106, test_loss: 0.014465, train_metric: 0.015, test_metric: 0.014\n",
            "----\n",
            "Epoch 1855/2500, lr=0.000782\n",
            "--> train_loss: 0.014949, test_loss: 0.014641, train_metric: 0.015, test_metric: 0.015\n",
            "----\n",
            "Epoch 1856/2500, lr=0.000782\n",
            "--> train_loss: 0.014801, test_loss: 0.014742, train_metric: 0.015, test_metric: 0.015\n",
            "----\n",
            "Epoch 1857/2500, lr=0.000781\n",
            "--> train_loss: 0.014787, test_loss: 0.014730, train_metric: 0.015, test_metric: 0.015\n",
            "----\n",
            "Epoch 1858/2500, lr=0.000780\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.014860, test_loss: 0.013924, train_metric: 0.015, test_metric: 0.014\n",
            "----\n",
            "Epoch 1859/2500, lr=0.000779\n",
            "--> train_loss: 0.014964, test_loss: 0.014682, train_metric: 0.015, test_metric: 0.015\n",
            "----\n",
            "Epoch 1860/2500, lr=0.000778\n",
            "--> train_loss: 0.014672, test_loss: 0.014267, train_metric: 0.015, test_metric: 0.014\n",
            "----\n",
            "Epoch 1861/2500, lr=0.000778\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.014714, test_loss: 0.013843, train_metric: 0.015, test_metric: 0.014\n",
            "----\n",
            "Epoch 1862/2500, lr=0.000777\n",
            "--> train_loss: 0.014583, test_loss: 0.014523, train_metric: 0.015, test_metric: 0.015\n",
            "----\n",
            "Epoch 1863/2500, lr=0.000776\n",
            "--> train_loss: 0.014747, test_loss: 0.014101, train_metric: 0.015, test_metric: 0.014\n",
            "----\n",
            "Epoch 1864/2500, lr=0.000775\n",
            "--> train_loss: 0.014742, test_loss: 0.014045, train_metric: 0.015, test_metric: 0.014\n",
            "----\n",
            "Epoch 1865/2500, lr=0.000775\n",
            "--> train_loss: 0.014893, test_loss: 0.014112, train_metric: 0.015, test_metric: 0.014\n",
            "----\n",
            "Epoch 1866/2500, lr=0.000774\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.014469, test_loss: 0.013798, train_metric: 0.014, test_metric: 0.014\n",
            "----\n",
            "Epoch 1867/2500, lr=0.000773\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.014639, test_loss: 0.013732, train_metric: 0.015, test_metric: 0.014\n",
            "----\n",
            "Epoch 1868/2500, lr=0.000772\n",
            "--> train_loss: 0.014533, test_loss: 0.014323, train_metric: 0.015, test_metric: 0.014\n",
            "----\n",
            "Epoch 1869/2500, lr=0.000771\n",
            "--> train_loss: 0.014249, test_loss: 0.013941, train_metric: 0.014, test_metric: 0.014\n",
            "----\n",
            "Epoch 1870/2500, lr=0.000771\n",
            "--> train_loss: 0.014757, test_loss: 0.013872, train_metric: 0.015, test_metric: 0.014\n",
            "----\n",
            "Epoch 1871/2500, lr=0.000770\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.014313, test_loss: 0.013285, train_metric: 0.014, test_metric: 0.013\n",
            "----\n",
            "Epoch 1872/2500, lr=0.000769\n",
            "--> train_loss: 0.014563, test_loss: 0.014866, train_metric: 0.015, test_metric: 0.015\n",
            "----\n",
            "Epoch 1873/2500, lr=0.000768\n",
            "--> train_loss: 0.014354, test_loss: 0.013499, train_metric: 0.014, test_metric: 0.013\n",
            "----\n",
            "Epoch 1874/2500, lr=0.000768\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.013868, test_loss: 0.012874, train_metric: 0.014, test_metric: 0.013\n",
            "----\n",
            "Epoch 1875/2500, lr=0.000767\n",
            "--> train_loss: 0.014122, test_loss: 0.013319, train_metric: 0.014, test_metric: 0.013\n",
            "----\n",
            "Epoch 1876/2500, lr=0.000766\n",
            "--> train_loss: 0.013819, test_loss: 0.013460, train_metric: 0.014, test_metric: 0.013\n",
            "----\n",
            "Epoch 1877/2500, lr=0.000765\n",
            "--> train_loss: 0.013787, test_loss: 0.013165, train_metric: 0.014, test_metric: 0.013\n",
            "----\n",
            "Epoch 1878/2500, lr=0.000765\n",
            "--> train_loss: 0.013722, test_loss: 0.013541, train_metric: 0.014, test_metric: 0.014\n",
            "----\n",
            "Epoch 1879/2500, lr=0.000764\n",
            "--> train_loss: 0.013770, test_loss: 0.012889, train_metric: 0.014, test_metric: 0.013\n",
            "----\n",
            "Epoch 1880/2500, lr=0.000763\n",
            "--> train_loss: 0.013649, test_loss: 0.013037, train_metric: 0.014, test_metric: 0.013\n",
            "----\n",
            "Epoch 1881/2500, lr=0.000762\n",
            "--> train_loss: 0.013688, test_loss: 0.013109, train_metric: 0.014, test_metric: 0.013\n",
            "----\n",
            "Epoch 1882/2500, lr=0.000761\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.013506, test_loss: 0.012518, train_metric: 0.014, test_metric: 0.013\n",
            "----\n",
            "Epoch 1883/2500, lr=0.000761\n",
            "--> train_loss: 0.013337, test_loss: 0.012687, train_metric: 0.013, test_metric: 0.013\n",
            "----\n",
            "Epoch 1884/2500, lr=0.000760\n",
            "--> train_loss: 0.013498, test_loss: 0.012799, train_metric: 0.013, test_metric: 0.013\n",
            "----\n",
            "Epoch 1885/2500, lr=0.000759\n",
            "--> train_loss: 0.013679, test_loss: 0.013274, train_metric: 0.014, test_metric: 0.013\n",
            "----\n",
            "Epoch 1886/2500, lr=0.000758\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.013612, test_loss: 0.012484, train_metric: 0.014, test_metric: 0.012\n",
            "----\n",
            "Epoch 1887/2500, lr=0.000758\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.013086, test_loss: 0.012291, train_metric: 0.013, test_metric: 0.012\n",
            "----\n",
            "Epoch 1888/2500, lr=0.000757\n",
            "--> train_loss: 0.013481, test_loss: 0.012571, train_metric: 0.013, test_metric: 0.013\n",
            "----\n",
            "Epoch 1889/2500, lr=0.000756\n",
            "--> train_loss: 0.013353, test_loss: 0.012325, train_metric: 0.013, test_metric: 0.012\n",
            "----\n",
            "Epoch 1890/2500, lr=0.000755\n",
            "--> train_loss: 0.012979, test_loss: 0.012614, train_metric: 0.013, test_metric: 0.013\n",
            "----\n",
            "Epoch 1891/2500, lr=0.000755\n",
            "--> train_loss: 0.013121, test_loss: 0.012371, train_metric: 0.013, test_metric: 0.012\n",
            "----\n",
            "Epoch 1892/2500, lr=0.000754\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.012813, test_loss: 0.012083, train_metric: 0.013, test_metric: 0.012\n",
            "----\n",
            "Epoch 1893/2500, lr=0.000753\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.013338, test_loss: 0.011923, train_metric: 0.013, test_metric: 0.012\n",
            "----\n",
            "Epoch 1894/2500, lr=0.000752\n",
            "--> train_loss: 0.013211, test_loss: 0.012850, train_metric: 0.013, test_metric: 0.013\n",
            "----\n",
            "Epoch 1895/2500, lr=0.000752\n",
            "--> train_loss: 0.012773, test_loss: 0.012240, train_metric: 0.013, test_metric: 0.012\n",
            "----\n",
            "Epoch 1896/2500, lr=0.000751\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.012713, test_loss: 0.011517, train_metric: 0.013, test_metric: 0.012\n",
            "----\n",
            "Epoch 1897/2500, lr=0.000750\n",
            "--> train_loss: 0.012729, test_loss: 0.011720, train_metric: 0.013, test_metric: 0.012\n",
            "----\n",
            "Epoch 1898/2500, lr=0.000749\n",
            "--> train_loss: 0.012539, test_loss: 0.012711, train_metric: 0.013, test_metric: 0.013\n",
            "----\n",
            "Epoch 1899/2500, lr=0.000749\n",
            "--> train_loss: 0.012634, test_loss: 0.012088, train_metric: 0.013, test_metric: 0.012\n",
            "----\n",
            "Epoch 1900/2500, lr=0.000748\n",
            "--> train_loss: 0.012620, test_loss: 0.011638, train_metric: 0.013, test_metric: 0.012\n",
            "----\n",
            "Epoch 1901/2500, lr=0.000747\n",
            "--> train_loss: 0.012474, test_loss: 0.011543, train_metric: 0.012, test_metric: 0.012\n",
            "----\n",
            "Epoch 1902/2500, lr=0.000746\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.012459, test_loss: 0.011309, train_metric: 0.012, test_metric: 0.011\n",
            "----\n",
            "Epoch 1903/2500, lr=0.000746\n",
            "--> train_loss: 0.012579, test_loss: 0.012028, train_metric: 0.013, test_metric: 0.012\n",
            "----\n",
            "Epoch 1904/2500, lr=0.000745\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.012526, test_loss: 0.011251, train_metric: 0.013, test_metric: 0.011\n",
            "----\n",
            "Epoch 1905/2500, lr=0.000744\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.012527, test_loss: 0.011228, train_metric: 0.013, test_metric: 0.011\n",
            "----\n",
            "Epoch 1906/2500, lr=0.000743\n",
            "--> train_loss: 0.012314, test_loss: 0.011470, train_metric: 0.012, test_metric: 0.011\n",
            "----\n",
            "Epoch 1907/2500, lr=0.000743\n",
            "--> train_loss: 0.012066, test_loss: 0.011700, train_metric: 0.012, test_metric: 0.012\n",
            "----\n",
            "Epoch 1908/2500, lr=0.000742\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.012345, test_loss: 0.011211, train_metric: 0.012, test_metric: 0.011\n",
            "----\n",
            "Epoch 1909/2500, lr=0.000741\n",
            "--> train_loss: 0.012005, test_loss: 0.011298, train_metric: 0.012, test_metric: 0.011\n",
            "----\n",
            "Epoch 1910/2500, lr=0.000740\n",
            "--> train_loss: 0.012095, test_loss: 0.011391, train_metric: 0.012, test_metric: 0.011\n",
            "----\n",
            "Epoch 1911/2500, lr=0.000740\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.012080, test_loss: 0.010977, train_metric: 0.012, test_metric: 0.011\n",
            "----\n",
            "Epoch 1912/2500, lr=0.000739\n",
            "--> train_loss: 0.011847, test_loss: 0.011007, train_metric: 0.012, test_metric: 0.011\n",
            "----\n",
            "Epoch 1913/2500, lr=0.000738\n",
            "--> train_loss: 0.012074, test_loss: 0.011041, train_metric: 0.012, test_metric: 0.011\n",
            "----\n",
            "Epoch 1914/2500, lr=0.000737\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.011906, test_loss: 0.010488, train_metric: 0.012, test_metric: 0.010\n",
            "----\n",
            "Epoch 1915/2500, lr=0.000737\n",
            "--> train_loss: 0.011994, test_loss: 0.011215, train_metric: 0.012, test_metric: 0.011\n",
            "----\n",
            "Epoch 1916/2500, lr=0.000736\n",
            "--> train_loss: 0.012083, test_loss: 0.011092, train_metric: 0.012, test_metric: 0.011\n",
            "----\n",
            "Epoch 1917/2500, lr=0.000735\n",
            "--> train_loss: 0.011726, test_loss: 0.010759, train_metric: 0.012, test_metric: 0.011\n",
            "----\n",
            "Epoch 1918/2500, lr=0.000735\n",
            "--> train_loss: 0.011715, test_loss: 0.010541, train_metric: 0.012, test_metric: 0.011\n",
            "----\n",
            "Epoch 1919/2500, lr=0.000734\n",
            "--> train_loss: 0.011519, test_loss: 0.010518, train_metric: 0.012, test_metric: 0.011\n",
            "----\n",
            "Epoch 1920/2500, lr=0.000733\n",
            "--> train_loss: 0.011727, test_loss: 0.010943, train_metric: 0.012, test_metric: 0.011\n",
            "----\n",
            "Epoch 1921/2500, lr=0.000732\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.011575, test_loss: 0.010478, train_metric: 0.012, test_metric: 0.010\n",
            "----\n",
            "Epoch 1922/2500, lr=0.000732\n",
            "--> train_loss: 0.011553, test_loss: 0.011079, train_metric: 0.012, test_metric: 0.011\n",
            "----\n",
            "Epoch 1923/2500, lr=0.000731\n",
            "--> train_loss: 0.011409, test_loss: 0.010537, train_metric: 0.011, test_metric: 0.011\n",
            "----\n",
            "Epoch 1924/2500, lr=0.000730\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.011447, test_loss: 0.010021, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1925/2500, lr=0.000729\n",
            "--> train_loss: 0.011257, test_loss: 0.010709, train_metric: 0.011, test_metric: 0.011\n",
            "----\n",
            "Epoch 1926/2500, lr=0.000729\n",
            "--> train_loss: 0.011132, test_loss: 0.010471, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1927/2500, lr=0.000728\n",
            "--> train_loss: 0.011070, test_loss: 0.010151, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1928/2500, lr=0.000727\n",
            "--> train_loss: 0.011198, test_loss: 0.010175, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1929/2500, lr=0.000726\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.010982, test_loss: 0.009833, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1930/2500, lr=0.000726\n",
            "--> train_loss: 0.010993, test_loss: 0.009975, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1931/2500, lr=0.000725\n",
            "--> train_loss: 0.010906, test_loss: 0.010379, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1932/2500, lr=0.000724\n",
            "--> train_loss: 0.011081, test_loss: 0.009858, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1933/2500, lr=0.000724\n",
            "--> train_loss: 0.011068, test_loss: 0.010350, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1934/2500, lr=0.000723\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.010789, test_loss: 0.009685, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1935/2500, lr=0.000722\n",
            "--> train_loss: 0.011355, test_loss: 0.010003, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1936/2500, lr=0.000721\n",
            "--> train_loss: 0.011232, test_loss: 0.009911, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1937/2500, lr=0.000721\n",
            "--> train_loss: 0.011075, test_loss: 0.010677, train_metric: 0.011, test_metric: 0.011\n",
            "----\n",
            "Epoch 1938/2500, lr=0.000720\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.010877, test_loss: 0.009610, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1939/2500, lr=0.000719\n",
            "--> train_loss: 0.010872, test_loss: 0.010167, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1940/2500, lr=0.000719\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.011492, test_loss: 0.009477, train_metric: 0.011, test_metric: 0.009\n",
            "----\n",
            "Epoch 1941/2500, lr=0.000718\n",
            "--> train_loss: 0.010552, test_loss: 0.009787, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1942/2500, lr=0.000717\n",
            "--> train_loss: 0.011453, test_loss: 0.011633, train_metric: 0.011, test_metric: 0.012\n",
            "----\n",
            "Epoch 1943/2500, lr=0.000716\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.011118, test_loss: 0.009189, train_metric: 0.011, test_metric: 0.009\n",
            "----\n",
            "Epoch 1944/2500, lr=0.000716\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.011030, test_loss: 0.009001, train_metric: 0.011, test_metric: 0.009\n",
            "----\n",
            "Epoch 1945/2500, lr=0.000715\n",
            "--> train_loss: 0.010870, test_loss: 0.010330, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1946/2500, lr=0.000714\n",
            "--> train_loss: 0.010471, test_loss: 0.009703, train_metric: 0.010, test_metric: 0.010\n",
            "----\n",
            "Epoch 1947/2500, lr=0.000714\n",
            "--> train_loss: 0.010407, test_loss: 0.009013, train_metric: 0.010, test_metric: 0.009\n",
            "----\n",
            "Epoch 1948/2500, lr=0.000713\n",
            "--> train_loss: 0.011094, test_loss: 0.009144, train_metric: 0.011, test_metric: 0.009\n",
            "----\n",
            "Epoch 1949/2500, lr=0.000712\n",
            "--> train_loss: 0.011153, test_loss: 0.011219, train_metric: 0.011, test_metric: 0.011\n",
            "----\n",
            "Epoch 1950/2500, lr=0.000711\n",
            "--> train_loss: 0.010779, test_loss: 0.009203, train_metric: 0.011, test_metric: 0.009\n",
            "----\n",
            "Epoch 1951/2500, lr=0.000711\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.010501, test_loss: 0.008818, train_metric: 0.011, test_metric: 0.009\n",
            "----\n",
            "Epoch 1952/2500, lr=0.000710\n",
            "--> train_loss: 0.010582, test_loss: 0.009607, train_metric: 0.011, test_metric: 0.010\n",
            "----\n",
            "Epoch 1953/2500, lr=0.000709\n",
            "--> train_loss: 0.010370, test_loss: 0.009402, train_metric: 0.010, test_metric: 0.009\n",
            "----\n",
            "Epoch 1954/2500, lr=0.000709\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.010128, test_loss: 0.008739, train_metric: 0.010, test_metric: 0.009\n",
            "----\n",
            "Epoch 1955/2500, lr=0.000708\n",
            "--> train_loss: 0.010261, test_loss: 0.009235, train_metric: 0.010, test_metric: 0.009\n",
            "----\n",
            "Epoch 1956/2500, lr=0.000707\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.010060, test_loss: 0.008619, train_metric: 0.010, test_metric: 0.009\n",
            "----\n",
            "Epoch 1957/2500, lr=0.000706\n",
            "--> train_loss: 0.010138, test_loss: 0.008861, train_metric: 0.010, test_metric: 0.009\n",
            "----\n",
            "Epoch 1958/2500, lr=0.000706\n",
            "--> train_loss: 0.009890, test_loss: 0.008835, train_metric: 0.010, test_metric: 0.009\n",
            "----\n",
            "Epoch 1959/2500, lr=0.000705\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.009850, test_loss: 0.008576, train_metric: 0.010, test_metric: 0.009\n",
            "----\n",
            "Epoch 1960/2500, lr=0.000704\n",
            "--> train_loss: 0.009780, test_loss: 0.008818, train_metric: 0.010, test_metric: 0.009\n",
            "----\n",
            "Epoch 1961/2500, lr=0.000704\n",
            "--> train_loss: 0.009793, test_loss: 0.008995, train_metric: 0.010, test_metric: 0.009\n",
            "----\n",
            "Epoch 1962/2500, lr=0.000703\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.009775, test_loss: 0.008396, train_metric: 0.010, test_metric: 0.008\n",
            "----\n",
            "Epoch 1963/2500, lr=0.000702\n",
            "--> train_loss: 0.009683, test_loss: 0.008541, train_metric: 0.010, test_metric: 0.009\n",
            "----\n",
            "Epoch 1964/2500, lr=0.000701\n",
            "--> train_loss: 0.010030, test_loss: 0.009217, train_metric: 0.010, test_metric: 0.009\n",
            "----\n",
            "Epoch 1965/2500, lr=0.000701\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.009516, test_loss: 0.008251, train_metric: 0.010, test_metric: 0.008\n",
            "----\n",
            "Epoch 1966/2500, lr=0.000700\n",
            "--> train_loss: 0.009579, test_loss: 0.008256, train_metric: 0.010, test_metric: 0.008\n",
            "----\n",
            "Epoch 1967/2500, lr=0.000699\n",
            "--> train_loss: 0.010220, test_loss: 0.009298, train_metric: 0.010, test_metric: 0.009\n",
            "----\n",
            "Epoch 1968/2500, lr=0.000699\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.009891, test_loss: 0.008222, train_metric: 0.010, test_metric: 0.008\n",
            "----\n",
            "Epoch 1969/2500, lr=0.000698\n",
            "--> train_loss: 0.009821, test_loss: 0.008307, train_metric: 0.010, test_metric: 0.008\n",
            "----\n",
            "Epoch 1970/2500, lr=0.000697\n",
            "--> train_loss: 0.009535, test_loss: 0.008456, train_metric: 0.010, test_metric: 0.008\n",
            "----\n",
            "Epoch 1971/2500, lr=0.000697\n",
            "--> train_loss: 0.010151, test_loss: 0.008792, train_metric: 0.010, test_metric: 0.009\n",
            "----\n",
            "Epoch 1972/2500, lr=0.000696\n",
            "--> train_loss: 0.009465, test_loss: 0.008359, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 1973/2500, lr=0.000695\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.009589, test_loss: 0.008217, train_metric: 0.010, test_metric: 0.008\n",
            "----\n",
            "Epoch 1974/2500, lr=0.000695\n",
            "--> train_loss: 0.009202, test_loss: 0.008580, train_metric: 0.009, test_metric: 0.009\n",
            "----\n",
            "Epoch 1975/2500, lr=0.000694\n",
            "--> train_loss: 0.009435, test_loss: 0.008402, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 1976/2500, lr=0.000693\n",
            "--> train_loss: 0.009263, test_loss: 0.008217, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 1977/2500, lr=0.000692\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.009204, test_loss: 0.007985, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 1978/2500, lr=0.000692\n",
            "--> train_loss: 0.009499, test_loss: 0.008448, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 1979/2500, lr=0.000691\n",
            "--> train_loss: 0.009385, test_loss: 0.008660, train_metric: 0.009, test_metric: 0.009\n",
            "----\n",
            "Epoch 1980/2500, lr=0.000690\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.008991, test_loss: 0.007558, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 1981/2500, lr=0.000690\n",
            "--> train_loss: 0.009367, test_loss: 0.007712, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 1982/2500, lr=0.000689\n",
            "--> train_loss: 0.009055, test_loss: 0.007999, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 1983/2500, lr=0.000688\n",
            "--> train_loss: 0.009633, test_loss: 0.008364, train_metric: 0.010, test_metric: 0.008\n",
            "----\n",
            "Epoch 1984/2500, lr=0.000688\n",
            "--> train_loss: 0.008979, test_loss: 0.008366, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 1985/2500, lr=0.000687\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.009091, test_loss: 0.007372, train_metric: 0.009, test_metric: 0.007\n",
            "----\n",
            "Epoch 1986/2500, lr=0.000686\n",
            "--> train_loss: 0.009185, test_loss: 0.008187, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 1987/2500, lr=0.000686\n",
            "--> train_loss: 0.008937, test_loss: 0.008067, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 1988/2500, lr=0.000685\n",
            "--> train_loss: 0.008898, test_loss: 0.007646, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 1989/2500, lr=0.000684\n",
            "--> train_loss: 0.008849, test_loss: 0.007935, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 1990/2500, lr=0.000683\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.008922, test_loss: 0.007243, train_metric: 0.009, test_metric: 0.007\n",
            "----\n",
            "Epoch 1991/2500, lr=0.000683\n",
            "--> train_loss: 0.008800, test_loss: 0.007873, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 1992/2500, lr=0.000682\n",
            "--> train_loss: 0.008820, test_loss: 0.007579, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 1993/2500, lr=0.000681\n",
            "--> train_loss: 0.008797, test_loss: 0.007786, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 1994/2500, lr=0.000681\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.008745, test_loss: 0.007202, train_metric: 0.009, test_metric: 0.007\n",
            "----\n",
            "Epoch 1995/2500, lr=0.000680\n",
            "--> train_loss: 0.009085, test_loss: 0.007239, train_metric: 0.009, test_metric: 0.007\n",
            "----\n",
            "Epoch 1996/2500, lr=0.000679\n",
            "--> train_loss: 0.008470, test_loss: 0.008136, train_metric: 0.008, test_metric: 0.008\n",
            "----\n",
            "Epoch 1997/2500, lr=0.000679\n",
            "--> train_loss: 0.008662, test_loss: 0.007225, train_metric: 0.009, test_metric: 0.007\n",
            "----\n",
            "Epoch 1998/2500, lr=0.000678\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.008325, test_loss: 0.007198, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 1999/2500, lr=0.000677\n",
            "--> train_loss: 0.009074, test_loss: 0.007203, train_metric: 0.009, test_metric: 0.007\n",
            "----\n",
            "Epoch 2000/2500, lr=0.000677\n",
            "--> train_loss: 0.008609, test_loss: 0.008043, train_metric: 0.009, test_metric: 0.008\n",
            "----\n",
            "Epoch 2001/2500, lr=0.000676\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.008298, test_loss: 0.007147, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2002/2500, lr=0.000675\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.008545, test_loss: 0.006820, train_metric: 0.009, test_metric: 0.007\n",
            "----\n",
            "Epoch 2003/2500, lr=0.000675\n",
            "--> train_loss: 0.008295, test_loss: 0.007469, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2004/2500, lr=0.000674\n",
            "--> train_loss: 0.008399, test_loss: 0.007188, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2005/2500, lr=0.000673\n",
            "--> train_loss: 0.008348, test_loss: 0.007429, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2006/2500, lr=0.000673\n",
            "--> train_loss: 0.008434, test_loss: 0.007514, train_metric: 0.008, test_metric: 0.008\n",
            "----\n",
            "Epoch 2007/2500, lr=0.000672\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.008379, test_loss: 0.006573, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2008/2500, lr=0.000671\n",
            "--> train_loss: 0.008080, test_loss: 0.006987, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2009/2500, lr=0.000671\n",
            "--> train_loss: 0.008268, test_loss: 0.007014, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2010/2500, lr=0.000670\n",
            "--> train_loss: 0.008102, test_loss: 0.007108, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2011/2500, lr=0.000669\n",
            "--> train_loss: 0.008267, test_loss: 0.007029, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2012/2500, lr=0.000669\n",
            "--> train_loss: 0.008656, test_loss: 0.006764, train_metric: 0.009, test_metric: 0.007\n",
            "----\n",
            "Epoch 2013/2500, lr=0.000668\n",
            "--> train_loss: 0.008170, test_loss: 0.007765, train_metric: 0.008, test_metric: 0.008\n",
            "----\n",
            "Epoch 2014/2500, lr=0.000667\n",
            "--> train_loss: 0.008376, test_loss: 0.007301, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2015/2500, lr=0.000667\n",
            "--> train_loss: 0.008337, test_loss: 0.007091, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2016/2500, lr=0.000666\n",
            "--> train_loss: 0.008010, test_loss: 0.006613, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2017/2500, lr=0.000665\n",
            "--> train_loss: 0.008043, test_loss: 0.006790, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2018/2500, lr=0.000665\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.008170, test_loss: 0.006460, train_metric: 0.008, test_metric: 0.006\n",
            "----\n",
            "Epoch 2019/2500, lr=0.000664\n",
            "--> train_loss: 0.008020, test_loss: 0.007516, train_metric: 0.008, test_metric: 0.008\n",
            "----\n",
            "Epoch 2020/2500, lr=0.000663\n",
            "--> train_loss: 0.008234, test_loss: 0.006683, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2021/2500, lr=0.000663\n",
            "--> train_loss: 0.007800, test_loss: 0.006683, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2022/2500, lr=0.000662\n",
            "--> train_loss: 0.007987, test_loss: 0.006615, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2023/2500, lr=0.000661\n",
            "--> train_loss: 0.008167, test_loss: 0.007098, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2024/2500, lr=0.000661\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.007924, test_loss: 0.006370, train_metric: 0.008, test_metric: 0.006\n",
            "----\n",
            "Epoch 2025/2500, lr=0.000660\n",
            "--> train_loss: 0.007607, test_loss: 0.006438, train_metric: 0.008, test_metric: 0.006\n",
            "----\n",
            "Epoch 2026/2500, lr=0.000659\n",
            "--> train_loss: 0.007809, test_loss: 0.006775, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2027/2500, lr=0.000659\n",
            "--> train_loss: 0.007555, test_loss: 0.006413, train_metric: 0.008, test_metric: 0.006\n",
            "----\n",
            "Epoch 2028/2500, lr=0.000658\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.007610, test_loss: 0.006045, train_metric: 0.008, test_metric: 0.006\n",
            "----\n",
            "Epoch 2029/2500, lr=0.000657\n",
            "--> train_loss: 0.007591, test_loss: 0.006764, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2030/2500, lr=0.000657\n",
            "--> train_loss: 0.007678, test_loss: 0.006761, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2031/2500, lr=0.000656\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.007470, test_loss: 0.005901, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2032/2500, lr=0.000655\n",
            "--> train_loss: 0.007629, test_loss: 0.005966, train_metric: 0.008, test_metric: 0.006\n",
            "----\n",
            "Epoch 2033/2500, lr=0.000655\n",
            "--> train_loss: 0.007343, test_loss: 0.006680, train_metric: 0.007, test_metric: 0.007\n",
            "----\n",
            "Epoch 2034/2500, lr=0.000654\n",
            "--> train_loss: 0.007507, test_loss: 0.006523, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2035/2500, lr=0.000653\n",
            "--> train_loss: 0.007260, test_loss: 0.005951, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2036/2500, lr=0.000653\n",
            "--> train_loss: 0.007410, test_loss: 0.006077, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2037/2500, lr=0.000652\n",
            "--> train_loss: 0.007538, test_loss: 0.006101, train_metric: 0.008, test_metric: 0.006\n",
            "----\n",
            "Epoch 2038/2500, lr=0.000651\n",
            "--> train_loss: 0.007246, test_loss: 0.006469, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2039/2500, lr=0.000651\n",
            "--> train_loss: 0.007311, test_loss: 0.006166, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2040/2500, lr=0.000650\n",
            "--> train_loss: 0.007572, test_loss: 0.005958, train_metric: 0.008, test_metric: 0.006\n",
            "----\n",
            "Epoch 2041/2500, lr=0.000649\n",
            "--> train_loss: 0.007184, test_loss: 0.006504, train_metric: 0.007, test_metric: 0.007\n",
            "----\n",
            "Epoch 2042/2500, lr=0.000649\n",
            "--> train_loss: 0.007921, test_loss: 0.007089, train_metric: 0.008, test_metric: 0.007\n",
            "----\n",
            "Epoch 2043/2500, lr=0.000648\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.007201, test_loss: 0.005752, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2044/2500, lr=0.000648\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.007333, test_loss: 0.005610, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2045/2500, lr=0.000647\n",
            "--> train_loss: 0.007147, test_loss: 0.005804, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2046/2500, lr=0.000646\n",
            "--> train_loss: 0.007475, test_loss: 0.006708, train_metric: 0.007, test_metric: 0.007\n",
            "----\n",
            "Epoch 2047/2500, lr=0.000646\n",
            "--> train_loss: 0.007020, test_loss: 0.005666, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2048/2500, lr=0.000645\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.007309, test_loss: 0.005598, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2049/2500, lr=0.000644\n",
            "--> train_loss: 0.007014, test_loss: 0.006222, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2050/2500, lr=0.000644\n",
            "--> train_loss: 0.007667, test_loss: 0.006361, train_metric: 0.008, test_metric: 0.006\n",
            "----\n",
            "Epoch 2051/2500, lr=0.000643\n",
            "--> train_loss: 0.007024, test_loss: 0.005677, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2052/2500, lr=0.000642\n",
            "--> train_loss: 0.007311, test_loss: 0.005685, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2053/2500, lr=0.000642\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.007005, test_loss: 0.005502, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2054/2500, lr=0.000641\n",
            "--> train_loss: 0.006965, test_loss: 0.005781, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2055/2500, lr=0.000640\n",
            "--> train_loss: 0.007135, test_loss: 0.006218, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2056/2500, lr=0.000640\n",
            "--> train_loss: 0.006841, test_loss: 0.005513, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2057/2500, lr=0.000639\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.007101, test_loss: 0.005491, train_metric: 0.007, test_metric: 0.005\n",
            "----\n",
            "Epoch 2058/2500, lr=0.000639\n",
            "--> train_loss: 0.006803, test_loss: 0.005651, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2059/2500, lr=0.000638\n",
            "--> train_loss: 0.006956, test_loss: 0.006232, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2060/2500, lr=0.000637\n",
            "--> train_loss: 0.006859, test_loss: 0.005668, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2061/2500, lr=0.000637\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.007269, test_loss: 0.005360, train_metric: 0.007, test_metric: 0.005\n",
            "----\n",
            "Epoch 2062/2500, lr=0.000636\n",
            "--> train_loss: 0.006832, test_loss: 0.005873, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2063/2500, lr=0.000635\n",
            "--> train_loss: 0.006823, test_loss: 0.005504, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2064/2500, lr=0.000635\n",
            "--> train_loss: 0.006848, test_loss: 0.005541, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2065/2500, lr=0.000634\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.006623, test_loss: 0.005023, train_metric: 0.007, test_metric: 0.005\n",
            "----\n",
            "Epoch 2066/2500, lr=0.000633\n",
            "--> train_loss: 0.006565, test_loss: 0.005560, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2067/2500, lr=0.000633\n",
            "--> train_loss: 0.006540, test_loss: 0.005661, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2068/2500, lr=0.000632\n",
            "--> train_loss: 0.007435, test_loss: 0.005068, train_metric: 0.007, test_metric: 0.005\n",
            "----\n",
            "Epoch 2069/2500, lr=0.000632\n",
            "--> train_loss: 0.006942, test_loss: 0.006071, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2070/2500, lr=0.000631\n",
            "--> train_loss: 0.006612, test_loss: 0.005806, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2071/2500, lr=0.000630\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.006588, test_loss: 0.004952, train_metric: 0.007, test_metric: 0.005\n",
            "----\n",
            "Epoch 2072/2500, lr=0.000630\n",
            "--> train_loss: 0.006690, test_loss: 0.005009, train_metric: 0.007, test_metric: 0.005\n",
            "----\n",
            "Epoch 2073/2500, lr=0.000629\n",
            "--> train_loss: 0.007266, test_loss: 0.006124, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2074/2500, lr=0.000628\n",
            "--> train_loss: 0.006762, test_loss: 0.005768, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2075/2500, lr=0.000628\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.006617, test_loss: 0.004904, train_metric: 0.007, test_metric: 0.005\n",
            "----\n",
            "Epoch 2076/2500, lr=0.000627\n",
            "--> train_loss: 0.006425, test_loss: 0.005301, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2077/2500, lr=0.000627\n",
            "--> train_loss: 0.006420, test_loss: 0.005431, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2078/2500, lr=0.000626\n",
            "--> train_loss: 0.006305, test_loss: 0.005134, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2079/2500, lr=0.000625\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.006685, test_loss: 0.004743, train_metric: 0.007, test_metric: 0.005\n",
            "----\n",
            "Epoch 2080/2500, lr=0.000625\n",
            "--> train_loss: 0.006272, test_loss: 0.005235, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2081/2500, lr=0.000624\n",
            "--> train_loss: 0.007304, test_loss: 0.006073, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2082/2500, lr=0.000623\n",
            "--> train_loss: 0.006435, test_loss: 0.005602, train_metric: 0.006, test_metric: 0.006\n",
            "----\n",
            "Epoch 2083/2500, lr=0.000623\n",
            "--> train_loss: 0.006936, test_loss: 0.005215, train_metric: 0.007, test_metric: 0.005\n",
            "----\n",
            "Epoch 2084/2500, lr=0.000622\n",
            "--> train_loss: 0.006880, test_loss: 0.005205, train_metric: 0.007, test_metric: 0.005\n",
            "----\n",
            "Epoch 2085/2500, lr=0.000622\n",
            "--> train_loss: 0.006915, test_loss: 0.005286, train_metric: 0.007, test_metric: 0.005\n",
            "----\n",
            "Epoch 2086/2500, lr=0.000621\n",
            "--> train_loss: 0.006803, test_loss: 0.006187, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2087/2500, lr=0.000620\n",
            "--> train_loss: 0.006686, test_loss: 0.005935, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2088/2500, lr=0.000620\n",
            "--> train_loss: 0.007141, test_loss: 0.004817, train_metric: 0.007, test_metric: 0.005\n",
            "----\n",
            "Epoch 2089/2500, lr=0.000619\n",
            "--> train_loss: 0.006724, test_loss: 0.005766, train_metric: 0.007, test_metric: 0.006\n",
            "----\n",
            "Epoch 2090/2500, lr=0.000618\n",
            "--> train_loss: 0.006848, test_loss: 0.004934, train_metric: 0.007, test_metric: 0.005\n",
            "----\n",
            "Epoch 2091/2500, lr=0.000618\n",
            "--> train_loss: 0.006235, test_loss: 0.005685, train_metric: 0.006, test_metric: 0.006\n",
            "----\n",
            "Epoch 2092/2500, lr=0.000617\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.006322, test_loss: 0.004720, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2093/2500, lr=0.000617\n",
            "--> train_loss: 0.005982, test_loss: 0.004930, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2094/2500, lr=0.000616\n",
            "--> train_loss: 0.006276, test_loss: 0.005247, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2095/2500, lr=0.000615\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.006007, test_loss: 0.004618, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2096/2500, lr=0.000615\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.006124, test_loss: 0.004559, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2097/2500, lr=0.000614\n",
            "--> train_loss: 0.005925, test_loss: 0.005043, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2098/2500, lr=0.000613\n",
            "--> train_loss: 0.006131, test_loss: 0.004815, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2099/2500, lr=0.000613\n",
            "--> train_loss: 0.005859, test_loss: 0.004815, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2100/2500, lr=0.000612\n",
            "--> train_loss: 0.006178, test_loss: 0.004904, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2101/2500, lr=0.000612\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.006149, test_loss: 0.004409, train_metric: 0.006, test_metric: 0.004\n",
            "----\n",
            "Epoch 2102/2500, lr=0.000611\n",
            "--> train_loss: 0.005715, test_loss: 0.004691, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2103/2500, lr=0.000610\n",
            "--> train_loss: 0.005786, test_loss: 0.005022, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2104/2500, lr=0.000610\n",
            "--> train_loss: 0.005967, test_loss: 0.004579, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2105/2500, lr=0.000609\n",
            "--> train_loss: 0.005784, test_loss: 0.004475, train_metric: 0.006, test_metric: 0.004\n",
            "----\n",
            "Epoch 2106/2500, lr=0.000609\n",
            "--> train_loss: 0.005867, test_loss: 0.004544, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2107/2500, lr=0.000608\n",
            "--> train_loss: 0.005700, test_loss: 0.004802, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2108/2500, lr=0.000607\n",
            "--> train_loss: 0.006033, test_loss: 0.004412, train_metric: 0.006, test_metric: 0.004\n",
            "----\n",
            "Epoch 2109/2500, lr=0.000607\n",
            "--> train_loss: 0.005971, test_loss: 0.004975, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2110/2500, lr=0.000606\n",
            "--> train_loss: 0.005702, test_loss: 0.004797, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2111/2500, lr=0.000606\n",
            "--> train_loss: 0.005715, test_loss: 0.004683, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2112/2500, lr=0.000605\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.005808, test_loss: 0.004173, train_metric: 0.006, test_metric: 0.004\n",
            "----\n",
            "Epoch 2113/2500, lr=0.000604\n",
            "--> train_loss: 0.005684, test_loss: 0.004710, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2114/2500, lr=0.000604\n",
            "--> train_loss: 0.005595, test_loss: 0.004425, train_metric: 0.006, test_metric: 0.004\n",
            "----\n",
            "Epoch 2115/2500, lr=0.000603\n",
            "--> train_loss: 0.005734, test_loss: 0.004549, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2116/2500, lr=0.000603\n",
            "--> train_loss: 0.005782, test_loss: 0.004236, train_metric: 0.006, test_metric: 0.004\n",
            "----\n",
            "Epoch 2117/2500, lr=0.000602\n",
            "--> train_loss: 0.005948, test_loss: 0.004358, train_metric: 0.006, test_metric: 0.004\n",
            "----\n",
            "Epoch 2118/2500, lr=0.000601\n",
            "--> train_loss: 0.005472, test_loss: 0.004415, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2119/2500, lr=0.000601\n",
            "--> train_loss: 0.005519, test_loss: 0.004507, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2120/2500, lr=0.000600\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.005783, test_loss: 0.004072, train_metric: 0.006, test_metric: 0.004\n",
            "----\n",
            "Epoch 2121/2500, lr=0.000600\n",
            "--> train_loss: 0.005456, test_loss: 0.004357, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2122/2500, lr=0.000599\n",
            "--> train_loss: 0.005543, test_loss: 0.004815, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2123/2500, lr=0.000598\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.005451, test_loss: 0.004039, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2124/2500, lr=0.000598\n",
            "--> train_loss: 0.005463, test_loss: 0.004202, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2125/2500, lr=0.000597\n",
            "--> train_loss: 0.005441, test_loss: 0.004301, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2126/2500, lr=0.000597\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.005368, test_loss: 0.004020, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2127/2500, lr=0.000596\n",
            "--> train_loss: 0.005512, test_loss: 0.004241, train_metric: 0.006, test_metric: 0.004\n",
            "----\n",
            "Epoch 2128/2500, lr=0.000595\n",
            "--> train_loss: 0.005706, test_loss: 0.004642, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2129/2500, lr=0.000595\n",
            "--> train_loss: 0.005476, test_loss: 0.004218, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2130/2500, lr=0.000594\n",
            "--> train_loss: 0.005395, test_loss: 0.004191, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2131/2500, lr=0.000594\n",
            "--> train_loss: 0.005310, test_loss: 0.004102, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2132/2500, lr=0.000593\n",
            "--> train_loss: 0.005306, test_loss: 0.004110, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2133/2500, lr=0.000592\n",
            "--> train_loss: 0.005471, test_loss: 0.004404, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2134/2500, lr=0.000592\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.005295, test_loss: 0.003930, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2135/2500, lr=0.000591\n",
            "--> train_loss: 0.005483, test_loss: 0.004240, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2136/2500, lr=0.000591\n",
            "--> train_loss: 0.005169, test_loss: 0.003932, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2137/2500, lr=0.000590\n",
            "--> train_loss: 0.005461, test_loss: 0.004211, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2138/2500, lr=0.000589\n",
            "--> train_loss: 0.005057, test_loss: 0.003992, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2139/2500, lr=0.000589\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.005089, test_loss: 0.003777, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2140/2500, lr=0.000588\n",
            "--> train_loss: 0.005020, test_loss: 0.003933, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2141/2500, lr=0.000588\n",
            "--> train_loss: 0.005091, test_loss: 0.004232, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2142/2500, lr=0.000587\n",
            "--> train_loss: 0.005058, test_loss: 0.004083, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2143/2500, lr=0.000586\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.004992, test_loss: 0.003705, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2144/2500, lr=0.000586\n",
            "--> train_loss: 0.005006, test_loss: 0.003856, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2145/2500, lr=0.000585\n",
            "--> train_loss: 0.004952, test_loss: 0.004158, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2146/2500, lr=0.000585\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.005215, test_loss: 0.003614, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2147/2500, lr=0.000584\n",
            "--> train_loss: 0.005032, test_loss: 0.004180, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2148/2500, lr=0.000584\n",
            "--> train_loss: 0.005058, test_loss: 0.004379, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2149/2500, lr=0.000583\n",
            "--> train_loss: 0.005001, test_loss: 0.003673, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2150/2500, lr=0.000582\n",
            "--> train_loss: 0.005149, test_loss: 0.003642, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2151/2500, lr=0.000582\n",
            "--> train_loss: 0.005095, test_loss: 0.003979, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2152/2500, lr=0.000581\n",
            "--> train_loss: 0.005062, test_loss: 0.003785, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2153/2500, lr=0.000581\n",
            "--> train_loss: 0.004785, test_loss: 0.003639, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2154/2500, lr=0.000580\n",
            "--> train_loss: 0.005021, test_loss: 0.003706, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2155/2500, lr=0.000579\n",
            "--> train_loss: 0.004922, test_loss: 0.004048, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2156/2500, lr=0.000579\n",
            "--> train_loss: 0.004955, test_loss: 0.003844, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2157/2500, lr=0.000578\n",
            "--> train_loss: 0.005100, test_loss: 0.003623, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2158/2500, lr=0.000578\n",
            "--> train_loss: 0.004784, test_loss: 0.003899, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2159/2500, lr=0.000577\n",
            "--> train_loss: 0.004989, test_loss: 0.004150, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2160/2500, lr=0.000577\n",
            "--> train_loss: 0.004898, test_loss: 0.003778, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2161/2500, lr=0.000576\n",
            "--> train_loss: 0.004930, test_loss: 0.003835, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2162/2500, lr=0.000575\n",
            "--> train_loss: 0.004945, test_loss: 0.003657, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2163/2500, lr=0.000575\n",
            "--> train_loss: 0.004822, test_loss: 0.003911, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2164/2500, lr=0.000574\n",
            "--> train_loss: 0.004705, test_loss: 0.003618, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2165/2500, lr=0.000574\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.004898, test_loss: 0.003317, train_metric: 0.005, test_metric: 0.003\n",
            "----\n",
            "Epoch 2166/2500, lr=0.000573\n",
            "--> train_loss: 0.004738, test_loss: 0.003804, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2167/2500, lr=0.000573\n",
            "--> train_loss: 0.004789, test_loss: 0.003761, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2168/2500, lr=0.000572\n",
            "--> train_loss: 0.004744, test_loss: 0.003580, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2169/2500, lr=0.000571\n",
            "--> train_loss: 0.004603, test_loss: 0.003559, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2170/2500, lr=0.000571\n",
            "--> train_loss: 0.004773, test_loss: 0.003643, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2171/2500, lr=0.000570\n",
            "--> train_loss: 0.004639, test_loss: 0.003635, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2172/2500, lr=0.000570\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.004802, test_loss: 0.003301, train_metric: 0.005, test_metric: 0.003\n",
            "----\n",
            "Epoch 2173/2500, lr=0.000569\n",
            "--> train_loss: 0.004669, test_loss: 0.003678, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2174/2500, lr=0.000569\n",
            "--> train_loss: 0.005059, test_loss: 0.004548, train_metric: 0.005, test_metric: 0.005\n",
            "----\n",
            "Epoch 2175/2500, lr=0.000568\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.005032, test_loss: 0.003284, train_metric: 0.005, test_metric: 0.003\n",
            "----\n",
            "Epoch 2176/2500, lr=0.000567\n",
            "--> train_loss: 0.004953, test_loss: 0.003580, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2177/2500, lr=0.000567\n",
            "--> train_loss: 0.004776, test_loss: 0.004214, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2178/2500, lr=0.000566\n",
            "--> train_loss: 0.004804, test_loss: 0.003310, train_metric: 0.005, test_metric: 0.003\n",
            "----\n",
            "Epoch 2179/2500, lr=0.000566\n",
            "--> train_loss: 0.004559, test_loss: 0.003591, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2180/2500, lr=0.000565\n",
            "--> train_loss: 0.004578, test_loss: 0.003534, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2181/2500, lr=0.000565\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.004575, test_loss: 0.003206, train_metric: 0.005, test_metric: 0.003\n",
            "----\n",
            "Epoch 2182/2500, lr=0.000564\n",
            "--> train_loss: 0.004675, test_loss: 0.003569, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2183/2500, lr=0.000563\n",
            "--> train_loss: 0.004588, test_loss: 0.003676, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2184/2500, lr=0.000563\n",
            "--> train_loss: 0.004625, test_loss: 0.003289, train_metric: 0.005, test_metric: 0.003\n",
            "----\n",
            "Epoch 2185/2500, lr=0.000562\n",
            "--> train_loss: 0.004660, test_loss: 0.003715, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2186/2500, lr=0.000562\n",
            "--> train_loss: 0.004574, test_loss: 0.003421, train_metric: 0.005, test_metric: 0.003\n",
            "----\n",
            "Epoch 2187/2500, lr=0.000561\n",
            "--> train_loss: 0.004691, test_loss: 0.003532, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2188/2500, lr=0.000561\n",
            "--> train_loss: 0.004552, test_loss: 0.003554, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2189/2500, lr=0.000560\n",
            "--> train_loss: 0.004617, test_loss: 0.003633, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2190/2500, lr=0.000560\n",
            "--> train_loss: 0.004551, test_loss: 0.003308, train_metric: 0.005, test_metric: 0.003\n",
            "----\n",
            "Epoch 2191/2500, lr=0.000559\n",
            "--> train_loss: 0.004823, test_loss: 0.003557, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2192/2500, lr=0.000558\n",
            "--> train_loss: 0.005719, test_loss: 0.004673, train_metric: 0.006, test_metric: 0.005\n",
            "----\n",
            "Epoch 2193/2500, lr=0.000558\n",
            "--> train_loss: 0.005881, test_loss: 0.003209, train_metric: 0.006, test_metric: 0.003\n",
            "----\n",
            "Epoch 2194/2500, lr=0.000557\n",
            "--> train_loss: 0.004594, test_loss: 0.003496, train_metric: 0.005, test_metric: 0.003\n",
            "----\n",
            "Epoch 2195/2500, lr=0.000557\n",
            "--> train_loss: 0.004539, test_loss: 0.003435, train_metric: 0.005, test_metric: 0.003\n",
            "----\n",
            "Epoch 2196/2500, lr=0.000556\n",
            "--> train_loss: 0.004329, test_loss: 0.003383, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2197/2500, lr=0.000556\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.004260, test_loss: 0.003094, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2198/2500, lr=0.000555\n",
            "--> train_loss: 0.004346, test_loss: 0.003133, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2199/2500, lr=0.000555\n",
            "--> train_loss: 0.004385, test_loss: 0.003605, train_metric: 0.004, test_metric: 0.004\n",
            "----\n",
            "Epoch 2200/2500, lr=0.000554\n",
            "--> train_loss: 0.004554, test_loss: 0.003109, train_metric: 0.005, test_metric: 0.003\n",
            "----\n",
            "Epoch 2201/2500, lr=0.000553\n",
            "--> train_loss: 0.004284, test_loss: 0.003218, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2202/2500, lr=0.000553\n",
            "--> train_loss: 0.004249, test_loss: 0.003318, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2203/2500, lr=0.000552\n",
            "--> train_loss: 0.004376, test_loss: 0.003133, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2204/2500, lr=0.000552\n",
            "--> train_loss: 0.004304, test_loss: 0.003186, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2205/2500, lr=0.000551\n",
            "--> train_loss: 0.004149, test_loss: 0.003318, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2206/2500, lr=0.000551\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.004129, test_loss: 0.003049, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2207/2500, lr=0.000550\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.004130, test_loss: 0.003016, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2208/2500, lr=0.000550\n",
            "--> train_loss: 0.004061, test_loss: 0.003300, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2209/2500, lr=0.000549\n",
            "--> train_loss: 0.004160, test_loss: 0.003090, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2210/2500, lr=0.000548\n",
            "--> train_loss: 0.004370, test_loss: 0.003405, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2211/2500, lr=0.000548\n",
            "--> train_loss: 0.004865, test_loss: 0.003051, train_metric: 0.005, test_metric: 0.003\n",
            "----\n",
            "Epoch 2212/2500, lr=0.000547\n",
            "--> train_loss: 0.004617, test_loss: 0.003505, train_metric: 0.005, test_metric: 0.004\n",
            "----\n",
            "Epoch 2213/2500, lr=0.000547\n",
            "--> train_loss: 0.004158, test_loss: 0.003114, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2214/2500, lr=0.000546\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.004184, test_loss: 0.002840, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2215/2500, lr=0.000546\n",
            "--> train_loss: 0.003975, test_loss: 0.003100, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2216/2500, lr=0.000545\n",
            "--> train_loss: 0.004145, test_loss: 0.003284, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2217/2500, lr=0.000545\n",
            "--> train_loss: 0.003999, test_loss: 0.002880, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2218/2500, lr=0.000544\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.003976, test_loss: 0.002816, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2219/2500, lr=0.000544\n",
            "--> train_loss: 0.003943, test_loss: 0.003003, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2220/2500, lr=0.000543\n",
            "--> train_loss: 0.004009, test_loss: 0.003120, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2221/2500, lr=0.000542\n",
            "--> train_loss: 0.004121, test_loss: 0.003106, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2222/2500, lr=0.000542\n",
            "--> train_loss: 0.004124, test_loss: 0.002964, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2223/2500, lr=0.000541\n",
            "--> train_loss: 0.004020, test_loss: 0.003160, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2224/2500, lr=0.000541\n",
            "--> train_loss: 0.003991, test_loss: 0.002888, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2225/2500, lr=0.000540\n",
            "--> train_loss: 0.004145, test_loss: 0.003016, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2226/2500, lr=0.000540\n",
            "--> train_loss: 0.004381, test_loss: 0.003229, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2227/2500, lr=0.000539\n",
            "--> train_loss: 0.004094, test_loss: 0.003254, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2228/2500, lr=0.000539\n",
            "--> train_loss: 0.004206, test_loss: 0.003372, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2229/2500, lr=0.000538\n",
            "--> train_loss: 0.004060, test_loss: 0.002986, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2230/2500, lr=0.000538\n",
            "--> train_loss: 0.004031, test_loss: 0.002837, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2231/2500, lr=0.000537\n",
            "--> train_loss: 0.004006, test_loss: 0.003153, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2232/2500, lr=0.000537\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.004153, test_loss: 0.002703, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2233/2500, lr=0.000536\n",
            "--> train_loss: 0.003869, test_loss: 0.002789, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2234/2500, lr=0.000535\n",
            "--> train_loss: 0.003942, test_loss: 0.003130, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2235/2500, lr=0.000535\n",
            "--> train_loss: 0.003880, test_loss: 0.002708, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2236/2500, lr=0.000534\n",
            "--> train_loss: 0.003761, test_loss: 0.002729, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2237/2500, lr=0.000534\n",
            "--> train_loss: 0.003823, test_loss: 0.002909, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2238/2500, lr=0.000533\n",
            "--> train_loss: 0.003895, test_loss: 0.002855, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2239/2500, lr=0.000533\n",
            "--> train_loss: 0.004107, test_loss: 0.002989, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2240/2500, lr=0.000532\n",
            "--> train_loss: 0.003811, test_loss: 0.002920, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2241/2500, lr=0.000532\n",
            "--> train_loss: 0.004188, test_loss: 0.002816, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2242/2500, lr=0.000531\n",
            "--> train_loss: 0.003947, test_loss: 0.002799, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2243/2500, lr=0.000531\n",
            "--> train_loss: 0.004006, test_loss: 0.003090, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2244/2500, lr=0.000530\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.003916, test_loss: 0.002633, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2245/2500, lr=0.000530\n",
            "--> train_loss: 0.003745, test_loss: 0.002723, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2246/2500, lr=0.000529\n",
            "--> train_loss: 0.003675, test_loss: 0.002839, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2247/2500, lr=0.000529\n",
            "--> train_loss: 0.003763, test_loss: 0.002654, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2248/2500, lr=0.000528\n",
            "--> train_loss: 0.003661, test_loss: 0.002742, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2249/2500, lr=0.000527\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.003569, test_loss: 0.002556, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2250/2500, lr=0.000527\n",
            "--> train_loss: 0.003575, test_loss: 0.002686, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2251/2500, lr=0.000526\n",
            "--> train_loss: 0.003649, test_loss: 0.002744, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2252/2500, lr=0.000526\n",
            "--> train_loss: 0.003687, test_loss: 0.002709, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2253/2500, lr=0.000525\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.003621, test_loss: 0.002466, train_metric: 0.004, test_metric: 0.002\n",
            "----\n",
            "Epoch 2254/2500, lr=0.000525\n",
            "--> train_loss: 0.003597, test_loss: 0.002651, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2255/2500, lr=0.000524\n",
            "--> train_loss: 0.003551, test_loss: 0.002782, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2256/2500, lr=0.000524\n",
            "--> train_loss: 0.003565, test_loss: 0.002560, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2257/2500, lr=0.000523\n",
            "--> train_loss: 0.003532, test_loss: 0.002573, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2258/2500, lr=0.000523\n",
            "--> train_loss: 0.003586, test_loss: 0.002736, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2259/2500, lr=0.000522\n",
            "--> train_loss: 0.003732, test_loss: 0.002891, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2260/2500, lr=0.000522\n",
            "--> train_loss: 0.003456, test_loss: 0.002525, train_metric: 0.003, test_metric: 0.003\n",
            "----\n",
            "Epoch 2261/2500, lr=0.000521\n",
            "--> train_loss: 0.004190, test_loss: 0.002656, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2262/2500, lr=0.000521\n",
            "--> train_loss: 0.003851, test_loss: 0.003036, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2263/2500, lr=0.000520\n",
            "--> train_loss: 0.003570, test_loss: 0.002747, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2264/2500, lr=0.000520\n",
            "--> train_loss: 0.003495, test_loss: 0.002534, train_metric: 0.003, test_metric: 0.003\n",
            "----\n",
            "Epoch 2265/2500, lr=0.000519\n",
            "--> train_loss: 0.003678, test_loss: 0.002635, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2266/2500, lr=0.000519\n",
            "--> train_loss: 0.003731, test_loss: 0.002514, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2267/2500, lr=0.000518\n",
            "--> train_loss: 0.003526, test_loss: 0.002744, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2268/2500, lr=0.000518\n",
            "--> train_loss: 0.003489, test_loss: 0.002496, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2269/2500, lr=0.000517\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.003501, test_loss: 0.002303, train_metric: 0.004, test_metric: 0.002\n",
            "----\n",
            "Epoch 2270/2500, lr=0.000516\n",
            "--> train_loss: 0.003587, test_loss: 0.002731, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2271/2500, lr=0.000516\n",
            "--> train_loss: 0.003415, test_loss: 0.002652, train_metric: 0.003, test_metric: 0.003\n",
            "----\n",
            "Epoch 2272/2500, lr=0.000515\n",
            "--> train_loss: 0.003328, test_loss: 0.002315, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2273/2500, lr=0.000515\n",
            "--> train_loss: 0.003540, test_loss: 0.002413, train_metric: 0.004, test_metric: 0.002\n",
            "----\n",
            "Epoch 2274/2500, lr=0.000514\n",
            "--> train_loss: 0.003499, test_loss: 0.002910, train_metric: 0.003, test_metric: 0.003\n",
            "----\n",
            "Epoch 2275/2500, lr=0.000514\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.003558, test_loss: 0.002247, train_metric: 0.004, test_metric: 0.002\n",
            "----\n",
            "Epoch 2276/2500, lr=0.000513\n",
            "--> train_loss: 0.003512, test_loss: 0.002447, train_metric: 0.004, test_metric: 0.002\n",
            "----\n",
            "Epoch 2277/2500, lr=0.000513\n",
            "--> train_loss: 0.003631, test_loss: 0.003084, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2278/2500, lr=0.000512\n",
            "--> train_loss: 0.003820, test_loss: 0.002277, train_metric: 0.004, test_metric: 0.002\n",
            "----\n",
            "Epoch 2279/2500, lr=0.000512\n",
            "--> train_loss: 0.003404, test_loss: 0.002429, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2280/2500, lr=0.000511\n",
            "--> train_loss: 0.003357, test_loss: 0.002568, train_metric: 0.003, test_metric: 0.003\n",
            "----\n",
            "Epoch 2281/2500, lr=0.000511\n",
            "--> train_loss: 0.003430, test_loss: 0.002559, train_metric: 0.003, test_metric: 0.003\n",
            "----\n",
            "Epoch 2282/2500, lr=0.000510\n",
            "--> train_loss: 0.003307, test_loss: 0.002326, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2283/2500, lr=0.000510\n",
            "--> train_loss: 0.003595, test_loss: 0.002335, train_metric: 0.004, test_metric: 0.002\n",
            "----\n",
            "Epoch 2284/2500, lr=0.000509\n",
            "--> train_loss: 0.003548, test_loss: 0.002721, train_metric: 0.004, test_metric: 0.003\n",
            "----\n",
            "Epoch 2285/2500, lr=0.000509\n",
            "--> train_loss: 0.003460, test_loss: 0.002400, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2286/2500, lr=0.000508\n",
            "--> train_loss: 0.003298, test_loss: 0.002294, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2287/2500, lr=0.000508\n",
            "--> train_loss: 0.003203, test_loss: 0.002435, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2288/2500, lr=0.000507\n",
            "--> train_loss: 0.003362, test_loss: 0.002769, train_metric: 0.003, test_metric: 0.003\n",
            "----\n",
            "Epoch 2289/2500, lr=0.000507\n",
            "--> train_loss: 0.003165, test_loss: 0.002272, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2290/2500, lr=0.000506\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.003233, test_loss: 0.002202, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2291/2500, lr=0.000506\n",
            "--> train_loss: 0.003271, test_loss: 0.002305, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2292/2500, lr=0.000505\n",
            "--> train_loss: 0.003236, test_loss: 0.002579, train_metric: 0.003, test_metric: 0.003\n",
            "----\n",
            "Epoch 2293/2500, lr=0.000505\n",
            "--> train_loss: 0.003221, test_loss: 0.002355, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2294/2500, lr=0.000504\n",
            "--> train_loss: 0.003184, test_loss: 0.002207, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2295/2500, lr=0.000504\n",
            "--> train_loss: 0.003265, test_loss: 0.002307, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2296/2500, lr=0.000503\n",
            "--> train_loss: 0.003117, test_loss: 0.002567, train_metric: 0.003, test_metric: 0.003\n",
            "----\n",
            "Epoch 2297/2500, lr=0.000503\n",
            "--> train_loss: 0.003410, test_loss: 0.002347, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2298/2500, lr=0.000502\n",
            "--> train_loss: 0.003173, test_loss: 0.002290, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2299/2500, lr=0.000502\n",
            "--> train_loss: 0.003172, test_loss: 0.002372, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2300/2500, lr=0.000501\n",
            "--> train_loss: 0.003213, test_loss: 0.002370, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2301/2500, lr=0.000501\n",
            "--> train_loss: 0.003405, test_loss: 0.002399, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2302/2500, lr=0.000500\n",
            "--> train_loss: 0.003272, test_loss: 0.002384, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2303/2500, lr=0.000500\n",
            "--> train_loss: 0.003146, test_loss: 0.002335, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2304/2500, lr=0.000499\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.003062, test_loss: 0.002163, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2305/2500, lr=0.000499\n",
            "--> train_loss: 0.003081, test_loss: 0.002187, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2306/2500, lr=0.000498\n",
            "--> train_loss: 0.003030, test_loss: 0.002243, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2307/2500, lr=0.000498\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.003065, test_loss: 0.002120, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2308/2500, lr=0.000497\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.003123, test_loss: 0.002110, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2309/2500, lr=0.000497\n",
            "--> train_loss: 0.002967, test_loss: 0.002383, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2310/2500, lr=0.000496\n",
            "--> train_loss: 0.003286, test_loss: 0.002595, train_metric: 0.003, test_metric: 0.003\n",
            "----\n",
            "Epoch 2311/2500, lr=0.000496\n",
            "--> train_loss: 0.003695, test_loss: 0.002271, train_metric: 0.004, test_metric: 0.002\n",
            "----\n",
            "Epoch 2312/2500, lr=0.000495\n",
            "--> train_loss: 0.003175, test_loss: 0.002377, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2313/2500, lr=0.000495\n",
            "--> train_loss: 0.003488, test_loss: 0.002791, train_metric: 0.003, test_metric: 0.003\n",
            "----\n",
            "Epoch 2314/2500, lr=0.000494\n",
            "--> train_loss: 0.003183, test_loss: 0.002157, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2315/2500, lr=0.000494\n",
            "--> train_loss: 0.003027, test_loss: 0.002155, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2316/2500, lr=0.000493\n",
            "--> train_loss: 0.002988, test_loss: 0.002144, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2317/2500, lr=0.000493\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.002895, test_loss: 0.002065, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2318/2500, lr=0.000492\n",
            "--> train_loss: 0.002931, test_loss: 0.002122, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2319/2500, lr=0.000492\n",
            "--> train_loss: 0.002994, test_loss: 0.002203, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2320/2500, lr=0.000491\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.003085, test_loss: 0.002051, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2321/2500, lr=0.000491\n",
            "--> train_loss: 0.003109, test_loss: 0.002637, train_metric: 0.003, test_metric: 0.003\n",
            "----\n",
            "Epoch 2322/2500, lr=0.000490\n",
            "--> train_loss: 0.003145, test_loss: 0.002231, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2323/2500, lr=0.000490\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.003067, test_loss: 0.001899, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2324/2500, lr=0.000489\n",
            "--> train_loss: 0.003357, test_loss: 0.002642, train_metric: 0.003, test_metric: 0.003\n",
            "----\n",
            "Epoch 2325/2500, lr=0.000489\n",
            "--> train_loss: 0.003015, test_loss: 0.002452, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2326/2500, lr=0.000488\n",
            "--> train_loss: 0.003101, test_loss: 0.001955, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2327/2500, lr=0.000488\n",
            "--> train_loss: 0.002992, test_loss: 0.002314, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2328/2500, lr=0.000487\n",
            "--> train_loss: 0.003310, test_loss: 0.002244, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2329/2500, lr=0.000487\n",
            "--> train_loss: 0.002897, test_loss: 0.002082, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2330/2500, lr=0.000486\n",
            "--> train_loss: 0.002973, test_loss: 0.002184, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2331/2500, lr=0.000486\n",
            "--> train_loss: 0.002975, test_loss: 0.002270, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2332/2500, lr=0.000485\n",
            "--> train_loss: 0.003011, test_loss: 0.002048, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2333/2500, lr=0.000485\n",
            "--> train_loss: 0.002939, test_loss: 0.002096, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2334/2500, lr=0.000484\n",
            "--> train_loss: 0.002947, test_loss: 0.002210, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2335/2500, lr=0.000484\n",
            "--> train_loss: 0.002991, test_loss: 0.002061, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2336/2500, lr=0.000483\n",
            "--> train_loss: 0.002988, test_loss: 0.002077, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2337/2500, lr=0.000483\n",
            "--> train_loss: 0.002919, test_loss: 0.002040, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2338/2500, lr=0.000483\n",
            "--> train_loss: 0.002993, test_loss: 0.002096, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2339/2500, lr=0.000482\n",
            "--> train_loss: 0.003043, test_loss: 0.002364, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2340/2500, lr=0.000482\n",
            "--> train_loss: 0.002815, test_loss: 0.001961, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2341/2500, lr=0.000481\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.002851, test_loss: 0.001863, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2342/2500, lr=0.000481\n",
            "--> train_loss: 0.002717, test_loss: 0.002023, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2343/2500, lr=0.000480\n",
            "--> train_loss: 0.002856, test_loss: 0.002256, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2344/2500, lr=0.000480\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.002871, test_loss: 0.001809, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2345/2500, lr=0.000479\n",
            "--> train_loss: 0.002757, test_loss: 0.001960, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2346/2500, lr=0.000479\n",
            "--> train_loss: 0.002852, test_loss: 0.002078, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2347/2500, lr=0.000478\n",
            "--> train_loss: 0.002823, test_loss: 0.001973, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2348/2500, lr=0.000478\n",
            "--> train_loss: 0.002992, test_loss: 0.002125, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2349/2500, lr=0.000477\n",
            "--> train_loss: 0.002960, test_loss: 0.001977, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2350/2500, lr=0.000477\n",
            "--> train_loss: 0.002794, test_loss: 0.001928, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2351/2500, lr=0.000476\n",
            "--> train_loss: 0.002712, test_loss: 0.001974, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2352/2500, lr=0.000476\n",
            "--> train_loss: 0.002689, test_loss: 0.001913, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2353/2500, lr=0.000475\n",
            "--> train_loss: 0.002755, test_loss: 0.002089, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2354/2500, lr=0.000475\n",
            "--> train_loss: 0.002704, test_loss: 0.001984, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2355/2500, lr=0.000474\n",
            "--> train_loss: 0.002738, test_loss: 0.001903, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2356/2500, lr=0.000474\n",
            "--> train_loss: 0.002684, test_loss: 0.001816, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2357/2500, lr=0.000473\n",
            "--> train_loss: 0.002641, test_loss: 0.001954, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2358/2500, lr=0.000473\n",
            "--> train_loss: 0.002693, test_loss: 0.001944, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2359/2500, lr=0.000472\n",
            "--> train_loss: 0.002724, test_loss: 0.001938, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2360/2500, lr=0.000472\n",
            "--> train_loss: 0.002747, test_loss: 0.001982, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2361/2500, lr=0.000472\n",
            "--> train_loss: 0.002580, test_loss: 0.001816, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2362/2500, lr=0.000471\n",
            "--> train_loss: 0.002633, test_loss: 0.001908, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2363/2500, lr=0.000471\n",
            "--> train_loss: 0.002600, test_loss: 0.001842, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2364/2500, lr=0.000470\n",
            "--> train_loss: 0.002656, test_loss: 0.001899, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2365/2500, lr=0.000470\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.002578, test_loss: 0.001783, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2366/2500, lr=0.000469\n",
            "--> train_loss: 0.002601, test_loss: 0.001957, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2367/2500, lr=0.000469\n",
            "--> train_loss: 0.002605, test_loss: 0.001970, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2368/2500, lr=0.000468\n",
            "--> train_loss: 0.002736, test_loss: 0.001961, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2369/2500, lr=0.000468\n",
            "--> train_loss: 0.002537, test_loss: 0.001785, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2370/2500, lr=0.000467\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.002656, test_loss: 0.001710, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2371/2500, lr=0.000467\n",
            "--> train_loss: 0.002548, test_loss: 0.001967, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2372/2500, lr=0.000466\n",
            "--> train_loss: 0.002562, test_loss: 0.001784, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2373/2500, lr=0.000466\n",
            "--> train_loss: 0.002509, test_loss: 0.001735, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2374/2500, lr=0.000465\n",
            "--> train_loss: 0.002527, test_loss: 0.001794, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2375/2500, lr=0.000465\n",
            "--> train_loss: 0.002744, test_loss: 0.002154, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2376/2500, lr=0.000465\n",
            "--> train_loss: 0.002832, test_loss: 0.001751, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2377/2500, lr=0.000464\n",
            "--> train_loss: 0.002686, test_loss: 0.001935, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2378/2500, lr=0.000464\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.002549, test_loss: 0.001691, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2379/2500, lr=0.000463\n",
            "--> train_loss: 0.002498, test_loss: 0.001909, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2380/2500, lr=0.000463\n",
            "--> train_loss: 0.002683, test_loss: 0.001798, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2381/2500, lr=0.000462\n",
            "--> train_loss: 0.002603, test_loss: 0.001858, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2382/2500, lr=0.000462\n",
            "--> train_loss: 0.002569, test_loss: 0.001861, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2383/2500, lr=0.000461\n",
            "--> train_loss: 0.002531, test_loss: 0.001714, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2384/2500, lr=0.000461\n",
            "--> train_loss: 0.002527, test_loss: 0.001761, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2385/2500, lr=0.000460\n",
            "--> train_loss: 0.002543, test_loss: 0.001984, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2386/2500, lr=0.000460\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.002693, test_loss: 0.001666, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2387/2500, lr=0.000459\n",
            "--> train_loss: 0.002450, test_loss: 0.001804, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2388/2500, lr=0.000459\n",
            "--> train_loss: 0.002593, test_loss: 0.001909, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2389/2500, lr=0.000459\n",
            "--> train_loss: 0.002371, test_loss: 0.001692, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2390/2500, lr=0.000458\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.002687, test_loss: 0.001660, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2391/2500, lr=0.000458\n",
            "--> train_loss: 0.002600, test_loss: 0.002072, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2392/2500, lr=0.000457\n",
            "--> train_loss: 0.002441, test_loss: 0.001704, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2393/2500, lr=0.000457\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.002407, test_loss: 0.001563, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2394/2500, lr=0.000456\n",
            "--> train_loss: 0.002390, test_loss: 0.001880, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2395/2500, lr=0.000456\n",
            "--> train_loss: 0.002457, test_loss: 0.001737, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2396/2500, lr=0.000455\n",
            "--> train_loss: 0.002383, test_loss: 0.001667, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2397/2500, lr=0.000455\n",
            "--> train_loss: 0.002400, test_loss: 0.001650, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2398/2500, lr=0.000454\n",
            "--> train_loss: 0.002465, test_loss: 0.001664, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2399/2500, lr=0.000454\n",
            "--> train_loss: 0.002298, test_loss: 0.001796, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2400/2500, lr=0.000453\n",
            "--> train_loss: 0.002454, test_loss: 0.001807, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2401/2500, lr=0.000453\n",
            "--> train_loss: 0.002497, test_loss: 0.001717, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2402/2500, lr=0.000453\n",
            "--> train_loss: 0.002374, test_loss: 0.001815, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2403/2500, lr=0.000452\n",
            "--> train_loss: 0.002576, test_loss: 0.001873, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2404/2500, lr=0.000452\n",
            "--> train_loss: 0.002555, test_loss: 0.001713, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2405/2500, lr=0.000451\n",
            "--> train_loss: 0.002548, test_loss: 0.001920, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2406/2500, lr=0.000451\n",
            "--> train_loss: 0.002587, test_loss: 0.001673, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2407/2500, lr=0.000450\n",
            "--> train_loss: 0.002414, test_loss: 0.001659, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2408/2500, lr=0.000450\n",
            "--> train_loss: 0.002646, test_loss: 0.001788, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2409/2500, lr=0.000449\n",
            "--> train_loss: 0.002637, test_loss: 0.001609, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2410/2500, lr=0.000449\n",
            "--> train_loss: 0.002504, test_loss: 0.001852, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2411/2500, lr=0.000449\n",
            "--> train_loss: 0.002522, test_loss: 0.001755, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2412/2500, lr=0.000448\n",
            "--> train_loss: 0.002556, test_loss: 0.001604, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2413/2500, lr=0.000448\n",
            "--> train_loss: 0.002591, test_loss: 0.001678, train_metric: 0.003, test_metric: 0.002\n",
            "----\n",
            "Epoch 2414/2500, lr=0.000447\n",
            "--> train_loss: 0.002782, test_loss: 0.002508, train_metric: 0.003, test_metric: 0.003\n",
            "----\n",
            "Epoch 2415/2500, lr=0.000447\n",
            "--> train_loss: 0.002396, test_loss: 0.001639, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2416/2500, lr=0.000446\n",
            "--> train_loss: 0.002392, test_loss: 0.001630, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2417/2500, lr=0.000446\n",
            "--> train_loss: 0.002464, test_loss: 0.001585, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2418/2500, lr=0.000445\n",
            "--> train_loss: 0.002217, test_loss: 0.001598, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2419/2500, lr=0.000445\n",
            "--> train_loss: 0.002223, test_loss: 0.001606, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2420/2500, lr=0.000445\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.002199, test_loss: 0.001498, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2421/2500, lr=0.000444\n",
            "--> train_loss: 0.002204, test_loss: 0.001564, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2422/2500, lr=0.000444\n",
            "--> train_loss: 0.002197, test_loss: 0.001551, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2423/2500, lr=0.000443\n",
            "--> train_loss: 0.002198, test_loss: 0.001520, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2424/2500, lr=0.000443\n",
            "--> train_loss: 0.002178, test_loss: 0.001605, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2425/2500, lr=0.000442\n",
            "--> train_loss: 0.002197, test_loss: 0.001588, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2426/2500, lr=0.000442\n",
            "--> train_loss: 0.002210, test_loss: 0.001563, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2427/2500, lr=0.000441\n",
            "--> train_loss: 0.002285, test_loss: 0.001566, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2428/2500, lr=0.000441\n",
            "--> train_loss: 0.002222, test_loss: 0.001654, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2429/2500, lr=0.000441\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.002199, test_loss: 0.001422, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2430/2500, lr=0.000440\n",
            "--> train_loss: 0.002140, test_loss: 0.001464, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2431/2500, lr=0.000440\n",
            "--> train_loss: 0.002120, test_loss: 0.001570, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2432/2500, lr=0.000439\n",
            "--> train_loss: 0.002148, test_loss: 0.001497, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2433/2500, lr=0.000439\n",
            "--> train_loss: 0.002449, test_loss: 0.001470, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2434/2500, lr=0.000438\n",
            "--> train_loss: 0.002139, test_loss: 0.001699, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2435/2500, lr=0.000438\n",
            "--> train_loss: 0.002184, test_loss: 0.001590, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2436/2500, lr=0.000437\n",
            "--> train_loss: 0.002245, test_loss: 0.001549, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2437/2500, lr=0.000437\n",
            "--> train_loss: 0.002210, test_loss: 0.001496, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2438/2500, lr=0.000437\n",
            "--> train_loss: 0.002120, test_loss: 0.001503, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2439/2500, lr=0.000436\n",
            "--> train_loss: 0.002062, test_loss: 0.001431, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2440/2500, lr=0.000436\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.002082, test_loss: 0.001391, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2441/2500, lr=0.000435\n",
            "--> train_loss: 0.002208, test_loss: 0.001580, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2442/2500, lr=0.000435\n",
            "--> train_loss: 0.002362, test_loss: 0.001470, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2443/2500, lr=0.000434\n",
            "--> train_loss: 0.002145, test_loss: 0.001620, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2444/2500, lr=0.000434\n",
            "--> train_loss: 0.002145, test_loss: 0.001518, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2445/2500, lr=0.000434\n",
            "--> train_loss: 0.002111, test_loss: 0.001498, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2446/2500, lr=0.000433\n",
            "--> train_loss: 0.002239, test_loss: 0.001509, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2447/2500, lr=0.000433\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.002126, test_loss: 0.001370, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2448/2500, lr=0.000432\n",
            "--> train_loss: 0.002058, test_loss: 0.001594, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2449/2500, lr=0.000432\n",
            "--> train_loss: 0.002239, test_loss: 0.001557, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2450/2500, lr=0.000431\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.002193, test_loss: 0.001287, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2451/2500, lr=0.000431\n",
            "--> train_loss: 0.002214, test_loss: 0.001817, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2452/2500, lr=0.000431\n",
            "--> train_loss: 0.002152, test_loss: 0.001436, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2453/2500, lr=0.000430\n",
            "--> train_loss: 0.002075, test_loss: 0.001343, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2454/2500, lr=0.000430\n",
            "--> train_loss: 0.001998, test_loss: 0.001501, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2455/2500, lr=0.000429\n",
            "--> train_loss: 0.002061, test_loss: 0.001402, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2456/2500, lr=0.000429\n",
            "--> train_loss: 0.001976, test_loss: 0.001319, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2457/2500, lr=0.000428\n",
            "--> train_loss: 0.001999, test_loss: 0.001434, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2458/2500, lr=0.000428\n",
            "--> train_loss: 0.002190, test_loss: 0.001467, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2459/2500, lr=0.000428\n",
            "--> train_loss: 0.002031, test_loss: 0.001573, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2460/2500, lr=0.000427\n",
            "--> train_loss: 0.002085, test_loss: 0.001340, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2461/2500, lr=0.000427\n",
            "--> train_loss: 0.002348, test_loss: 0.001504, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2462/2500, lr=0.000426\n",
            "--> train_loss: 0.002095, test_loss: 0.001543, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2463/2500, lr=0.000426\n",
            "--> train_loss: 0.002123, test_loss: 0.001433, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2464/2500, lr=0.000425\n",
            "--> train_loss: 0.002070, test_loss: 0.001400, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2465/2500, lr=0.000425\n",
            "--> train_loss: 0.002116, test_loss: 0.001434, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2466/2500, lr=0.000425\n",
            "--> train_loss: 0.001932, test_loss: 0.001326, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2467/2500, lr=0.000424\n",
            "--> train_loss: 0.001969, test_loss: 0.001344, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2468/2500, lr=0.000424\n",
            "--> train_loss: 0.002030, test_loss: 0.001352, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2469/2500, lr=0.000423\n",
            "--> train_loss: 0.001984, test_loss: 0.001529, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2470/2500, lr=0.000423\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.001955, test_loss: 0.001274, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2471/2500, lr=0.000422\n",
            "--> train_loss: 0.002067, test_loss: 0.001342, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2472/2500, lr=0.000422\n",
            "--> train_loss: 0.002284, test_loss: 0.001601, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2473/2500, lr=0.000422\n",
            "--> train_loss: 0.001956, test_loss: 0.001482, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2474/2500, lr=0.000421\n",
            "--> train_loss: 0.002022, test_loss: 0.001507, train_metric: 0.002, test_metric: 0.002\n",
            "----\n",
            "Epoch 2475/2500, lr=0.000421\n",
            "--> train_loss: 0.001927, test_loss: 0.001307, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2476/2500, lr=0.000420\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.001923, test_loss: 0.001266, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2477/2500, lr=0.000420\n",
            "--> train_loss: 0.001879, test_loss: 0.001340, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2478/2500, lr=0.000419\n",
            "--> train_loss: 0.001863, test_loss: 0.001364, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2479/2500, lr=0.000419\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.002025, test_loss: 0.001254, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2480/2500, lr=0.000419\n",
            "--> train_loss: 0.001997, test_loss: 0.001480, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2481/2500, lr=0.000418\n",
            "--> train_loss: 0.001955, test_loss: 0.001438, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2482/2500, lr=0.000418\n",
            "--> train_loss: 0.001926, test_loss: 0.001276, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2483/2500, lr=0.000417\n",
            "--> train_loss: 0.001889, test_loss: 0.001301, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2484/2500, lr=0.000417\n",
            "--> train_loss: 0.001856, test_loss: 0.001367, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2485/2500, lr=0.000417\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.001882, test_loss: 0.001252, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2486/2500, lr=0.000416\n",
            "--> train_loss: 0.001819, test_loss: 0.001323, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2487/2500, lr=0.000416\n",
            "--> train_loss: 0.001884, test_loss: 0.001290, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2488/2500, lr=0.000415\n",
            "--> train_loss: 0.001851, test_loss: 0.001309, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2489/2500, lr=0.000415\n",
            "--> train_loss: 0.001814, test_loss: 0.001260, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2490/2500, lr=0.000414\n",
            "--> train_loss: 0.001880, test_loss: 0.001267, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2491/2500, lr=0.000414\n",
            "--> train_loss: 0.001833, test_loss: 0.001342, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2492/2500, lr=0.000414\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.001839, test_loss: 0.001220, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2493/2500, lr=0.000413\n",
            "--> train_loss: 0.001793, test_loss: 0.001221, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2494/2500, lr=0.000413\n",
            "--> train_loss: 0.001805, test_loss: 0.001303, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2495/2500, lr=0.000412\n",
            "--> train_loss: 0.001862, test_loss: 0.001264, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2496/2500, lr=0.000412\n",
            "--> model improved! --> saved to ./weights_regression.pt\n",
            "--> train_loss: 0.001953, test_loss: 0.001133, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2497/2500, lr=0.000412\n",
            "--> train_loss: 0.001827, test_loss: 0.001451, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2498/2500, lr=0.000411\n",
            "--> train_loss: 0.001862, test_loss: 0.001273, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2499/2500, lr=0.000411\n",
            "--> train_loss: 0.001850, test_loss: 0.001229, train_metric: 0.002, test_metric: 0.001\n",
            "----\n",
            "Epoch 2500/2500, lr=0.000410\n",
            "--> train_loss: 0.001816, test_loss: 0.001325, train_metric: 0.002, test_metric: 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVEs0jRlPtzn",
        "outputId": "db557631-6268-4758-aac0-48c23bc7c2db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fig_loss = go.Figure()\n",
        "fig_metric = go.Figure()\n",
        "\n",
        "x = [i+1 for i in range(params[\"num_epochs\"])]\n",
        "\n",
        "fig_loss.add_traces( go.Scatter(x=x,y=loss_history[\"train\"], name=\"train loss\", mode=\"lines+markers\" ) )\n",
        "fig_loss.add_traces( go.Scatter(x=x,y=loss_history[\"test\"], name=\"test loss\"  , mode=\"lines+markers\") )\n",
        "fig_loss.update_layout(title=\"Loss Results\", xaxis_title=\"epochs\", hovermode=\"x\")\n",
        "fig_loss.show()\n",
        "\n",
        "fig_metric.add_traces( go.Scatter(x=x,y=metric_history[\"train\"], name=\"train metric\", mode=\"lines+markers\") )\n",
        "fig_metric.add_traces( go.Scatter(x=x,y=metric_history[\"test\"], name=\"test_metric\" , mode=\"lines+markers\") )\n",
        "fig_metric.update_layout(title=\"Metric Results\", xaxis_title=\"epochs\", hovermode=\"x\")\n",
        "fig_metric.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"106ebaa5-1868-4304-8fb3-e3072ead6928\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"106ebaa5-1868-4304-8fb3-e3072ead6928\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '106ebaa5-1868-4304-8fb3-e3072ead6928',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"train loss\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500], \"y\": [46.36821219308036, 43.90594552176339, 40.41241978236607, 35.574229213169644, 29.738605259486608, 23.27469046456473, 19.23122994559152, 16.429647216796877, 14.645842633928572, 12.679490966796875, 11.514110194614956, 10.526168212890624, 9.800698678152902, 9.368781302315849, 8.982340785435268, 8.605166625976562, 8.2094921875, 7.820752563476563, 7.503487548828125, 7.12577157156808, 6.760778895786831, 6.420605032784598, 6.057600359235491, 5.733860430036272, 5.406892874581473, 5.1260347638811385, 4.84750501360212, 4.593295157296317, 4.349742954799107, 4.119705505371094, 3.9194035993303573, 3.7311397443498886, 3.573667471749442, 3.4021844482421875, 3.2478165544782365, 3.1160074288504465, 2.9767427716936385, 2.85044921875, 2.745955396379743, 2.6341566249302457, 2.5292847115652903, 2.438119790213449, 2.3552522495814734, 2.2600003705705913, 2.189977329799107, 2.107057560511998, 2.0340401894705638, 1.9697842407226562, 1.8988257489885603, 1.8442888968331472, 1.7941903904506138, 1.7394907924107144, 1.6805846840994698, 1.632930701119559, 1.5887119402204242, 1.5436863599504744, 1.4840298897879465, 1.43723390851702, 1.389861809866769, 1.3479748753138951, 1.3087105451311385, 1.263764430454799, 1.229765592302595, 1.1894364384242466, 1.1601344517299108, 1.124200897216797, 1.0906924765450614, 1.052590103149414, 1.0189311000279018, 0.9936233466012138, 0.965865729195731, 0.936808122907366, 0.905583986554827, 0.8807545580182756, 0.8559468678065709, 0.8357360621861049, 0.8072822625296456, 0.7872065625871931, 0.7605782699584961, 0.7442789241245815, 0.729240733555385, 0.7096070970807756, 0.692313883645194, 0.6695498875209264, 0.650404646737235, 0.6290452684674944, 0.6121289770943777, 0.5993786185128348, 0.5814649963378906, 0.5716612080165319, 0.5502351433890207, 0.5354100908551898, 0.5215390014648438, 0.5082269886561802, 0.4943825367518834, 0.485976379939488, 0.4711132049560547, 0.4584753118242536, 0.4474017606462751, 0.43846543175833563, 0.42686565399169923, 0.41670227595738, 0.40819266183035713, 0.39887866156441826, 0.3936685507638114, 0.38201021194458007, 0.3723147773742676, 0.36120328630719867, 0.3535775075639997, 0.34601059232439313, 0.3404566383361816, 0.330804021017892, 0.32348457063947406, 0.31611788068498886, 0.3095668751852853, 0.30135526384626116, 0.2956281198774065, 0.2918312644958496, 0.28649751663208006, 0.27768254143851145, 0.2727182688031878, 0.2652851867675781, 0.2594434710911342, 0.25442586081368584, 0.24751547949654715, 0.24162058694022043, 0.2360012858254569, 0.2302475643157959, 0.22663773127964565, 0.2223401151384626, 0.21576052529471262, 0.20962865829467772, 0.20464753559657506, 0.19892668042864117, 0.19441109384809221, 0.18857369695390974, 0.1822873946598598, 0.17652669497898646, 0.17018299511500767, 0.1666423797607422, 0.1641934040614537, 0.15615478924342563, 0.15283263615199497, 0.14900700092315675, 0.14710386753082275, 0.14580002648489815, 0.1440871729169573, 0.14462626865931918, 0.14748310770307269, 0.13241400037493026, 0.12869652611868723, 0.12365900993347168, 0.1223223774773734, 0.12564601489475796, 0.11813249588012695, 0.11801718848092216, 0.11061334201267788, 0.11089506830487933, 0.10923939909253802, 0.11174476896013533, 0.10720829691205706, 0.10275191170828683, 0.10418776648385183, 0.10272130761827741, 0.10200949464525495, 0.10475369930267334, 0.10832846369062152, 0.10788681847708566, 0.09892970289502825, 0.09614357812064035, 0.1035553434916905, 0.0949453387941633, 0.09746796539851597, 0.09240402357918875, 0.09010105950491769, 0.08986598287309919, 0.08937621048518589, 0.08738527434212821, 0.08684120314461845, 0.0854069403239659, 0.08559965678623745, 0.0869472278867449, 0.08417435305459159, 0.08924751894814628, 0.09146420955657959, 0.0847879103251866, 0.08732511792864119, 0.09625880718231201, 0.10088944094521658, 0.10096257618495397, 0.08629834856305804, 0.0931707239151001, 0.09054594448634556, 0.08900967393602643, 0.08087711334228516, 0.07979658194950649, 0.07901221752166748, 0.08169464451926095, 0.0780160209110805, 0.08043797424861363, 0.07799841267721994, 0.0776465858731951, 0.07589816774640765, 0.07613541262490409, 0.07555217300142561, 0.07589692354202271, 0.07485817159925189, 0.07493943827492851, 0.07894512040274484, 0.0751534230368478, 0.07616147245679583, 0.07820206437792097, 0.0787170376096453, 0.07671666213444302, 0.07543404034205846, 0.0785599388395037, 0.07516611303601946, 0.07550081253051757, 0.07319216319492886, 0.07190898724964687, 0.0730559389931815, 0.07232740197862898, 0.07169262545449392, 0.07580098901476179, 0.08732834679739816, 0.09219073840550014, 0.10115245682852608, 0.09647097383226667, 0.08015576362609864, 0.0758475467136928, 0.07506599971226284, 0.0708243669782366, 0.0703593921661377, 0.07053630862917219, 0.07221040521349226, 0.07253284522465298, 0.07165329251970563, 0.07183931146349226, 0.07031696387699672, 0.0704291296005249, 0.07130595752171108, 0.07162898744855609, 0.06935253075190953, 0.06978920664106096, 0.06917884417942592, 0.06864435911178589, 0.06908376693725586, 0.06949414321354458, 0.06858310971941267, 0.06807703426906041, 0.06924778120858328, 0.06872353962489537, 0.06919773987361363, 0.07077561923435756, 0.0687465476989746, 0.06941096680504935, 0.06997359480176653, 0.06940182209014893, 0.0691613507270813, 0.07022213663373675, 0.06995981012071882, 0.06784551552363804, 0.06779595511300224, 0.06772755418504987, 0.06676831449781145, 0.06660307203020369, 0.06654790946415493, 0.06695077010563441, 0.06612280709402901, 0.06802234751837594, 0.06589542695454188, 0.06577533347266061, 0.0665087788445609, 0.0680543851852417, 0.06665525538580758, 0.06637078932353428, 0.0682836982182094, 0.0665828207560948, 0.06743156126567296, 0.0670070539202009, 0.06608349936349052, 0.0671429112979344, 0.06670205593109131, 0.06945629460471017, 0.07069058963230677, 0.06763413088662283, 0.06584130082811628, 0.06645680972508022, 0.06869668279375349, 0.06814191341400147, 0.06630301952362061, 0.06646142210279192, 0.06756897756031581, 0.06697601931435722, 0.06753172738211495, 0.06727065358843122, 0.06785050187792097, 0.06671822684151786, 0.0657848082269941, 0.06666580745152065, 0.0662010931968689, 0.06536633968353271, 0.0650276163646153, 0.06518309491021293, 0.06544029644557407, 0.06454730136053903, 0.06528410298483713, 0.06528337819235666, 0.06460976191929409, 0.06543804577418737, 0.06487319707870483, 0.06496838433401925, 0.06623366798673357, 0.06546248504093716, 0.06584039075034005, 0.0662308052607945, 0.06618545702525548, 0.06644688538142612, 0.06680419785635812, 0.06579285689762661, 0.06468032496316092, 0.06516532557351248, 0.06473050321851458, 0.06481585230146135, 0.06415434053965978, 0.0642956829071045, 0.06416493960789271, 0.06435465097427367, 0.06422974245888846, 0.06440455538885934, 0.06427092552185058, 0.06497855833598545, 0.06496204376220703, 0.06510740756988526, 0.0653657647541591, 0.06498205287115914, 0.0697113037109375, 0.06780453852244786, 0.06410026618412563, 0.06658153329576764, 0.06594159194401332, 0.06593445573534285, 0.0644353665624346, 0.06412898812975203, 0.06447755438940865, 0.0643896552494594, 0.06536085401262556, 0.06590259892599923, 0.06643761907305036, 0.06842854022979736, 0.06615490095955985, 0.06607183626719884, 0.06443596533366612, 0.0637988921574184, 0.06341183798653739, 0.0634416743687221, 0.06337817702974592, 0.06453380857195173, 0.06689624752317157, 0.06564581836972917, 0.06770061390740531, 0.06703689234597342, 0.06885900974273682, 0.06463925191334316, 0.06604219130107335, 0.06501430238996234, 0.06608805860791887, 0.06768434456416539, 0.06724574157169887, 0.06743010078157698, 0.06866642304829189, 0.06713406017848424, 0.06667220796857562, 0.0651965400150844, 0.06463667869567871, 0.06409064361027308, 0.06496277809143067, 0.06418874502182007, 0.06528749636241368, 0.06537959507533482, 0.06453422342027937, 0.06621934618268695, 0.06772943087986537, 0.06591020209448678, 0.06517259734017508, 0.06413801908493041, 0.06308705977031163, 0.0639111532483782, 0.06497869116919382, 0.06307701962334769, 0.0650807877949306, 0.06447876759937832, 0.0627075481414795, 0.0655809429713658, 0.06379883033888681, 0.0634181751523699, 0.06494906152997698, 0.06478565318243844, 0.06361095292227609, 0.0640546921321324, 0.0633789450781686, 0.06498689106532506, 0.06471878698893956, 0.06369448355266026, 0.06481901679720198, 0.06548219953264509, 0.06776724747249058, 0.06829174177987235, 0.06900068010602678, 0.06566924912588937, 0.06690524646214076, 0.06746597698756626, 0.06589485543114798, 0.06447306769234794, 0.06335742780140469, 0.06422091824667794, 0.06567604541778564, 0.06603662695203509, 0.06517738546643938, 0.06442756073815482, 0.06484405313219344, 0.06560856512614659, 0.06395444427217756, 0.06460234676088605, 0.06409617696489607, 0.06512541055679322, 0.06403672252382551, 0.06343341827392578, 0.06375898769923619, 0.06336382116590228, 0.06454042434692382, 0.06337279626301356, 0.06364209651947021, 0.0630767263684954, 0.06342561926160539, 0.06391877106257847, 0.06280005659375872, 0.06291216543742588, 0.06326904092516218, 0.06407839093889509, 0.06297404016767229, 0.06308216299329485, 0.06270749977656773, 0.06382901191711426, 0.06393033811024257, 0.0633651396206447, 0.0638213450568063, 0.06405569996152605, 0.06458610602787562, 0.06304234266281128, 0.06332082271575928, 0.06393670150211879, 0.06275314058576312, 0.0641796452658517, 0.06312010901314871, 0.06269660064152309, 0.06352509600775583, 0.06285812411989485, 0.06362537588391985, 0.06495100225721087, 0.06687602962766374, 0.06531905651092529, 0.06512645551136562, 0.06561138936451504, 0.06455253192356654, 0.06351979391915458, 0.0635592988559178, 0.06401683092117309, 0.06422441823141915, 0.06554246425628663, 0.0654137475149972, 0.06559917177472796, 0.06496970176696777, 0.0660574974332537, 0.06327368497848511, 0.06511035067694528, 0.06829510893140521, 0.06683719566890171, 0.06382112503051758, 0.06388071366718837, 0.06451302085603987, 0.06514664854322161, 0.0640462030683245, 0.06239530495234898, 0.06371393271854946, 0.06673888002123152, 0.06518268210547311, 0.06440660374505179, 0.06425623025212969, 0.06341326100485666, 0.0634328235898699, 0.06379946231842042, 0.06446641513279507, 0.06364458833421979, 0.06332249402999877, 0.0626617227281843, 0.06245957783290318, 0.06225857053484236, 0.0638544613974435, 0.06671805245535714, 0.06658794334956578, 0.06515150274549211, 0.06468146664755685, 0.06572867597852435, 0.06352100304194859, 0.06259255971227373, 0.06362633909497942, 0.06870083945138114, 0.07109918798719134, 0.07526816231863839, 0.0702477581160409, 0.06696706908089774, 0.06678202050072807, 0.06500866310937065, 0.06483133860996791, 0.0641224581854684, 0.06382579258510045, 0.06424424954823085, 0.06467657157352992, 0.06484288896833147, 0.06417341845376151, 0.06340957828930446, 0.06347353049686977, 0.06303416184016636, 0.0637700891494751, 0.06364249365670341, 0.06372372831617083, 0.0635024585042681, 0.06336120912006923, 0.0635587808064052, 0.06373567853655134, 0.06268148115703037, 0.06368881327765329, 0.06765407494136265, 0.06532652718680246, 0.06407770224979946, 0.0625258367402213, 0.062132463455200196, 0.06307868071964809, 0.06306581122534616, 0.06354437828063965, 0.06292183228901455, 0.06324055876050677, 0.0625355202811105, 0.06412274837493896, 0.06248332125799996, 0.064564368724823, 0.0628348207473755, 0.06294020584651402, 0.06237468787602016, 0.06400196552276612, 0.06443114825657435, 0.06312591552734376, 0.0629327392578125, 0.06403539453233992, 0.0632896648134504, 0.06353294372558593, 0.06612039634159633, 0.06696903092520577, 0.06491294724600656, 0.06376467909131732, 0.06412218741008213, 0.06540159566061837, 0.06445650236947195, 0.06423028094427927, 0.06602693625858852, 0.06388631071363177, 0.06374062265668597, 0.06644128663199289, 0.06885984557015555, 0.06781847681318011, 0.0650217124394008, 0.06480682236807687, 0.06526181834084648, 0.06451797451291766, 0.06422290904181344, 0.06277849980763027, 0.06278989042554582, 0.06306323119572231, 0.06289228745869228, 0.06439071893692017, 0.0632910486630031, 0.0630754246030535, 0.06284278052193779, 0.06318086045128958, 0.06240389551435198, 0.06332127332687378, 0.06238416450364249, 0.06299584661211287, 0.06222292900085449, 0.06419852665492466, 0.06252263137272426, 0.06306325435638428, 0.06224576950073242, 0.062356469631195066, 0.06236954348427909, 0.06254712445395333, 0.06312641518456595, 0.06347946473530361, 0.06333948407854352, 0.06348593779972621, 0.06334842068808419, 0.06301314524241856, 0.0622248101234436, 0.06248570374080113, 0.0628363425391061, 0.06216445990971157, 0.06288361208779471, 0.06287221397672381, 0.06264288561684744, 0.061864208153315954, 0.06359018053327288, 0.06328623294830323, 0.06276458365576608, 0.06342257499694824, 0.06374501160212925, 0.06414570774350847, 0.06663039309637887, 0.06413365738732474, 0.06335602351597378, 0.06430227824619839, 0.06386246987751552, 0.06425237621579852, 0.06471854312079293, 0.06859091758728027, 0.06481839861188617, 0.0638009980746678, 0.06429336615971157, 0.06270611218043735, 0.06249655587332589, 0.06199520826339722, 0.062130263873509, 0.06210372652326311, 0.0620529351915632, 0.06267974853515625, 0.06392819370542253, 0.06407154423849923, 0.06492824145725795, 0.06260930980954851, 0.062408419677189415, 0.06244921054158892, 0.06488086427961078, 0.06632123538425991, 0.06419852001326425, 0.06458972454071045, 0.0627660928453718, 0.06425795657294137, 0.06330001762935093, 0.0683697601727077, 0.06681907006672451, 0.06462008544376918, 0.066885267666408, 0.06524825845445906, 0.06564267499106272, 0.06416009494236537, 0.06368882043021065, 0.06200624465942383, 0.062358794893537246, 0.062037170955113005, 0.06132810388292585, 0.06150935207094465, 0.061114325523376464, 0.06116297517504011, 0.06043680667877197, 0.06087618044444493, 0.06175603321620396, 0.06380532877785819, 0.06685241086142404, 0.06842894009181431, 0.07030091490064348, 0.06821995462690081, 0.06701194763183593, 0.06833242961338588, 0.06865293434688023, 0.0644802795137678, 0.06304398911339897, 0.06229256936482021, 0.062439556121826174, 0.06283036402293614, 0.06213277305875506, 0.06135971307754517, 0.0602705192565918, 0.05999945572444371, 0.059671128817967004, 0.05965736525399344, 0.06003879036222185, 0.06366304465702602, 0.060211337975093294, 0.06196988991328648, 0.05972659690039499, 0.06250414405550275, 0.06195837225232805, 0.06234357629503522, 0.059214942114693775, 0.059713643618992396, 0.06133876051221575, 0.059252303327832906, 0.058645548479897634, 0.0590398907661438, 0.05870863812310355, 0.05934695380074637, 0.059012458324432374, 0.05973607063293457, 0.0584568875176566, 0.05892944063459124, 0.05930745124816895, 0.05869853155953544, 0.05904993976865496, 0.0591836759022304, 0.05818562848227365, 0.058199126720428464, 0.05889493771961757, 0.05931927442550659, 0.059056538684027535, 0.05861508505684989, 0.05919908727918352, 0.05816350902829851, 0.05868548904146467, 0.058782053334372385, 0.061148245675223215, 0.058613157953534806, 0.05722967147827149, 0.05831251382827759, 0.05706486293247768, 0.05813643148967198, 0.06031728676387242, 0.0589307849747794, 0.059786943367549354, 0.05938974073954991, 0.05952216761452811, 0.05829007693699428, 0.056941381181989395, 0.05668613297598703, 0.05819518600191389, 0.05823028087615967, 0.05689572640827724, 0.056535686084202355, 0.05686795728547232, 0.05592171566826957, 0.05620971270969936, 0.05605193887438093, 0.055987355709075926, 0.05739298684256417, 0.058488289288112096, 0.0561120673588344, 0.05735077176775251, 0.0548871990612575, 0.05561485699244908, 0.05376740523747035, 0.05408542803355626, 0.05391432591847011, 0.05525698866162981, 0.05400399582726615, 0.05373429536819458, 0.055016403879438125, 0.053099737507956365, 0.05382481404713222, 0.05579645531518119, 0.054788779871804374, 0.06191093717302595, 0.06549022912979126, 0.06384527615138462, 0.05610815831593105, 0.05368925503322056, 0.05406123127256121, 0.056578372887202674, 0.06063879081181117, 0.0586541908127921, 0.05328040361404419, 0.05507486070905413, 0.052370257718222485, 0.05177151305334909, 0.051709507192884176, 0.053004472255706786, 0.05324892061097281, 0.05468840599060058, 0.05317122459411621, 0.051083832468305314, 0.051165283066885815, 0.0514681305204119, 0.05290367637361799, 0.052480580466134207, 0.054812086990901406, 0.0531279993057251, 0.05246721182550703, 0.051834890842437746, 0.05129691668919155, 0.05302257980619158, 0.050440740585327146, 0.05211416789463588, 0.050972184453691755, 0.050745013441358296, 0.050206358773367744, 0.050421007020132885, 0.049252317292349676, 0.04892998729433332, 0.04934319257736206, 0.050018679584775655, 0.05174440179552351, 0.04905108758381435, 0.049227592434201925, 0.049436242239815846, 0.048305413041796, 0.04975669997079032, 0.04919318897383554, 0.04911865677152361, 0.04934447271483285, 0.04986999647957938, 0.04812261887959072, 0.048537533283233646, 0.051812238693237304, 0.04973094940185547, 0.0484907933643886, 0.048466406209128246, 0.04713406903403146, 0.047433806657791136, 0.04700997301510402, 0.046910858154296874, 0.04658522299357823, 0.047557006563459125, 0.04852080379213606, 0.0464275506564549, 0.04794631719589233, 0.047107667241777694, 0.0478815621989114, 0.04715836541993278, 0.046861886978149414, 0.04745099357196263, 0.046922479697636195, 0.04694358331816537, 0.04673383508409772, 0.047216025079999654, 0.047734660591397964, 0.047750711270741056, 0.04671300768852234, 0.046559398174285886, 0.04576589141573225, 0.045938576459884646, 0.04539359842027937, 0.04556214349610465, 0.046166234357016424, 0.04761828660964966, 0.04791619385991778, 0.04540989228657314, 0.045773526089532036, 0.045753129380089894, 0.044329385927745275, 0.044724807143211365, 0.0448526873758861, 0.045021045804023746, 0.04533246176583426, 0.04599188532148089, 0.04532868794032505, 0.044061055524008616, 0.044520758986473084, 0.04484649658203125, 0.044807130609239854, 0.04412667955671038, 0.04475275235516685, 0.04512809651238578, 0.04432812111718314, 0.04399162973676409, 0.04346194778169905, 0.04337187443460737, 0.04365150996616909, 0.04390965257372175, 0.04427447966166905, 0.04353451388222831, 0.04475775480270386, 0.043823292766298566, 0.04382106167929513, 0.044107195479529245, 0.045279493161610194, 0.04372899634497506, 0.043759845495224, 0.04307517579623631, 0.04278115017073495, 0.04394088625907898, 0.04367064373833793, 0.04406029956681388, 0.04382178374699184, 0.0448619270324707, 0.04432557531765529, 0.04427122609955924, 0.04414390257426671, 0.04373362370899746, 0.04491802181516375, 0.044678447587149485, 0.04379385556493487, 0.04496464473860604, 0.04310569320406232, 0.044202971628734045, 0.042797648225511825, 0.0427216249704361, 0.04249587195260184, 0.04213568074362618, 0.042097821065357754, 0.042435419389179774, 0.041801913636071344, 0.04166870049067906, 0.04195583888462612, 0.04171469398907253, 0.04211654833384922, 0.042797270502362934, 0.0427106739793505, 0.04234492199761527, 0.0418386732680457, 0.04130074075290135, 0.04171618172100612, 0.04190046633992876, 0.04251285723277501, 0.04300551533699035, 0.04317948971475874, 0.04416034936904907, 0.043645754882267546, 0.04402142030852182, 0.04299005006040846, 0.042073933737618585, 0.04146366391863142, 0.04151589615004403, 0.04134750604629517, 0.04176919851984297, 0.041772227372441975, 0.041410996403012955, 0.04123504136289869, 0.04153074741363525, 0.04278398505278996, 0.04268986438001905, 0.04146299515451704, 0.04085013091564178, 0.04142448808465685, 0.04094698633466448, 0.041123933621815274, 0.04146667629480362, 0.04117213095937457, 0.04139160786356245, 0.04072435872895377, 0.040592435428074426, 0.040623984336853027, 0.041821977751595636, 0.04120811768940517, 0.04080935708114079, 0.0409546434879303, 0.0407282543182373, 0.04055874977793012, 0.040453835896083284, 0.0402948614529201, 0.040365621192114697, 0.04006585768290928, 0.040930380140032085, 0.04068450433867318, 0.040112454550606864, 0.040068129811968126, 0.039891836302621025, 0.040223963260650634, 0.039598145825522284, 0.039910668986184256, 0.03992694914340973, 0.04012560861451285, 0.0397832076890128, 0.03980089489902769, 0.04069074690341949, 0.040996663059507096, 0.039598975777626035, 0.040008895908083236, 0.04033650321619851, 0.03982313769204276, 0.04002001455851963, 0.03966081346784319, 0.040300657195704324, 0.04009645666394915, 0.04012132968221392, 0.03983576910836356, 0.04022334260599954, 0.0395652151959283, 0.039462299006325856, 0.03928027834211077, 0.03919077481542315, 0.039364459684916905, 0.03927738768713815, 0.03910207254546029, 0.03883783519268036, 0.03889639207295009, 0.038970803448132106, 0.0388394991840635, 0.03934506348201207, 0.03881735750607082, 0.03885905282838004, 0.03879004784992763, 0.03882184633186885, 0.03884404642241342, 0.03896565505436488, 0.03859352401324681, 0.03888705798557827, 0.039474080886159627, 0.03918878248759679, 0.03895990473883493, 0.04006586057799203, 0.039955402953284126, 0.039042472583906994, 0.0400252251965659, 0.03960304541247232, 0.039132579394749234, 0.03948983243533543, 0.039022083793367655, 0.03950230819838388, 0.038727305105754305, 0.0390294987814767, 0.039316410337175645, 0.03909552761486598, 0.039742990136146544, 0.03947856834956578, 0.039183887158121385, 0.039331372124808174, 0.03948797115257808, 0.04087369816643851, 0.04160677586283003, 0.040976581062589376, 0.04091941782406398, 0.040521975755691525, 0.04020661302975246, 0.0397024519102914, 0.040238492488861084, 0.03975424323763166, 0.03874758311680385, 0.03913327745028904, 0.03908762429441724, 0.038260000773838586, 0.039212641886302406, 0.03845437969480242, 0.03818292277199881, 0.03838324708598, 0.038172324384961806, 0.038179154651505606, 0.03781725866453988, 0.038091959612710134, 0.03816293239593506, 0.03816819931779589, 0.03878603066716876, 0.03870755706514631, 0.03865058728626796, 0.038819213935307094, 0.038609643493379864, 0.03889205353600638, 0.03826437098639352, 0.038385058982031685, 0.038193818841661724, 0.03877495918955122, 0.03832001677581242, 0.038063557893037794, 0.038053704159600396, 0.037845187187194824, 0.03806148988859994, 0.03857513502240181, 0.03851570240088872, 0.03891713993889945, 0.038817182779312134, 0.038585257147039684, 0.038444584267480035, 0.03893132516316005, 0.03884226952280317, 0.038378393309456964, 0.03796160195555006, 0.039177969694137574, 0.03944914647511073, 0.038011491724423, 0.03814779758453369, 0.03823925861290523, 0.03824127972126007, 0.03889732497079032, 0.038155009405953545, 0.03815358757972717, 0.03770915848868234, 0.03790385927472796, 0.03779295223099845, 0.0378874203988484, 0.03778170943260193, 0.03795475593635014, 0.037969747270856585, 0.037944193737847465, 0.03755356294768197, 0.037550710950578964, 0.03742757797241211, 0.037304939883095876, 0.0373383709362575, 0.03826522103377751, 0.03804975271224976, 0.037677321093423026, 0.038110026461737494, 0.03765640701566424, 0.037521747861589703, 0.037331029857907976, 0.03776880732604435, 0.0376056694984436, 0.03739384140287127, 0.037750580992017474, 0.03720359836305891, 0.037730769259589056, 0.037211845687457494, 0.03706405392714909, 0.0370281788281032, 0.03713151114327567, 0.037168829441070556, 0.037256974067006794, 0.03722973040172032, 0.03800014214856284, 0.03772796979972294, 0.03820170589855739, 0.038101344960076465, 0.03810804946081979, 0.037864998238427296, 0.037998124190739224, 0.03755294782774789, 0.03705554434231349, 0.036863156982830596, 0.03709757123674665, 0.03727797372000558, 0.037498534406934465, 0.03754649017538343, 0.0377388380255018, 0.037588100603648596, 0.03724111097199576, 0.03704059115477971, 0.036974211250032696, 0.037054655381611415, 0.03718822087560381, 0.03747434420245034, 0.03744106190545218, 0.037413987261908394, 0.037284146887915474, 0.03774334217820849, 0.03765146357672555, 0.03702390104532242, 0.03692407625062125, 0.03682022507701601, 0.036940151027270725, 0.036762229885373796, 0.0369681122473308, 0.0370006947857993, 0.03695574828556606, 0.036955151983669825, 0.03690450957843235, 0.03707773881299155, 0.036954159779208046, 0.03698160341807774, 0.036912287047931126, 0.03674100135053907, 0.037347322617258344, 0.03734089068004063, 0.03708640609468732, 0.037377276782478604, 0.0372121581860951, 0.03698210920606341, 0.03703835879053388, 0.03724170207977295, 0.03703582533768245, 0.03720481600080218, 0.03726017713546753, 0.03676730488027845, 0.03732721699135644, 0.03952733142035348, 0.039068516067096166, 0.0381574547290802, 0.038668094873428344, 0.03991604047162192, 0.04078904960836683, 0.03989991971424648, 0.03896645358630589, 0.037851601328168595, 0.038446189676012314, 0.03827766605785915, 0.03762973879064833, 0.03693083201135908, 0.036856936386653355, 0.03705291364874159, 0.036956318616867066, 0.03691858811037881, 0.036590649144990106, 0.03673882092748369, 0.03682136263166155, 0.037084153550011774, 0.03703210847718375, 0.03646373212337494, 0.036705657754625594, 0.036386493614741736, 0.036669551134109496, 0.03657812959381512, 0.03668529459408351, 0.036785240003040856, 0.03641466583524432, 0.036694340194974626, 0.036362879957471575, 0.03646442932741983, 0.03656216212681362, 0.03652617037296295, 0.03619788357189723, 0.03617293749536787, 0.0362851163319179, 0.036603912625994, 0.03667729254279818, 0.03687120077865464, 0.03625255322882107, 0.03643064328602382, 0.03639195842402322, 0.036344738432339256, 0.03639551699161529, 0.03658783963748387, 0.03683278407369341, 0.03752248074327196, 0.03726248570850917, 0.03720463139670236, 0.036867719548089165, 0.03729684169803347, 0.03642413233007703, 0.036267811059951784, 0.03661230572632381, 0.03673193335533142, 0.03646186662571771, 0.03646415114402771, 0.03633962486471449, 0.03623035975864956, 0.03659422210284642, 0.036740357620375495, 0.0363616372857775, 0.03662586765629905, 0.03639247076851981, 0.03653228717190879, 0.03625691413879394, 0.03639422569956098, 0.03627928222928729, 0.0365260922908783, 0.03634703738348825, 0.03717707089015416, 0.03698763941015516, 0.037239447917257036, 0.03767192227499826, 0.03645065145833152, 0.03700558185577393, 0.036411291105406626, 0.036242585011890954, 0.037741750617112435, 0.037530040059770856, 0.036927592413766044, 0.03731354658092771, 0.03655005386897496, 0.036568540505000524, 0.03645080523831504, 0.036683127880096436, 0.03699197334902627, 0.03785839148930141, 0.036968318309102736, 0.03659529788153512, 0.03645628954683031, 0.036512529658419744, 0.03641727975436619, 0.03664481750556401, 0.03654364994594029, 0.03632664867809841, 0.036315038885389055, 0.036743721110480174, 0.03661212376185826, 0.03657125702926091, 0.036455188819340296, 0.03645770102739334, 0.036436729516301836, 0.03752173577036176, 0.03748874034200396, 0.03661270443882261, 0.03731371624129159, 0.037502042821475436, 0.0371460440329143, 0.037287945236478534, 0.03711551657744817, 0.03679011344909668, 0.03771941798073905, 0.03696671664714813, 0.03649359541279929, 0.03690262522016253, 0.03637776379074369, 0.036216533301132066, 0.036268343584878106, 0.03649072766304016, 0.036559976850237166, 0.03656834994043623, 0.03688531909670149, 0.03625809320381709, 0.03671312860080174, 0.03599726097924369, 0.03600705610854285, 0.03592844120093754, 0.036101552418300084, 0.03612829497882298, 0.03578328354018075, 0.03593373405081885, 0.035758942535945346, 0.03576170144336564, 0.03573419025966099, 0.03575018899781363, 0.03585702385221209, 0.03602905531014715, 0.03571583833013262, 0.03567078837326595, 0.03637643132890974, 0.03586897236960275, 0.03573832895074572, 0.0361015670640128, 0.03605636707374028, 0.03588893800973892, 0.03580253192356655, 0.0356551547561373, 0.03572525680065155, 0.035865369950022014, 0.0358495996253831, 0.03612384370395116, 0.0363554436819894, 0.03678630036967141, 0.036416010430880955, 0.03645140345607485, 0.03664949246815273, 0.03632026110376631, 0.03592037882123675, 0.03663846594946725, 0.03689243180411202, 0.036755137060369765, 0.03640754290989467, 0.03660608904702323, 0.036523741739136834, 0.03619371473789215, 0.03603570767811366, 0.03589788453919547, 0.036195939864431106, 0.035836592401776994, 0.03595819541386196, 0.035947997399738855, 0.03580265551805496, 0.035864452464239936, 0.03570853625025068, 0.035656329904283794, 0.03580137278352465, 0.035726932457515174, 0.03567023055894034, 0.03549320495554379, 0.03597267976828984, 0.03617350548505783, 0.03582785436085292, 0.036223422799791606, 0.036044411190918516, 0.03577433404113565, 0.03584747076034546, 0.035820083703313556, 0.03567177483013698, 0.03568336750779833, 0.0360062198979514, 0.03657114429133279, 0.03661294153758458, 0.036085624013628276, 0.03683799965041024, 0.03602322016443525, 0.03573185997349875, 0.03576376071998051, 0.03566843603338514, 0.03587455110890525, 0.03576675363949367, 0.035946581406252724, 0.03604012889521462, 0.036210248810904365, 0.035942017223153794, 0.03594618729182652, 0.036210055777004785, 0.03594308997903552, 0.03577835542815072, 0.03573585118566241, 0.03665803483554295, 0.036109852705683024, 0.0364812525681087, 0.03706816707338605, 0.037054821252822875, 0.03638859876564571, 0.03608119070529938, 0.03605174720287323, 0.03562901445797512, 0.03625162363052368, 0.03604699168886457, 0.0359052677665438, 0.03611994658197675, 0.03611913689545223, 0.035516805308205744, 0.03548647852880614, 0.03564961884702955, 0.035389130541256496, 0.035424415000847406, 0.03542536965438298, 0.03543239457266671, 0.03533712846892221, 0.03597294807434082, 0.03661446213722229, 0.0364371006829398, 0.03580697979245867, 0.03599860923630851, 0.03548522514956338, 0.03526196803365435, 0.035363882184028625, 0.03537211554391043, 0.035319259081568036, 0.035456227915627615, 0.035193354146821156, 0.03528982354061944, 0.03578418254852295, 0.03599529266357422, 0.03628290729863303, 0.035865372845104763, 0.03594693626676287, 0.036320565938949584, 0.035714691366468154, 0.035547943796430316, 0.035378096529415676, 0.03536350011825561, 0.035266785536493574, 0.036156514925616126, 0.03620525019509452, 0.0357534110546112, 0.03579030769211906, 0.035531532881515365, 0.03533063965184348, 0.03550758923803057, 0.0355809109551566, 0.035571190118789675, 0.03551051463399615, 0.035624001792499, 0.035523804085595266, 0.035660833631243026, 0.03572226754256657, 0.03538741443838392, 0.03556373085294451, 0.03542298078536987, 0.03562618357794625, 0.0358902781350272, 0.0355850944348744, 0.035706164581435065, 0.03519383805138724, 0.035520534153495514, 0.03548805373055594, 0.03570433173860822, 0.03657223335334233, 0.03684343133653913, 0.036750986916678295, 0.03668531545570918, 0.03637307694980076, 0.035579654148646764, 0.03585796598877226, 0.03593386914048876, 0.03562041963849749, 0.03574249250548227, 0.03539808077471597, 0.035878311310495646, 0.03559056554521833, 0.035883423941476006, 0.03549590621675764, 0.03538918478148324, 0.035303308963775634, 0.035271697384970525, 0.03621077409812382, 0.03524883606604167, 0.03521719732454845, 0.03522194462163108, 0.03550349082265582, 0.03529216655663082, 0.03543718559401376, 0.03551736439977373, 0.035181238651275634, 0.03527702348572867, 0.03523232306752886, 0.03540276050567627, 0.03554934586797442, 0.0352696338721684, 0.03541447503226144, 0.035465797356196815, 0.03599968995366778, 0.03560601200376238, 0.035657631669725694, 0.03524917585509164, 0.03531935828072684, 0.03528523755925042, 0.03574079777513232, 0.03578438758850098, 0.03577229235853468, 0.03599946462682315, 0.03550090994153704, 0.03529134716306414, 0.035116058077131, 0.03542440491063254, 0.035136342218944, 0.035432036774499076, 0.0349889612197876, 0.03545716966901507, 0.03525421679019928, 0.03597752179418291, 0.03525448731013707, 0.03567628971167973, 0.03537249041455132, 0.0348501672063555, 0.035096037558146886, 0.034815045935767036, 0.03470987183707101, 0.03490785258156913, 0.03490052853311811, 0.034734552928379604, 0.03518984147480556, 0.03510999143123627, 0.03559698432683945, 0.035230713060923985, 0.03549155260835375, 0.03512691182749612, 0.03552374797207969, 0.03515869515282767, 0.03520698896476201, 0.03493923902511597, 0.034641247051102775, 0.03479997720037188, 0.0347696812238012, 0.03481665228094374, 0.03486234324319022, 0.03497129108224596, 0.03460723672594343, 0.03469684273004532, 0.03464195447308677, 0.03489874754633222, 0.034670237983976095, 0.034607239280428206, 0.03437441170215607, 0.03481695592403412, 0.03478646431650434, 0.03506265474217279, 0.03473339242594583, 0.034683527520724705, 0.034536782375403814, 0.034438030890056065, 0.03436820915767125, 0.034901824167796545, 0.03451706596783229, 0.03430685698986054, 0.03430512828486306, 0.03446510374546051, 0.03519286896501269, 0.03486751207283565, 0.03494847395590373, 0.034664866562400545, 0.034463841574532646, 0.03438208750316075, 0.034119827577045986, 0.03411584888185774, 0.03378209437642778, 0.03421028801373073, 0.03435147830418178, 0.03428097878183637, 0.03401668718882969, 0.034141189626285005, 0.03396380445786885, 0.034212111915860856, 0.03396827127252306, 0.03401659676006862, 0.034276722414152964, 0.03426506280899048, 0.034900463138307844, 0.03377790170056479, 0.03429340600967407, 0.03431627937725612, 0.03359725866998945, 0.034610873120171685, 0.03509414383343288, 0.034549076897757394, 0.034778424331120085, 0.034239181109837126, 0.0337492653301784, 0.034107835377965655, 0.03446515730449132, 0.034299495645931787, 0.033834359007222314, 0.03385981542723519, 0.03366983013493674, 0.03343656165259225, 0.03363481402397156, 0.03429826966353825, 0.03408126984323774, 0.03362175992556981, 0.034287623507635936, 0.034423764433179585, 0.03445346589599337, 0.03339426866599492, 0.03286436200141907, 0.033014379910060336, 0.032782315696988786, 0.03241228290966579, 0.032501692771911624, 0.03280847694192614, 0.032315690091678076, 0.03239873647689819, 0.03287548524992807, 0.032678655896868025, 0.0324445264680045, 0.032389628035681586, 0.032870850477899824, 0.03328661927155086, 0.03297398728983743, 0.03304543239729745, 0.03252414788518633, 0.03292911980833326, 0.032655556414808544, 0.03197312831878662, 0.03223488679953984, 0.03183557621070317, 0.03171211889811924, 0.03189393877983093, 0.03163995255317007, 0.03162215667111533, 0.03181986042431423, 0.03212875391755785, 0.03132551065513066, 0.03143147349357605, 0.03131552185331072, 0.031024366787501746, 0.031173091658524103, 0.031108603818075998, 0.03126214419092451, 0.030890506420816696, 0.03124258692775454, 0.030743127380098614, 0.03070611025605883, 0.030891976143632618, 0.030850578887122018, 0.030530506031853814, 0.03034783022744315, 0.030633321149008616, 0.03067452209336417, 0.030519540905952453, 0.030624033893857684, 0.030373044865471977, 0.030364858763558523, 0.03034772208758763, 0.030965962750571115, 0.02996946198599679, 0.030062862294060844, 0.029904042993273053, 0.02977897082056318, 0.02956930309534073, 0.029723271131515502, 0.029427531957626343, 0.029463278651237487, 0.029486544472830636, 0.029648407995700836, 0.029521405100822448, 0.03015821158885956, 0.030508141687938146, 0.029656072301524027, 0.02937143623828888, 0.029484540351799555, 0.02934282898902893, 0.02907006459576743, 0.02921596850667681, 0.028666650141988482, 0.029035118222236634, 0.028453681213515145, 0.02863917521068028, 0.02888971388339996, 0.028720313693795886, 0.029278420039585657, 0.02845349975994655, 0.028185699496950423, 0.027958517159734454, 0.028001088670321875, 0.0284165606541293, 0.028017132537705557, 0.027792860865592958, 0.02775865456887654, 0.02852878472634724, 0.027534611906324113, 0.027978884662900654, 0.028246936116899764, 0.027203989582402367, 0.02708686249596732, 0.02715916565486363, 0.02710832540478025, 0.027067762187549044, 0.027264814121382576, 0.026967025995254516, 0.02713244072028569, 0.026721219292708806, 0.026886217849595207, 0.02647469665322985, 0.02670540043285915, 0.026436008257525307, 0.026497101613453457, 0.026269532952989852, 0.02631903712238584, 0.026405562247548784, 0.026691511613982064, 0.02680415059838976, 0.026851014920643398, 0.026917580366134643, 0.02649596197264535, 0.026211066884653907, 0.02687421705041613, 0.026265244909695215, 0.02676106427397047, 0.026228736128125874, 0.02642823836633137, 0.02616586487208094, 0.025030627165521895, 0.025529478873525347, 0.02551721692085266, 0.025575803220272066, 0.02516916837011065, 0.025007669712815966, 0.024713038482836316, 0.025000558878694262, 0.024919970503875188, 0.025172845040048873, 0.025649328465972628, 0.024875291330473764, 0.02457898335797446, 0.024533937999180386, 0.02430329837969371, 0.024241368770599365, 0.02450677441699164, 0.024127183694924628, 0.023907207250595093, 0.024037799154009137, 0.02418349862098694, 0.024388965368270874, 0.024289728105068208, 0.023917181534426554, 0.02392256566456386, 0.024651787281036376, 0.02384429505893162, 0.02360524092401777, 0.02318814524582454, 0.023614669527326312, 0.023331588166100637, 0.023390837184020453, 0.02409477208341871, 0.02287139347621373, 0.022690516327108657, 0.02290630349091121, 0.02243223828928811, 0.02224382698535919, 0.02247887900897435, 0.02316664559500558, 0.022456463745662143, 0.022365305764334542, 0.021944793335029057, 0.021910789353506904, 0.02180420858519418, 0.02189301269395011, 0.02176391716514315, 0.021583428084850313, 0.021616900520665304, 0.021468310483864377, 0.021620413150106158, 0.021519872971943448, 0.021613534135477883, 0.021553407737186978, 0.02189488185303552, 0.022055435947009497, 0.02183947844164712, 0.021199114280087607, 0.020890939916883195, 0.02134159641606467, 0.020999215756143844, 0.02087384364434651, 0.02103108627455575, 0.020529117924826486, 0.020766095157180513, 0.02106422117778233, 0.020896036028862, 0.02059541140283857, 0.020344161071947642, 0.020324960861887252, 0.02015163847378322, 0.020239353605679102, 0.020036278452192035, 0.02081233105489186, 0.020309515254838127, 0.020183158069849015, 0.01958301365375519, 0.019509086949484687, 0.019273621759244373, 0.019533742240497046, 0.01937256438391549, 0.019590516814163753, 0.01958717622927257, 0.019380760746342797, 0.019420370289257593, 0.01966128442968641, 0.01915087171963283, 0.01928983015673501, 0.01889512864606721, 0.018965085574558803, 0.018467515792165483, 0.018675257308142526, 0.018476778950010028, 0.018433032844747815, 0.018405019044876098, 0.018915411574499948, 0.018774247765541076, 0.018422368722302573, 0.018115420298916954, 0.018238452289785656, 0.01826911551611764, 0.018049830538885935, 0.01912556924990245, 0.01766613760164806, 0.01811464488506317, 0.018114709854125978, 0.017581684504236493, 0.017711796334811618, 0.017561709455081394, 0.01747770145535469, 0.017273285984992982, 0.017772555010659354, 0.01779447853565216, 0.01750277838536671, 0.017020468094519205, 0.017160245435578484, 0.017216926429952893, 0.0175992579971041, 0.01747755983046123, 0.017051398072923933, 0.016715653240680695, 0.01665619956595557, 0.016476757675409317, 0.016495666205883025, 0.016626935856682915, 0.01717450371810368, 0.01704972833395004, 0.01663643091917038, 0.017005601355007716, 0.016236687643187385, 0.016856206910950796, 0.016265658949102674, 0.016833341121673583, 0.015864671085562023, 0.016296362493719373, 0.0157597136923245, 0.01588952613728387, 0.01620193489960262, 0.015348281902926308, 0.015480091656957354, 0.015975099291120256, 0.016257183189902987, 0.015604439633233207, 0.01593412086367607, 0.015465826647622244, 0.015410261920520238, 0.015106383434363773, 0.014948757461139134, 0.014800845703908376, 0.014787316918373108, 0.014860007954495293, 0.01496375846011298, 0.014672267607280185, 0.01471382098538535, 0.014582938296454294, 0.014747005105018616, 0.01474245514188494, 0.014893373250961304, 0.01446897846779653, 0.014638914806502206, 0.014532672390341759, 0.014249085869107928, 0.014756901924099241, 0.014313077671187264, 0.014563394316605159, 0.014353778532573155, 0.013867933409554618, 0.01412165179848671, 0.013819206271852767, 0.013786618326391492, 0.013721806066376822, 0.01377038380929402, 0.013648737158094134, 0.013687708207539149, 0.013505775162151882, 0.013336501568555831, 0.013497942643506186, 0.013679172481809344, 0.013612149975129537, 0.013085923375827925, 0.013480547666549682, 0.013352917305060795, 0.012978785506316594, 0.013121216424873896, 0.012812923618725368, 0.013338228464126587, 0.013211378284863063, 0.012772731887442724, 0.012713252220835004, 0.012729124277830123, 0.012539034145218985, 0.012634031644889286, 0.012619568748133523, 0.012474196127482823, 0.012459448959146227, 0.01257855543068477, 0.012525732261793954, 0.012526500310216631, 0.012314261496067047, 0.012065977241311754, 0.01234530210494995, 0.012004797884396144, 0.012094816224915641, 0.012080189628260476, 0.011847180489982878, 0.01207439197493451, 0.011906342889581407, 0.01199372730084828, 0.012082595122711998, 0.011725637944681304, 0.011714944328580584, 0.011519235287393842, 0.01172718923006739, 0.011575166774647577, 0.011552950590848923, 0.01140900854553495, 0.011447491603238241, 0.011256554169314249, 0.011131723821163177, 0.011069554133074625, 0.01119822655405317, 0.010981521627732686, 0.010993222040789468, 0.010905692832810537, 0.011080627611705234, 0.011067524743931634, 0.010789339755262648, 0.011355388121945517, 0.011232251737798963, 0.011075452012675149, 0.010876512953213282, 0.010871590695210865, 0.011491990302290235, 0.010552480944565365, 0.011452673162732805, 0.011118333893162864, 0.011030267498322895, 0.010870185473135539, 0.01047149087701525, 0.010406563367162431, 0.01109358976994242, 0.011153015153748648, 0.010779251754283905, 0.010501493087836674, 0.01058197843176978, 0.010369694083929062, 0.010128150965486254, 0.010260909966060093, 0.010059685749667032, 0.010137779372079032, 0.009890213693891254, 0.009850109262125832, 0.009779947570392064, 0.009792987853288651, 0.009775440096855164, 0.009682641540254866, 0.010029944224017007, 0.009515877004180636, 0.009578783512115479, 0.010219695908682687, 0.009890519423144204, 0.009821441727025168, 0.009535133434193476, 0.010151032592569078, 0.009465067727225168, 0.009588977226189205, 0.009201554719890866, 0.00943517655134201, 0.009262701028159686, 0.009203766924994333, 0.009498646344457354, 0.009385382043463843, 0.00899073737008231, 0.009367447580610002, 0.009055489642279488, 0.009632566177419253, 0.00897889216031347, 0.009090964666434697, 0.009184756193842207, 0.008937103705746787, 0.008898095041513444, 0.0088490101482187, 0.008921805300882884, 0.008800221596445355, 0.008819951074463981, 0.008797217692647661, 0.008745395690202712, 0.009084960051945278, 0.008469814189842769, 0.008661889423217092, 0.008325150523866925, 0.009074317067861557, 0.008609237117426735, 0.008297902771404811, 0.00854536976133074, 0.008294513268130166, 0.008399135725838797, 0.008348416132586344, 0.00843437373638153, 0.008378865484680448, 0.008080381112439292, 0.008267753848007747, 0.008101903987782343, 0.008266801408358983, 0.008655850120953152, 0.00816956319979259, 0.008375633231231144, 0.008337208884102958, 0.008010313255446298, 0.008043083037648882, 0.008170127017157419, 0.008019842335156032, 0.008233603877680643, 0.007800143605896405, 0.007987378048045295, 0.008167215585708619, 0.007923814505338668, 0.0076072809738772255, 0.007809437151466097, 0.007555246566023146, 0.007609834117548806, 0.007590930440596172, 0.007678014082568033, 0.007469926391329084, 0.007629362685339792, 0.007343425750732422, 0.0075071055229221076, 0.0072600759140082766, 0.007409673482179642, 0.007538346807871546, 0.007246049983160837, 0.007310796550342015, 0.007572159937449864, 0.0071843321621418, 0.007921326096568789, 0.007201159085546221, 0.007332722826727799, 0.007146854368703706, 0.0074752024454729895, 0.007019596163715635, 0.007309012753622873, 0.007014106171471732, 0.0076673718435423715, 0.007023740538528988, 0.007310612031391689, 0.007005185272012438, 0.006965168075902121, 0.007135461355958666, 0.006840608758585794, 0.007101293615996838, 0.006803037183625358, 0.006956059017351695, 0.0068593411360468185, 0.007269072575228555, 0.006832425253731864, 0.0068225616748843875, 0.006847549229860306, 0.006623260165963854, 0.00656546026468277, 0.006539715613637652, 0.007435213880879538, 0.0069422399997711185, 0.006612169380698885, 0.006587646986757006, 0.006690473194633211, 0.007266107840197427, 0.006761522952999388, 0.006616884384836469, 0.006425039172172547, 0.006420116530997413, 0.006304964146443776, 0.006685463339090347, 0.006271639870745795, 0.007303517545972552, 0.00643538304737636, 0.006936047162328447, 0.006880467917237963, 0.006914924851485661, 0.006803022176027298, 0.006686096361705235, 0.007141486321176801, 0.006724284951175962, 0.006847700434071677, 0.006235472304480416, 0.0063222567737102504, 0.005981609172054699, 0.006275971361568996, 0.006007228493690491, 0.0061241017494882855, 0.005925294671739851, 0.006131020784378052, 0.005859402758734567, 0.0061778223940304345, 0.00614888425384249, 0.005715334862470627, 0.005785815566778183, 0.005967057145067623, 0.005784193937267576, 0.005867464851055827, 0.005699750589472907, 0.0060326238828045985, 0.005971264126045363, 0.00570204211132867, 0.005714542248419353, 0.005808117389678955, 0.005683924497238227, 0.005594666855675834, 0.005734328946896961, 0.005782041102647782, 0.005948271879128047, 0.005471887801374708, 0.0055186977769647325, 0.005782866179943085, 0.005456235110759735, 0.0055426011553832465, 0.005451455584594181, 0.005463048326117652, 0.0054408268204757144, 0.005367931382996695, 0.005511544368096761, 0.005705663476671491, 0.005475733258894512, 0.005394985207489558, 0.005310075623648507, 0.005306115363325392, 0.005470654517412185, 0.005294583759137562, 0.005483447419745582, 0.0051685972086020875, 0.005460527901138578, 0.005057065561413765, 0.005089411501373564, 0.005019711149590356, 0.0050906130032879965, 0.005057851150631905, 0.00499185494014195, 0.005006173274346761, 0.004952044295413153, 0.005214591228536197, 0.005031854787043163, 0.0050584822254521504, 0.005001062644379479, 0.00514861062169075, 0.0050951110039438524, 0.005061827727726528, 0.004785365739039012, 0.005021118968725205, 0.004922094281230654, 0.0049545534806592125, 0.0050995574465819765, 0.004783809844936643, 0.004989166323627744, 0.004898118163858141, 0.004930127561092377, 0.0049447306990623476, 0.0048224433830806185, 0.004704706051519939, 0.004898223791803632, 0.0047378340789249965, 0.00478927099278995, 0.004743581563234329, 0.004602684250899724, 0.004772739027227674, 0.004639493865626199, 0.004802356639078686, 0.00466876996415002, 0.00505853288940021, 0.005031847315175193, 0.004952712612492698, 0.004776255552257811, 0.0048035186529159545, 0.0045593320684773585, 0.004577544693435942, 0.0045753670164517, 0.004674943887761661, 0.004588202429669244, 0.004625104516744614, 0.004660179402147021, 0.004573954769543239, 0.0046912251625742234, 0.004552494606801441, 0.004616518254790987, 0.004551108373062951, 0.004823446571826935, 0.0057185739278793335, 0.005881492389099938, 0.0045938906073570256, 0.004538575240543911, 0.004329119005373546, 0.004259971050279481, 0.004346110054424831, 0.004384851370538984, 0.004553912015897887, 0.004284453988075257, 0.004248562519039427, 0.004375535781894411, 0.004303573369979858, 0.004148654852594648, 0.004129337263958794, 0.004129724800586701, 0.004060898572206497, 0.004159534105232784, 0.004369581639766693, 0.0048650403959410535, 0.004617012483733041, 0.004157648235559463, 0.0041839699447154995, 0.003974976731198175, 0.00414516425558499, 0.003998910976307733, 0.003975517643349511, 0.003942937393273626, 0.004009489949260439, 0.004121015306030001, 0.004124395943113736, 0.004019957716975893, 0.003991031369992665, 0.004144981088382857, 0.004380585380962917, 0.004093503153749875, 0.0042064459728343145, 0.004059887869017465, 0.0040307152697018215, 0.004006394424608775, 0.004153067916631698, 0.0038687369067754063, 0.003942463163818632, 0.0038796432529176984, 0.0037614661242280686, 0.0038229510400976455, 0.003894508129784039, 0.0041069716853754865, 0.003810868433543614, 0.004188147549118315, 0.0039470510663730755, 0.00400627538561821, 0.003915516755410603, 0.0037447742479188102, 0.0036750410071441104, 0.0037630164410386766, 0.003661326595715114, 0.0035687886657459395, 0.003574824162891933, 0.0036487472695963725, 0.003686535486153194, 0.0036212813854217527, 0.0035969671607017518, 0.0035514613134520396, 0.0035649416595697403, 0.0035321923451764243, 0.003586090196456228, 0.003732468464544841, 0.0034555378130504064, 0.004189671277999878, 0.003850928544998169, 0.003569804387433188, 0.0034945612294333323, 0.003678220467908042, 0.0037309891411236354, 0.00352636707680566, 0.0034888796508312225, 0.0035007998773029874, 0.003586674439055579, 0.003415008091500827, 0.0033281378660883223, 0.0035401539185217447, 0.003498986054744039, 0.0035580820270947047, 0.0035121973497526985, 0.003630620241165161, 0.0038201191808496203, 0.0034044213167258672, 0.003356881705777986, 0.003429917906011854, 0.0033069168350526265, 0.0035945642633097514, 0.0035478914848395757, 0.0034602306676762445, 0.0032978596218994688, 0.003203474687678473, 0.0033623147649424418, 0.003165385339941297, 0.003233032513942037, 0.003271301112004689, 0.003235795774630138, 0.003220984318426677, 0.003183810476745878, 0.003264748901128769, 0.0031173736921378545, 0.0034095653146505355, 0.0031727920366185053, 0.003172363724027361, 0.0032131059467792513, 0.0034045201327119554, 0.003271979476724352, 0.0031458396996770586, 0.00306197821029595, 0.003081179229276521, 0.0030304241180419923, 0.003065043240785599, 0.00312343781547887, 0.002967087998986244, 0.00328602491744927, 0.003695218882390431, 0.003174519911408424, 0.0034879755760942187, 0.003183350541761943, 0.0030273361610514775, 0.002988404908350536, 0.0028952345890658243, 0.002930780574679375, 0.0029936498084238597, 0.0030854331063372748, 0.0031094645602362495, 0.003144930207303592, 0.0030673016607761384, 0.0033570683641093117, 0.0030152113416365214, 0.0031010987290314264, 0.0029919763122286114, 0.003309712665421622, 0.0028969577699899675, 0.002973315715789795, 0.0029751472813742503, 0.0030108876526355743, 0.0029387277471167703, 0.0029465152748993466, 0.002991000690630504, 0.0029884049615689686, 0.0029189916806561605, 0.002992890120616981, 0.0030426728512559617, 0.0028153057928596223, 0.0028507151561123984, 0.0027169151497738703, 0.0028557729508195606, 0.0028706987095730645, 0.002757034280470439, 0.0028516431472131185, 0.0028229877139840805, 0.0029924691787787845, 0.0029600831121206284, 0.0027939876062529428, 0.002712216611419405, 0.002689066476055554, 0.0027552633945431026, 0.0027038973037685665, 0.0027384306703295024, 0.0026844502133982523, 0.0026407837122678756, 0.0026932978789721215, 0.0027244550096137184, 0.002747056973831994, 0.002580141073891095, 0.0026325038501194544, 0.0025996283388563567, 0.002656433167202132, 0.0025784142847572055, 0.002601464007581983, 0.0026049950772098133, 0.0027355374981250084, 0.002537362660680498, 0.0026556898121322904, 0.00254813018654074, 0.0025623097749693053, 0.0025094274005719596, 0.002527491237436022, 0.0027441465428897313, 0.0028322519361972808, 0.0026861564176423208, 0.002548896963042872, 0.002497738227248192, 0.0026831625827721185, 0.0026034763987575257, 0.0025690088527543205, 0.0025305382907390593, 0.002526502694402422, 0.0025432610724653516, 0.002692909251366343, 0.0024497416402612412, 0.0025932951590844563, 0.0023710830083915166, 0.002686965124947684, 0.002599560001066753, 0.0024412866362503597, 0.002406540523682322, 0.0023901823163032534, 0.002456630053264754, 0.0023829878121614458, 0.002400362129722323, 0.002465152218937874, 0.00229763782450131, 0.002454071513244084, 0.0024966018327644895, 0.002374167750988688, 0.002576308601668903, 0.002554505861231259, 0.0025481879711151123, 0.002586871523942266, 0.0024142918416431972, 0.0026457479915448596, 0.0026371718517371585, 0.00250370911189488, 0.00252195805311203, 0.00255622070814882, 0.0025906667858362196, 0.0027818457356521063, 0.0023956456035375594, 0.0023924167454242706, 0.002464424531374659, 0.002217326866728919, 0.002222648917564324, 0.0021992555899279457, 0.0022035034000873564, 0.0021969182470015117, 0.0021975536750895638, 0.002177918760904244, 0.0021973361926419395, 0.0022104462023292267, 0.002285181762916701, 0.002222141016806875, 0.002198918323431696, 0.0021397764555045537, 0.00212040085877691, 0.0021482894516416957, 0.0024489536029951913, 0.0021393960820777075, 0.0021841373613902502, 0.0022450373428208486, 0.0022095280779259544, 0.0021201893155063903, 0.0020617384782859256, 0.002081544888871057, 0.002207610521997724, 0.0023622113413044386, 0.0021451220235654284, 0.0021450238142694746, 0.002111422089593751, 0.00223940977028438, 0.0021263188123703002, 0.0020579307100602557, 0.0022385934101683755, 0.0021931063490254537, 0.0022142017632722855, 0.002151936026556151, 0.0020752470940351487, 0.0019976868267570225, 0.0020612356066703795, 0.001975549321089472, 0.001998564675450325, 0.0021896024474075864, 0.00203055313123124, 0.002085412634270532, 0.002348028974873679, 0.002094558285815375, 0.002123050083007131, 0.002070491686463356, 0.0021163833567074365, 0.0019319984636136463, 0.001969060152769089, 0.002030039674469403, 0.0019837323895522526, 0.0019547009095549584, 0.002067003388489996, 0.002283699054803167, 0.0019563267699309757, 0.0020223847989525113, 0.0019266372599772044, 0.0019230839822973523, 0.0018787723353930882, 0.0018631958748613086, 0.002024618610739708, 0.001996811968939645, 0.0019551378488540648, 0.0019256074726581574, 0.0018893616540091379, 0.001856384766953332, 0.0018817951530218124, 0.0018190525685037886, 0.0018843223580292292, 0.0018511288187333516, 0.0018135620281100272, 0.0018795327310051238, 0.001833283028432301, 0.0018388761899300985, 0.0017930107776607785, 0.0018053179340703147, 0.0018617389564003263, 0.00195346403334822, 0.0018274847630943571, 0.0018619651773146221, 0.001850401306790965, 0.0018161817054663386]}, {\"mode\": \"lines+markers\", \"name\": \"test loss\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500], \"y\": [45.44391031901041, 42.52290283203125, 38.40029215494792, 32.0791015625, 25.783380533854167, 19.60687032063802, 15.836796468098958, 14.293351033528646, 11.93027587890625, 10.609971618652343, 9.450999043782552, 8.542799886067709, 8.006918029785156, 7.608972880045573, 7.276858011881511, 6.993723704020183, 6.741330668131511, 6.379528554280599, 6.179785868326823, 5.800402119954427, 5.522646942138672, 5.25742182413737, 4.98782475789388, 4.742707061767578, 4.476126352945964, 4.2677082824707036, 4.040258560180664, 3.837067209879557, 3.6650565592447917, 3.4835020955403646, 3.3358389790852865, 3.1804549153645834, 3.0564522043863933, 2.928159891764323, 2.8001726786295573, 2.6932354227701825, 2.5925677744547526, 2.499085693359375, 2.407196095784505, 2.3214836883544923, 2.246223627726237, 2.166055107116699, 2.0933719126383465, 2.029104919433594, 1.9638909657796224, 1.9011312357584635, 1.8433692932128907, 1.7878907012939453, 1.7411700948079427, 1.68232053120931, 1.636246592203776, 1.6042938105265299, 1.5450938924153645, 1.499143009185791, 1.4552593231201172, 1.417827428181966, 1.3703770192464193, 1.3305844243367513, 1.3050157928466797, 1.2542629369099936, 1.2161363474527995, 1.1943805694580079, 1.1490159225463867, 1.1166375541687013, 1.0850351079305014, 1.05238650004069, 1.0202974700927734, 0.9925791422526041, 0.9744871075948079, 0.9391834767659505, 0.9135560989379883, 0.8894112650553385, 0.8614734331766765, 0.8366105270385742, 0.8162078221638998, 0.7924658139546712, 0.7685951105753581, 0.7542390060424805, 0.7341680018107096, 0.7134714253743489, 0.7015699132283528, 0.6738901583353678, 0.6767760276794433, 0.6506855201721191, 0.6300948270161947, 0.6037105242411296, 0.5891882006327311, 0.5787834866841635, 0.5660809739430746, 0.546856722831726, 0.5344074312845866, 0.5186117808024089, 0.5064041328430176, 0.4951948134104411, 0.48667068481445314, 0.4700464884440104, 0.4578810977935791, 0.44653916358947754, 0.4376692008972168, 0.43138689041137696, 0.41730790774027504, 0.4064635483423869, 0.39940746943155925, 0.3894127368927002, 0.37973527908325194, 0.37261393388112385, 0.36398846944173174, 0.35510183334350587, 0.34738712628682455, 0.3481880791982015, 0.3318548043568929, 0.3234843413035075, 0.3160823599497477, 0.3104459603627523, 0.30161172231038413, 0.29453259309132895, 0.28883358001708986, 0.2884225718180339, 0.27835203806559244, 0.2705985101064046, 0.2652547597885132, 0.25951340357462566, 0.2564745775858561, 0.24787041187286377, 0.2416262912750244, 0.23538471539815267, 0.23031744003295898, 0.22622080643971762, 0.22136897404988606, 0.21361524105072022, 0.2073229726155599, 0.2039631732304891, 0.19736622015635172, 0.19180401166280112, 0.18576603571573894, 0.18196029663085939, 0.17514897346496583, 0.16980000178019206, 0.16366486072540284, 0.1597981389363607, 0.15691054821014405, 0.15040341059366863, 0.1464380892117818, 0.1456976588567098, 0.14205946207046508, 0.1364527376492818, 0.13412150700887043, 0.15133220513661702, 0.13057939688364664, 0.126918998559316, 0.12324043114980061, 0.12052989959716796, 0.12481125831604004, 0.11764923254648844, 0.11493801275889079, 0.11034592072168986, 0.11054800192515056, 0.10633888403574626, 0.11088672002156576, 0.10513456503550211, 0.10077269713083903, 0.10040454546610514, 0.10108327786127726, 0.10133620659510295, 0.09753204584121704, 0.10077878793080648, 0.09335575103759766, 0.10339872280756633, 0.09347591559092204, 0.09793750921885172, 0.09195206006368001, 0.09041444301605224, 0.09101369937260946, 0.08958681941032409, 0.08775610367457072, 0.08876316626866658, 0.0863871161142985, 0.08653059641520182, 0.08403550545374552, 0.08448682228724162, 0.08425728956858317, 0.08812374711036682, 0.08496585925420125, 0.08450762510299682, 0.08233175913492839, 0.08500083128611247, 0.08928048451741537, 0.09163218100865682, 0.10641684850056966, 0.08006296634674072, 0.0896164321899414, 0.08160550117492676, 0.09909943421681722, 0.08067988713582357, 0.08004240433375041, 0.0772265100479126, 0.08118877172470093, 0.07917763749758402, 0.07820285638173421, 0.07766543944676717, 0.07653461615244547, 0.07673419952392578, 0.0743130898475647, 0.07491818269093832, 0.07441524585088094, 0.07642295360565185, 0.07500986735026041, 0.0808606505393982, 0.07346078793207804, 0.07527246554692586, 0.07299911816914877, 0.08018040776252747, 0.07743483702341715, 0.07344271183013916, 0.07443172057469685, 0.07253602425257365, 0.07625532825787862, 0.07221531987190247, 0.0720255200068156, 0.07229564110438029, 0.07200074593226115, 0.07165353457132975, 0.07182802200317383, 0.08000906705856323, 0.07760783116022746, 0.0912924591700236, 0.10375655094782511, 0.08065959274768829, 0.07304824352264404, 0.07593583265940348, 0.07106061697006226, 0.07026283860206604, 0.07074055433273316, 0.07275503873825073, 0.07198660612106324, 0.07111458162466686, 0.07275405009587606, 0.07127079010009765, 0.07096299807230631, 0.06880009373029074, 0.07551954666773478, 0.07114518841107687, 0.07004942258199055, 0.07173773606618246, 0.06935535589853922, 0.0693936848640442, 0.06876548488934835, 0.06756957749525706, 0.06817926049232483, 0.06864583412806192, 0.06991180340449016, 0.0689262600739797, 0.06913368264834086, 0.06899904012680054, 0.07021461645762125, 0.06861514965693156, 0.06927793900171916, 0.06876382907231648, 0.0695395032564799, 0.06750421524047852, 0.06794477701187134, 0.07037706772486368, 0.06854799747467041, 0.06773040930430095, 0.06781342109044393, 0.06838934461275736, 0.06704684495925903, 0.06676218708356221, 0.06855499664942423, 0.06736103534698486, 0.06725791196028391, 0.06615812619527181, 0.06744236985842388, 0.06714773178100586, 0.06660079479217529, 0.06778921484947205, 0.06624642610549927, 0.06917679309844971, 0.06885914405186971, 0.0663949696222941, 0.06635336637496948, 0.06713797728220622, 0.06634917894999186, 0.07170140266418457, 0.0696131940682729, 0.06593314131100972, 0.06741478125254313, 0.06741929054260254, 0.0674716571966807, 0.06933230320612589, 0.06722471793492635, 0.06677262544631958, 0.0682680066426595, 0.066383509238561, 0.06919283151626587, 0.0672233812014262, 0.06626412789026896, 0.06637672742207845, 0.06718198696772258, 0.06600107034047445, 0.06707827806472778, 0.06578773101170858, 0.0651156226793925, 0.06523110528786977, 0.0658824090162913, 0.06602250039577484, 0.06728021581967672, 0.06568348129590353, 0.06536932349205017, 0.06503812154134114, 0.0658574875195821, 0.06924636085828145, 0.06614045778910319, 0.06674845019976298, 0.0662495493888855, 0.06598427693049112, 0.06672878424326578, 0.06552185932795207, 0.06663405736287435, 0.06492262681325277, 0.06517751137415569, 0.0641913123925527, 0.06560153404871623, 0.06673440774281819, 0.06597525080045065, 0.06508702437082926, 0.06575048645337422, 0.06516179521878561, 0.06509529829025268, 0.06564192771911621, 0.06517626682917277, 0.0654302207628886, 0.06622945308685303, 0.06511573870976765, 0.066074667374293, 0.0670743203163147, 0.06726465066274007, 0.06475247263908386, 0.0667914326985677, 0.06652928074200948, 0.06539415756861369, 0.06583486278851827, 0.06515449523925781, 0.06515783230463663, 0.06537001172701518, 0.06583752860625584, 0.0666999779144923, 0.06678088665008546, 0.06604144334793091, 0.06667587677637736, 0.06785593509674072, 0.06627456585566203, 0.06487331291039784, 0.06408724268277487, 0.06479875127474467, 0.06481645147005717, 0.06469001332918803, 0.06747332334518433, 0.06527276674906413, 0.06609214107195537, 0.06750400861104329, 0.06952843109766642, 0.0644903979698817, 0.06691755970319112, 0.06542698224385579, 0.06684089183807373, 0.065344899892807, 0.06754973193009695, 0.06751183390617371, 0.06498437523841857, 0.070464661916097, 0.06900143186251323, 0.06525652209917704, 0.06576901237169902, 0.06514221986134847, 0.06532112042109171, 0.06394737561543783, 0.06603620926539103, 0.0654953372478485, 0.06508925278981527, 0.0654534375667572, 0.06619437555472056, 0.06886655886967977, 0.0664363177617391, 0.06377163251241048, 0.06407216548919678, 0.06506198108196258, 0.06424355139334996, 0.06450823704401652, 0.06406293551127117, 0.06443589329719543, 0.06399502595265706, 0.0662644890944163, 0.06572198748588562, 0.06403627872467041, 0.06406950116157532, 0.06616599003473918, 0.06432996908823649, 0.06362273871898651, 0.06461310823758443, 0.06435922503471375, 0.06557968278725942, 0.06428927381833395, 0.06632944345474243, 0.06512495517730713, 0.06870321869850159, 0.06634234706560771, 0.06801086107889812, 0.0648187279701233, 0.06693430225054423, 0.06391063849131266, 0.06667115489641826, 0.06523555755615235, 0.06493995229403178, 0.06418663620948792, 0.06617962559064229, 0.06489583253860473, 0.06741076072057088, 0.06305130084355672, 0.06550020456314087, 0.06682482639948527, 0.06693761428197224, 0.066172254383564, 0.06443901856740315, 0.06546542525291443, 0.0643720289071401, 0.0645532989501953, 0.06347242633501689, 0.06350361347198487, 0.06482616504033406, 0.06351013580958048, 0.06491652488708496, 0.06352657894293467, 0.06385883828004202, 0.06689109474420547, 0.06375755230585733, 0.06367250442504883, 0.06457241574923198, 0.06417487462361654, 0.06452119032541911, 0.06341564814249674, 0.06437747836112977, 0.0639183270931244, 0.06407874981562296, 0.06462730884552002, 0.06447955886522928, 0.06599993785222372, 0.06455240209897359, 0.06507686138153077, 0.06369730591773987, 0.0641361161073049, 0.0626826282342275, 0.06593916893005371, 0.06514746606349946, 0.0642958378791809, 0.06374479134877523, 0.06287220637003581, 0.06424085338910421, 0.06387579202651977, 0.06844484170277913, 0.06364491820335388, 0.06699034889539082, 0.06452558318773906, 0.06410576303799947, 0.06371354659398397, 0.06399991114934285, 0.06408428271611531, 0.06438883145650227, 0.06437211607893308, 0.06629546999931335, 0.06541643897692362, 0.06675192674001058, 0.06579181949297587, 0.06628736873467764, 0.0631803834438324, 0.06704808314641317, 0.07135261217753093, 0.06687636454900106, 0.06406387984752655, 0.06415328105290731, 0.06561309933662414, 0.06371224919954936, 0.06304552396138509, 0.06384801665941874, 0.06743608057498932, 0.06518088102340698, 0.06545489231745402, 0.06543715635935465, 0.06381773352622985, 0.06374210556348164, 0.0647394057114919, 0.0664339280128479, 0.06434342126051584, 0.06389695644378662, 0.06353775223096211, 0.06317992051442464, 0.06432065308094025, 0.06375117858250935, 0.06446696639060974, 0.06788438280423482, 0.06390936732292175, 0.06867415110270182, 0.0651611328125, 0.06388803164164225, 0.06432365874449412, 0.06624755859375, 0.06796222448348999, 0.07382798592249552, 0.07082342624664306, 0.06961680094401042, 0.06632106979688009, 0.06888424634933471, 0.06487842718760173, 0.06418877998987833, 0.06445368607838949, 0.06349161744117737, 0.06409584601720174, 0.06737066109975179, 0.06628449440002442, 0.06391638179620107, 0.06439401090145111, 0.06388673504193623, 0.06464330514272054, 0.06404127756754557, 0.06322008490562439, 0.06455458442370098, 0.0636611545085907, 0.06460662603378296, 0.06579621235529581, 0.06332067569096883, 0.06320734103520712, 0.06456280986467998, 0.06332475264867146, 0.0670876391728719, 0.06405203421910603, 0.06339156309763591, 0.06275930166244507, 0.0629798158009847, 0.06444967269897461, 0.06491884271303813, 0.06443045139312745, 0.0628298838933309, 0.06273804267247518, 0.06361777583758037, 0.06499647895495096, 0.06323402961095174, 0.06268024762471516, 0.06378649711608887, 0.06484736204147339, 0.06369099626938501, 0.0626462988058726, 0.06443461259206136, 0.0634087069829305, 0.06450753331184388, 0.0635662325223287, 0.06297427852948506, 0.06351702332496643, 0.06863462527592977, 0.06520402272542318, 0.06415821075439453, 0.06451148430506388, 0.06458499729633331, 0.06539343436559042, 0.06513370205958684, 0.06429720759391784, 0.06489452958106995, 0.0636270785331726, 0.06408096114794413, 0.06351451595624288, 0.06627485513687134, 0.06614276647567749, 0.07010199069976807, 0.06906210025151571, 0.06466465950012207, 0.06413549582163493, 0.0633605182170868, 0.0637626059850057, 0.0632942259311676, 0.06300129453341166, 0.0645002269744873, 0.0638181201616923, 0.06400010148684183, 0.06300189057985942, 0.06382730364799499, 0.06267107486724853, 0.06429083426793417, 0.06301530758539836, 0.06331592281659444, 0.06405600825945536, 0.06305713415145874, 0.06213904579480489, 0.06284878412882487, 0.06457958658536275, 0.0638042438030243, 0.062119293808937076, 0.06221431255340576, 0.0631823992729187, 0.06455530285835266, 0.06480100790659586, 0.06291691263516744, 0.06498317817846934, 0.06302450259526571, 0.06357115228970846, 0.06307355463504791, 0.06348256369431814, 0.0633421152830124, 0.06346350570519765, 0.06319946606953938, 0.0627125608921051, 0.06272092262903849, 0.06292524258295695, 0.06485249181588491, 0.0631035190820694, 0.062383208672205606, 0.06421736796696981, 0.06302549719810485, 0.06631262342135112, 0.06326801180839539, 0.06528672456741333, 0.06396626909573873, 0.06457081238428751, 0.06371003071467082, 0.06407638351122538, 0.06886213938395182, 0.06436908801396687, 0.06554914196332295, 0.06273624122142792, 0.0639713990688324, 0.06333090623219807, 0.06239261964956919, 0.06250738620758056, 0.06362900793552399, 0.0628555178642273, 0.0626124362150828, 0.06364948908487955, 0.0624563988049825, 0.06566311796506245, 0.06338105916976929, 0.06366145213445028, 0.06357594132423401, 0.06435082256793975, 0.06338794430096945, 0.06525783260663351, 0.06347449203332266, 0.06407426714897156, 0.06360910336176555, 0.063044193983078, 0.06314199884732564, 0.06654381712277731, 0.06291260639826457, 0.07035076936086018, 0.06510253667831421, 0.06662354628245036, 0.06340271512667338, 0.06437168041865031, 0.06253405849138896, 0.062306047280629474, 0.0622945765654246, 0.061707686185836795, 0.06188539961973826, 0.06117551406224569, 0.06128550529479981, 0.06092534780502319, 0.06194661438465118, 0.06343926548957825, 0.06136691411336263, 0.06429376284281413, 0.06221260070800781, 0.06302377382914225, 0.06980457305908203, 0.06655725081761678, 0.07576685150464375, 0.07202724814414978, 0.06644745230674744, 0.06888809482256572, 0.06315234661102295, 0.06302362004915874, 0.06391825556755065, 0.06355176289876302, 0.06118181188901266, 0.06186902602513631, 0.06125890930493673, 0.06015502214431763, 0.0609316356976827, 0.06021581729253133, 0.06483270153403282, 0.060156272252400715, 0.06494487722714742, 0.06327808698018392, 0.060376266638437905, 0.06226873159408569, 0.0600083331267039, 0.05959574977556865, 0.061557933886845904, 0.06160665929317474, 0.060887213150660195, 0.06244983116785685, 0.061927882035573326, 0.06026432593663533, 0.058771708806355794, 0.0603818158308665, 0.06057270646095276, 0.05930097421010335, 0.05910265676677227, 0.06085869828859965, 0.059950734774271645, 0.05890275637308757, 0.059179602464040124, 0.06192671696345011, 0.060304749806722006, 0.05932203491528829, 0.05891797383626302, 0.061355546315511066, 0.05842691779136658, 0.058109509944915774, 0.06177476644515991, 0.0588531756401062, 0.05803365627924601, 0.06040096084276835, 0.06060326377550761, 0.05976331869761149, 0.05884829660256704, 0.059557158946990964, 0.05877775589625041, 0.060961941679318746, 0.058707163731257124, 0.06223055760065715, 0.060177427927652995, 0.059233415921529135, 0.05934769868850708, 0.059908775289853416, 0.057975090344746905, 0.056850032409032185, 0.0620816695690155, 0.056921457449595134, 0.05772351384162903, 0.058546809951464336, 0.05729273160298665, 0.05699596186478933, 0.05819690704345703, 0.056674938599268594, 0.056882713635762534, 0.05814996361732483, 0.05661752472321192, 0.058788485924402875, 0.060332541465759275, 0.05549669583638509, 0.05500649313131968, 0.056613974173863726, 0.05636020223299662, 0.055389870007832846, 0.05600273450215658, 0.05613683541615804, 0.05508845210075378, 0.05553804715474447, 0.05764014085133871, 0.054289714495340986, 0.056673672199249264, 0.059479057391484576, 0.06346242626508077, 0.062641881108284, 0.06059770981470744, 0.05567890882492065, 0.055595401922861734, 0.05517602403958639, 0.05858212649822235, 0.05947176774342855, 0.05484790484110514, 0.0594037667910258, 0.05360754370689392, 0.05341279298067093, 0.0542767858505249, 0.053400062918663026, 0.05399375100930532, 0.053870583772659304, 0.055415143370628354, 0.05392962098121643, 0.052617823282877604, 0.05332033554712931, 0.05285132805506388, 0.05536385039488474, 0.054068227807680766, 0.053898295561472576, 0.0579198956489563, 0.054710734287897744, 0.05244100709756216, 0.05538650830586751, 0.05251238147417704, 0.05331476092338562, 0.053227663437525434, 0.054206689596176146, 0.051901950041453045, 0.054841458797454834, 0.05244750579198201, 0.051773248116175334, 0.05226024031639099, 0.05289525796969732, 0.05321092406908671, 0.05423648238182068, 0.05159659425417582, 0.05146381378173828, 0.051882333159446715, 0.05177140653133392, 0.05101512769858042, 0.05288335005442302, 0.05304246584574381, 0.050771100918451946, 0.05126514613628388, 0.050447067022323606, 0.05162308911482493, 0.05274985472361247, 0.05307554334402084, 0.05080833156903585, 0.050459502935409545, 0.050795046289761864, 0.05047161599000295, 0.05011534730593364, 0.05011235555013021, 0.04925684014956156, 0.04974605242411296, 0.05349524309237798, 0.050388693362474445, 0.05051904181639354, 0.05056842386722565, 0.05084878106911977, 0.05107056240240733, 0.05013468662897746, 0.050557883580525716, 0.05043216864267985, 0.04942121282219887, 0.05054121474424998, 0.05113121112187703, 0.052895376284917195, 0.05008593534429868, 0.04996803601582845, 0.050377027789751685, 0.04877982616424561, 0.04915125826994578, 0.04886932770411174, 0.04955634911855062, 0.04958213478326798, 0.04995283087094625, 0.05232305596272151, 0.04803034881750742, 0.04885671963294347, 0.04913455307483673, 0.04769750515619914, 0.048858388264973955, 0.05072180559237798, 0.0476228525241216, 0.04839115967353185, 0.048907795151074726, 0.047527249256769814, 0.04923993011315664, 0.047807363669077556, 0.04734622836112976, 0.049562584161758426, 0.0477370415131251, 0.048869826396306354, 0.048017859756946564, 0.04861970106760661, 0.04725134551525116, 0.04765243709087372, 0.04723086734612783, 0.04786633968353272, 0.047441280980904894, 0.0479600997765859, 0.04811742583910624, 0.04753836353619893, 0.04817697683970133, 0.04731626192728678, 0.0489947364727656, 0.04664408326148987, 0.04894347141186396, 0.04690923050045967, 0.046719699303309124, 0.0474559219678243, 0.04632286310195923, 0.048673850198586784, 0.04814299722512563, 0.04983766138553619, 0.04818330744902293, 0.04921781102816264, 0.04750148773193359, 0.049040898482004804, 0.0476101964712143, 0.04812134385108948, 0.0482980485757192, 0.048735017577807106, 0.04914861897627513, 0.046845898032188416, 0.047174213528633116, 0.047010763486226397, 0.047400191227595014, 0.047029236356417335, 0.04646383742491404, 0.04702821850776672, 0.0460082620382309, 0.04672754396994909, 0.045812984456618626, 0.045798877080281575, 0.047771625717480976, 0.04640564143657684, 0.04705555240313212, 0.04786882042884827, 0.04632040162881215, 0.046018649041652676, 0.04633779486020406, 0.046389816403388975, 0.04672463893890381, 0.046730089137951535, 0.04749418983856837, 0.04888347218434016, 0.04849458952744802, 0.04783666988213857, 0.04942492693662644, 0.04636786679426829, 0.04582051614920298, 0.04642466882864634, 0.046406765977541606, 0.046800563732783, 0.04644298295180003, 0.04563860883315404, 0.04623777382075787, 0.04578462759653727, 0.04651474475860596, 0.0483345165848732, 0.04623239835103353, 0.04638065417607625, 0.0463153201341629, 0.04569118519624074, 0.04524319112300873, 0.04554136291146278, 0.04605126226941744, 0.045117212931315105, 0.04566356455286344, 0.045229668319225313, 0.04500990817944209, 0.04615075627962748, 0.04473756939172745, 0.04664788544178009, 0.04590145786603292, 0.045804307858149214, 0.04452724933624268, 0.04577170352141063, 0.044856578807036085, 0.045251582662264506, 0.045475163360436756, 0.04523757080237071, 0.045962833364804584, 0.045175260106722515, 0.04496195614337921, 0.044900293151537575, 0.044718724886576335, 0.045241005420684814, 0.04437640209992726, 0.044588112235069276, 0.04539581596851349, 0.04432470122973124, 0.045074427227179206, 0.044713003039360044, 0.04393117610365152, 0.04604445874691009, 0.04426545401414236, 0.04448243528604508, 0.04527313490708669, 0.0441195622086525, 0.04514036377271016, 0.04437165200710297, 0.04514354288578033, 0.04597446858882904, 0.044649247924486796, 0.04420827547709147, 0.045610297620296475, 0.04400587161382039, 0.044253854552904766, 0.04422226848701636, 0.044572790463765465, 0.044405163129170734, 0.044125359853108725, 0.04388260434071223, 0.04390883396069209, 0.043853271007537845, 0.04346366287519535, 0.04421730637550354, 0.044403391480445864, 0.043542272249857586, 0.04416383018096288, 0.04362102727095286, 0.04399350146452586, 0.044106442828973136, 0.04350111097097397, 0.04437912672758102, 0.04393303294976552, 0.04366651693979899, 0.044365545511245726, 0.043320386012395226, 0.04571181813875834, 0.04388247152169546, 0.045348746379216515, 0.044621531963348386, 0.044060176014900206, 0.044207054475943246, 0.043835134704907734, 0.043696182568868, 0.0437958163022995, 0.043600603193044665, 0.04443719923496246, 0.043911337951819104, 0.04533913572629293, 0.04349102934201558, 0.045429927110671994, 0.04402849555015564, 0.04373750805854797, 0.044631236592928565, 0.04544099489847819, 0.04580877820650737, 0.045373271306355795, 0.04598041991392771, 0.04526434898376465, 0.045356926719347636, 0.044353224833806354, 0.04497363522648811, 0.04471323529879252, 0.04404292464256287, 0.043413305779298146, 0.044592854181925455, 0.04324711074431737, 0.04356039663155874, 0.04454189916451772, 0.043151695330937705, 0.043328893383344017, 0.043788278698921206, 0.04264830559492111, 0.04322381297747294, 0.043690362572669984, 0.04297232627868652, 0.04327369491259257, 0.043333183924357095, 0.04310046553611755, 0.04420305023590724, 0.043130657101670904, 0.0441106973340114, 0.04413875897725423, 0.04303384264310201, 0.043308611611525216, 0.043578070551157, 0.04363991230726242, 0.04289507488409678, 0.04311579763889313, 0.04331269860267639, 0.04264150202274322, 0.043021576503912605, 0.04393478294213613, 0.04306057373682658, 0.043992451826731366, 0.043618280291557315, 0.04293444663286209, 0.043154219388961794, 0.04311737993111213, 0.0438372419277827, 0.04280278484026591, 0.04318374673525492, 0.04337770134210586, 0.04266686556239923, 0.04423764685789744, 0.04277478595574697, 0.04257326434055964, 0.043364391922950746, 0.04243818995853265, 0.04340835372606913, 0.042461503135661285, 0.042889851530392965, 0.042645934770504636, 0.042777458826700844, 0.04300917486349742, 0.04338859510918458, 0.04316205014785131, 0.04208159824212392, 0.04301387190818787, 0.042540692749122776, 0.04239952027797699, 0.0423821234703064, 0.04216859112183253, 0.04211746315161387, 0.043147958914438886, 0.04234983245531718, 0.04354561845461528, 0.04343715538581212, 0.04271860400835673, 0.04283232678969701, 0.04219689438740412, 0.04239517688751221, 0.04286693861087163, 0.04238885433723529, 0.04191484361886978, 0.04371475532650947, 0.042612136602401735, 0.04216113011042277, 0.042410749395688375, 0.04199338436126709, 0.04198609779278437, 0.042235809167226156, 0.042046889066696166, 0.0423811200261116, 0.04281478802363078, 0.04356072465578715, 0.04361312488714854, 0.04241147875785828, 0.04205081005891164, 0.04387999475002289, 0.042510370016098024, 0.0423018761475881, 0.042215874592463176, 0.04194934030373891, 0.04241538571814696, 0.042382345000902814, 0.04256399734566609, 0.04277931813150644, 0.04184910933176676, 0.04255170007546743, 0.042138091921806335, 0.04211121012767156, 0.04197329799334208, 0.041832072039445244, 0.04256532420714696, 0.04207652879257997, 0.042278214295705156, 0.04293399790922801, 0.04219640838603179, 0.042248575687408446, 0.04284580051898956, 0.04175769532720248, 0.04195381601651509, 0.04179223040739695, 0.041857620775699614, 0.042033902804056805, 0.041596934720873835, 0.042642639180024464, 0.04228987912336985, 0.041862020393212636, 0.04170437475045522, 0.041930806934833524, 0.04179972842335701, 0.04204809804757436, 0.041889192859331764, 0.04162605067094167, 0.041864322622617085, 0.04309216698010763, 0.0420190699895223, 0.04220615208148956, 0.04255277022719383, 0.04163419902324676, 0.04222432206074397, 0.04224984725316366, 0.04157507677872976, 0.04156856040159861, 0.041539169549942016, 0.042142637471357984, 0.04391174018383026, 0.045759998957316084, 0.04296278278032939, 0.042046234607696534, 0.04328675587972005, 0.044389236172040304, 0.0434497207403183, 0.043886127769947054, 0.04212423622608185, 0.04213322321573893, 0.04296172892053922, 0.042713671922683716, 0.041915408745408056, 0.042245955591400465, 0.042124592661857606, 0.04146304865678151, 0.04209751792252064, 0.041539615392684935, 0.04170768737792969, 0.04240763532618681, 0.04166965270414948, 0.04153618057568868, 0.04152684653798739, 0.0411748460928599, 0.04167646725972494, 0.04135945121447245, 0.04126975800842047, 0.0415959099928538, 0.04144175052642822, 0.04121957888205846, 0.041833589461942516, 0.04151857051377495, 0.04135340270896753, 0.04125822047392527, 0.04175375486413638, 0.04113951755066713, 0.04122083048025767, 0.04140622178713481, 0.041347439686457314, 0.0410086929363509, 0.0416853599747022, 0.04096527814865112, 0.041539275844891865, 0.041394667426745094, 0.041631392041842144, 0.04126347192873558, 0.04102071513732274, 0.04114478687445323, 0.0414287719130516, 0.04230315605799357, 0.041967519720395405, 0.041298720041910805, 0.04163116643826167, 0.04143360614776612, 0.0414778429766496, 0.041676722566286725, 0.04196455339590709, 0.04163986961046855, 0.04133361412833134, 0.04152919312318166, 0.041112289130687714, 0.041817733844121296, 0.04129620343446731, 0.0414043690264225, 0.04153789346416791, 0.04121435234944026, 0.041042165358861286, 0.04125162700812022, 0.041464553475379945, 0.04108089059591293, 0.04166231781244278, 0.04143682638804118, 0.040957713524500526, 0.04300182441870371, 0.041011261443297066, 0.0414136419693629, 0.04193638451397419, 0.04108886957168579, 0.041960457861423495, 0.04123865634202957, 0.04140061060587565, 0.04205672661463419, 0.041990300019582115, 0.04168598751227061, 0.0413615345209837, 0.041766397555669146, 0.0414299601316452, 0.0415647413333257, 0.041399562855561574, 0.04236587166786194, 0.04216842293739319, 0.042064467817544936, 0.041365504662195844, 0.041117976059516274, 0.0420260093609492, 0.041087144513924916, 0.04094313532114029, 0.04173048496246338, 0.04102485160032908, 0.041821867525577545, 0.04168043777346611, 0.041412623624006904, 0.04122529099384944, 0.0413876344760259, 0.04183901225527128, 0.041698200007279716, 0.04199342186252276, 0.04131230448683103, 0.04128495285908381, 0.04182563081383705, 0.041064642270406085, 0.04278874079386393, 0.041828638315200804, 0.04188050816456477, 0.041834361751874286, 0.041630036532878875, 0.04136012869576613, 0.040966861347357435, 0.041159039636452996, 0.04090605835119883, 0.040846860508124035, 0.04119882086912791, 0.04118992884953817, 0.04051658471425374, 0.04131944010655085, 0.04096099058787028, 0.04117321620384852, 0.041516662538051606, 0.04074201037486394, 0.040873360633850095, 0.04060765457650026, 0.040413359788556896, 0.04083640237649282, 0.040613823731740314, 0.0406756229326129, 0.040933950543403624, 0.040648034314314525, 0.040483261694510775, 0.040827979346116386, 0.0406337743687133, 0.04072870681683222, 0.04073849081993103, 0.04072402416417996, 0.04063422369460265, 0.041322231491406756, 0.04056578656037649, 0.04055588662624359, 0.041918763717015584, 0.04040802260239919, 0.04050552099943161, 0.040804736614227295, 0.040636057356993355, 0.04070829381545385, 0.041170668005943295, 0.04174682954947154, 0.04108938882748286, 0.041576324502627056, 0.04175760040680567, 0.040649097859859463, 0.041072330474853515, 0.04163676266868909, 0.04069148341814677, 0.04122236688931783, 0.04158367882172267, 0.04051618980864684, 0.04062086164951324, 0.041799804170926415, 0.04086252411206563, 0.041133981049060825, 0.041288290296991666, 0.04085657159487406, 0.040929040412108104, 0.04039744275932511, 0.04085489372412364, 0.041392445862293244, 0.040541538757582506, 0.04060864478349686, 0.04074299563964208, 0.04042945365111033, 0.040273177213966845, 0.04067110339800517, 0.04025451600551605, 0.040901417272786296, 0.04103294918934504, 0.04092617027461529, 0.04110343337059021, 0.04111971567074458, 0.04047580222288767, 0.040667987664540606, 0.040467835528155166, 0.040312793410072725, 0.040415980853140354, 0.0404536767800649, 0.04075454821189244, 0.0413353685537974, 0.040838184952735904, 0.041148984730243685, 0.040658180117607114, 0.040909432967503864, 0.04044528524080912, 0.040788314541180926, 0.04092208333313465, 0.040787738064924875, 0.04071508510659139, 0.04047207474708557, 0.04124953707059224, 0.04107958285758893, 0.040420641899108885, 0.040581778685251875, 0.04083625184992949, 0.04066729644934337, 0.04064983348051707, 0.03997543176015218, 0.040604323744773865, 0.041747097820043565, 0.042856913010279336, 0.0415235428015391, 0.041773613492647806, 0.04113971769809723, 0.04028396591544151, 0.04063107034191489, 0.04051936368147532, 0.041279913981755574, 0.04097366531689962, 0.04050608764092128, 0.040532352129618324, 0.04057887425025304, 0.040278987487157185, 0.04012906829516093, 0.04052371740341187, 0.040118835270404815, 0.04025200923283895, 0.04022821471095085, 0.04043312003215154, 0.040113638391097384, 0.040477753678957624, 0.04114535629749298, 0.040717628225684165, 0.04064414978027344, 0.04034405255069335, 0.039900929859528936, 0.040338495870431264, 0.040218347112337745, 0.040409530028700825, 0.040303313434123994, 0.040006937086582185, 0.04028427203496297, 0.04008442550897598, 0.04050699104865392, 0.040961395700772604, 0.040298954248428345, 0.04066965997219086, 0.040803985248009364, 0.04128516654173533, 0.040648821194966635, 0.040268231891095636, 0.04009714663028717, 0.04000869398315748, 0.03999904304742813, 0.04039835166186094, 0.04005972315867742, 0.041587741275628407, 0.0405799917380015, 0.0401857982079188, 0.04052564925203721, 0.04090884466965993, 0.04023279411097368, 0.04013566076755524, 0.040140447070201236, 0.040633960217237475, 0.039974809885025026, 0.04057868090768655, 0.03988728225231171, 0.04008083939552307, 0.040133319695790606, 0.040474325393637024, 0.04017816066741944, 0.040012346605459846, 0.04025190452734629, 0.04054552177588145, 0.040042857068280376, 0.03978575949246685, 0.04003408789634705, 0.04138358473777771, 0.041416572829087575, 0.041585586468378705, 0.040964993039766945, 0.03993609940012296, 0.04076511879762014, 0.04061071790754795, 0.04073095599810282, 0.04023709833621979, 0.04058014869689942, 0.04043201526006063, 0.04028706327080726, 0.04041762630144755, 0.040356216132640836, 0.040333463350931804, 0.03997265388568243, 0.03975255072116852, 0.03994366149107615, 0.040132906883955, 0.0406086144844691, 0.04009876628716787, 0.03977882146835327, 0.03988184948762258, 0.039988184173901875, 0.0404012127717336, 0.04057666222254435, 0.04037910908460617, 0.04011266401658455, 0.04017859637737274, 0.03997307360172272, 0.039823153217633564, 0.03989057799180349, 0.040123486071825025, 0.04006322662035624, 0.04070446362098058, 0.04025699034333229, 0.03993928441156944, 0.03967392603556315, 0.04030386090278625, 0.03987832024693489, 0.04015653751790524, 0.040145620703697205, 0.04031643728415171, 0.040492482980092365, 0.039777782360712687, 0.04024191975593567, 0.039674701491991676, 0.04003719041744868, 0.040129444437722366, 0.03959324836730957, 0.03988399883111318, 0.03946901351834337, 0.039964170654614765, 0.04026432300607363, 0.039990252455075585, 0.03995122759292523, 0.0398223178088665, 0.03972566952308019, 0.039799721837043764, 0.039436253110567726, 0.03945900976657867, 0.03981938699881236, 0.03968556235233943, 0.03961868205418189, 0.039298467735449476, 0.0394034743309021, 0.0396783850590388, 0.039390827218691506, 0.04059994141260783, 0.0402071879307429, 0.040550882617632546, 0.03991438746452332, 0.03972538655002912, 0.03967725766201814, 0.039243316960831484, 0.03931663731733958, 0.03921170055866242, 0.03953588445981344, 0.03925653874874115, 0.039634236097335816, 0.039467299381891884, 0.039109898805618284, 0.03931366294622421, 0.03919016838073731, 0.03908246380587419, 0.03907311280568441, 0.03891899081567923, 0.039171302715937294, 0.03924012412627538, 0.03914267390966415, 0.03885182873656352, 0.039200581709543866, 0.03893033782641093, 0.03892352600892385, 0.038861261506875355, 0.038985371589660645, 0.0389068881670634, 0.03889253040154775, 0.03897396182020505, 0.038899928629398346, 0.038804558515548704, 0.03891194547216097, 0.038563281980653605, 0.03878367006778717, 0.039141463041305544, 0.03942423522472382, 0.03850409984588623, 0.03843575060367584, 0.038214980860551195, 0.038697055727243426, 0.03901349127292633, 0.0386457860780259, 0.03847629219293594, 0.03863812744617462, 0.03827666645248731, 0.03810849865277608, 0.03861594378948212, 0.038315387678643065, 0.03828607322648168, 0.03825595686833064, 0.03811795800924301, 0.03845206422110398, 0.038126864582300184, 0.03840495025118192, 0.0380052159478267, 0.038597839375336965, 0.03909588893254598, 0.03893535981575648, 0.0385826634367307, 0.038321568568547563, 0.0378263991077741, 0.038131173054377236, 0.03846364716688792, 0.03855608493089676, 0.03861565510431925, 0.03803003614147504, 0.037678929766019185, 0.03824576571583748, 0.038285438815752665, 0.037406661907831824, 0.037730577811598776, 0.03766447429855665, 0.03764933109283447, 0.038635026613871255, 0.03956732382376989, 0.03757597575585048, 0.03687405372659365, 0.03659783492485682, 0.03682192489504814, 0.03689553052186966, 0.036333904669930535, 0.03655930082003275, 0.03667966107527415, 0.03621793190638224, 0.03639407296975454, 0.03679473519325256, 0.03604603846867879, 0.03671303073565165, 0.03655792827407519, 0.036680249273777006, 0.03627341502656539, 0.03643405834833781, 0.036197320719559986, 0.036637959331274034, 0.03641638713578383, 0.03645376622676849, 0.0356807483235995, 0.03580275786419709, 0.03552507345875104, 0.0353985582664609, 0.03525619516770045, 0.03575284659862518, 0.03542969763278961, 0.03515439917643865, 0.03561345518877109, 0.03494372293973962, 0.03513445317745209, 0.03480718453725179, 0.0346089909474055, 0.03452170252799988, 0.03456286350886027, 0.035044814720749856, 0.03466545214255651, 0.034605151365200676, 0.034447399377822874, 0.03431345274051031, 0.03420422027508418, 0.03408506147563457, 0.03397530714670817, 0.03416470776001612, 0.03379290322462718, 0.033891872465610505, 0.03391504724820455, 0.033909133076667784, 0.03387146731217702, 0.033404809733231865, 0.034105225801467895, 0.033435210958123206, 0.03336256106694539, 0.03361653139193853, 0.03281873246033987, 0.03321300983428955, 0.033108506947755814, 0.03300202923516433, 0.03266350467999776, 0.03259615997473399, 0.03252573365966479, 0.032909300525983176, 0.032609944144884745, 0.03260052879651387, 0.032856001059214275, 0.03249952220047514, 0.03215818246205648, 0.03210788706938426, 0.03190568986038367, 0.03168003927916288, 0.03187195042769114, 0.031966658333937324, 0.032087920308113096, 0.031732696890831, 0.03132456248005231, 0.03136443207661311, 0.03134974340597788, 0.03171256020665169, 0.030984786450862885, 0.03085756942629814, 0.030759386668602624, 0.030879296213388443, 0.03117525706688563, 0.030586694876352946, 0.030554057012001673, 0.030345227792859076, 0.031055087099472682, 0.030374027689297995, 0.030069294969240823, 0.030386977593104044, 0.030230830122406283, 0.029740161101023357, 0.02978225921591123, 0.029870572984218597, 0.02960260902841886, 0.029692129418253898, 0.029775945246219637, 0.029203102389971414, 0.029007181028525033, 0.02892368992169698, 0.029191588225464027, 0.029130483667055767, 0.028769209086894988, 0.02871474136908849, 0.02887175271908442, 0.028976746499538422, 0.028590830117464064, 0.028309099475542703, 0.02867172082265218, 0.029740141034126283, 0.028355071544647215, 0.028660721679528555, 0.028938680353264013, 0.028736874957879386, 0.028640424609184267, 0.029412868420283, 0.027938175896803537, 0.02852256717781226, 0.028281428615252176, 0.02746064727505048, 0.027374317646026613, 0.02716053356726964, 0.027697408298651378, 0.026752142906188964, 0.026787216464678448, 0.02713030199209849, 0.026552115182081858, 0.026775271197160087, 0.026492699732383093, 0.026988076468308766, 0.026240843037764233, 0.02662043571472168, 0.026020810107390085, 0.025891433556874593, 0.026235563059647877, 0.025652886231740314, 0.02557866950829824, 0.025566818018754323, 0.025400587916374208, 0.025505586912234625, 0.025895768602689107, 0.02538724462191264, 0.025115257104237874, 0.025504072507222492, 0.026302970498800277, 0.02491799424091975, 0.024468065996964772, 0.02476935918132464, 0.024731877893209457, 0.02454545927544435, 0.024301420797904334, 0.02497631589571635, 0.024289969702561698, 0.023899887055158615, 0.023725911589960257, 0.023868181115637224, 0.02365556581566731, 0.02354262928167979, 0.023513376712799072, 0.02398213278502226, 0.023370832254489264, 0.02337557444969813, 0.02310326079527537, 0.02295486249650518, 0.02319206764300664, 0.0230826698243618, 0.02249798039595286, 0.02247765064239502, 0.02278120239575704, 0.022723678797483444, 0.022685111463069917, 0.022075598041216532, 0.022716819445292157, 0.02405344585577647, 0.02203519145647685, 0.022142338107029598, 0.021860885495940844, 0.02201761056979497, 0.02246027593811353, 0.021374773184458414, 0.02182652324438095, 0.021116547373433908, 0.022155236701170603, 0.02135381169617176, 0.021103829046090445, 0.021757674117883045, 0.021090983971953393, 0.020833092282215755, 0.020794372459252676, 0.020650112827618916, 0.020770238041877748, 0.02037670244773229, 0.0209881404787302, 0.020864047706127167, 0.01999340418105324, 0.02056481547653675, 0.019952589332436522, 0.019860823427637418, 0.02010415921608607, 0.019770067731539408, 0.019625978767871855, 0.020326767563819886, 0.019621251871188482, 0.019884290993213653, 0.019402346846958, 0.019546792556842166, 0.019586177070935566, 0.01900808647274971, 0.019099960426489513, 0.01915626049041748, 0.019417109539111455, 0.018993597278992334, 0.018465994000434874, 0.018435201545556387, 0.018753665685653686, 0.019556879500548043, 0.01834108545134465, 0.01807888425886631, 0.018057086964448292, 0.018562691311041515, 0.01829103688398997, 0.017814152638117472, 0.017701495389143625, 0.019597432265679043, 0.01773088362067938, 0.01723701337973277, 0.01757324264695247, 0.01795873204867045, 0.01756695091724396, 0.017177732735872267, 0.01749181292951107, 0.01719004233678182, 0.01725717638929685, 0.017364490230878195, 0.0173583772033453, 0.01688471871117751, 0.01778294990460078, 0.01672612359126409, 0.016920314133167268, 0.016966579457124074, 0.01681713968515396, 0.016595824162165325, 0.016424990569551785, 0.01621951036155224, 0.017066539774338405, 0.01588713233669599, 0.015896914017697175, 0.017020313814282418, 0.01599029791230957, 0.015322989622751871, 0.016221287405739228, 0.01756809324026108, 0.015095166638493538, 0.015000730554262797, 0.015968565319975217, 0.015698694934447605, 0.014964981737236182, 0.01545875952889522, 0.015572858353455862, 0.01527051467448473, 0.015073591222365698, 0.014591748664776484, 0.01634141811480125, 0.014983716383576394, 0.014587344874938329, 0.014464673356463512, 0.014641167049606642, 0.014741832638780277, 0.014729793270428976, 0.013924140160282454, 0.014681518425544103, 0.014266690500080585, 0.013842740009228388, 0.014523005237181981, 0.014101239542166391, 0.014045111536979675, 0.014111579209566117, 0.013797880609830221, 0.013731645519534747, 0.014322624976436297, 0.013941279674569765, 0.013872144569953282, 0.013284552519520125, 0.014866004387537638, 0.013499329040447872, 0.012873518268267313, 0.013319213849802811, 0.013460373903314272, 0.013164774347096682, 0.013540711502234142, 0.012889425953229268, 0.013036720206340153, 0.013109429826339086, 0.01251751812795798, 0.012687383933613698, 0.012799439206719398, 0.013274201055367788, 0.012484326461950938, 0.012290685921907425, 0.012571399193257093, 0.012324798467258612, 0.012614197706182799, 0.012370704313119252, 0.012083233514179786, 0.011923039803902307, 0.01285015453894933, 0.012240018509328366, 0.01151729978621006, 0.011720432365934053, 0.012711473181843758, 0.012087819563845793, 0.011638073176145554, 0.01154301236073176, 0.011308923264344533, 0.012028336971998215, 0.011251464560627937, 0.011227905725439389, 0.011470032210151354, 0.011699868763486544, 0.01121067335208257, 0.011298414518435796, 0.011390736494213342, 0.01097669060031573, 0.011006843134139976, 0.011041228671868642, 0.010488079860806465, 0.011214884022871654, 0.011092423051595688, 0.010759174178044001, 0.01054050048192342, 0.010518456945816676, 0.010943262825409571, 0.010478062729040782, 0.011079202195008596, 0.010537027468283972, 0.010021026208996774, 0.010708539684613546, 0.010471311807632446, 0.010151343817512194, 0.010175456007321676, 0.009832647256553173, 0.009975154995918273, 0.010378666470448177, 0.009857898006836573, 0.01034960076212883, 0.009685428440570831, 0.01000329094628493, 0.00991050566236178, 0.010676623359322547, 0.009610360339283943, 0.010166735748449961, 0.009477201203505198, 0.00978694553176562, 0.0116329787671566, 0.009188782498240471, 0.00900104915102323, 0.010330211967229844, 0.009703439871470133, 0.00901329072813193, 0.00914357508222262, 0.011219228406747182, 0.009202748388051987, 0.008817896097898483, 0.009606558655699093, 0.009401555632551512, 0.008738689621289571, 0.009235161145528158, 0.008619308869043985, 0.008860807120800018, 0.008834809809923173, 0.008575766223172347, 0.008818495273590087, 0.00899493628491958, 0.008396263867616653, 0.008541264726469914, 0.009217361857493719, 0.008250631168484688, 0.00825575360407432, 0.009298321002473435, 0.008222240308920542, 0.008306663185358047, 0.008455920467774073, 0.008791991919279098, 0.00835852900519967, 0.008217298065622648, 0.008580151963979005, 0.008401650339365005, 0.0082173756758372, 0.00798488383491834, 0.008448438843091328, 0.00866048431644837, 0.00755801809951663, 0.007712115049362183, 0.007998638475934664, 0.008364483453333378, 0.008366194839278857, 0.007371826320886612, 0.00818718301753203, 0.008067121654748917, 0.007645551810661952, 0.007935225802163283, 0.00724313755830129, 0.007873347476124764, 0.007578508059183756, 0.0077858125666777295, 0.007202330306172371, 0.007239471077919006, 0.00813632495701313, 0.007224940334757169, 0.007197902450958887, 0.007203046381473541, 0.008043002188205719, 0.007146912291646003, 0.006819542075196902, 0.007469474275906881, 0.007188413466016451, 0.007428779155015945, 0.007514265477657318, 0.0065734035521745685, 0.006986918840557337, 0.007013930479685466, 0.0071081824849049255, 0.007028856600324313, 0.006763950188954671, 0.007765206148227056, 0.007300544579823812, 0.007091241764525572, 0.006613218635320664, 0.006789897978305817, 0.0064601974189281465, 0.007516237422823906, 0.006682647975782554, 0.00668341226875782, 0.006615166614452998, 0.007098463500539462, 0.00637045747290055, 0.006437756046652794, 0.006775153502821922, 0.006412799507379532, 0.0060445679724216465, 0.006764222408334414, 0.006760532694558303, 0.005901454115907351, 0.005965851545333863, 0.0066804072260856625, 0.006523040160536766, 0.005951030726234118, 0.0060765205634136996, 0.006101297239462534, 0.006469481984774272, 0.0061660999183853465, 0.005958143770694733, 0.00650420847038428, 0.007089041844010353, 0.00575238823890686, 0.005610075667500496, 0.005803836956620217, 0.006708348393440247, 0.005666458532214164, 0.005598202993472417, 0.006221693605184555, 0.006361067742109299, 0.005677448809146881, 0.005685485228896141, 0.005501611282428106, 0.005781345032155514, 0.006217780237396558, 0.005513276904821396, 0.005491119548678398, 0.005650772290925185, 0.006232254666586717, 0.005668202638626099, 0.005359976962208748, 0.005872812966505686, 0.005504481187090278, 0.00554115512718757, 0.005023112061123053, 0.0055599687745173775, 0.005660624528924624, 0.005068073098858198, 0.0060708328212300934, 0.005806129698952039, 0.004951765934626261, 0.005008985549211502, 0.006123716806372007, 0.005767592613895734, 0.004904246833175421, 0.005301237180829048, 0.005431221512456735, 0.005134363795320193, 0.004743471679588159, 0.005234746417651574, 0.006072772517800331, 0.0056015429397424064, 0.005214877650141716, 0.005205065831542015, 0.005285872916380565, 0.006187384575605392, 0.0059347038964430495, 0.004816931709647179, 0.005766102174917857, 0.004934170817335447, 0.005685086796681086, 0.004720143439869086, 0.004929661651452383, 0.005247490480542183, 0.004617678001523018, 0.004559132630626361, 0.005043194169799487, 0.00481495146950086, 0.004815227662523587, 0.004903521661957105, 0.00440883435929815, 0.004690661629041036, 0.005022309720516205, 0.004578731184204419, 0.0044746352483828866, 0.004543775829176108, 0.0048024095594882965, 0.004411739880839984, 0.0049745953703920045, 0.00479728435476621, 0.004682716156045596, 0.00417342039446036, 0.0047100857769449556, 0.004424558642009894, 0.004548865060011546, 0.004236402089397112, 0.004357778703173001, 0.0044146546100576715, 0.0045067935685316725, 0.004072094584504763, 0.004357003023227056, 0.004814896310369174, 0.004038725681602955, 0.004202412891512116, 0.004300898090004921, 0.004019940694173177, 0.004240739954014619, 0.004641804204632839, 0.004218385790785153, 0.004191160437961419, 0.004101738681395848, 0.004109619359175364, 0.004403570021192233, 0.003929976224899292, 0.004240490247805913, 0.003932357616722584, 0.004210810909668605, 0.00399191644663612, 0.003776757021745046, 0.003932586709658305, 0.004232182167470455, 0.004082710581521193, 0.0037047277266780533, 0.0038560585429271064, 0.004158473610877991, 0.003613750810424487, 0.004180148256321748, 0.00437911673138539, 0.0036726033377150694, 0.0036418817254404226, 0.003979268098870913, 0.003784701240559419, 0.0036388017237186433, 0.0037064075221618017, 0.004047831296920776, 0.0038435522839426993, 0.003623135002950827, 0.003898568625251452, 0.004149786109725634, 0.0037775476773579913, 0.003835151543219884, 0.003657153621315956, 0.00391125296552976, 0.0036180476595958073, 0.0033170620848735175, 0.0038035390774408978, 0.0037613972028096515, 0.0035796502232551576, 0.003559494142731031, 0.003642746830979983, 0.0036346127154926458, 0.003300857419768969, 0.0036777183413505554, 0.004548397585749626, 0.003284425487120946, 0.003579634204506874, 0.00421390600502491, 0.003310177947084109, 0.003590533969302972, 0.003534011182685693, 0.0032059346139431, 0.0035692310084899266, 0.003675803517301877, 0.0032885575294494627, 0.0037147372340162596, 0.003420905942718188, 0.0035320589443047844, 0.003553930955628554, 0.003632776973148187, 0.0033084273462494216, 0.003556709140539169, 0.004673118516802788, 0.003208963101108869, 0.003496426319082578, 0.003434883306423823, 0.0033833579222361247, 0.0030944778273502985, 0.0031328174596031507, 0.003605121597647667, 0.003109440878033638, 0.0032184256426990034, 0.003318168247739474, 0.003133064340800047, 0.0031855381838977338, 0.003317927159368992, 0.0030486593892176945, 0.0030155534110963343, 0.0033004324759046237, 0.0030895545457800227, 0.0034046831727027895, 0.0030513624846935272, 0.0035046221067508063, 0.003114498586704334, 0.002840219164888064, 0.003099924350778262, 0.00328365637610356, 0.0028804682195186617, 0.0028156132996082304, 0.003002675026655197, 0.003119801658516129, 0.0031059150646130245, 0.0029638737440109255, 0.0031601018582781155, 0.00288814431677262, 0.003015579196314017, 0.0032289066910743715, 0.0032537818948427835, 0.003371610070268313, 0.0029856995368997257, 0.002837470807135105, 0.0031528294334808985, 0.00270291176935037, 0.002789275006701549, 0.003130042633662621, 0.002707776017487049, 0.002729384998480479, 0.0029087311029434206, 0.002854521491875251, 0.002989328367014726, 0.002920152706404527, 0.0028159006436665854, 0.0027986862510442734, 0.0030895994355281194, 0.0026333607857426007, 0.002723465400437514, 0.0028393339986602464, 0.0026535703614354133, 0.002742465635140737, 0.0025562157481908796, 0.0026860026518503825, 0.002743895649909973, 0.0027087262148658433, 0.0024663182720541955, 0.002650583349168301, 0.0027818514903386435, 0.002559653992454211, 0.002572824570039908, 0.002736311616996924, 0.002890859308342139, 0.0025246507426102956, 0.002656472846865654, 0.0030361587926745416, 0.0027465555258095265, 0.0025337163358926775, 0.0026345780678093435, 0.0025138585579892, 0.0027443052207430205, 0.002495772211501996, 0.0023030578220884007, 0.002731270343065262, 0.002651727596918742, 0.002314818724989891, 0.002413160080711047, 0.0029103054478764536, 0.002247290915499131, 0.0024469556597371896, 0.003084173866858085, 0.0022774429060518743, 0.0024286407232284547, 0.0025680149036149185, 0.0025591675750911234, 0.0023255599848926068, 0.002334620008865992, 0.002721299317975839, 0.0023997849350174266, 0.002294345845778783, 0.0024350758269429205, 0.002769240407894055, 0.0022721382634093364, 0.0022017824028929076, 0.002304999530315399, 0.002579260418812434, 0.002354661027590434, 0.0022068385034799574, 0.002307056412100792, 0.00256666611880064, 0.0023471458194156487, 0.0022899665062626204, 0.002371829239030679, 0.002370289961496989, 0.002399114730457465, 0.002384031390150388, 0.002334834548334281, 0.0021632887174685795, 0.0021866912270585696, 0.0022430734957257907, 0.0021198832305769125, 0.0021099270942310493, 0.002383281427125136, 0.0025948903833826384, 0.002270540480191509, 0.0023774254073699315, 0.0027906198675433794, 0.002156567828108867, 0.002155480533838272, 0.0021439789359768233, 0.0020648515100280445, 0.002121928110718727, 0.002203083597123623, 0.002050980751713117, 0.0026371130595604577, 0.0022306301693121593, 0.0018993062029282251, 0.00264176403482755, 0.0024523855621616047, 0.0019547959168752034, 0.0023135135571161905, 0.0022439658517638844, 0.0020815538614988327, 0.0021843962433437504, 0.002269749597956737, 0.002047826548417409, 0.002096271403133869, 0.002209876316289107, 0.0020612772678335507, 0.002077385609348615, 0.0020398514717817305, 0.002095613119502862, 0.0023641853344937166, 0.0019612116118272144, 0.0018632910524805388, 0.0020225387811660766, 0.0022562873736023904, 0.0018087465812762578, 0.001960324918230375, 0.0020777370408177377, 0.001973410944143931, 0.0021248655393719673, 0.00197738990187645, 0.0019276048863927522, 0.0019738714521129927, 0.0019129506746927898, 0.002089006577928861, 0.001983922297755877, 0.0019027827928463617, 0.001816349339981874, 0.0019535231838623685, 0.0019438926316797734, 0.0019377455612023672, 0.001982264245549838, 0.0018162220095594725, 0.001908442291120688, 0.001842359813551108, 0.0018992724238584438, 0.0017826152468721072, 0.001957241098086039, 0.001969633543243011, 0.0019613917544484137, 0.0017850333638489246, 0.001710434357325236, 0.0019672999841471514, 0.0017841825758417448, 0.0017347753668824832, 0.0017942642917235692, 0.002153782012561957, 0.0017508329016466936, 0.001935170814394951, 0.0016913009869555632, 0.0019093800460298857, 0.0017980351174871126, 0.0018577418848872184, 0.0018611330228547255, 0.0017140731339653334, 0.0017610627909501395, 0.0019837489351630213, 0.0016655346502860388, 0.0018040451407432556, 0.0019089146393040815, 0.0016920488824446996, 0.00166035079707702, 0.0020723492900530495, 0.0017042236278454464, 0.0015632628401120504, 0.0018800656776875257, 0.0017368984160323938, 0.001667271740734577, 0.0016497438649336496, 0.0016644277299443881, 0.001795665044337511, 0.0018066070787608624, 0.0017171581089496612, 0.0018153507386644682, 0.0018731361938019594, 0.0017134196311235428, 0.0019197708244125047, 0.001672891415655613, 0.0016593507428963978, 0.001787805613130331, 0.001608684075375398, 0.0018519609918196997, 0.001754865013062954, 0.0016044400011499722, 0.0016776277124881745, 0.002508409507572651, 0.0016386538247267406, 0.001630345198015372, 0.0015849353621403377, 0.0015983789414167405, 0.0016064640258749326, 0.0014975825076301893, 0.0015640663603941599, 0.0015509880147874355, 0.001520198651899894, 0.001604589279741049, 0.0015884997447331746, 0.001562595758587122, 0.0015658152351776759, 0.0016538615959386031, 0.0014218702850242455, 0.0014644692093133927, 0.0015700600451479356, 0.0014973967894911767, 0.0014696397818624973, 0.0016986308122674625, 0.0015898673919339974, 0.0015490989262859026, 0.0014962328101197878, 0.0015031595652302105, 0.0014309400816758474, 0.0013909686108430227, 0.0015801333822309971, 0.0014698297033707301, 0.0016204973061879476, 0.0015178281503419081, 0.0014984565849105516, 0.0015087663382291794, 0.001369868355492751, 0.0015944197277228038, 0.0015571076143532992, 0.0012868942009905974, 0.0018166651266316573, 0.0014362261630594731, 0.0013432201743125915, 0.00150130204235514, 0.0014024334276715915, 0.0013191104959696532, 0.001434454433619976, 0.0014672891795635223, 0.0015731542805830637, 0.0013398802218337854, 0.0015042037144303321, 0.001542896901567777, 0.00143277358263731, 0.0013996201194822787, 0.0014338054383794467, 0.001325525746991237, 0.0013440323372681936, 0.0013517983568211397, 0.0015294482248524824, 0.0012738749695320923, 0.001342475547765692, 0.0016007824738820393, 0.0014824756110707919, 0.0015071936479459206, 0.0013073032225171726, 0.00126645440235734, 0.0013400666539867719, 0.0013644397864118218, 0.0012538303559025128, 0.0014799639830986658, 0.001437929371992747, 0.0012761049469312033, 0.0013009329264362652, 0.0013667952517668405, 0.0012519717278579871, 0.001323373137662808, 0.0012899585502843063, 0.0013086155864099661, 0.0012596211691076557, 0.0012668001403411228, 0.0013420302979648114, 0.001220444447050492, 0.0012208696951468785, 0.0013025292754173278, 0.0012637621412674585, 0.0011326445701221625, 0.001451323429743449, 0.001273096582541863, 0.00122948977475365, 0.0013250169282158215]}],\n",
              "                        {\"hovermode\": \"x\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Loss Results\"}, \"xaxis\": {\"title\": {\"text\": \"epochs\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('106ebaa5-1868-4304-8fb3-e3072ead6928');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"4f3b4923-b0bc-4641-87be-c3e83479fd2a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"4f3b4923-b0bc-4641-87be-c3e83479fd2a\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '4f3b4923-b0bc-4641-87be-c3e83479fd2a',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"train metric\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500], \"y\": [46.36820983886719, 43.90594482421875, 40.41242218017578, 35.5742301940918, 29.738605499267578, 23.274690628051758, 19.231229782104492, 16.429645538330078, 14.645842552185059, 12.67949104309082, 11.51410961151123, 10.526168823242188, 9.800698280334473, 9.368782043457031, 8.982340812683105, 8.6051664352417, 8.209491729736328, 7.820753574371338, 7.503487586975098, 7.125771522521973, 6.76077938079834, 6.420604705810547, 6.057600498199463, 5.733860492706299, 5.406892776489258, 5.126034736633301, 4.84750509262085, 4.593295097351074, 4.349742889404297, 4.119705677032471, 3.919403553009033, 3.731139898300171, 3.5736677646636963, 3.402184247970581, 3.247816801071167, 3.1160075664520264, 2.976743221282959, 2.850449323654175, 2.7459557056427, 2.6341567039489746, 2.529284715652466, 2.438119649887085, 2.355252265930176, 2.260000467300415, 2.1899774074554443, 2.107057809829712, 2.0340404510498047, 1.9697842597961426, 1.8988256454467773, 1.8442888259887695, 1.7941901683807373, 1.7394907474517822, 1.6805846691131592, 1.6329307556152344, 1.5887119770050049, 1.5436863899230957, 1.4840298891067505, 1.4372339248657227, 1.3898617029190063, 1.3479747772216797, 1.3087104558944702, 1.263764500617981, 1.2297656536102295, 1.1894365549087524, 1.1601343154907227, 1.1242008209228516, 1.0906925201416016, 1.052590012550354, 1.0189311504364014, 0.9936233758926392, 0.9658657312393188, 0.9368080496788025, 0.9055840373039246, 0.8807545900344849, 0.8559468984603882, 0.835736095905304, 0.8072822690010071, 0.7872065901756287, 0.7605781555175781, 0.7442789077758789, 0.7292406558990479, 0.7096071243286133, 0.6923138499259949, 0.6695498824119568, 0.650404691696167, 0.6290452480316162, 0.6121290326118469, 0.5993786454200745, 0.5814650058746338, 0.5716611742973328, 0.5502351522445679, 0.5354100465774536, 0.5215389728546143, 0.5082269906997681, 0.4943825304508209, 0.48597633838653564, 0.4711132049560547, 0.45847535133361816, 0.4474017918109894, 0.4384654462337494, 0.42686566710472107, 0.4167022705078125, 0.4081926643848419, 0.39887866377830505, 0.3936685621738434, 0.38201019167900085, 0.3723147511482239, 0.36120328307151794, 0.35357752442359924, 0.34601059556007385, 0.34045663475990295, 0.3308040201663971, 0.3234845697879791, 0.31611791253089905, 0.309566855430603, 0.3013552725315094, 0.2956281304359436, 0.29183128476142883, 0.2864975333213806, 0.2776825428009033, 0.27271828055381775, 0.2652851641178131, 0.259443461894989, 0.25442588329315186, 0.247515469789505, 0.24162057042121887, 0.23600128293037415, 0.23024755716323853, 0.22663775086402893, 0.22234013676643372, 0.2157605141401291, 0.2096286416053772, 0.20464757084846497, 0.1989266723394394, 0.19441109895706177, 0.1885736882686615, 0.18228739500045776, 0.17652669548988342, 0.170183002948761, 0.16664239764213562, 0.16419340670108795, 0.1561547815799713, 0.15283262729644775, 0.14900700747966766, 0.14710387587547302, 0.14580002427101135, 0.14408718049526215, 0.14462625980377197, 0.14748311042785645, 0.13241399824619293, 0.1286965161561966, 0.12365901470184326, 0.12232237309217453, 0.12564601004123688, 0.11813250184059143, 0.11801718920469284, 0.11061334609985352, 0.1108950674533844, 0.10923939198255539, 0.11174476891756058, 0.10720829665660858, 0.10275190323591232, 0.10418776422739029, 0.10272131860256195, 0.10200949758291245, 0.10475370287895203, 0.10832847654819489, 0.10788681358098984, 0.09892970323562622, 0.09614358097314835, 0.1035553440451622, 0.09494534134864807, 0.09746796637773514, 0.09240402281284332, 0.09010104835033417, 0.08986597508192062, 0.08937621116638184, 0.08738527446985245, 0.08684121072292328, 0.08540693670511246, 0.08559966087341309, 0.0869472324848175, 0.08417434990406036, 0.08924752473831177, 0.09146421402692795, 0.08478790521621704, 0.08732511103153229, 0.09625880420207977, 0.10088943690061569, 0.1009625792503357, 0.08629834651947021, 0.09317073225975037, 0.09054594486951828, 0.08900967240333557, 0.08087711036205292, 0.07979658246040344, 0.07901221513748169, 0.08169464021921158, 0.0780160203576088, 0.08043797314167023, 0.07799840718507767, 0.07764658331871033, 0.075898177921772, 0.07613541185855865, 0.07555217295885086, 0.07589692622423172, 0.0748581662774086, 0.07493943721055984, 0.07894512265920639, 0.0751534253358841, 0.0761614739894867, 0.07820206135511398, 0.07871703803539276, 0.07671666890382767, 0.07543404400348663, 0.07855993509292603, 0.07516610622406006, 0.07550081610679626, 0.07319215685129166, 0.07190898805856705, 0.0730559378862381, 0.0723273977637291, 0.07169261574745178, 0.07580099254846573, 0.08732835203409195, 0.09219073504209518, 0.1011524498462677, 0.09647098183631897, 0.08015576004981995, 0.0758475512266159, 0.07506600022315979, 0.07082436978816986, 0.07035939395427704, 0.070536307990551, 0.0722104012966156, 0.07253284752368927, 0.07165329158306122, 0.07183931767940521, 0.07031695544719696, 0.07042913138866425, 0.07130595296621323, 0.07162898778915405, 0.06935253739356995, 0.06978920847177505, 0.06917884200811386, 0.06864435970783234, 0.06908377259969711, 0.06949414312839508, 0.06858311593532562, 0.06807703524827957, 0.0692477822303772, 0.06872353702783585, 0.06919773668050766, 0.07077562808990479, 0.06874655187129974, 0.06941096484661102, 0.06997358798980713, 0.0694018229842186, 0.06916134059429169, 0.07022213190793991, 0.06995981186628342, 0.06784551590681076, 0.06779596209526062, 0.06772755831480026, 0.06676831096410751, 0.06660307943820953, 0.06654790788888931, 0.06695076823234558, 0.06612280756235123, 0.06802235543727875, 0.06589542329311371, 0.06577533483505249, 0.06650877743959427, 0.06805438548326492, 0.06665525585412979, 0.06637079268693924, 0.06828369945287704, 0.0665828138589859, 0.06743156164884567, 0.06700705736875534, 0.06608349829912186, 0.06714291125535965, 0.06670206040143967, 0.06945629417896271, 0.0706905946135521, 0.06763412803411484, 0.06584130227565765, 0.06645680963993073, 0.06869668513536453, 0.06814191490411758, 0.06630302220582962, 0.06646142154932022, 0.06756898015737534, 0.06697601824998856, 0.06753171980381012, 0.06727065145969391, 0.0678505077958107, 0.06671822816133499, 0.06578481197357178, 0.06666580587625504, 0.0662010982632637, 0.06536634266376495, 0.065027616918087, 0.06518310308456421, 0.06544029712677002, 0.06454730778932571, 0.06528410315513611, 0.0652833804488182, 0.06460975855588913, 0.06543804705142975, 0.06487319618463516, 0.06496838480234146, 0.06623366475105286, 0.06546248495578766, 0.06584038585424423, 0.06623080372810364, 0.06618545204401016, 0.06644688546657562, 0.0668042004108429, 0.06579285860061646, 0.0646803230047226, 0.06516532599925995, 0.06473050266504288, 0.06481585651636124, 0.06415434926748276, 0.0642956867814064, 0.06416494399309158, 0.06435465067625046, 0.06422974169254303, 0.06440455466508865, 0.06427092105150223, 0.06497856229543686, 0.06496204435825348, 0.06510741263628006, 0.06536576896905899, 0.06498205661773682, 0.06971130520105362, 0.06780453771352768, 0.06410027295351028, 0.06658153235912323, 0.06594159454107285, 0.06593445688486099, 0.06443536281585693, 0.06412898749113083, 0.06447755545377731, 0.0643896535038948, 0.06536085158586502, 0.06590259820222855, 0.06643761694431305, 0.06842853873968124, 0.06615489721298218, 0.06607183068990707, 0.06443596631288528, 0.06379889696836472, 0.06341183930635452, 0.0634416714310646, 0.06337817758321762, 0.06453380733728409, 0.06689624488353729, 0.06564581394195557, 0.06770061701536179, 0.06703688204288483, 0.06885901093482971, 0.06463925540447235, 0.06604219228029251, 0.0650143027305603, 0.06608805805444717, 0.0676843449473381, 0.06724574416875839, 0.06743010133504868, 0.06866642087697983, 0.0671340599656105, 0.06667221337556839, 0.06519654393196106, 0.06463667005300522, 0.06409064680337906, 0.06496278196573257, 0.06418874114751816, 0.0652875006198883, 0.06537959724664688, 0.06453422456979752, 0.06621934473514557, 0.06772943586111069, 0.06591019779443741, 0.06517259031534195, 0.0641380175948143, 0.06308706104755402, 0.06391114741563797, 0.064978688955307, 0.06307702511548996, 0.06508079171180725, 0.06447876244783401, 0.06270754337310791, 0.06558094918727875, 0.06379882991313934, 0.06341817229986191, 0.06494906544685364, 0.06478565186262131, 0.06361095607280731, 0.0640546903014183, 0.0633789449930191, 0.06498689204454422, 0.06471879035234451, 0.06369448453187943, 0.06481902301311493, 0.06548219919204712, 0.06776724755764008, 0.06829173862934113, 0.06900068372488022, 0.06566925346851349, 0.06690524518489838, 0.06746597588062286, 0.06589484959840775, 0.06447307020425797, 0.06335742771625519, 0.06422092020511627, 0.06567604094743729, 0.0660366341471672, 0.06517738103866577, 0.06442756205797195, 0.0648440569639206, 0.06560856103897095, 0.0639544427394867, 0.06460235267877579, 0.06409617513418198, 0.06512541323900223, 0.06403672695159912, 0.06343341618776321, 0.063758984208107, 0.06336382031440735, 0.06454042345285416, 0.06337279081344604, 0.06364209204912186, 0.06307672709226608, 0.06342562288045883, 0.06391877681016922, 0.06280004978179932, 0.06291215866804123, 0.06326904147863388, 0.06407839059829712, 0.06297404319047928, 0.06308215856552124, 0.06270749866962433, 0.06382901221513748, 0.06393034011125565, 0.063365139067173, 0.06382134556770325, 0.06405569612979889, 0.0645861029624939, 0.06304235011339188, 0.0633208230137825, 0.06393670290708542, 0.06275314092636108, 0.06417964398860931, 0.06312011182308197, 0.06269659847021103, 0.06352510303258896, 0.06285812705755234, 0.06362537294626236, 0.06495100259780884, 0.06687603890895844, 0.06531905382871628, 0.0651264488697052, 0.06561138480901718, 0.06455253064632416, 0.06351979076862335, 0.06355930119752884, 0.06401683390140533, 0.06422442197799683, 0.06554245948791504, 0.06541375070810318, 0.06559917330741882, 0.06496970355510712, 0.06605749577283859, 0.06327368319034576, 0.06511035561561584, 0.06829510629177094, 0.06683719158172607, 0.06382112205028534, 0.06388071179389954, 0.06451302021741867, 0.06514664739370346, 0.06404619663953781, 0.062395308166742325, 0.0637139305472374, 0.06673888117074966, 0.06518268585205078, 0.06440660357475281, 0.0642562285065651, 0.06341326236724854, 0.06343282759189606, 0.06379946321249008, 0.06446640938520432, 0.06364458054304123, 0.06332249194383621, 0.06266172230243683, 0.06245957687497139, 0.06225856766104698, 0.06385445594787598, 0.06671804934740067, 0.06658794730901718, 0.06515150517225266, 0.06468147039413452, 0.06572867929935455, 0.06352100521326065, 0.06259255856275558, 0.06362634152173996, 0.06870083510875702, 0.071099191904068, 0.07526816427707672, 0.07024776190519333, 0.06696707010269165, 0.06678202748298645, 0.06500866264104843, 0.06483133882284164, 0.06412245333194733, 0.06382579356431961, 0.06424424052238464, 0.06467656791210175, 0.06484288722276688, 0.06417342275381088, 0.06340957432985306, 0.06347353011369705, 0.06303416192531586, 0.06377008557319641, 0.06364249438047409, 0.06372373551130295, 0.06350246071815491, 0.06336120516061783, 0.06355878710746765, 0.06373567879199982, 0.06268148124217987, 0.06368881464004517, 0.0676540806889534, 0.06532652676105499, 0.0640777051448822, 0.06252583861351013, 0.06213245913386345, 0.06307868659496307, 0.06306580454111099, 0.0635443776845932, 0.06292183697223663, 0.06324055790901184, 0.06253551691770554, 0.0641227513551712, 0.062483325600624084, 0.06456436961889267, 0.06283482164144516, 0.06294020265340805, 0.06237468868494034, 0.06400196254253387, 0.06443113833665848, 0.06312591582536697, 0.06293273717164993, 0.06403539329767227, 0.06328966468572617, 0.06353294104337692, 0.06612040102481842, 0.06696902960538864, 0.06491294503211975, 0.06376468390226364, 0.06412218511104584, 0.065401591360569, 0.0644565001130104, 0.06423027813434601, 0.06602694094181061, 0.06388630717992783, 0.06374061852693558, 0.06644129008054733, 0.06885983794927597, 0.06781847029924393, 0.06502170860767365, 0.06480682641267776, 0.06526181846857071, 0.06451796740293503, 0.06422290951013565, 0.06277850270271301, 0.06278988718986511, 0.06306323409080505, 0.06289228796958923, 0.06439071893692017, 0.0632910504937172, 0.06307543069124222, 0.06284277886152267, 0.06318086385726929, 0.06240389868617058, 0.06332127749919891, 0.062384165823459625, 0.06299584358930588, 0.0622229278087616, 0.06419852375984192, 0.06252263486385345, 0.06306325644254684, 0.06224576756358147, 0.06235646829009056, 0.06236954778432846, 0.06254712492227554, 0.06312641501426697, 0.0634794607758522, 0.0633394867181778, 0.06348594278097153, 0.06334841996431351, 0.06301314383745193, 0.06222481280565262, 0.0624857060611248, 0.06283633410930634, 0.06216445937752724, 0.06288361549377441, 0.06287220865488052, 0.0626428872346878, 0.061864208430051804, 0.06359018385410309, 0.06328622996807098, 0.06276458501815796, 0.06342257559299469, 0.06374500691890717, 0.06414570659399033, 0.06663039326667786, 0.0641336590051651, 0.06335602700710297, 0.06430228054523468, 0.06386246532201767, 0.06425237655639648, 0.06471854448318481, 0.0685909166932106, 0.06481839716434479, 0.06380100548267365, 0.06429336220026016, 0.0627061054110527, 0.06249655410647392, 0.06199520826339722, 0.062130264937877655, 0.06210372969508171, 0.06205293536186218, 0.06267974525690079, 0.06392819434404373, 0.06407154351472855, 0.06492824107408524, 0.06260930746793747, 0.062408413738012314, 0.06244920939207077, 0.0648808628320694, 0.0663212388753891, 0.06419852375984192, 0.064589723944664, 0.06276609003543854, 0.06425795704126358, 0.06330002099275589, 0.06836975365877151, 0.06681907176971436, 0.06462009251117706, 0.06688527017831802, 0.06524825841188431, 0.06564266979694366, 0.06416009366512299, 0.06368881464004517, 0.06200624257326126, 0.0623587891459465, 0.06203717365860939, 0.061328109353780746, 0.061509352177381516, 0.061114322394132614, 0.06116297468543053, 0.060436803847551346, 0.060876183211803436, 0.06175604090094566, 0.06380532681941986, 0.06685240566730499, 0.06842894107103348, 0.07030091434717178, 0.06821995228528976, 0.06701194494962692, 0.06833243370056152, 0.0686529353260994, 0.06448028236627579, 0.0630439892411232, 0.062292568385601044, 0.06243955343961716, 0.0628303661942482, 0.06213276833295822, 0.0613597147166729, 0.0602705180644989, 0.05999945476651192, 0.059671130031347275, 0.05965736508369446, 0.060038790106773376, 0.0636630430817604, 0.06021134555339813, 0.06196989491581917, 0.059726595878601074, 0.06250414252281189, 0.061958372592926025, 0.062343575060367584, 0.059214942157268524, 0.05971364676952362, 0.06133876368403435, 0.0592522993683815, 0.058645546436309814, 0.0590398907661438, 0.05870863050222397, 0.05934695526957512, 0.059012457728385925, 0.05973607301712036, 0.058456890285015106, 0.058929432183504105, 0.05930744856595993, 0.05869852751493454, 0.05904994532465935, 0.05918367579579353, 0.0581856332719326, 0.05819912627339363, 0.05889493599534035, 0.05931926891207695, 0.059056539088487625, 0.05861508473753929, 0.05919909477233887, 0.058163512498140335, 0.05868549272418022, 0.05878205597400665, 0.06114824488759041, 0.05861315503716469, 0.0572296679019928, 0.058312512934207916, 0.05706486850976944, 0.058136433362960815, 0.06031728908419609, 0.058930784463882446, 0.05978694185614586, 0.059389736503362656, 0.05952216684818268, 0.05829008296132088, 0.056941382586956024, 0.05668614059686661, 0.05819518491625786, 0.05823028087615967, 0.056895725429058075, 0.05653568357229233, 0.05686796084046364, 0.05592171102762222, 0.056209709495306015, 0.056051939725875854, 0.05598735436797142, 0.05739298835396767, 0.05848829448223114, 0.05611206218600273, 0.057350773364305496, 0.05488719791173935, 0.055614858865737915, 0.053767405450344086, 0.05408542603254318, 0.053914327174425125, 0.05525698885321617, 0.0540039986371994, 0.053734295070171356, 0.055016402155160904, 0.05309973657131195, 0.053824812173843384, 0.05579645559191704, 0.054788779467344284, 0.06191093474626541, 0.0654902383685112, 0.06384527683258057, 0.05610815808176994, 0.05368925258517265, 0.05406123027205467, 0.0565783753991127, 0.06063878908753395, 0.05865418538451195, 0.05328040570020676, 0.05507485941052437, 0.052370261400938034, 0.05177151411771774, 0.051709506660699844, 0.05300447344779968, 0.053248923271894455, 0.054688405245542526, 0.053171224892139435, 0.05108383297920227, 0.05116528272628784, 0.0514681302011013, 0.0529036708176136, 0.052480582147836685, 0.05481208860874176, 0.05312799662351608, 0.05246720835566521, 0.051834892481565475, 0.05129691958427429, 0.053022582083940506, 0.05044073984026909, 0.05211417004466057, 0.05097218602895737, 0.05074501037597656, 0.05020635947585106, 0.050421006977558136, 0.04925231263041496, 0.04892998933792114, 0.04934319108724594, 0.05001867935061455, 0.05174440145492554, 0.049051087349653244, 0.04922759532928467, 0.04943624138832092, 0.048305414617061615, 0.049756698310375214, 0.049193184822797775, 0.049118656665086746, 0.04934447258710861, 0.049869995564222336, 0.04812261834740639, 0.04853753373026848, 0.05181223899126053, 0.049730949103832245, 0.048490796238183975, 0.04846640303730965, 0.04713406786322594, 0.04743380472064018, 0.04700997471809387, 0.046910859644412994, 0.04658522084355354, 0.047557007521390915, 0.048520803451538086, 0.04642755165696144, 0.04794631898403168, 0.04710766300559044, 0.047881558537483215, 0.047158367931842804, 0.046861883252859116, 0.047450993210077286, 0.046922482550144196, 0.046943582594394684, 0.046733833849430084, 0.0472160242497921, 0.047734662890434265, 0.04775071144104004, 0.046713005751371384, 0.046559400856494904, 0.045765891671180725, 0.04593857750296593, 0.045393601059913635, 0.04556214064359665, 0.04616623371839523, 0.04761828854680061, 0.047916192561388016, 0.04540989175438881, 0.045773521065711975, 0.045753128826618195, 0.044329386204481125, 0.044724803417921066, 0.044852688908576965, 0.045021045953035355, 0.04533246532082558, 0.04599188640713692, 0.04532868415117264, 0.04406105726957321, 0.04452076181769371, 0.04484649375081062, 0.044807132333517075, 0.04412667825818062, 0.04475275054574013, 0.045128095895051956, 0.04432811960577965, 0.04399162903428078, 0.04346194863319397, 0.04337187483906746, 0.043651510030031204, 0.04390965402126312, 0.044274475425481796, 0.04353451356291771, 0.04475774988532066, 0.04382329434156418, 0.0438210628926754, 0.04410719498991966, 0.04527949169278145, 0.04372899606823921, 0.043759845197200775, 0.04307517781853676, 0.042781151831150055, 0.04394088685512543, 0.043670639395713806, 0.04406030476093292, 0.043821781873703, 0.04486193135380745, 0.0443255789577961, 0.04427122697234154, 0.044143904000520706, 0.0437336228787899, 0.04491802304983139, 0.044678449630737305, 0.043793853372335434, 0.04496464505791664, 0.04310569539666176, 0.04420297220349312, 0.042797643691301346, 0.042721621692180634, 0.04249586910009384, 0.042135678231716156, 0.042097821831703186, 0.0424354188144207, 0.04180191457271576, 0.04166870191693306, 0.04195583984255791, 0.04171469435095787, 0.042116548866033554, 0.0427972674369812, 0.042710673063993454, 0.042344920337200165, 0.04183867201209068, 0.04130074009299278, 0.041716184467077255, 0.04190046712756157, 0.042512860149145126, 0.04300551116466522, 0.04317948967218399, 0.04416034743189812, 0.04364575073122978, 0.04402142018079758, 0.042990051209926605, 0.04207393527030945, 0.041463665664196014, 0.04151589423418045, 0.04134750738739967, 0.041769202798604965, 0.041772227734327316, 0.04141099750995636, 0.04123504087328911, 0.04153074696660042, 0.042783986777067184, 0.042689863592386246, 0.04146299511194229, 0.04085013270378113, 0.04142449051141739, 0.04094698652625084, 0.04112393409013748, 0.04146667569875717, 0.04117213562130928, 0.041391607373952866, 0.04072435572743416, 0.040592435747385025, 0.0406239852309227, 0.041821978986263275, 0.04120812192559242, 0.04080935940146446, 0.04095464199781418, 0.04072825610637665, 0.040558747947216034, 0.04045383632183075, 0.04029485955834389, 0.04036562144756317, 0.04006585478782654, 0.040930382907390594, 0.04068450257182121, 0.04011245444417, 0.0400681272149086, 0.03989183530211449, 0.04022396355867386, 0.039598144590854645, 0.03991066664457321, 0.03992694988846779, 0.04012560844421387, 0.039783209562301636, 0.039800893515348434, 0.040690746158361435, 0.04099665954709053, 0.0395989790558815, 0.040008895099163055, 0.040336500853300095, 0.03982314094901085, 0.040020011365413666, 0.03966081142425537, 0.04030065983533859, 0.040096454322338104, 0.040121328085660934, 0.03983577340841293, 0.040223341435194016, 0.03956521674990654, 0.03946229815483093, 0.039280276745557785, 0.03919077664613724, 0.03936446085572243, 0.03927738592028618, 0.03910207375884056, 0.03883783519268036, 0.03889639303088188, 0.03897080570459366, 0.03883949667215347, 0.03934505954384804, 0.038817357271909714, 0.0388590507209301, 0.03879004716873169, 0.03882184252142906, 0.038844045251607895, 0.038965657353401184, 0.03859352320432663, 0.038887061178684235, 0.03947408124804497, 0.03918878361582756, 0.03895990550518036, 0.040065862238407135, 0.03995540365576744, 0.03904247283935547, 0.04002522677183151, 0.03960304707288742, 0.039132580161094666, 0.039489831775426865, 0.03902208432555199, 0.03950231149792671, 0.038727305829524994, 0.039029497653245926, 0.039316412061452866, 0.03909552842378616, 0.03974299132823944, 0.039478570222854614, 0.03918388485908508, 0.03933136910200119, 0.03948797285556793, 0.0408736988902092, 0.04160677641630173, 0.04097658023238182, 0.04091941937804222, 0.040521975606679916, 0.04020661488175392, 0.03970245271921158, 0.04023849219083786, 0.039754245430231094, 0.03874758630990982, 0.03913327306509018, 0.039087627083063126, 0.03826000168919563, 0.03921264037489891, 0.03845437988638878, 0.03818292170763016, 0.03838324546813965, 0.03817232325673103, 0.038179151713848114, 0.03781725838780403, 0.038091957569122314, 0.03816293552517891, 0.038168199360370636, 0.03878603130578995, 0.03870755806565285, 0.03865058720111847, 0.038819216191768646, 0.038609642535448074, 0.038892053067684174, 0.03826437145471573, 0.038385059684515, 0.03819381818175316, 0.03877496346831322, 0.03832001984119415, 0.03806355968117714, 0.03805370256304741, 0.0378451831638813, 0.03806149214506149, 0.03857513517141342, 0.03851570188999176, 0.038917139172554016, 0.038817182183265686, 0.03858525678515434, 0.03844458609819412, 0.03893132507801056, 0.03884226828813553, 0.03837839514017105, 0.037961602210998535, 0.039177969098091125, 0.039449144154787064, 0.03801149129867554, 0.03814779967069626, 0.0382392592728138, 0.03824127838015556, 0.0388973243534565, 0.03815501183271408, 0.03815358877182007, 0.03770916163921356, 0.037903860211372375, 0.03779295086860657, 0.03788742050528526, 0.03778171166777611, 0.03795475512742996, 0.03796974569559097, 0.03794419392943382, 0.037553563714027405, 0.037550706416368484, 0.03742758184671402, 0.03730493783950806, 0.03733837231993675, 0.03826522082090378, 0.03804975375533104, 0.03767732158303261, 0.038110025227069855, 0.037656404078006744, 0.03752174228429794, 0.037331029772758484, 0.037768807262182236, 0.03760566934943199, 0.03739384189248085, 0.03775057941675186, 0.0372035950422287, 0.03773076832294464, 0.03721184656023979, 0.03706405311822891, 0.037028178572654724, 0.03713151067495346, 0.03716883063316345, 0.03725697472691536, 0.03722973167896271, 0.03800014406442642, 0.037727970629930496, 0.038201700896024704, 0.03810134530067444, 0.03810804709792137, 0.03786499798297882, 0.037998124957084656, 0.03755294531583786, 0.037055544555187225, 0.03686315938830376, 0.037097569555044174, 0.03727797046303749, 0.037498533725738525, 0.037546489387750626, 0.03773883730173111, 0.037588100880384445, 0.03724111244082451, 0.0370405949652195, 0.036974210292100906, 0.03705465421080589, 0.037188220769166946, 0.03747434541583061, 0.03744106367230415, 0.03741398826241493, 0.03728414699435234, 0.03774334490299225, 0.037651460617780685, 0.03702390193939209, 0.036924075335264206, 0.03682022541761398, 0.03694014996290207, 0.03676223009824753, 0.036968111991882324, 0.03700069338083267, 0.03695574775338173, 0.036955151706933975, 0.036904510110616684, 0.03707773610949516, 0.036954160779714584, 0.036981601268053055, 0.03691228851675987, 0.03674100339412689, 0.03734732046723366, 0.03734089061617851, 0.037086404860019684, 0.03737727552652359, 0.03721215948462486, 0.036982107907533646, 0.03703835606575012, 0.037241701036691666, 0.037035826593637466, 0.037204816937446594, 0.03726017847657204, 0.03676730394363403, 0.03732721880078316, 0.03952733054757118, 0.039068516343832016, 0.03815745562314987, 0.03866809606552124, 0.039916038513183594, 0.040789052844047546, 0.03989991918206215, 0.038966454565525055, 0.03785160183906555, 0.03844618797302246, 0.03827766701579094, 0.03762974217534065, 0.03693082928657532, 0.036856938153505325, 0.03705291077494621, 0.036956317722797394, 0.03691859170794487, 0.03659064695239067, 0.03673882037401199, 0.036821361631155014, 0.037084151059389114, 0.0370321087539196, 0.03646373376250267, 0.036705661565065384, 0.03638649359345436, 0.036669548600912094, 0.03657813370227814, 0.036685291677713394, 0.03678524121642113, 0.03641466423869133, 0.03669434040784836, 0.03636287897825241, 0.03646443039178848, 0.03656216338276863, 0.036526165902614594, 0.036197882145643234, 0.03617293760180473, 0.03628511726856232, 0.03660391643643379, 0.036677293479442596, 0.036871202290058136, 0.03625255450606346, 0.03643064200878143, 0.036391958594322205, 0.0363447368144989, 0.036395516246557236, 0.03658784180879593, 0.0368327833712101, 0.03752248361706734, 0.037262484431266785, 0.03720463439822197, 0.036867719143629074, 0.0372968427836895, 0.03642413392663002, 0.03626781329512596, 0.03661230579018593, 0.036731936037540436, 0.03646186739206314, 0.0364641509950161, 0.03633962199091911, 0.03623035550117493, 0.036594223231077194, 0.03674035891890526, 0.03636163845658302, 0.03662586584687233, 0.036392468959093094, 0.036532286554574966, 0.03625691682100296, 0.03639422729611397, 0.036279283463954926, 0.03652609512209892, 0.03634703904390335, 0.03717707097530365, 0.03698763623833656, 0.037239447236061096, 0.03767191991209984, 0.03645065054297447, 0.037005580961704254, 0.03641128912568092, 0.03624258562922478, 0.037741754204034805, 0.03753003850579262, 0.036927592009305954, 0.0373135469853878, 0.03655005246400833, 0.0365685410797596, 0.03645080327987671, 0.03668312728404999, 0.03699197620153427, 0.03785839304327965, 0.03696831688284874, 0.03659529611468315, 0.03645629063248634, 0.03651253134012222, 0.03641727939248085, 0.036644820123910904, 0.036543648689985275, 0.03632664680480957, 0.03631503880023956, 0.03674372285604477, 0.036612123250961304, 0.03657125309109688, 0.0364551916718483, 0.03645770251750946, 0.03643673285841942, 0.037521734833717346, 0.03748873993754387, 0.036612704396247864, 0.03731371834874153, 0.037502042949199677, 0.03714604675769806, 0.03728794679045677, 0.037115518003702164, 0.036790113896131516, 0.03771941736340523, 0.0369667150080204, 0.03649359568953514, 0.03690262511372566, 0.036377765238285065, 0.036216527223587036, 0.03626834228634834, 0.036490730941295624, 0.036559976637363434, 0.036568351089954376, 0.03688531741499901, 0.036258094012737274, 0.0367131270468235, 0.035997264087200165, 0.03600705415010452, 0.03592844307422638, 0.036101553589105606, 0.036128297448158264, 0.03578328713774681, 0.035933732986450195, 0.03575894236564636, 0.035761699080467224, 0.03573419153690338, 0.03575018793344498, 0.03585702180862427, 0.03602905571460724, 0.03571583703160286, 0.03567078709602356, 0.036376431584358215, 0.03586897253990173, 0.03573833033442497, 0.0361015684902668, 0.03605636581778526, 0.03588894009590149, 0.035802531987428665, 0.035655152052640915, 0.03572525829076767, 0.03586537018418312, 0.03584959730505943, 0.036123842000961304, 0.03635544329881668, 0.03678630292415619, 0.036416009068489075, 0.03645140677690506, 0.036649491637945175, 0.03632026165723801, 0.03592037782073021, 0.03663846477866173, 0.03689243271946907, 0.03675513714551926, 0.03640754148364067, 0.036606088280677795, 0.03652374446392059, 0.036193713545799255, 0.03603570535778999, 0.035897884517908096, 0.036195941269397736, 0.0358365923166275, 0.035958193242549896, 0.035947997123003006, 0.035802654922008514, 0.0358644537627697, 0.035708535462617874, 0.03565632924437523, 0.035801373422145844, 0.03572693467140198, 0.03567022830247879, 0.03549320623278618, 0.035972680896520615, 0.0361735038459301, 0.03582785651087761, 0.03622342273592949, 0.0360444113612175, 0.03577433526515961, 0.03584747016429901, 0.03582008183002472, 0.03567177429795265, 0.035683367401361465, 0.03600621968507767, 0.03657114505767822, 0.036612942814826965, 0.03608562424778938, 0.036837998777627945, 0.03602321818470955, 0.035731859505176544, 0.03576376289129257, 0.03566844016313553, 0.03587455302476883, 0.03576675429940224, 0.03594658151268959, 0.03604012727737427, 0.03621025010943413, 0.035942018032073975, 0.035946182906627655, 0.03621005639433861, 0.03594308719038963, 0.03577835485339165, 0.035735853016376495, 0.03665803372859955, 0.03610984981060028, 0.03648125380277634, 0.037068165838718414, 0.037054818123579025, 0.03638859838247299, 0.036081187427043915, 0.03605174645781517, 0.035629019141197205, 0.03625161945819855, 0.03604699298739433, 0.035905271768569946, 0.03611995279788971, 0.036119140684604645, 0.035516805946826935, 0.035486478358507156, 0.0356496199965477, 0.03538912534713745, 0.03542441129684448, 0.035425372421741486, 0.035432394593954086, 0.03533713147044182, 0.035972949117422104, 0.03661446273326874, 0.036437101662158966, 0.03580697998404503, 0.03599861264228821, 0.035485222935676575, 0.03526196628808975, 0.035363879054784775, 0.035372115671634674, 0.035319261252880096, 0.03545622527599335, 0.03519335016608238, 0.03528982400894165, 0.03578418120741844, 0.03599529340863228, 0.03628290817141533, 0.03586537390947342, 0.035946935415267944, 0.03632056713104248, 0.03571468964219093, 0.03554794192314148, 0.035378094762563705, 0.03536349907517433, 0.03526678681373596, 0.03615651652216911, 0.03620525076985359, 0.035753410309553146, 0.03579030558466911, 0.03553153574466705, 0.0353306420147419, 0.03550758957862854, 0.035580914467573166, 0.03557119145989418, 0.03551051393151283, 0.035624001175165176, 0.035523805767297745, 0.03566083312034607, 0.0357222706079483, 0.03538741171360016, 0.03556372970342636, 0.03542298078536987, 0.035626184195280075, 0.03589027747511864, 0.03558509424328804, 0.03570616617798805, 0.03519383817911148, 0.035520534962415695, 0.035488054156303406, 0.03570433333516121, 0.036572232842445374, 0.0368434302508831, 0.03675098717212677, 0.03668531775474548, 0.0363730750977993, 0.035579655319452286, 0.03585796803236008, 0.03593387082219124, 0.035620417445898056, 0.035742491483688354, 0.035398080945014954, 0.03587831184267998, 0.03559056296944618, 0.03588342294096947, 0.035495903342962265, 0.03538918495178223, 0.03530330955982208, 0.03527170047163963, 0.03621077537536621, 0.035248834639787674, 0.035217199474573135, 0.035221945494413376, 0.03550349175930023, 0.035292163491249084, 0.03543718531727791, 0.035517361015081406, 0.03518124297261238, 0.035277023911476135, 0.03523232415318489, 0.03540275990962982, 0.035549346357584, 0.03526963293552399, 0.035414475947618484, 0.035465799272060394, 0.035999689251184464, 0.035606011748313904, 0.03565762937068939, 0.03524917736649513, 0.035319358110427856, 0.035285238176584244, 0.035740796476602554, 0.035784389823675156, 0.03577229008078575, 0.035999465733766556, 0.0355009101331234, 0.03529134765267372, 0.035116057842969894, 0.035424403846263885, 0.03513634204864502, 0.035432036966085434, 0.03498896211385727, 0.035457171499729156, 0.03525421768426895, 0.03597752004861832, 0.03525448590517044, 0.03567628934979439, 0.03537248820066452, 0.034850165247917175, 0.03509603813290596, 0.03481505066156387, 0.0347098745405674, 0.034907855093479156, 0.03490052744746208, 0.03473455458879471, 0.03518984094262123, 0.035109993070364, 0.03559698536992073, 0.03523071110248566, 0.03549155220389366, 0.03512691333889961, 0.03552374616265297, 0.03515869751572609, 0.03520698845386505, 0.0349392369389534, 0.034641243517398834, 0.034799974411726, 0.034769680351018906, 0.03481665626168251, 0.03486233949661255, 0.034971293061971664, 0.034607239067554474, 0.03469684720039368, 0.03464195132255554, 0.03489874675869942, 0.03467023745179176, 0.034607239067554474, 0.0343744121491909, 0.03481695428490639, 0.03478646278381348, 0.035062652081251144, 0.03473339229822159, 0.034683529287576675, 0.034536782652139664, 0.03443802893161774, 0.034368209540843964, 0.03490182384848595, 0.034517064690589905, 0.034306857734918594, 0.03430512920022011, 0.03446510434150696, 0.03519286960363388, 0.03486751392483711, 0.034948475658893585, 0.03466486558318138, 0.03446384146809578, 0.034382086247205734, 0.034119825810194016, 0.03411584720015526, 0.03378209099173546, 0.034210290759801865, 0.03435147926211357, 0.03428097814321518, 0.0340166836977005, 0.034141190350055695, 0.033963803201913834, 0.03421211242675781, 0.03396826982498169, 0.034016598016023636, 0.034276723861694336, 0.03426505997776985, 0.03490046411752701, 0.03377790004014969, 0.03429340943694115, 0.0343162827193737, 0.03359726071357727, 0.03461087495088577, 0.03509414196014404, 0.034549079835414886, 0.034778427332639694, 0.034239184111356735, 0.03374926373362541, 0.03410783410072327, 0.034465156495571136, 0.034299496561288834, 0.03383436053991318, 0.03385981544852257, 0.03366983309388161, 0.03343656286597252, 0.033634815365076065, 0.03429826721549034, 0.03408127278089523, 0.033621761947870255, 0.03428762033581734, 0.034423764795064926, 0.034453462809324265, 0.03339426964521408, 0.03286436200141907, 0.03301438316702843, 0.03278231620788574, 0.03241228312253952, 0.032501693814992905, 0.03280847892165184, 0.0323156900703907, 0.03239873796701431, 0.032875485718250275, 0.03267865628004074, 0.03244452551007271, 0.032389625906944275, 0.03287084773182869, 0.03328661993145943, 0.032973986119031906, 0.03304542973637581, 0.032524146139621735, 0.03292912244796753, 0.03265555948019028, 0.03197312727570534, 0.032234884798526764, 0.03183557838201523, 0.0317121185362339, 0.03189393877983093, 0.0316399522125721, 0.031622156500816345, 0.03181986138224602, 0.03212875500321388, 0.03132551163434982, 0.031431473791599274, 0.03131552413105965, 0.031024368479847908, 0.031173095107078552, 0.03110860474407673, 0.031262144446372986, 0.030890505760908127, 0.03124258667230606, 0.030743127688765526, 0.03070611134171486, 0.03089197725057602, 0.03085057996213436, 0.030530504882335663, 0.030347829684615135, 0.030633321031928062, 0.030674520879983902, 0.03051953949034214, 0.030624032020568848, 0.030373044312000275, 0.030364856123924255, 0.03034771978855133, 0.030965963378548622, 0.029969461262226105, 0.030062859877943993, 0.029904043301939964, 0.029778970405459404, 0.02956930361688137, 0.02972327172756195, 0.029427533969283104, 0.029463279992341995, 0.02948654443025589, 0.02964840829372406, 0.029521405696868896, 0.030158212408423424, 0.030508141964673996, 0.029656073078513145, 0.029371432960033417, 0.029484542086720467, 0.029342828318476677, 0.029070066288113594, 0.029215967282652855, 0.028666649013757706, 0.029035119339823723, 0.028453679755330086, 0.02863917499780655, 0.028889713808894157, 0.028720315545797348, 0.02927841804921627, 0.02845349721610546, 0.02818569913506508, 0.027958521619439125, 0.0280010886490345, 0.02841656096279621, 0.028017131611704826, 0.027792859822511673, 0.027758656069636345, 0.028528785333037376, 0.02753460966050625, 0.027978885918855667, 0.028246933594346046, 0.027203990146517754, 0.027086863294243813, 0.02715916745364666, 0.027108322829008102, 0.02706776186823845, 0.02726481296122074, 0.02696702629327774, 0.027132440358400345, 0.02672121860086918, 0.026886217296123505, 0.026474693790078163, 0.02670539915561676, 0.026436006650328636, 0.02649710513651371, 0.02626953460276127, 0.02631903812289238, 0.026405563578009605, 0.026691509410738945, 0.026804152876138687, 0.02685101330280304, 0.026917580515146255, 0.026495961472392082, 0.026211068034172058, 0.02687421441078186, 0.02626524493098259, 0.026761066168546677, 0.026228735223412514, 0.026428239420056343, 0.026165863499045372, 0.025030625984072685, 0.02552947774529457, 0.025517215952277184, 0.025575803592801094, 0.025169167667627335, 0.025007670745253563, 0.02471303939819336, 0.0250005591660738, 0.024919969961047173, 0.025172844529151917, 0.025649327784776688, 0.02487529255449772, 0.024578982964158058, 0.024533936753869057, 0.02430330030620098, 0.02424137108027935, 0.02450677566230297, 0.024127183482050896, 0.02390720695257187, 0.02403780072927475, 0.024183496832847595, 0.02438896708190441, 0.024289727210998535, 0.023917181417346, 0.023922566324472427, 0.0246517863124609, 0.023844296112656593, 0.023605240508913994, 0.023188143968582153, 0.02361467108130455, 0.02333158813416958, 0.023390838876366615, 0.024094771593809128, 0.022871393710374832, 0.022690515965223312, 0.02290630340576172, 0.02243223786354065, 0.02224382758140564, 0.022478878498077393, 0.02316664531826973, 0.022456465288996696, 0.022365305572748184, 0.021944794803857803, 0.021910786628723145, 0.0218042079359293, 0.021893013268709183, 0.021763915196061134, 0.021583428606390953, 0.021616900339722633, 0.021468309685587883, 0.021620413288474083, 0.021519871428608894, 0.021613534539937973, 0.02155340649187565, 0.02189488150179386, 0.022055435925722122, 0.021839478984475136, 0.021199112758040428, 0.020890939980745316, 0.02134159579873085, 0.02099921554327011, 0.020873842760920525, 0.021031087264418602, 0.020529119297862053, 0.02076609618961811, 0.02106422372162342, 0.02089603617787361, 0.020595410838723183, 0.020344160497188568, 0.020324960350990295, 0.020151637494564056, 0.02023935504257679, 0.020036278292536736, 0.02081233076751232, 0.020309513434767723, 0.020183159038424492, 0.019583014771342278, 0.01950908452272415, 0.01927362196147442, 0.01953374408185482, 0.019372563809156418, 0.01959051750600338, 0.01958717778325081, 0.019380759447813034, 0.01942036859691143, 0.01966128498315811, 0.0191508736461401, 0.019289828836917877, 0.01889512874186039, 0.018965084105730057, 0.01846751570701599, 0.01867525838315487, 0.018476778641343117, 0.018433034420013428, 0.018405018374323845, 0.018915412947535515, 0.0187742467969656, 0.018422367051243782, 0.018115419894456863, 0.018238451331853867, 0.018269116058945656, 0.018049830570816994, 0.019125569611787796, 0.01766613870859146, 0.018114645034074783, 0.018114712089300156, 0.01758168451488018, 0.01771179586648941, 0.01756170764565468, 0.017477702349424362, 0.01727328635752201, 0.017772555351257324, 0.017794478684663773, 0.01750277914106846, 0.017020467668771744, 0.017160246148705482, 0.01721692830324173, 0.017599258571863174, 0.01747755892574787, 0.017051398754119873, 0.016715653240680695, 0.01665619947016239, 0.01647675596177578, 0.01649566739797592, 0.016626935452222824, 0.017174502834677696, 0.017049727961421013, 0.016636429354548454, 0.01700560189783573, 0.016236687079072, 0.016856206580996513, 0.016265658661723137, 0.016833340749144554, 0.015864670276641846, 0.01629636436700821, 0.01575971394777298, 0.015889525413513184, 0.016201935708522797, 0.01534828171133995, 0.01548009179532528, 0.015975099056959152, 0.016257181763648987, 0.015604439191520214, 0.01593412086367607, 0.015465826727449894, 0.015410262160003185, 0.01510638277977705, 0.014948757365345955, 0.0148008456453681, 0.014787317253649235, 0.014860006980597973, 0.014963757246732712, 0.01467226818203926, 0.01471382100135088, 0.014582937583327293, 0.014747006818652153, 0.014742453582584858, 0.014893372543156147, 0.01446897815912962, 0.014638914726674557, 0.014532672241330147, 0.014249086380004883, 0.014756901189684868, 0.014313077554106712, 0.014563394710421562, 0.014353778213262558, 0.013867934234440327, 0.014121652580797672, 0.01381920650601387, 0.0137866185978055, 0.013721806928515434, 0.013770383782684803, 0.013648737221956253, 0.013687708415091038, 0.013505776412785053, 0.013336502015590668, 0.013497942127287388, 0.013679173775017262, 0.013612150214612484, 0.013085922226309776, 0.013480547815561295, 0.01335291750729084, 0.01297878660261631, 0.013121216557919979, 0.012812923640012741, 0.01333822961896658, 0.013211378827691078, 0.012772731482982635, 0.012713250704109669, 0.01272912323474884, 0.012539033778011799, 0.012634032405912876, 0.012619568966329098, 0.012474196031689644, 0.012459449470043182, 0.012578555382788181, 0.01252573262900114, 0.012526500970125198, 0.012314260937273502, 0.012065975926816463, 0.012345301918685436, 0.012004797346889973, 0.012094816192984581, 0.012080189771950245, 0.011847180314362049, 0.012074391357600689, 0.011906343512237072, 0.011993726715445518, 0.012082595378160477, 0.011725638061761856, 0.011714944615960121, 0.011519234627485275, 0.011727189645171165, 0.011575166136026382, 0.01155295129865408, 0.011409008875489235, 0.011447491124272346, 0.011256554163992405, 0.01113172434270382, 0.011069553904235363, 0.011198226362466812, 0.01098152156919241, 0.010993222706019878, 0.010905692353844643, 0.011080627329647541, 0.011067523621022701, 0.010789339430630207, 0.011355387978255749, 0.011232251301407814, 0.011075451970100403, 0.01087651215493679, 0.010871590115129948, 0.011491990648210049, 0.010552480816841125, 0.011452672071754932, 0.011118333786725998, 0.011030267924070358, 0.010870185680687428, 0.010471491143107414, 0.010406563989818096, 0.01109358947724104, 0.011153014376759529, 0.010779252275824547, 0.010501492768526077, 0.010581978596746922, 0.010369694791734219, 0.010128151625394821, 0.010260910727083683, 0.010059685446321964, 0.010137779638171196, 0.00989021360874176, 0.009850109927356243, 0.009779946878552437, 0.009792988188564777, 0.009775439277291298, 0.009682641364634037, 0.010029944591224194, 0.009515875950455666, 0.009578783065080643, 0.010219695046544075, 0.009890520013868809, 0.00982144195586443, 0.00953513290733099, 0.010151032358407974, 0.009465068578720093, 0.009588977321982384, 0.009201554581522942, 0.009435176849365234, 0.00926270242780447, 0.009203767403960228, 0.00949864648282528, 0.009385382756590843, 0.008990737609565258, 0.009367447346448898, 0.009055488742887974, 0.009632566012442112, 0.008978892117738724, 0.009090964682400227, 0.009184756316244602, 0.008937103673815727, 0.008898094296455383, 0.008849010802805424, 0.008921804837882519, 0.008800221607089043, 0.008819951675832272, 0.008797218091785908, 0.008745395578444004, 0.00908496044576168, 0.00846981443464756, 0.008661889471113682, 0.008325150236487389, 0.0090743163600564, 0.008609237149357796, 0.008297902531921864, 0.00854536984115839, 0.008294512517750263, 0.008399136364459991, 0.008348415605723858, 0.008434373885393143, 0.008378865197300911, 0.008080380968749523, 0.008267753757536411, 0.008101903833448887, 0.008266801945865154, 0.0086558498442173, 0.008169563487172127, 0.008375633507966995, 0.00833720900118351, 0.008010312914848328, 0.008043083362281322, 0.008170127868652344, 0.008019842207431793, 0.008233604021370411, 0.007800143212080002, 0.007987378165125847, 0.008167214691638947, 0.007923814468085766, 0.007607280742377043, 0.007809437345713377, 0.007555246353149414, 0.007609834894537926, 0.007590930908918381, 0.007678014226257801, 0.007469926495105028, 0.00762936333194375, 0.007343425881117582, 0.00750710628926754, 0.007260075304657221, 0.007409673649817705, 0.007538347039371729, 0.007246050983667374, 0.007310796529054642, 0.007572160568088293, 0.007184332236647606, 0.007921326905488968, 0.007201159372925758, 0.007332722656428814, 0.007146854419261217, 0.007475202437490225, 0.007019596174359322, 0.007309012580662966, 0.007014106493443251, 0.00766737200319767, 0.0070237405598163605, 0.007310611195862293, 0.007005185354501009, 0.006965167820453644, 0.007135461550205946, 0.006840609014034271, 0.007101293187588453, 0.006803037598729134, 0.006956059020012617, 0.006859340704977512, 0.007269072346389294, 0.006832425482571125, 0.006822561379522085, 0.006847549229860306, 0.006623259745538235, 0.006565460469573736, 0.006539715453982353, 0.007435213308781385, 0.006942240055650473, 0.006612169556319714, 0.006587646901607513, 0.0066904728300869465, 0.007266107946634293, 0.006761522963643074, 0.006616883911192417, 0.006425039377063513, 0.006420116405934095, 0.0063049644231796265, 0.00668546324595809, 0.006271639373153448, 0.007303517311811447, 0.006435383576899767, 0.00693604676052928, 0.006880467291921377, 0.006914924830198288, 0.00680302269756794, 0.006686096079647541, 0.007141486741602421, 0.006724285427480936, 0.006847700569778681, 0.006235471926629543, 0.006322256289422512, 0.00598160969093442, 0.006275971420109272, 0.006007228512316942, 0.006124102044850588, 0.005925294943153858, 0.006131020840257406, 0.005859402474015951, 0.006177822593599558, 0.006148884072899818, 0.005715334787964821, 0.005785815883427858, 0.0059670573100447655, 0.0057841939851641655, 0.005867464933544397, 0.005699750501662493, 0.00603262335062027, 0.005971264094114304, 0.005702042020857334, 0.005714541766792536, 0.005808116868138313, 0.005683924537152052, 0.0055946665816009045, 0.0057343291118741035, 0.005782041698694229, 0.005948271602392197, 0.005471887532621622, 0.005518697667866945, 0.005782865919172764, 0.005456235259771347, 0.005542600993067026, 0.005451455246657133, 0.005463047884404659, 0.005440826527774334, 0.005367931444197893, 0.005511544179171324, 0.005705663468688726, 0.005475733429193497, 0.005394984968006611, 0.00531007582321763, 0.005306115373969078, 0.005470654461532831, 0.005294583737850189, 0.005483448039740324, 0.005168597213923931, 0.005460528191179037, 0.005057065282016993, 0.005089411046355963, 0.00501971086487174, 0.005090612918138504, 0.005057851318269968, 0.004991855006664991, 0.00500617315992713, 0.004952044226229191, 0.005214591510593891, 0.005031854845583439, 0.005058481823652983, 0.005001062527298927, 0.005148610565811396, 0.0050951107405126095, 0.005061827600002289, 0.004785365890711546, 0.005021118558943272, 0.004922094289213419, 0.004954553209245205, 0.005099557340145111, 0.0047838096506893635, 0.0049891662783920765, 0.004898118320852518, 0.004930127877742052, 0.004944731015712023, 0.004822443705052137, 0.004704705905169249, 0.004898224025964737, 0.004737834446132183, 0.004789270926266909, 0.004743581637740135, 0.004602684173732996, 0.004772739019244909, 0.0046394942328333855, 0.004802356474101543, 0.004668770357966423, 0.005058532580733299, 0.005031847395002842, 0.004952712450176477, 0.00477625522762537, 0.0048035187646746635, 0.00455933203920722, 0.0045775449834764, 0.004575367085635662, 0.004674944095313549, 0.004588202573359013, 0.004625104367733002, 0.0046601793728768826, 0.004573954734951258, 0.0046912250109016895, 0.004552494268864393, 0.004616518039256334, 0.004551108460873365, 0.004823446739464998, 0.0057185739278793335, 0.005881492048501968, 0.004593890625983477, 0.004538575187325478, 0.004329118877649307, 0.004259971436113119, 0.004346109926700592, 0.004384851083159447, 0.004553912207484245, 0.004284454043954611, 0.0042485627345740795, 0.004375535994768143, 0.004303573630750179, 0.004148655105382204, 0.0041293371468782425, 0.00412972504273057, 0.004060898441821337, 0.0041595338843762875, 0.004369581583887339, 0.0048650405369699, 0.004617012571543455, 0.004157647956162691, 0.004183970391750336, 0.003974976483732462, 0.004145164508372545, 0.003998911008238792, 0.0039755175821483135, 0.00394293712452054, 0.00400948990136385, 0.004121015314012766, 0.00412439601495862, 0.00401995750144124, 0.003991031087934971, 0.004144981037825346, 0.004380585625767708, 0.00409350311383605, 0.004206445999443531, 0.004059887491166592, 0.004030715208500624, 0.004006394650787115, 0.004153067711740732, 0.003868736792355776, 0.003942463081330061, 0.0038796435110270977, 0.0037614658940583467, 0.003822950879111886, 0.0038945081178098917, 0.004106971900910139, 0.003810868365690112, 0.004188147839158773, 0.003947051241993904, 0.004006275441497564, 0.003915516659617424, 0.0037447744980454445, 0.003675040788948536, 0.003763016313314438, 0.0036613265983760357, 0.0035687885247170925, 0.0035748244263231754, 0.003648747457191348, 0.0036865356378257275, 0.003621281124651432, 0.003596966853365302, 0.0035514612682163715, 0.003564941929653287, 0.003532192436978221, 0.0035860904026776552, 0.0037324687000364065, 0.003455537836998701, 0.004189671017229557, 0.0038509282749146223, 0.0035698043648153543, 0.003494561417028308, 0.003678220324218273, 0.0037309895269572735, 0.003526367014274001, 0.0034888796508312225, 0.0035007998812943697, 0.003586674341931939, 0.003415008308365941, 0.0033281377982348204, 0.003540154080837965, 0.003498986130580306, 0.0035580818075686693, 0.0035121976397931576, 0.0036306199617683887, 0.0038201191928237677, 0.0034044210333377123, 0.0033568814396858215, 0.0034299178514629602, 0.003306916682049632, 0.003594564273953438, 0.0035478915087878704, 0.003460230538621545, 0.0032978595700114965, 0.003203474683687091, 0.0033623147755861282, 0.003165385453030467, 0.0032330325338989496, 0.003271301044151187, 0.0032357957679778337, 0.003220984246581793, 0.0031838102731853724, 0.0032647489570081234, 0.0031173736788332462, 0.003409565193578601, 0.0031727920286357403, 0.003172363853082061, 0.003213105956092477, 0.003404519986361265, 0.0032719792798161507, 0.003145839786157012, 0.003061978379264474, 0.003081178991124034, 0.0030304240062832832, 0.00306504312902689, 0.0031234377529472113, 0.002967087784782052, 0.0032860247883945704, 0.0036952185910195112, 0.0031745198648422956, 0.003487975336611271, 0.0031833506654947996, 0.0030273362062871456, 0.0029884048271924257, 0.0028952346183359623, 0.0029307804070413113, 0.0029936498031020164, 0.0030854330398142338, 0.003109464654698968, 0.0031449303496629, 0.0030673015862703323, 0.0033570684026926756, 0.0030152113176882267, 0.0031010988168418407, 0.0029919759836047888, 0.0033097125124186277, 0.0028969577979296446, 0.0029733157716691494, 0.0029751472175121307, 0.0030108876526355743, 0.002938727615401149, 0.0029465151019394398, 0.0029910006560385227, 0.0029884048271924257, 0.0029189917258918285, 0.0029928903095424175, 0.003042672760784626, 0.0028153059538453817, 0.002850715070962906, 0.0027169152162969112, 0.0028557730838656425, 0.002870698692277074, 0.002757034031674266, 0.00285164313390851, 0.002822987735271454, 0.0029924693517386913, 0.0029600830748677254, 0.0027939877472817898, 0.002712216693907976, 0.0026890665758401155, 0.0027552631217986345, 0.002703897189348936, 0.0027384308632463217, 0.0026844502426683903, 0.0026407837867736816, 0.0026932978071272373, 0.0027244549710303545, 0.0027470572385936975, 0.0025801409501582384, 0.0026325038634240627, 0.00259962840937078, 0.002656433265656233, 0.0025784142781049013, 0.002601464046165347, 0.0026049951557070017, 0.002735537476837635, 0.0025373627431690693, 0.002655689837411046, 0.002548130229115486, 0.002562309615314007, 0.0025094274897128344, 0.0025274911895394325, 0.0027441466227173805, 0.0028322520665824413, 0.002686156425625086, 0.002548897173255682, 0.002497737994417548, 0.0026831624563783407, 0.0026034764014184475, 0.0025690088514238596, 0.0025305382441729307, 0.0025265023577958345, 0.0025432610418647528, 0.0026929096784442663, 0.002449741819873452, 0.002593295183032751, 0.00237108301371336, 0.0026869650464504957, 0.0025995599571615458, 0.0024412868078798056, 0.002406540559604764, 0.0023901821114122868, 0.0024566298816353083, 0.002382987644523382, 0.0024003619328141212, 0.002465152181684971, 0.002297637751325965, 0.0024540715385228395, 0.0024966017808765173, 0.0023741675540804863, 0.0025763087905943394, 0.00255450583063066, 0.0025481877382844687, 0.0025868716184049845, 0.0024142914917320013, 0.002645747968927026, 0.002637171884998679, 0.0025037091691046953, 0.002521958202123642, 0.0025562208611518145, 0.0025906667578965425, 0.0027818456292152405, 0.002395645482465625, 0.0023924168199300766, 0.002464424818754196, 0.0022173267789185047, 0.002222648821771145, 0.0021992556285113096, 0.002203503390774131, 0.002196918474510312, 0.0021975538693368435, 0.0021779187954962254, 0.0021973364055156708, 0.0022104461677372456, 0.002285182010382414, 0.0022221410181373358, 0.002198918256908655, 0.0021397764794528484, 0.0021204007789492607, 0.002148289233446121, 0.002448953688144684, 0.0021393962670117617, 0.002184137236326933, 0.0022450373508036137, 0.0022095281165093184, 0.002120189368724823, 0.0020617383997887373, 0.0020815448369830847, 0.002207610523328185, 0.0023622114676982164, 0.0021451220382004976, 0.0021450237836688757, 0.0021114221308380365, 0.002239409601315856, 0.0021263191010802984, 0.002057930687442422, 0.002238593529909849, 0.0021931063383817673, 0.002214201958850026, 0.0021519360598176718, 0.002075247000902891, 0.0019976866897195578, 0.002061235485598445, 0.001975549152120948, 0.0019985646940767765, 0.00218960247002542, 0.002030553063377738, 0.0020854126196354628, 0.0023480288218706846, 0.0020945582073181868, 0.0021230499260127544, 0.0020704916678369045, 0.002116383286193013, 0.0019319984130561352, 0.0019690601620823145, 0.002030039671808481, 0.0019837322179228067, 0.001954701030626893, 0.0020670031663030386, 0.0022836991120129824, 0.001956326887011528, 0.0020223846659064293, 0.0019266372546553612, 0.0019230837933719158, 0.0018787722801789641, 0.0018631959101185203, 0.0020246184431016445, 0.00199681194499135, 0.0019551378209143877, 0.001925607561133802, 0.0018893616506829858, 0.0018563847988843918, 0.0018817951204255223, 0.0018190523842349648, 0.001884322497062385, 0.0018511288799345493, 0.0018135618884116411, 0.0018795327050611377, 0.0018332829931750894, 0.0018388761673122644, 0.001793010625988245, 0.0018053180538117886, 0.0018617389723658562, 0.00195346400141716, 0.0018274849280714989, 0.0018619652837514877, 0.001850401284173131, 0.0018161815823987126]}, {\"mode\": \"lines+markers\", \"name\": \"test_metric\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500], \"y\": [45.44390869140625, 42.52290344238281, 38.400291442871094, 32.0791015625, 25.78338050842285, 19.606870651245117, 15.836796760559082, 14.293351173400879, 11.930276870727539, 10.609971046447754, 9.450998306274414, 8.542799949645996, 8.006917953491211, 7.608973026275635, 7.276857852935791, 6.9937238693237305, 6.741330623626709, 6.379528522491455, 6.17978572845459, 5.80040168762207, 5.522646427154541, 5.257421970367432, 4.9878249168396, 4.742707252502441, 4.476126194000244, 4.267708778381348, 4.040258884429932, 3.8370673656463623, 3.6650564670562744, 3.483502149581909, 3.335839033126831, 3.180454969406128, 3.0564522743225098, 2.9281599521636963, 2.8001725673675537, 2.693235397338867, 2.5925679206848145, 2.4990856647491455, 2.407196044921875, 2.321483612060547, 2.2462234497070312, 2.166055202484131, 2.093372106552124, 2.0291049480438232, 1.963890790939331, 1.9011311531066895, 1.8433693647384644, 1.7878906726837158, 1.7411702871322632, 1.6823205947875977, 1.636246681213379, 1.6042938232421875, 1.5450937747955322, 1.499143123626709, 1.4552593231201172, 1.4178276062011719, 1.3703769445419312, 1.3305844068527222, 1.3050158023834229, 1.254262924194336, 1.2161364555358887, 1.1943806409835815, 1.1490159034729004, 1.1166375875473022, 1.085034966468811, 1.0523865222930908, 1.0202974081039429, 0.9925791621208191, 0.9744871258735657, 0.9391834735870361, 0.9135560393333435, 0.889411211013794, 0.8614733815193176, 0.8366104960441589, 0.8162078261375427, 0.7924658060073853, 0.7685950994491577, 0.754239022731781, 0.7341679334640503, 0.7134714126586914, 0.7015699148178101, 0.6738901734352112, 0.6767759919166565, 0.6506855487823486, 0.6300948262214661, 0.6037105321884155, 0.5891881585121155, 0.5787834525108337, 0.5660809278488159, 0.546856701374054, 0.5344074368476868, 0.5186117887496948, 0.5064041018486023, 0.49519479274749756, 0.48667067289352417, 0.4700464904308319, 0.45788106322288513, 0.44653916358947754, 0.43766921758651733, 0.4313868582248688, 0.41730788350105286, 0.4064635634422302, 0.3994074761867523, 0.3894127309322357, 0.3797352910041809, 0.37261393666267395, 0.3639884293079376, 0.3551018238067627, 0.3473871350288391, 0.3481881022453308, 0.33185482025146484, 0.3234843611717224, 0.3160823881626129, 0.31044596433639526, 0.3016117215156555, 0.2945325970649719, 0.2888335585594177, 0.2884225845336914, 0.2783520221710205, 0.27059850096702576, 0.2652547359466553, 0.25951340794563293, 0.2564745843410492, 0.24787040054798126, 0.24162627756595612, 0.2353847175836563, 0.23031742870807648, 0.22622080147266388, 0.2213689684867859, 0.21361523866653442, 0.20732298493385315, 0.20396316051483154, 0.19736622273921967, 0.19180402159690857, 0.18576602637767792, 0.1819602996110916, 0.17514897882938385, 0.16979999840259552, 0.16366486251354218, 0.15979814529418945, 0.15691052377223969, 0.15040341019630432, 0.1464380919933319, 0.14569765329360962, 0.14205946028232574, 0.13645273447036743, 0.13412150740623474, 0.15133219957351685, 0.1305793970823288, 0.1269190013408661, 0.12324043363332748, 0.1205298975110054, 0.12481124699115753, 0.11764922738075256, 0.11493801325559616, 0.11034591495990753, 0.1105480045080185, 0.10633888095617294, 0.11088672280311584, 0.10513456165790558, 0.10077270865440369, 0.10040454566478729, 0.10108327865600586, 0.10133620351552963, 0.09753204137086868, 0.10077878832817078, 0.09335575252771378, 0.10339873284101486, 0.09347591549158096, 0.09793751686811447, 0.09195205569267273, 0.09041444957256317, 0.09101369976997375, 0.08958681672811508, 0.08775609731674194, 0.08876316249370575, 0.08638712018728256, 0.08653059601783752, 0.08403550833463669, 0.08448681980371475, 0.08425728976726532, 0.08812373876571655, 0.08496585488319397, 0.08450762182474136, 0.08233176171779633, 0.08500083535909653, 0.08928048610687256, 0.091632179915905, 0.10641685128211975, 0.08006297051906586, 0.08961643278598785, 0.08160550147294998, 0.09909942746162415, 0.08067988604307175, 0.08004239946603775, 0.07722650468349457, 0.08118877559900284, 0.07917764037847519, 0.07820285111665726, 0.07766544073820114, 0.07653461396694183, 0.07673420011997223, 0.07431308925151825, 0.07491818070411682, 0.07441524416208267, 0.07642295211553574, 0.07500986754894257, 0.08086065202951431, 0.07346078753471375, 0.07527245581150055, 0.07299911230802536, 0.08018040657043457, 0.07743483781814575, 0.07344271242618561, 0.07443171739578247, 0.07253601402044296, 0.07625532895326614, 0.07221531867980957, 0.07202552258968353, 0.072295643389225, 0.07200074195861816, 0.07165353000164032, 0.07182802259922028, 0.08000906556844711, 0.07760783284902573, 0.09129245579242706, 0.10375655442476273, 0.08065958321094513, 0.07304824143648148, 0.07593582570552826, 0.07106062024831772, 0.0702628418803215, 0.07074055075645447, 0.07275503873825073, 0.0719866082072258, 0.07111457735300064, 0.07275404781103134, 0.07127079367637634, 0.07096299529075623, 0.06880009174346924, 0.07551953941583633, 0.0711451917886734, 0.07004941999912262, 0.07173773646354675, 0.06935535371303558, 0.06939368695020676, 0.06876549124717712, 0.06756958365440369, 0.06817925721406937, 0.06864583492279053, 0.06991180777549744, 0.06892626732587814, 0.0691336840391159, 0.0689990371465683, 0.07021461427211761, 0.0686151534318924, 0.06927794218063354, 0.06876382976770401, 0.06953950226306915, 0.06750421226024628, 0.06794477254152298, 0.07037706673145294, 0.06854799389839172, 0.06773041188716888, 0.06781341880559921, 0.06838934868574142, 0.06704683601856232, 0.06676218658685684, 0.06855499744415283, 0.06736103445291519, 0.06725791096687317, 0.06615813076496124, 0.06744237244129181, 0.06714773178100586, 0.06660079210996628, 0.0677892193198204, 0.06624642759561539, 0.06917679309844971, 0.06885913759469986, 0.06639496982097626, 0.06635336577892303, 0.06713797152042389, 0.06634917855262756, 0.07170140743255615, 0.06961318850517273, 0.06593314558267593, 0.0674147829413414, 0.06741929054260254, 0.06747165322303772, 0.0693323090672493, 0.06722471863031387, 0.06677262485027313, 0.06826800853013992, 0.06638351082801819, 0.06919283419847488, 0.06722337752580643, 0.06626413017511368, 0.06637672334909439, 0.06718198955059052, 0.06600106507539749, 0.06707827746868134, 0.06578773260116577, 0.06511562317609787, 0.06523110717535019, 0.06588241457939148, 0.06602250039577484, 0.06728021055459976, 0.06568347662687302, 0.06536932289600372, 0.06503812223672867, 0.06585747748613358, 0.06924636662006378, 0.06614045798778534, 0.06674845516681671, 0.0662495493888855, 0.06598427146673203, 0.06672878563404083, 0.06552185863256454, 0.06663405895233154, 0.06492262333631516, 0.06517750769853592, 0.0641913115978241, 0.06560154259204865, 0.06673440337181091, 0.06597525626420975, 0.06508702039718628, 0.06575048714876175, 0.065161794424057, 0.06509529054164886, 0.06564192473888397, 0.06517626345157623, 0.06543022394180298, 0.0662294551730156, 0.06511573493480682, 0.0660746619105339, 0.06707432121038437, 0.06726465374231339, 0.06475246697664261, 0.06679143011569977, 0.0665292739868164, 0.06539416313171387, 0.06583485752344131, 0.06515449285507202, 0.06515782326459885, 0.06537000834941864, 0.06583753228187561, 0.06669998168945312, 0.06678088754415512, 0.06604144722223282, 0.06667587906122208, 0.06785593181848526, 0.06627456843852997, 0.06487331539392471, 0.06408724188804626, 0.0647987499833107, 0.06481645256280899, 0.0646900162100792, 0.06747332215309143, 0.06527276337146759, 0.06609213352203369, 0.06750401109457016, 0.06952843070030212, 0.06449040025472641, 0.06691756099462509, 0.06542698293924332, 0.06684089452028275, 0.065344899892807, 0.06754972785711288, 0.06751184165477753, 0.06498438119888306, 0.07046465575695038, 0.06900143623352051, 0.0652565211057663, 0.0657690018415451, 0.06514222174882889, 0.06532111763954163, 0.06394737958908081, 0.06603620946407318, 0.0654953345656395, 0.06508924812078476, 0.065453439950943, 0.06619437783956528, 0.0688665583729744, 0.06643631309270859, 0.06377162784337997, 0.06407216936349869, 0.06506197899580002, 0.06424355506896973, 0.06450823694467545, 0.0640629380941391, 0.06443589180707932, 0.06399502605199814, 0.06626448780298233, 0.06572199612855911, 0.0640362799167633, 0.06406950205564499, 0.066165991127491, 0.06432997435331345, 0.06362273544073105, 0.06461310386657715, 0.06435922533273697, 0.06557969003915787, 0.06428927183151245, 0.06632944941520691, 0.06512495875358582, 0.06870321184396744, 0.06634234637022018, 0.0680108591914177, 0.06481873244047165, 0.06693430244922638, 0.06391063332557678, 0.06667116284370422, 0.06523555517196655, 0.0649399533867836, 0.06418664008378983, 0.06617961823940277, 0.06489583104848862, 0.06741075962781906, 0.06305129826068878, 0.06550019979476929, 0.06682482361793518, 0.06693761795759201, 0.06617225706577301, 0.06443902105093002, 0.06546542048454285, 0.06437203288078308, 0.06455329805612564, 0.06347241997718811, 0.06350361555814743, 0.06482616066932678, 0.06351013481616974, 0.06491652131080627, 0.06352658569812775, 0.06385884433984756, 0.06689109653234482, 0.06375755369663239, 0.0636725127696991, 0.06457240879535675, 0.06417487561702728, 0.0645211860537529, 0.06341564655303955, 0.06437747925519943, 0.0639183297753334, 0.06407874822616577, 0.06462731212377548, 0.06447955965995789, 0.06599994003772736, 0.06455240398645401, 0.06507686525583267, 0.06369730830192566, 0.06413611769676208, 0.0626826286315918, 0.06593917310237885, 0.06514746695756912, 0.06429584324359894, 0.06374479085206985, 0.06287220120429993, 0.06424085050821304, 0.06387579441070557, 0.06844484061002731, 0.06364491581916809, 0.06699035316705704, 0.06452558189630508, 0.06410575658082962, 0.06371354311704636, 0.06399990618228912, 0.06408429145812988, 0.06438883394002914, 0.06437211483716965, 0.06629547476768494, 0.06541644036769867, 0.06675192713737488, 0.06579181551933289, 0.06628736853599548, 0.06318037956953049, 0.0670480877161026, 0.07135261595249176, 0.0668763667345047, 0.06406387686729431, 0.0641532763838768, 0.06561309844255447, 0.06371224671602249, 0.06304552406072617, 0.06384801119565964, 0.06743607670068741, 0.06518088281154633, 0.0654548928141594, 0.06543715298175812, 0.06381773948669434, 0.06374210119247437, 0.0647394061088562, 0.06643392890691757, 0.0643434226512909, 0.06389695405960083, 0.06353774666786194, 0.06317991763353348, 0.0643206462264061, 0.06375117599964142, 0.06446696817874908, 0.06788437813520432, 0.0639093667268753, 0.06867415457963943, 0.06516113132238388, 0.06388802826404572, 0.06432365626096725, 0.06624756008386612, 0.06796222925186157, 0.07382798194885254, 0.07082342356443405, 0.06961680203676224, 0.06632106751203537, 0.06888424605131149, 0.0648784264922142, 0.06418877840042114, 0.06445368379354477, 0.06349162012338638, 0.06409583985805511, 0.06737066060304642, 0.06628450006246567, 0.06391637772321701, 0.064394012093544, 0.06388673186302185, 0.06464330852031708, 0.06404127925634384, 0.06322009116411209, 0.06455458700656891, 0.06366115808486938, 0.06460662186145782, 0.06579621881246567, 0.06332067400217056, 0.06320734322071075, 0.06456281244754791, 0.06332474946975708, 0.06708763539791107, 0.0640520378947258, 0.06339156627655029, 0.06275930255651474, 0.0629798173904419, 0.06444967538118362, 0.06491884589195251, 0.06443045288324356, 0.0628298819065094, 0.06273803859949112, 0.0636177733540535, 0.06499648094177246, 0.06323403120040894, 0.06268025189638138, 0.06378649175167084, 0.06484736502170563, 0.06369099020957947, 0.0626462996006012, 0.06443461775779724, 0.06340871006250381, 0.06450753659009933, 0.06356623023748398, 0.06297428160905838, 0.0635170266032219, 0.06863462179899216, 0.06520402431488037, 0.06415820866823196, 0.0645114928483963, 0.06458500027656555, 0.06539343297481537, 0.06513369828462601, 0.06429720669984818, 0.06489452719688416, 0.06362707912921906, 0.06408096104860306, 0.06351451575756073, 0.06627485901117325, 0.06614276766777039, 0.07010199129581451, 0.06906209886074066, 0.06466466188430786, 0.06413549184799194, 0.06336052715778351, 0.0637625977396965, 0.06329422444105148, 0.06300129741430283, 0.06450022757053375, 0.06381811946630478, 0.06400010734796524, 0.06300189346075058, 0.06382729858160019, 0.06267107278108597, 0.0642908364534378, 0.06301531195640564, 0.06331592798233032, 0.06405600905418396, 0.06305713951587677, 0.06213904544711113, 0.0628487765789032, 0.06457959115505219, 0.0638042464852333, 0.062119293957948685, 0.06221431866288185, 0.06318240612745285, 0.06455530226230621, 0.06480100750923157, 0.06291690468788147, 0.06498317420482635, 0.06302450597286224, 0.06357115507125854, 0.0630735531449318, 0.06348256766796112, 0.06334211677312851, 0.06346350163221359, 0.0631994679570198, 0.06271255761384964, 0.06272092461585999, 0.06292524188756943, 0.06485249102115631, 0.06310352683067322, 0.06238320842385292, 0.06421737372875214, 0.06302549690008163, 0.06631262600421906, 0.0632680207490921, 0.06528672575950623, 0.06396626681089401, 0.06457080692052841, 0.06371002644300461, 0.06407638639211655, 0.06886214017868042, 0.0643690899014473, 0.06554913520812988, 0.06273623555898666, 0.06397140026092529, 0.06333090364933014, 0.062392622232437134, 0.06250739097595215, 0.06362900882959366, 0.06285551935434341, 0.06261243671178818, 0.063649483025074, 0.0624563992023468, 0.06566312164068222, 0.06338105350732803, 0.06366145610809326, 0.06357593834400177, 0.06435082107782364, 0.06338794529438019, 0.06525783240795135, 0.06347449868917465, 0.06407427042722702, 0.06360910087823868, 0.06304419785737991, 0.06314200162887573, 0.0665438175201416, 0.06291260570287704, 0.0703507661819458, 0.06510253995656967, 0.06662354618310928, 0.06340271979570389, 0.06437168270349503, 0.06253405660390854, 0.06230604648590088, 0.06229458004236221, 0.06170768663287163, 0.06188540160655975, 0.06117551028728485, 0.061285506933927536, 0.060925357043743134, 0.06194661557674408, 0.0634392648935318, 0.06136691942811012, 0.0642937570810318, 0.06221260130405426, 0.06302377581596375, 0.06980457156896591, 0.06655725091695786, 0.07576685398817062, 0.07202724367380142, 0.06644745171070099, 0.0688880980014801, 0.06315235048532486, 0.0630236268043518, 0.06391825526952744, 0.06355176120996475, 0.06118181720376015, 0.06186902895569801, 0.06125890836119652, 0.06015502288937569, 0.06093163788318634, 0.06021581590175629, 0.06483270227909088, 0.06015627458691597, 0.06494487822055817, 0.06327808648347855, 0.06037626788020134, 0.062268733978271484, 0.06000832840800285, 0.0595957487821579, 0.06155793368816376, 0.06160665675997734, 0.06088721752166748, 0.06244983524084091, 0.06192788481712341, 0.06026432663202286, 0.058771707117557526, 0.06038181856274605, 0.060572706162929535, 0.059300970286130905, 0.059102654457092285, 0.060858700424432755, 0.059950731694698334, 0.05890275910496712, 0.059179600328207016, 0.061926715075969696, 0.06030474230647087, 0.05932203307747841, 0.058917973190546036, 0.06135554611682892, 0.05842692032456398, 0.05810951068997383, 0.06177476420998573, 0.05885317549109459, 0.05803365632891655, 0.060400962829589844, 0.06060326099395752, 0.05976331606507301, 0.0588482990860939, 0.05955715849995613, 0.05877775698900223, 0.06096193939447403, 0.058707162737846375, 0.06223055347800255, 0.06017742678523064, 0.0592334158718586, 0.05934770032763481, 0.059908777475357056, 0.05797508731484413, 0.05685003101825714, 0.06208166852593422, 0.05692145600914955, 0.0577235110104084, 0.05854681134223938, 0.05729273334145546, 0.05699596554040909, 0.05819690600037575, 0.05667493864893913, 0.05688271299004555, 0.05814995989203453, 0.056617528200149536, 0.0587884820997715, 0.060332540422677994, 0.0554967001080513, 0.055006492882966995, 0.0566139742732048, 0.05636020004749298, 0.055389873683452606, 0.05600273236632347, 0.056136831641197205, 0.05508844926953316, 0.05553805083036423, 0.057640139013528824, 0.054289717227220535, 0.056673672050237656, 0.05947905406355858, 0.06346242129802704, 0.06264188140630722, 0.06059771403670311, 0.05567890778183937, 0.05559539794921875, 0.0551760233938694, 0.0585821270942688, 0.059471771121025085, 0.05484790727496147, 0.05940376967191696, 0.05360754579305649, 0.053412795066833496, 0.054276783019304276, 0.05340006574988365, 0.053993746638298035, 0.05387058109045029, 0.05541514232754707, 0.05392962321639061, 0.05261782184243202, 0.05332033336162567, 0.05285132676362991, 0.05536384508013725, 0.05406823009252548, 0.053898297250270844, 0.05791989713907242, 0.05471073091030121, 0.05244100093841553, 0.0553865060210228, 0.052512381225824356, 0.05331476032733917, 0.05322766304016113, 0.054206687957048416, 0.05190195143222809, 0.054841455072164536, 0.05244750529527664, 0.051773250102996826, 0.05226023867726326, 0.052895255386829376, 0.053210921585559845, 0.054236482828855515, 0.051596593111753464, 0.051463812589645386, 0.05188233405351639, 0.05177140608429909, 0.05101512745022774, 0.05288334935903549, 0.053042467683553696, 0.05077109858393669, 0.05126514285802841, 0.050447069108486176, 0.051623087376356125, 0.05274985358119011, 0.05307554081082344, 0.05080833286046982, 0.05045950412750244, 0.0507950484752655, 0.05047161877155304, 0.050115346908569336, 0.05011235550045967, 0.049256838858127594, 0.04974605143070221, 0.053495243191719055, 0.05038869380950928, 0.05051903799176216, 0.05056842043995857, 0.05084878206253052, 0.05107055976986885, 0.05013468489050865, 0.05055788531899452, 0.05043216794729233, 0.04942121356725693, 0.05054121091961861, 0.05113121122121811, 0.052895378321409225, 0.05008593574166298, 0.049968037754297256, 0.050377026200294495, 0.04877982288599014, 0.049151256680488586, 0.04886932671070099, 0.04955634847283363, 0.04958213493227959, 0.049952831119298935, 0.05232305824756622, 0.048030346632003784, 0.048856720328330994, 0.04913455247879028, 0.04769750311970711, 0.048858389258384705, 0.05072180554270744, 0.04762285202741623, 0.04839115962386131, 0.048907794058322906, 0.0475272461771965, 0.049239929765462875, 0.04780736193060875, 0.04734622687101364, 0.04956258833408356, 0.04773704335093498, 0.04886982962489128, 0.04801785945892334, 0.04861970245838165, 0.047251343727111816, 0.047652434557676315, 0.04723086580634117, 0.0478663370013237, 0.0474412776529789, 0.04796009883284569, 0.04811742901802063, 0.04753836244344711, 0.048176977783441544, 0.047316260635852814, 0.04899473488330841, 0.04664408043026924, 0.048943474888801575, 0.046909231692552567, 0.04671969637274742, 0.04745592176914215, 0.046322859823703766, 0.048673853278160095, 0.048142995685338974, 0.049837663769721985, 0.04818330705165863, 0.04921781271696091, 0.04750148579478264, 0.04904089495539665, 0.047610193490982056, 0.04812134802341461, 0.04829804599285126, 0.04873502254486084, 0.049148619174957275, 0.046845901757478714, 0.04717421159148216, 0.04701076075434685, 0.04740018770098686, 0.04702923819422722, 0.04646383598446846, 0.04702822118997574, 0.046008262783288956, 0.04672754183411598, 0.04581298679113388, 0.04579887539148331, 0.047771621495485306, 0.04640563949942589, 0.047055549919605255, 0.047868821769952774, 0.04632040113210678, 0.046018652617931366, 0.04633779078722, 0.04638981446623802, 0.04672463983297348, 0.04673008993268013, 0.04749418795108795, 0.04888347163796425, 0.04849458858370781, 0.04783666878938675, 0.049424927681684494, 0.04636786878108978, 0.04582051560282707, 0.04642466455698013, 0.046406764537096024, 0.046800561249256134, 0.04644298180937767, 0.045638613402843475, 0.046237774193286896, 0.04578462988138199, 0.04651474580168724, 0.0483345165848732, 0.046232398599386215, 0.046380653977394104, 0.04631532356142998, 0.04569118469953537, 0.04524318873882294, 0.04554136097431183, 0.0460512638092041, 0.04511721432209015, 0.04566356539726257, 0.04522966966032982, 0.045009907335042953, 0.046150755137205124, 0.044737569987773895, 0.04664788395166397, 0.04590145871043205, 0.045804303139448166, 0.04452725127339363, 0.04577169939875603, 0.04485658183693886, 0.04525158181786537, 0.04547516256570816, 0.045237574726343155, 0.04596283286809921, 0.045175258070230484, 0.04496195167303085, 0.04490029439330101, 0.044718727469444275, 0.04524100571870804, 0.04437640681862831, 0.0445881150662899, 0.04539581388235092, 0.04432469978928566, 0.045074429363012314, 0.04471300169825554, 0.04393117502331734, 0.04604445397853851, 0.04426545277237892, 0.044482436031103134, 0.04527313634753227, 0.044119562953710556, 0.04514036700129509, 0.044371653348207474, 0.04514354094862938, 0.04597447067499161, 0.04464924335479736, 0.04420827701687813, 0.045610301196575165, 0.0440058708190918, 0.04425385966897011, 0.04422226920723915, 0.04457279294729233, 0.04440516233444214, 0.04412535950541496, 0.04388260096311569, 0.04390883445739746, 0.0438532717525959, 0.04346366226673126, 0.0442173071205616, 0.04440339282155037, 0.043542273342609406, 0.04416383057832718, 0.04362102970480919, 0.0439935028553009, 0.04410644248127937, 0.0435011088848114, 0.04437912628054619, 0.04393303021788597, 0.04366651549935341, 0.044365543872117996, 0.04332038760185242, 0.04571181908249855, 0.043882470577955246, 0.045348744839429855, 0.044621530920267105, 0.044060178101062775, 0.04420705884695053, 0.04383513331413269, 0.04369617998600006, 0.04379581660032272, 0.04360060393810272, 0.04443719983100891, 0.04391133785247803, 0.04533913731575012, 0.04349103197455406, 0.04542992636561394, 0.044028494507074356, 0.0437375083565712, 0.0446312353014946, 0.04544099047780037, 0.04580877721309662, 0.04537327215075493, 0.04598041996359825, 0.0452643483877182, 0.045356929302215576, 0.04435322433710098, 0.04497363418340683, 0.04471323639154434, 0.0440429262816906, 0.043413303792476654, 0.044592853635549545, 0.04324711114168167, 0.04356039687991142, 0.04454189911484718, 0.043151695281267166, 0.043328892439603806, 0.043788280338048935, 0.042648304253816605, 0.04322381317615509, 0.04369036480784416, 0.04297232627868652, 0.043273694813251495, 0.043333183974027634, 0.04310046508908272, 0.044203050434589386, 0.04313065856695175, 0.04411069676280022, 0.04413875937461853, 0.043033841997385025, 0.04330861568450928, 0.043578073382377625, 0.043639909476041794, 0.04289507493376732, 0.04311579838395119, 0.04331269860267639, 0.04264150187373161, 0.04302157834172249, 0.04393478110432625, 0.04306057468056679, 0.04399245232343674, 0.043618276715278625, 0.042934443801641464, 0.04315422102808952, 0.043117377907037735, 0.04383724182844162, 0.04280278459191322, 0.04318374767899513, 0.04337770491838455, 0.04266686737537384, 0.0442376472055912, 0.04277478903532028, 0.042573265731334686, 0.04336439445614815, 0.042438190430402756, 0.0434083566069603, 0.04246150329709053, 0.042889855802059174, 0.04264593869447708, 0.04277746379375458, 0.04300917312502861, 0.043388593941926956, 0.04316205158829689, 0.042081598192453384, 0.04301387071609497, 0.042540691792964935, 0.04239952191710472, 0.042382121086120605, 0.04216859117150307, 0.042117465287446976, 0.04314795881509781, 0.04234982654452324, 0.04354562237858772, 0.043437156826257706, 0.04271860420703888, 0.042832329869270325, 0.04219689592719078, 0.042395178228616714, 0.042866937816143036, 0.042388856410980225, 0.041914843022823334, 0.04371475428342819, 0.04261213541030884, 0.04216112941503525, 0.042410749942064285, 0.041993383318185806, 0.04198610037565231, 0.042235810309648514, 0.04204689338803291, 0.04238112270832062, 0.04281478747725487, 0.043560728430747986, 0.04361312463879585, 0.042411480098962784, 0.0420508049428463, 0.04387999698519707, 0.042510367929935455, 0.04230187460780144, 0.042215876281261444, 0.04194933921098709, 0.04241538792848587, 0.04238234460353851, 0.04256400093436241, 0.04277931898832321, 0.04184911027550697, 0.04255170002579689, 0.04213808849453926, 0.04211121052503586, 0.0419732965528965, 0.04183206707239151, 0.042565327137708664, 0.042076531797647476, 0.042278215289115906, 0.04293399676680565, 0.04219640791416168, 0.042248573154211044, 0.04284580051898956, 0.041757695376873016, 0.04195381700992584, 0.04179222881793976, 0.04185762256383896, 0.04203389957547188, 0.04159693419933319, 0.042642638087272644, 0.04228987544775009, 0.041862018406391144, 0.04170437902212143, 0.04193081334233284, 0.04179972782731056, 0.042048096656799316, 0.04188919439911842, 0.04162605106830597, 0.04186432436108589, 0.043092165142297745, 0.0420190691947937, 0.04220614954829216, 0.04255276918411255, 0.041634202003479004, 0.04222432151436806, 0.04224984347820282, 0.041575074195861816, 0.041568562388420105, 0.04153916984796524, 0.04214264079928398, 0.04391174018383026, 0.04576000198721886, 0.042962782084941864, 0.042046234011650085, 0.0432867556810379, 0.044389236718416214, 0.04344971850514412, 0.043886128813028336, 0.04212423786520958, 0.04213322326540947, 0.0429617278277874, 0.042713671922683716, 0.0419154055416584, 0.042245958000421524, 0.04212459549307823, 0.04146304726600647, 0.04209751635789871, 0.04153961315751076, 0.041707687079906464, 0.04240763559937477, 0.041669655591249466, 0.041536178439855576, 0.04152685031294823, 0.04117484390735626, 0.041676465421915054, 0.04135945066809654, 0.04126976057887077, 0.04159591346979141, 0.04144174978137016, 0.04121958091855049, 0.04183358699083328, 0.041518568992614746, 0.04135340452194214, 0.04125821962952614, 0.04175375774502754, 0.04113951325416565, 0.041220828890800476, 0.04140622168779373, 0.0413474403321743, 0.041008688509464264, 0.04168535768985748, 0.04096527770161629, 0.041539277881383896, 0.0413946695625782, 0.041631389409303665, 0.04126347228884697, 0.041020717471838, 0.04114478826522827, 0.04142877086997032, 0.04230315610766411, 0.04196751490235329, 0.041298720985651016, 0.04163116589188576, 0.041433606296777725, 0.04147784411907196, 0.04167672246694565, 0.04196454957127571, 0.041639868170022964, 0.04133361577987671, 0.04152919352054596, 0.04111228883266449, 0.04181773215532303, 0.041296202689409256, 0.041404370218515396, 0.04153789207339287, 0.04121435061097145, 0.04104216396808624, 0.04125162586569786, 0.04146455228328705, 0.041080888360738754, 0.041662316769361496, 0.041436824947595596, 0.04095771536231041, 0.043001823127269745, 0.0410112589597702, 0.04141364246606827, 0.04193638637661934, 0.04108886793255806, 0.0419604554772377, 0.041238658130168915, 0.04140060767531395, 0.042056724429130554, 0.04199029877781868, 0.04168598726391792, 0.04136153683066368, 0.04176639765501022, 0.04142995923757553, 0.04156474024057388, 0.04139956086874008, 0.04236587509512901, 0.04216841980814934, 0.04206446558237076, 0.04136550426483154, 0.04111797362565994, 0.042026009410619736, 0.04108714312314987, 0.04094313457608223, 0.04173048213124275, 0.04102484881877899, 0.04182186722755432, 0.041680436581373215, 0.04141262546181679, 0.04122529178857803, 0.04138763248920441, 0.04183901101350784, 0.04169820249080658, 0.04199342057108879, 0.04131230711936951, 0.0412849523127079, 0.04182562977075577, 0.04106464236974716, 0.04278874024748802, 0.041828643530607224, 0.04188051074743271, 0.04183436185121536, 0.041630037128925323, 0.04136012867093086, 0.040966860949993134, 0.04115904122591019, 0.04090606048703194, 0.04084686189889908, 0.04119881987571716, 0.04118993133306503, 0.040516581386327744, 0.04131944105029106, 0.040960993617773056, 0.041173215955495834, 0.041516661643981934, 0.04074200987815857, 0.040873363614082336, 0.04060765355825424, 0.04041336104273796, 0.040836405009031296, 0.040613822638988495, 0.040675628930330276, 0.04093395173549652, 0.04064803570508957, 0.0404832661151886, 0.040827978402376175, 0.04063377529382706, 0.04072871059179306, 0.04073848947882652, 0.040724024176597595, 0.04063422232866287, 0.04132223129272461, 0.04056578502058983, 0.040555886924266815, 0.04191875830292702, 0.040408022701740265, 0.04050552472472191, 0.04080473631620407, 0.040636058896780014, 0.04070829227566719, 0.041170667856931686, 0.0417468324303627, 0.041089389473199844, 0.0415763258934021, 0.041757598519325256, 0.04064910113811493, 0.04107233136892319, 0.04163676127791405, 0.04069148376584053, 0.04122236743569374, 0.04158367961645126, 0.040516193956136703, 0.04062085971236229, 0.04179980605840683, 0.04086252674460411, 0.041133981198072433, 0.04128829017281532, 0.04085657000541687, 0.04092904180288315, 0.040397439152002335, 0.04085489735007286, 0.04139244556427002, 0.04054154083132744, 0.04060864821076393, 0.04074299708008766, 0.040429454296827316, 0.04027317836880684, 0.04067109897732735, 0.04025451838970184, 0.040901415050029755, 0.04103294759988785, 0.040926169604063034, 0.041103433817625046, 0.04111971706151962, 0.04047580063343048, 0.04066798835992813, 0.04046783596277237, 0.04031279310584068, 0.04041597992181778, 0.040453676134347916, 0.040754545480012894, 0.04133537411689758, 0.04083818569779396, 0.04114898294210434, 0.04065817594528198, 0.040909431874752045, 0.04044528678059578, 0.04078831523656845, 0.04092208668589592, 0.04078774154186249, 0.04071508347988129, 0.04047207534313202, 0.041249535977840424, 0.04107958450913429, 0.040420643985271454, 0.040581777691841125, 0.04083625227212906, 0.04066729545593262, 0.040649835020303726, 0.03997543454170227, 0.04060431942343712, 0.04174710065126419, 0.042856913059949875, 0.04152354225516319, 0.041773613542318344, 0.041139718145132065, 0.04028397053480148, 0.040631070733070374, 0.040519360452890396, 0.04127991199493408, 0.04097366705536842, 0.040506087243556976, 0.04053235054016113, 0.040578875690698624, 0.04027898609638214, 0.04012906923890114, 0.0405237190425396, 0.04011883586645126, 0.04025200754404068, 0.040228214114904404, 0.040433119982481, 0.04011363908648491, 0.04047775641083717, 0.041145358234643936, 0.040717627853155136, 0.04064415022730827, 0.04034405201673508, 0.03990093246102333, 0.04033849388360977, 0.04021834582090378, 0.04040953144431114, 0.0403033122420311, 0.040006935596466064, 0.04028427228331566, 0.04008442535996437, 0.0405069924890995, 0.04096139594912529, 0.0402989536523819, 0.04066966101527214, 0.04080398380756378, 0.04128516465425491, 0.040648818016052246, 0.04026823490858078, 0.04009714722633362, 0.04000869393348694, 0.039999041706323624, 0.04039835184812546, 0.04005972295999527, 0.04158774018287659, 0.04057999327778816, 0.040185797959566116, 0.0405256487429142, 0.040908847004175186, 0.04023279249668121, 0.04013565927743912, 0.04014045000076294, 0.04063395783305168, 0.03997480869293213, 0.040578678250312805, 0.039887282997369766, 0.04008084163069725, 0.040133316069841385, 0.04047432541847229, 0.04017815738916397, 0.04001234844326973, 0.04025190323591232, 0.040545519441366196, 0.04004285857081413, 0.03978576138615608, 0.040034085512161255, 0.04138358682394028, 0.04141657054424286, 0.041585586965084076, 0.040964994579553604, 0.03993609920144081, 0.04076511785387993, 0.040610719472169876, 0.040730953216552734, 0.040237098932266235, 0.0405801497399807, 0.04043201729655266, 0.04028706252574921, 0.0404176265001297, 0.040356215089559555, 0.04033346101641655, 0.03997265174984932, 0.039752550423145294, 0.03994366154074669, 0.040132906287908554, 0.04060861095786095, 0.04009876400232315, 0.03977882117033005, 0.03988184779882431, 0.03998818248510361, 0.040401212871074677, 0.04057666286826134, 0.0403791107237339, 0.04011266306042671, 0.04017859697341919, 0.039973076432943344, 0.039823152124881744, 0.039890579879283905, 0.040123485028743744, 0.04006322845816612, 0.04070446267724037, 0.040256988257169724, 0.039939284324645996, 0.03967392444610596, 0.04030385985970497, 0.03987831994891167, 0.0401565358042717, 0.0401456244289875, 0.04031644016504288, 0.04049248248338699, 0.03977778181433678, 0.040241919457912445, 0.039674703031778336, 0.040037188678979874, 0.04012944549322128, 0.039593249559402466, 0.03988399729132652, 0.03946901112794876, 0.03996416926383972, 0.040264327079057693, 0.039990250021219254, 0.039951227605342865, 0.03982231765985489, 0.03972566872835159, 0.03979972377419472, 0.039436254650354385, 0.03945901244878769, 0.03981938585639, 0.03968556225299835, 0.03961867839097977, 0.039298467338085175, 0.03940347209572792, 0.039678383618593216, 0.03939083218574524, 0.040599942207336426, 0.040207188576459885, 0.04055088385939598, 0.039914388209581375, 0.03972538560628891, 0.03967726230621338, 0.039243318140506744, 0.03931663557887077, 0.0392116978764534, 0.03953588381409645, 0.039256539195775986, 0.039634235203266144, 0.03946730121970177, 0.03910989686846733, 0.0393136665225029, 0.03919016942381859, 0.03908246383070946, 0.03907310962677002, 0.03891899064183235, 0.03917130082845688, 0.03924012556672096, 0.0391426719725132, 0.03885182738304138, 0.03920058161020279, 0.03893033787608147, 0.03892352432012558, 0.038861263543367386, 0.038985371589660645, 0.03890688717365265, 0.03889252990484238, 0.038973964750766754, 0.03889992833137512, 0.0388045571744442, 0.03891194611787796, 0.038563285022974014, 0.03878366947174072, 0.0391414612531662, 0.03942423686385155, 0.03850409761071205, 0.03843574970960617, 0.03821498155593872, 0.03869705647230148, 0.039013490080833435, 0.03864578530192375, 0.03847628831863403, 0.038638126105070114, 0.03827666863799095, 0.03810849413275719, 0.03861594200134277, 0.038315389305353165, 0.03828607499599457, 0.0382559560239315, 0.03811796009540558, 0.038452062755823135, 0.0381268635392189, 0.038404952734708786, 0.03800521418452263, 0.03859783709049225, 0.039095889776945114, 0.03893535956740379, 0.038582660257816315, 0.03832156956195831, 0.03782640025019646, 0.03813117370009422, 0.03846364840865135, 0.038556087762117386, 0.03861565515398979, 0.03803003579378128, 0.037678930908441544, 0.03824576362967491, 0.03828543797135353, 0.037406664341688156, 0.03773057833313942, 0.03766447305679321, 0.037649333477020264, 0.038635026663541794, 0.03956732526421547, 0.03757597506046295, 0.03687405213713646, 0.036597833037376404, 0.03682192042469978, 0.03689552843570709, 0.036333903670310974, 0.03655930235981941, 0.03667966276407242, 0.036217931658029556, 0.03639407455921173, 0.03679473698139191, 0.03604603931307793, 0.03671302646398544, 0.03655792772769928, 0.036680251359939575, 0.03627341613173485, 0.03643405809998512, 0.036197319626808167, 0.036637961864471436, 0.03641638904809952, 0.036453764885663986, 0.035680752247571945, 0.03580275923013687, 0.03552507236599922, 0.03539855778217316, 0.035256195813417435, 0.03575284779071808, 0.0354297012090683, 0.035154398530721664, 0.03561345487833023, 0.03494372218847275, 0.035134457051754, 0.03480718284845352, 0.03460898995399475, 0.03452170267701149, 0.034562863409519196, 0.03504481539130211, 0.03466545417904854, 0.034605152904987335, 0.03444739803671837, 0.03431345149874687, 0.03420421853661537, 0.03408506512641907, 0.033975306898355484, 0.034164708107709885, 0.033792901784181595, 0.03389187157154083, 0.03391505032777786, 0.033909134566783905, 0.03387146815657616, 0.03340481221675873, 0.03410522639751434, 0.03343521058559418, 0.03336256369948387, 0.033616531640291214, 0.03281873092055321, 0.03321301192045212, 0.033108506351709366, 0.03300202637910843, 0.032663505524396896, 0.0325961597263813, 0.03252573311328888, 0.03290930017828941, 0.03260994330048561, 0.0326005257666111, 0.03285599872469902, 0.0324995219707489, 0.03215818479657173, 0.0321078859269619, 0.03190569207072258, 0.03168003633618355, 0.03187195211648941, 0.03196665644645691, 0.03208792209625244, 0.031732697039842606, 0.031324565410614014, 0.03136443346738815, 0.03134974464774132, 0.03171256184577942, 0.03098478354513645, 0.030857572332024574, 0.030759384855628014, 0.030879294499754906, 0.03117525763809681, 0.030586697161197662, 0.030554059892892838, 0.03034522756934166, 0.031055087223649025, 0.03037402778863907, 0.030069293454289436, 0.03038697876036167, 0.03023083135485649, 0.029740162193775177, 0.029782259836792946, 0.029870573431253433, 0.02960260957479477, 0.02969212830066681, 0.029775943607091904, 0.029203103855252266, 0.02900717966258526, 0.02892369031906128, 0.02919158898293972, 0.0291304811835289, 0.02876920998096466, 0.02871474251151085, 0.028871752321720123, 0.028976745903491974, 0.02859082818031311, 0.02830910123884678, 0.028671719133853912, 0.029740143567323685, 0.0283550713211298, 0.028660720214247704, 0.02893868088722229, 0.02873687446117401, 0.028640426695346832, 0.029412871226668358, 0.027938175946474075, 0.028522571548819542, 0.028281427919864655, 0.027460645884275436, 0.02737431786954403, 0.027160532772541046, 0.027697406709194183, 0.02675214409828186, 0.02678721770644188, 0.02713029831647873, 0.026552116498351097, 0.026775270700454712, 0.02649269811809063, 0.02698807790875435, 0.026240842416882515, 0.02662043645977974, 0.02602081000804901, 0.025891432538628578, 0.026235563680529594, 0.02565288543701172, 0.02557866834104061, 0.025566820055246353, 0.025400588288903236, 0.025505587458610535, 0.02589576691389084, 0.025387246161699295, 0.025115255266427994, 0.02550407312810421, 0.026302970945835114, 0.024917995557188988, 0.024468064308166504, 0.024769358336925507, 0.02473187819123268, 0.0245454590767622, 0.024301420897245407, 0.024976316839456558, 0.024289971217513084, 0.02389988675713539, 0.02372591197490692, 0.02386818267405033, 0.023655567318201065, 0.023542627692222595, 0.023513376712799072, 0.023982133716344833, 0.023370832204818726, 0.02337557263672352, 0.02310326136648655, 0.02295486442744732, 0.023192068561911583, 0.0230826698243618, 0.022497979924082756, 0.02247765101492405, 0.02278120256960392, 0.02272367849946022, 0.022685110569000244, 0.022075597196817398, 0.022716820240020752, 0.02405344508588314, 0.02203519269824028, 0.022142337635159492, 0.021860884502530098, 0.02201761119067669, 0.022460274398326874, 0.021374773234128952, 0.021826522424817085, 0.02111654542386532, 0.022155236452817917, 0.021353811025619507, 0.02110382914543152, 0.02175767533481121, 0.021090984344482422, 0.02083309181034565, 0.020794374868273735, 0.020650113001465797, 0.02077023871243, 0.020376700907945633, 0.020988140255212784, 0.02086404711008072, 0.019993403926491737, 0.020564815029501915, 0.01995258778333664, 0.019860824570059776, 0.02010415866971016, 0.019770067185163498, 0.019625980406999588, 0.02032676711678505, 0.01962125115096569, 0.019884292036294937, 0.01940234564244747, 0.019546791911125183, 0.019586175680160522, 0.0190080888569355, 0.01909996010363102, 0.019156262278556824, 0.019417108967900276, 0.018993597477674484, 0.01846599392592907, 0.018435202538967133, 0.018753664568066597, 0.019556879997253418, 0.018341083079576492, 0.018078884109854698, 0.018057087436318398, 0.018562691286206245, 0.018291037529706955, 0.01781415194272995, 0.017701495438814163, 0.019597431644797325, 0.01773088425397873, 0.017237013205885887, 0.017573243007063866, 0.017958732321858406, 0.017566950991749763, 0.017177730798721313, 0.017491813749074936, 0.01719004288315773, 0.017257176339626312, 0.017364488914608955, 0.01735837757587433, 0.01688471995294094, 0.017782950773835182, 0.016726123169064522, 0.016920313239097595, 0.016966579481959343, 0.016817141324281693, 0.01659582369029522, 0.016424989327788353, 0.01621951162815094, 0.01706654019653797, 0.01588713377714157, 0.015896914526820183, 0.017020313069224358, 0.015990298241376877, 0.015322990715503693, 0.016221288591623306, 0.017568092793226242, 0.015095166862010956, 0.015000730752944946, 0.01596856489777565, 0.015698693692684174, 0.014964981004595757, 0.015458758920431137, 0.015572859905660152, 0.015270514413714409, 0.015073591843247414, 0.014591747894883156, 0.016341419890522957, 0.014983716420829296, 0.014587345533072948, 0.014464673586189747, 0.014641167595982552, 0.01474183239042759, 0.01472979225218296, 0.013924140483140945, 0.014681519009172916, 0.01426669117063284, 0.013842741958796978, 0.014523005113005638, 0.01410123985260725, 0.014045110903680325, 0.014111579395830631, 0.013797881081700325, 0.013731644488871098, 0.01432262547314167, 0.01394127868115902, 0.013872143812477589, 0.013284552842378616, 0.014866005629301071, 0.013499329797923565, 0.012873517349362373, 0.013319214805960655, 0.013460373505949974, 0.013164774514734745, 0.013540711253881454, 0.012889426201581955, 0.013036719523370266, 0.013109429739415646, 0.012517519295215607, 0.012687384150922298, 0.012799439020454884, 0.013274201191961765, 0.012484326027333736, 0.01229068636894226, 0.012571398168802261, 0.012324798852205276, 0.01261419802904129, 0.012370704673230648, 0.012083233334124088, 0.011923039332032204, 0.01285015419125557, 0.01224001869559288, 0.011517299339175224, 0.01172043289989233, 0.012711472809314728, 0.012087819166481495, 0.011638072319328785, 0.011543011292815208, 0.011308923363685608, 0.012028337456285954, 0.011251465417444706, 0.011227905750274658, 0.01147003285586834, 0.01169986929744482, 0.011210673488676548, 0.011298414319753647, 0.011390735395252705, 0.010976690798997879, 0.01100684329867363, 0.011041229590773582, 0.010488079860806465, 0.011214883998036385, 0.01109242346137762, 0.010759174823760986, 0.010540500283241272, 0.010518456809222698, 0.01094326376914978, 0.010478062555193901, 0.011079201474785805, 0.01053702738136053, 0.01002102717757225, 0.010708540678024292, 0.010471311397850513, 0.01015134435147047, 0.01017545536160469, 0.009832647629082203, 0.009975154884159565, 0.01037866622209549, 0.009857897646725178, 0.01034959964454174, 0.009685428813099861, 0.010003290139138699, 0.009910505264997482, 0.010676623322069645, 0.009610360488295555, 0.01016673631966114, 0.009477200917899609, 0.009786944836378098, 0.011632978916168213, 0.009188782423734665, 0.009001048281788826, 0.010330211371183395, 0.00970343966037035, 0.009013291448354721, 0.009143575094640255, 0.011219228617846966, 0.009202748537063599, 0.00881789717823267, 0.0096065578982234, 0.009401555173099041, 0.008738690055906773, 0.009235161356627941, 0.008619308471679688, 0.008860806934535503, 0.008834809996187687, 0.008575767278671265, 0.008818495087325573, 0.008994936011731625, 0.008396264165639877, 0.008541264571249485, 0.00921736191958189, 0.008250630460679531, 0.00825575366616249, 0.009298321790993214, 0.008222240023314953, 0.008306663483381271, 0.008455920964479446, 0.008791991509497166, 0.008358529768884182, 0.008217298425734043, 0.008580151945352554, 0.008401650004088879, 0.008217375725507736, 0.007984884083271027, 0.008448438718914986, 0.008660484105348587, 0.007558018434792757, 0.007712115533649921, 0.007998637855052948, 0.008364483714103699, 0.008366195484995842, 0.007371826097369194, 0.008187183178961277, 0.008067121729254723, 0.007645551580935717, 0.007935225032269955, 0.007243137340992689, 0.007873347960412502, 0.007578507997095585, 0.0077858129516243935, 0.007202330511063337, 0.007239471189677715, 0.0081363245844841, 0.007224940229207277, 0.007197902072221041, 0.007203046698123217, 0.008043002337217331, 0.007146912626922131, 0.006819542031735182, 0.007469474337995052, 0.007188413292169571, 0.007428778801113367, 0.007514266297221184, 0.006573403254151344, 0.006986918393522501, 0.007013930473476648, 0.007108183111995459, 0.007028856314718723, 0.006763949990272522, 0.007765206508338451, 0.0073005445301532745, 0.007091241888701916, 0.006613218225538731, 0.006789898034185171, 0.006460196804255247, 0.007516237907111645, 0.006682647857815027, 0.006683411542326212, 0.006615166552364826, 0.007098463363945484, 0.0063704573549330235, 0.0064377556554973125, 0.006775153335183859, 0.006412799470126629, 0.006044568028301001, 0.00676422193646431, 0.006760532036423683, 0.0059014540165662766, 0.005965851247310638, 0.00668040756136179, 0.006523040123283863, 0.005951030645519495, 0.006076520774513483, 0.006101297680288553, 0.006469481624662876, 0.0061660995706915855, 0.005958144087344408, 0.006504208315163851, 0.007089041639119387, 0.005752388387918472, 0.005610075779259205, 0.005803836975246668, 0.006708348635584116, 0.005666458513587713, 0.0055982028134167194, 0.006221693474799395, 0.00636106776073575, 0.0056774490512907505, 0.0056854854337871075, 0.005501611158251762, 0.005781345535069704, 0.006217780523002148, 0.0055132764391601086, 0.005491119809448719, 0.005650772247463465, 0.006232254672795534, 0.005668202415108681, 0.005359977018088102, 0.005872813053429127, 0.00550448102876544, 0.005541155114769936, 0.005023112520575523, 0.005559968762099743, 0.005660624708980322, 0.00506807304918766, 0.006070832721889019, 0.0058061303570866585, 0.004951765760779381, 0.005008986219763756, 0.006123716942965984, 0.005767592694610357, 0.004904246889054775, 0.005301237106323242, 0.005431221332401037, 0.005134364124387503, 0.004743471741676331, 0.005234746262431145, 0.006072772666811943, 0.005601543001830578, 0.005214877426624298, 0.005205065477639437, 0.005285872612148523, 0.006187384482473135, 0.005934704095125198, 0.004816931672394276, 0.0057661025784909725, 0.004934171214699745, 0.005685086827725172, 0.004720143508166075, 0.004929661750793457, 0.005247490480542183, 0.004617677535861731, 0.00455913320183754, 0.005043193697929382, 0.004814951214939356, 0.004815227817744017, 0.004903521854430437, 0.004408834036439657, 0.0046906620264053345, 0.005022309720516205, 0.004578731022775173, 0.004474635235965252, 0.00454377569258213, 0.0048024095594882965, 0.0044117397628724575, 0.004974594805389643, 0.0047972844913601875, 0.004682715982198715, 0.0041734203696250916, 0.004710086155682802, 0.004424558486789465, 0.004548865370452404, 0.004236402455717325, 0.004357778467237949, 0.004414654802531004, 0.004506793804466724, 0.004072094801813364, 0.004357003141194582, 0.004814896732568741, 0.0040387255139648914, 0.004202412907034159, 0.004300897940993309, 0.004019940737634897, 0.0042407396249473095, 0.004641803912818432, 0.004218386020511389, 0.0041911606676876545, 0.004101738333702087, 0.004109619650989771, 0.0044035702012479305, 0.003929976373910904, 0.0042404904961586, 0.003932357765734196, 0.004210811108350754, 0.00399191677570343, 0.003776756813749671, 0.003932586405426264, 0.004232182167470455, 0.004082710947841406, 0.0037047278601676226, 0.0038560584653168917, 0.004158474039286375, 0.0036137509159743786, 0.004180148243904114, 0.004379116464406252, 0.0036726032849401236, 0.0036418819800019264, 0.00397926801815629, 0.003784700995311141, 0.0036388016305863857, 0.0037064070347696543, 0.004047831520438194, 0.0038435522001236677, 0.003623135155066848, 0.0038985684514045715, 0.0041497861966490746, 0.003777547739446163, 0.003835151670500636, 0.003657153807580471, 0.003911252599209547, 0.003618047572672367, 0.003317062044516206, 0.0038035388570278883, 0.003761397209018469, 0.003579650307074189, 0.00355949392542243, 0.003642746713012457, 0.0036346125416457653, 0.0033008574973791838, 0.003677718574181199, 0.00454839738085866, 0.003284425474703312, 0.0035796340089291334, 0.004213906358927488, 0.003310177940875292, 0.0035905337426811457, 0.0035340110771358013, 0.0032059343066066504, 0.0035692311357706785, 0.0036758037749677896, 0.003288557752966881, 0.0037147370167076588, 0.0034209061414003372, 0.0035320587921887636, 0.003553930902853608, 0.0036327766720205545, 0.0033084270544350147, 0.0035567092709243298, 0.004673118703067303, 0.0032089629676193, 0.0034964263904839754, 0.0034348834306001663, 0.003383358009159565, 0.003094477578997612, 0.003132817568257451, 0.0036051212809979916, 0.0031094409059733152, 0.003218425437808037, 0.0033181682229042053, 0.003133064368739724, 0.0031855381093919277, 0.0033179272431880236, 0.003048659535124898, 0.003015553578734398, 0.0033004325814545155, 0.0030895546078681946, 0.0034046832006424665, 0.0030513624660670757, 0.0035046220291405916, 0.0031144986860454082, 0.0028402190655469894, 0.0030999244190752506, 0.003283656435087323, 0.0028804680332541466, 0.002815613290295005, 0.0030026747845113277, 0.003119801636785269, 0.003105914918705821, 0.002963873790577054, 0.0031601020600646734, 0.002888144226744771, 0.0030155791901052, 0.003228906774893403, 0.0032537817023694515, 0.003371609840542078, 0.0029856995679438114, 0.002837470732629299, 0.003152829362079501, 0.0027029116172343493, 0.002789275022223592, 0.0031300424598157406, 0.002707775915041566, 0.002729385159909725, 0.002908731112256646, 0.0028545213863253593, 0.002989328233525157, 0.0029201528523117304, 0.0028159006033092737, 0.0027986865025013685, 0.003089599311351776, 0.0026333609130233526, 0.0027234654407948256, 0.002839334076270461, 0.002653570380061865, 0.0027424658183008432, 0.002556215738877654, 0.0026860025245696306, 0.0027438956312835217, 0.0027087260968983173, 0.0024663181975483894, 0.0026505833957344294, 0.002781851449981332, 0.0025596541818231344, 0.0025728242471814156, 0.002736311638727784, 0.0028908594977110624, 0.0025246506556868553, 0.002656472846865654, 0.0030361588578671217, 0.0027465554885566235, 0.002533716382458806, 0.0026345776859670877, 0.002513858489692211, 0.002744305180385709, 0.0024957722052931786, 0.0023030578158795834, 0.002731270156800747, 0.0026517275255173445, 0.0023148187901824713, 0.002413160167634487, 0.0029103055130690336, 0.0022472909186035395, 0.0024469555355608463, 0.0030841738916933537, 0.0022774427197873592, 0.0024286406114697456, 0.0025680148974061012, 0.0025591677986085415, 0.0023255599662661552, 0.0023346198722720146, 0.0027212994173169136, 0.002399784978479147, 0.0022943459916859865, 0.002435075817629695, 0.0027692406438291073, 0.0022721383720636368, 0.002201782539486885, 0.002304999390617013, 0.0025792603846639395, 0.002354661002755165, 0.002206838456913829, 0.0023070566821843386, 0.0025666661094874144, 0.002347145928069949, 0.0022899664472788572, 0.0023718292359262705, 0.002370289759710431, 0.0023991146590560675, 0.0023840314242988825, 0.0023348345421254635, 0.002163288649171591, 0.002186691155657172, 0.002243073657155037, 0.002119883196428418, 0.002109927125275135, 0.0023832814767956734, 0.0025948903057724237, 0.0022705404553562403, 0.0023774253204464912, 0.0027906198520213366, 0.002156567759811878, 0.0021554806735366583, 0.0021439790725708008, 0.002064851578325033, 0.00212192814797163, 0.002203083597123623, 0.002050980692729354, 0.0026371132116764784, 0.002230630023404956, 0.001899306196719408, 0.0026417640037834644, 0.002452385611832142, 0.001954796025529504, 0.0023135135415941477, 0.0022439658641815186, 0.0020815536845475435, 0.002184396144002676, 0.002269749529659748, 0.002047826535999775, 0.002096271375194192, 0.002209876198321581, 0.002061277162283659, 0.002077385550364852, 0.002039851387962699, 0.0020956131629645824, 0.002364185405895114, 0.001961211673915386, 0.0018632910214364529, 0.0020225385669618845, 0.0022562872618436813, 0.0018087466014549136, 0.0019603248219937086, 0.0020777371246367693, 0.0019734110683202744, 0.0021248655393719673, 0.0019773899111896753, 0.0019276050152257085, 0.001973871374502778, 0.001912950654514134, 0.002089006593450904, 0.0019839222077280283, 0.0019027827074751258, 0.0018163492204621434, 0.001953523140400648, 0.001943892682902515, 0.0019377456046640873, 0.001982264220714569, 0.0018162220949307084, 0.0019084421219304204, 0.0018423596629872918, 0.0018992724362760782, 0.001782615203410387, 0.0019572412129491568, 0.0019696333911269903, 0.0019613916520029306, 0.0017850333824753761, 0.0017104343278333545, 0.001967300195246935, 0.0017841826193034649, 0.0017347752582281828, 0.0017942643025889993, 0.0021537821739912033, 0.0017508327728137374, 0.0019351707305759192, 0.0016913008876144886, 0.0019093801965937018, 0.0017980353441089392, 0.0018577418522909284, 0.001861133030615747, 0.0017140732379630208, 0.0017610628856346011, 0.00198374898172915, 0.0016655345680192113, 0.0018040452850982547, 0.0019089144188910723, 0.0016920488560572267, 0.0016603507101535797, 0.0020723494235426188, 0.0017042235704138875, 0.0015632628928869963, 0.0018800657708197832, 0.0017368984408676624, 0.001667271601036191, 0.0016497437609359622, 0.0016644276911392808, 0.001795665011741221, 0.0018066070042550564, 0.0017171581275761127, 0.001815350609831512, 0.001873136148788035, 0.0017134195659309626, 0.001919770846143365, 0.0016728914342820644, 0.0016593508189544082, 0.001787805580534041, 0.0016086839605122805, 0.0018519610166549683, 0.001754865050315857, 0.0016044401563704014, 0.0016776277916505933, 0.002508409321308136, 0.0016386539209634066, 0.0016303452430292964, 0.0015849353512749076, 0.001598378992639482, 0.0016064640367403626, 0.001497582532465458, 0.0015640663914382458, 0.001550988177768886, 0.0015201987698674202, 0.0016045891679823399, 0.0015884998720139265, 0.001562595716677606, 0.00156581518240273, 0.0016538616036996245, 0.0014218701981008053, 0.001464469125494361, 0.0015700600342825055, 0.001497396850027144, 0.0014696397120133042, 0.0016986308619379997, 0.0015898675192147493, 0.0015490988735109568, 0.0014962328132241964, 0.001503159524872899, 0.0014309401158243418, 0.0013909685658290982, 0.0015801334520801902, 0.0014698297018185258, 0.0016204972052946687, 0.0015178282046690583, 0.0014984564622864127, 0.0015087664360180497, 0.0013698682887479663, 0.001594419707544148, 0.0015571076655760407, 0.0012868941994383931, 0.0018166651716455817, 0.0014362260699272156, 0.00134322012308985, 0.0015013020019978285, 0.0014024333795532584, 0.0013191105099394917, 0.0014344545779749751, 0.0014672892866656184, 0.0015731543535366654, 0.001339880283921957, 0.0015042036538943648, 0.0015428969636559486, 0.0014327734243124723, 0.0013996202033013105, 0.0014338054461404681, 0.0013255256926640868, 0.0013440322363749146, 0.0013517984189093113, 0.0015294482000172138, 0.0012738750083371997, 0.0013424755306914449, 0.001600782503373921, 0.0014824755489826202, 0.0015071936650201678, 0.0013073030859231949, 0.0012664544628933072, 0.001340066664852202, 0.0013644397258758545, 0.0012538302689790726, 0.001479964004829526, 0.0014379292260855436, 0.0012761049438267946, 0.0013009329559281468, 0.0013667952734977007, 0.0012519716983661056, 0.0013233731733635068, 0.0012899585999548435, 0.0013086155522614717, 0.0012596212327480316, 0.0012668002163991332, 0.0013420303585007787, 0.0012204445665702224, 0.0012208695989102125, 0.0013025292428210378, 0.0012637621257454157, 0.0011326445965096354, 0.0014513233909383416, 0.0012730966554954648, 0.0012294896878302097, 0.0013250169577077031]}],\n",
              "                        {\"hovermode\": \"x\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Metric Results\"}, \"xaxis\": {\"title\": {\"text\": \"epochs\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4f3b4923-b0bc-4641-87be-c3e83479fd2a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQQ_3WAafsvW"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "nF8f30Pdfb53",
        "outputId": "d37daa0f-d334-4a5c-9e15-9edd10ed8eae"
      },
      "source": [
        "# Run on cpu\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Load Regression\n",
        "model = RegressionNet(num_inputs=1)\n",
        "weights = torch.load(\"weights_regression.pt\")\n",
        "model.load_state_dict(weights)\n",
        "model = model.to(device)\n",
        "\n",
        "# Predict Regression\n",
        "Xt = torch.from_numpy(X).type(torch.float32).unsqueeze(1)\n",
        "Xt = (Xt - R_mean) / R_std\n",
        "Y_hat = model.forward(Xt).detach().numpy().reshape(-1)\n",
        "\n",
        "# Visualize Regression\n",
        "fig = go.Figure()\n",
        "fig.add_traces( go.Scatter(x=X, y=Y, name=\"Real\",hovertemplate='x: %{x} <br>y: %{y}') )\n",
        "fig.add_traces( go.Scatter(x=X, y=Y_hat, name=\"Predicted\",hovertemplate='x: %{x} <br>y: %{y}') )\n",
        "fig.update_layout(title=\"Regression Results\",hovermode=\"x\")\n",
        "fig.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"df3c1d73-b183-45ff-95d0-6501fb044c05\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"df3c1d73-b183-45ff-95d0-6501fb044c05\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'df3c1d73-b183-45ff-95d0-6501fb044c05',\n",
              "                        [{\"hovertemplate\": \"x: %{x} <br>y: %{y}\", \"name\": \"Real\", \"type\": \"scatter\", \"x\": [-2.0, -1.98998998998999, -1.97997997997998, -1.96996996996997, -1.95995995995996, -1.94994994994995, -1.93993993993994, -1.92992992992993, -1.91991991991992, -1.90990990990991, -1.8998998998999, -1.88988988988989, -1.87987987987988, -1.86986986986987, -1.85985985985986, -1.84984984984985, -1.83983983983984, -1.82982982982983, -1.8198198198198199, -1.8098098098098099, -1.7997997997997999, -1.7897897897897899, -1.7797797797797799, -1.7697697697697699, -1.7597597597597598, -1.7497497497497498, -1.7397397397397398, -1.7297297297297298, -1.7197197197197198, -1.7097097097097098, -1.6996996996996998, -1.6896896896896898, -1.6796796796796798, -1.6696696696696698, -1.6596596596596598, -1.6496496496496498, -1.6396396396396398, -1.6296296296296298, -1.6196196196196198, -1.6096096096096097, -1.5995995995995997, -1.5895895895895895, -1.5795795795795795, -1.5695695695695695, -1.5595595595595595, -1.5495495495495495, -1.5395395395395395, -1.5295295295295295, -1.5195195195195195, -1.5095095095095095, -1.4994994994994995, -1.4894894894894894, -1.4794794794794794, -1.4694694694694694, -1.4594594594594594, -1.4494494494494494, -1.4394394394394394, -1.4294294294294294, -1.4194194194194194, -1.4094094094094094, -1.3993993993993994, -1.3893893893893894, -1.3793793793793794, -1.3693693693693694, -1.3593593593593594, -1.3493493493493494, -1.3393393393393394, -1.3293293293293293, -1.3193193193193193, -1.3093093093093093, -1.2992992992992993, -1.2892892892892893, -1.2792792792792793, -1.2692692692692693, -1.2592592592592593, -1.2492492492492493, -1.2392392392392393, -1.2292292292292293, -1.2192192192192193, -1.2092092092092093, -1.1991991991991993, -1.189189189189189, -1.179179179179179, -1.169169169169169, -1.159159159159159, -1.149149149149149, -1.139139139139139, -1.129129129129129, -1.119119119119119, -1.109109109109109, -1.099099099099099, -1.089089089089089, -1.079079079079079, -1.069069069069069, -1.059059059059059, -1.049049049049049, -1.039039039039039, -1.029029029029029, -1.019019019019019, -1.009009009009009, -0.9989989989989989, -0.9889889889889889, -0.9789789789789789, -0.9689689689689689, -0.9589589589589589, -0.9489489489489489, -0.9389389389389389, -0.9289289289289289, -0.9189189189189189, -0.9089089089089089, -0.8988988988988988, -0.8888888888888888, -0.8788788788788788, -0.8688688688688688, -0.8588588588588588, -0.8488488488488488, -0.8388388388388388, -0.8288288288288288, -0.8188188188188188, -0.8088088088088088, -0.7987987987987988, -0.7887887887887888, -0.7787787787787788, -0.7687687687687688, -0.7587587587587588, -0.7487487487487487, -0.7387387387387387, -0.7287287287287287, -0.7187187187187187, -0.7087087087087087, -0.6986986986986987, -0.6886886886886887, -0.6786786786786787, -0.6686686686686687, -0.6586586586586587, -0.6486486486486487, -0.6386386386386387, -0.6286286286286287, -0.6186186186186187, -0.6086086086086087, -0.5985985985985987, -0.5885885885885886, -0.5785785785785786, -0.5685685685685686, -0.5585585585585586, -0.5485485485485486, -0.5385385385385386, -0.5285285285285286, -0.5185185185185186, -0.5085085085085086, -0.4984984984984986, -0.4884884884884886, -0.4784784784784786, -0.46846846846846857, -0.45845845845845856, -0.44844844844844856, -0.43843843843843855, -0.42842842842842854, -0.41841841841841854, -0.40840840840840853, -0.3983983983983985, -0.3883883883883883, -0.3783783783783783, -0.3683683683683683, -0.3583583583583583, -0.34834834834834827, -0.33833833833833826, -0.32832832832832826, -0.31831831831831825, -0.30830830830830824, -0.29829829829829824, -0.28828828828828823, -0.2782782782782782, -0.2682682682682682, -0.2582582582582582, -0.2482482482482482, -0.2382382382382382, -0.2282282282282282, -0.21821821821821819, -0.20820820820820818, -0.19819819819819817, -0.18818818818818817, -0.17817817817817816, -0.16816816816816815, -0.15815815815815815, -0.14814814814814814, -0.13813813813813813, -0.12812812812812813, -0.11811811811811812, -0.10810810810810811, -0.09809809809809811, -0.0880880880880881, -0.0780780780780781, -0.06806806806806809, -0.05805805805805808, -0.048048048048048075, -0.03803803803803807, -0.028028028028028062, -0.018018018018018056, -0.00800800800800805, 0.002002002002002179, 0.012012012012012185, 0.022022022022022192, 0.0320320320320322, 0.042042042042042205, 0.05205205205205221, 0.06206206206206222, 0.07207207207207222, 0.08208208208208223, 0.09209209209209224, 0.10210210210210224, 0.11211211211211225, 0.12212212212212226, 0.13213213213213226, 0.14214214214214227, 0.15215215215215228, 0.16216216216216228, 0.1721721721721723, 0.1821821821821823, 0.1921921921921923, 0.2022022022022023, 0.21221221221221231, 0.22222222222222232, 0.23223223223223233, 0.24224224224224233, 0.25225225225225234, 0.26226226226226235, 0.27227227227227235, 0.28228228228228236, 0.29229229229229237, 0.3023023023023024, 0.3123123123123124, 0.3223223223223224, 0.3323323323323324, 0.3423423423423424, 0.3523523523523524, 0.3623623623623624, 0.3723723723723724, 0.3823823823823824, 0.39239239239239243, 0.40240240240240244, 0.41241241241241244, 0.42242242242242245, 0.43243243243243246, 0.44244244244244246, 0.45245245245245247, 0.4624624624624625, 0.4724724724724725, 0.4824824824824825, 0.4924924924924925, 0.5025025025025025, 0.5125125125125125, 0.5225225225225225, 0.5325325325325325, 0.5425425425425425, 0.5525525525525525, 0.5625625625625625, 0.5725725725725725, 0.5825825825825826, 0.5925925925925926, 0.6026026026026026, 0.6126126126126126, 0.6226226226226226, 0.6326326326326326, 0.6426426426426426, 0.6526526526526526, 0.6626626626626626, 0.6726726726726726, 0.6826826826826826, 0.6926926926926926, 0.7027027027027026, 0.7127127127127126, 0.7227227227227226, 0.7327327327327327, 0.7427427427427427, 0.7527527527527527, 0.7627627627627627, 0.7727727727727727, 0.7827827827827827, 0.7927927927927927, 0.8028028028028027, 0.8128128128128127, 0.8228228228228227, 0.8328328328328327, 0.8428428428428427, 0.8528528528528527, 0.8628628628628627, 0.8728728728728727, 0.8828828828828827, 0.8928928928928928, 0.9029029029029028, 0.9129129129129128, 0.9229229229229228, 0.9329329329329328, 0.9429429429429428, 0.9529529529529528, 0.9629629629629628, 0.9729729729729728, 0.9829829829829828, 0.9929929929929928, 1.0030030030030028, 1.0130130130130128, 1.0230230230230228, 1.0330330330330328, 1.0430430430430429, 1.0530530530530529, 1.0630630630630629, 1.0730730730730729, 1.0830830830830829, 1.0930930930930929, 1.1031031031031029, 1.113113113113113, 1.123123123123123, 1.133133133133133, 1.143143143143143, 1.153153153153153, 1.163163163163163, 1.173173173173173, 1.183183183183183, 1.193193193193193, 1.203203203203203, 1.2132132132132134, 1.2232232232232234, 1.2332332332332334, 1.2432432432432434, 1.2532532532532534, 1.2632632632632634, 1.2732732732732734, 1.2832832832832834, 1.2932932932932935, 1.3033033033033035, 1.3133133133133135, 1.3233233233233235, 1.3333333333333335, 1.3433433433433435, 1.3533533533533535, 1.3633633633633635, 1.3733733733733735, 1.3833833833833835, 1.3933933933933935, 1.4034034034034035, 1.4134134134134135, 1.4234234234234235, 1.4334334334334335, 1.4434434434434436, 1.4534534534534536, 1.4634634634634636, 1.4734734734734736, 1.4834834834834836, 1.4934934934934936, 1.5035035035035036, 1.5135135135135136, 1.5235235235235236, 1.5335335335335336, 1.5435435435435436, 1.5535535535535536, 1.5635635635635636, 1.5735735735735736, 1.5835835835835836, 1.5935935935935936, 1.6036036036036037, 1.6136136136136137, 1.6236236236236237, 1.6336336336336337, 1.6436436436436437, 1.6536536536536537, 1.6636636636636637, 1.6736736736736737, 1.6836836836836837, 1.6936936936936937, 1.7037037037037037, 1.7137137137137137, 1.7237237237237237, 1.7337337337337337, 1.7437437437437437, 1.7537537537537538, 1.7637637637637638, 1.7737737737737738, 1.7837837837837838, 1.7937937937937938, 1.8038038038038038, 1.8138138138138138, 1.8238238238238238, 1.8338338338338338, 1.8438438438438438, 1.8538538538538538, 1.8638638638638638, 1.8738738738738738, 1.8838838838838838, 1.8938938938938938, 1.9039039039039038, 1.9139139139139139, 1.9239239239239239, 1.9339339339339339, 1.9439439439439439, 1.9539539539539539, 1.9639639639639639, 1.973973973973974, 1.983983983983984, 1.993993993993994, 2.0040040040040044, 2.0140140140140144, 2.0240240240240244, 2.0340340340340344, 2.0440440440440444, 2.0540540540540544, 2.0640640640640644, 2.0740740740740744, 2.0840840840840844, 2.0940940940940944, 2.1041041041041044, 2.1141141141141144, 2.1241241241241244, 2.1341341341341344, 2.1441441441441444, 2.1541541541541545, 2.1641641641641645, 2.1741741741741745, 2.1841841841841845, 2.1941941941941945, 2.2042042042042045, 2.2142142142142145, 2.2242242242242245, 2.2342342342342345, 2.2442442442442445, 2.2542542542542545, 2.2642642642642645, 2.2742742742742745, 2.2842842842842845, 2.2942942942942945, 2.3043043043043046, 2.3143143143143146, 2.3243243243243246, 2.3343343343343346, 2.3443443443443446, 2.3543543543543546, 2.3643643643643646, 2.3743743743743746, 2.3843843843843846, 2.3943943943943946, 2.4044044044044046, 2.4144144144144146, 2.4244244244244246, 2.4344344344344346, 2.4444444444444446, 2.4544544544544546, 2.4644644644644647, 2.4744744744744747, 2.4844844844844847, 2.4944944944944947, 2.5045045045045047, 2.5145145145145147, 2.5245245245245247, 2.5345345345345347, 2.5445445445445447, 2.5545545545545547, 2.5645645645645647, 2.5745745745745747, 2.5845845845845847, 2.5945945945945947, 2.6046046046046047, 2.6146146146146148, 2.6246246246246248, 2.6346346346346348, 2.6446446446446448, 2.6546546546546548, 2.664664664664665, 2.674674674674675, 2.684684684684685, 2.694694694694695, 2.704704704704705, 2.714714714714715, 2.724724724724725, 2.734734734734735, 2.744744744744745, 2.754754754754755, 2.764764764764765, 2.774774774774775, 2.784784784784785, 2.794794794794795, 2.804804804804805, 2.814814814814815, 2.824824824824825, 2.834834834834835, 2.844844844844845, 2.854854854854855, 2.864864864864865, 2.874874874874875, 2.884884884884885, 2.894894894894895, 2.904904904904905, 2.914914914914915, 2.924924924924925, 2.934934934934935, 2.944944944944945, 2.954954954954955, 2.964964964964965, 2.974974974974975, 2.984984984984985, 2.994994994994995, 3.005005005005005, 3.015015015015015, 3.025025025025025, 3.035035035035035, 3.045045045045045, 3.055055055055055, 3.065065065065065, 3.075075075075075, 3.085085085085085, 3.095095095095095, 3.105105105105105, 3.115115115115115, 3.125125125125125, 3.135135135135135, 3.145145145145145, 3.155155155155155, 3.165165165165165, 3.175175175175175, 3.185185185185185, 3.195195195195195, 3.205205205205205, 3.215215215215215, 3.225225225225225, 3.235235235235235, 3.245245245245245, 3.255255255255255, 3.265265265265265, 3.275275275275275, 3.285285285285285, 3.295295295295295, 3.305305305305305, 3.315315315315315, 3.325325325325325, 3.335335335335335, 3.3453453453453452, 3.3553553553553552, 3.3653653653653652, 3.3753753753753752, 3.3853853853853852, 3.3953953953953953, 3.4054054054054053, 3.4154154154154153, 3.4254254254254253, 3.4354354354354353, 3.4454454454454453, 3.4554554554554553, 3.4654654654654653, 3.4754754754754753, 3.4854854854854853, 3.4954954954954953, 3.5055055055055053, 3.5155155155155153, 3.5255255255255253, 3.5355355355355353, 3.5455455455455454, 3.5555555555555554, 3.5655655655655654, 3.5755755755755754, 3.5855855855855854, 3.5955955955955954, 3.6056056056056054, 3.6156156156156154, 3.6256256256256254, 3.6356356356356354, 3.6456456456456454, 3.6556556556556554, 3.6656656656656654, 3.6756756756756754, 3.6856856856856854, 3.6956956956956954, 3.7057057057057055, 3.7157157157157155, 3.7257257257257255, 3.7357357357357355, 3.7457457457457455, 3.7557557557557555, 3.7657657657657655, 3.7757757757757755, 3.7857857857857855, 3.7957957957957955, 3.8058058058058055, 3.8158158158158155, 3.8258258258258255, 3.8358358358358355, 3.8458458458458455, 3.8558558558558556, 3.8658658658658656, 3.8758758758758756, 3.8858858858858856, 3.8958958958958956, 3.9059059059059056, 3.9159159159159156, 3.9259259259259256, 3.9359359359359356, 3.9459459459459456, 3.9559559559559556, 3.9659659659659656, 3.9759759759759756, 3.9859859859859856, 3.9959959959959956, 4.006006006006006, 4.016016016016016, 4.026026026026026, 4.036036036036036, 4.046046046046046, 4.056056056056056, 4.066066066066066, 4.076076076076076, 4.086086086086086, 4.096096096096096, 4.106106106106106, 4.116116116116116, 4.126126126126126, 4.136136136136136, 4.146146146146146, 4.156156156156156, 4.166166166166166, 4.176176176176176, 4.186186186186186, 4.196196196196196, 4.206206206206206, 4.216216216216216, 4.226226226226226, 4.236236236236236, 4.246246246246246, 4.256256256256256, 4.266266266266266, 4.276276276276276, 4.286286286286286, 4.296296296296296, 4.306306306306306, 4.316316316316316, 4.326326326326326, 4.336336336336336, 4.346346346346346, 4.356356356356356, 4.366366366366366, 4.376376376376376, 4.386386386386386, 4.396396396396396, 4.406406406406406, 4.416416416416417, 4.426426426426427, 4.436436436436437, 4.446446446446447, 4.456456456456457, 4.466466466466467, 4.476476476476477, 4.486486486486487, 4.496496496496497, 4.506506506506507, 4.516516516516517, 4.526526526526527, 4.536536536536537, 4.546546546546547, 4.556556556556557, 4.566566566566567, 4.576576576576577, 4.586586586586587, 4.596596596596597, 4.606606606606607, 4.616616616616617, 4.626626626626627, 4.636636636636637, 4.646646646646647, 4.656656656656657, 4.666666666666667, 4.676676676676677, 4.686686686686687, 4.696696696696697, 4.706706706706707, 4.716716716716717, 4.726726726726727, 4.736736736736737, 4.746746746746747, 4.756756756756757, 4.766766766766767, 4.776776776776777, 4.786786786786787, 4.796796796796797, 4.806806806806807, 4.816816816816817, 4.826826826826827, 4.836836836836837, 4.846846846846847, 4.856856856856857, 4.866866866866867, 4.876876876876877, 4.886886886886887, 4.896896896896897, 4.906906906906907, 4.916916916916917, 4.926926926926927, 4.936936936936937, 4.946946946946947, 4.956956956956957, 4.966966966966967, 4.976976976976977, 4.986986986986987, 4.996996996996997, 5.007007007007007, 5.017017017017017, 5.027027027027027, 5.037037037037037, 5.047047047047047, 5.057057057057057, 5.067067067067067, 5.077077077077077, 5.087087087087087, 5.097097097097097, 5.107107107107107, 5.117117117117117, 5.127127127127127, 5.137137137137137, 5.147147147147147, 5.157157157157157, 5.167167167167167, 5.177177177177177, 5.187187187187187, 5.197197197197197, 5.207207207207207, 5.217217217217217, 5.227227227227227, 5.237237237237237, 5.247247247247247, 5.257257257257257, 5.267267267267267, 5.277277277277277, 5.287287287287287, 5.297297297297297, 5.307307307307307, 5.317317317317317, 5.327327327327327, 5.337337337337337, 5.347347347347347, 5.357357357357357, 5.367367367367367, 5.377377377377377, 5.387387387387387, 5.397397397397397, 5.407407407407407, 5.4174174174174174, 5.4274274274274275, 5.4374374374374375, 5.4474474474474475, 5.4574574574574575, 5.4674674674674675, 5.4774774774774775, 5.4874874874874875, 5.4974974974974975, 5.5075075075075075, 5.5175175175175175, 5.5275275275275275, 5.5375375375375375, 5.5475475475475475, 5.5575575575575575, 5.5675675675675675, 5.5775775775775776, 5.587587587587588, 5.597597597597598, 5.607607607607608, 5.617617617617618, 5.627627627627628, 5.637637637637638, 5.647647647647648, 5.657657657657658, 5.667667667667668, 5.677677677677678, 5.687687687687688, 5.697697697697698, 5.707707707707708, 5.717717717717718, 5.727727727727728, 5.737737737737738, 5.747747747747748, 5.757757757757758, 5.767767767767768, 5.777777777777778, 5.787787787787788, 5.797797797797798, 5.807807807807808, 5.817817817817818, 5.827827827827828, 5.837837837837838, 5.847847847847848, 5.857857857857858, 5.867867867867868, 5.877877877877878, 5.887887887887888, 5.897897897897898, 5.907907907907908, 5.917917917917918, 5.927927927927928, 5.937937937937938, 5.947947947947948, 5.957957957957958, 5.967967967967968, 5.977977977977978, 5.987987987987988, 5.997997997997998, 6.008008008008009, 6.018018018018019, 6.028028028028029, 6.038038038038039, 6.048048048048049, 6.058058058058059, 6.068068068068069, 6.078078078078079, 6.088088088088089, 6.098098098098099, 6.108108108108109, 6.118118118118119, 6.128128128128129, 6.138138138138139, 6.148148148148149, 6.158158158158159, 6.168168168168169, 6.178178178178179, 6.188188188188189, 6.198198198198199, 6.208208208208209, 6.218218218218219, 6.228228228228229, 6.238238238238239, 6.248248248248249, 6.258258258258259, 6.268268268268269, 6.278278278278279, 6.288288288288289, 6.298298298298299, 6.308308308308309, 6.318318318318319, 6.328328328328329, 6.338338338338339, 6.348348348348349, 6.358358358358359, 6.368368368368369, 6.378378378378379, 6.388388388388389, 6.398398398398399, 6.408408408408409, 6.418418418418419, 6.428428428428429, 6.438438438438439, 6.448448448448449, 6.458458458458459, 6.468468468468469, 6.478478478478479, 6.488488488488489, 6.498498498498499, 6.508508508508509, 6.518518518518519, 6.528528528528529, 6.538538538538539, 6.548548548548549, 6.558558558558559, 6.568568568568569, 6.578578578578579, 6.588588588588589, 6.598598598598599, 6.608608608608609, 6.618618618618619, 6.628628628628629, 6.638638638638639, 6.648648648648649, 6.658658658658659, 6.668668668668669, 6.678678678678679, 6.688688688688689, 6.698698698698699, 6.708708708708709, 6.718718718718719, 6.728728728728729, 6.738738738738739, 6.748748748748749, 6.758758758758759, 6.768768768768769, 6.778778778778779, 6.788788788788789, 6.798798798798799, 6.808808808808809, 6.818818818818819, 6.828828828828829, 6.838838838838839, 6.848848848848849, 6.858858858858859, 6.868868868868869, 6.878878878878879, 6.888888888888889, 6.898898898898899, 6.908908908908909, 6.918918918918919, 6.928928928928929, 6.938938938938939, 6.948948948948949, 6.958958958958959, 6.968968968968969, 6.978978978978979, 6.988988988988989, 6.998998998998999, 7.009009009009009, 7.019019019019019, 7.029029029029029, 7.039039039039039, 7.049049049049049, 7.059059059059059, 7.069069069069069, 7.079079079079079, 7.089089089089089, 7.099099099099099, 7.109109109109109, 7.119119119119119, 7.129129129129129, 7.1391391391391394, 7.1491491491491495, 7.1591591591591595, 7.1691691691691695, 7.1791791791791795, 7.1891891891891895, 7.1991991991991995, 7.2092092092092095, 7.2192192192192195, 7.2292292292292295, 7.2392392392392395, 7.2492492492492495, 7.2592592592592595, 7.2692692692692695, 7.2792792792792795, 7.2892892892892895, 7.2992992992992995, 7.3093093093093096, 7.31931931931932, 7.32932932932933, 7.33933933933934, 7.34934934934935, 7.35935935935936, 7.36936936936937, 7.37937937937938, 7.38938938938939, 7.3993993993994, 7.40940940940941, 7.41941941941942, 7.42942942942943, 7.43943943943944, 7.44944944944945, 7.45945945945946, 7.46946946946947, 7.47947947947948, 7.48948948948949, 7.4994994994995, 7.50950950950951, 7.51951951951952, 7.52952952952953, 7.53953953953954, 7.54954954954955, 7.55955955955956, 7.56956956956957, 7.57957957957958, 7.58958958958959, 7.5995995995996, 7.60960960960961, 7.61961961961962, 7.62962962962963, 7.63963963963964, 7.64964964964965, 7.65965965965966, 7.66966966966967, 7.67967967967968, 7.68968968968969, 7.6996996996997, 7.70970970970971, 7.71971971971972, 7.72972972972973, 7.73973973973974, 7.74974974974975, 7.75975975975976, 7.76976976976977, 7.77977977977978, 7.78978978978979, 7.7997997997998, 7.80980980980981, 7.81981981981982, 7.82982982982983, 7.83983983983984, 7.84984984984985, 7.85985985985986, 7.86986986986987, 7.87987987987988, 7.88988988988989, 7.8998998998999, 7.90990990990991, 7.91991991991992, 7.92992992992993, 7.93993993993994, 7.94994994994995, 7.95995995995996, 7.96996996996997, 7.97997997997998, 7.98998998998999, 8.0], \"y\": [4.348766175087199, 4.277192966093969, 4.205102954463674, 4.132487146126188, 4.059336629423321, 3.9856425908123354, 3.911396330578623, 3.8365892785425917, 3.7612130097457555, 3.6852592601009544, 3.6087199419916254, 3.5315871598050066, 3.4538532253841696, 3.375510673383772, 3.296552276514456, 3.2169710606608466, 3.1367603198581673, 3.055913631112543, 2.974424869050148, 2.892288220380445, 2.8094981981588774, 2.7260496558344807, 2.641937801068032, 2.5571582093064924, 2.4717068370996693, 2.385580035145178, 2.2987745610480106, 2.211287591781181, 2.123116735834155, 2.0342600450359956, 1.9447160260403917, 1.8544836514599983, 1.763562370637775, 1.6719521200432987, 1.579653333282303, 1.4866669507080115, 1.392994428623144, 1.2986377480617897, 1.2035994231406981, 1.1078825089698654, 1.0114906091126674, 0.9144278825861474, 0.8166990503924698, 0.7183094015728864, 0.6192647987760354, 0.5195716833327357, 0.41923707982988656, 0.31826860017649905, 0.21667444715532413, 0.11446341745397193, 0.011644904169876624, -0.09177110121609311, -0.19577400740211237, -0.3003526223682337, -0.40549515302337114, -0.5111892053997501, -0.6174217853976534, -0.7241793000828051, -0.8314475595382479, -0.9392117792720592, -1.0474565831817677, -1.1561660070758175, -1.2653235027519267, -1.3749119426316847, -1.484913624950209, -1.595310279499198, -1.706083073921177, -1.8172126205522456, -1.9286789838101188, -2.0404616881237443, -2.15253972640026, -2.2648915690245763, -2.377495173386338, -2.4903279939285246, -2.6033669927114618, -2.716588650485514, -2.8299689782652266, -2.9434835293972257, -3.0571074121136768, -3.1708153025626418, -3.284581458306205, -3.3983797322767635, -3.5121835871814295, -3.6259661103440575, -3.7397000289739224, -3.8533577258496927, -3.9669112554068846, -4.080332360216582, -4.193592487842789, -4.3066628080653935, -4.419514230455329, -4.532117422288148, -4.644442826781849, -4.756460681644461, -4.868141037916503, -4.9794537790931885, -5.0903686405108175, -5.200855228981597, -5.310883042660767, -5.420421491129657, -5.529439915678046, -5.637907609768911, -5.745793839668446, -5.8530678652239985, -5.959698960772354, -6.065656436160612, -6.170909657861748, -6.275428070166713, -6.379181216434895, -6.482138760384516, -6.584270507404513, -6.685546425869305, -6.7859366684377775, -6.88541159331777, -6.983941785477266, -7.081498077783479, -7.178051572051032, -7.2735736599803715, -7.368036043967623, -7.461410757767144, -7.553670186988011, -7.644787089405842, -7.734734615071362, -7.823486326197288, -7.9110162168051685, -7.997298732114024, -8.082308787652714, -8.166021788078188, -8.24841364568196, -8.329460798567293, -8.409140228479904, -8.487429478275137, -8.564306669004896, -8.639750516607812, -8.71374034818652, -8.78625611785608, -8.857278422148063, -8.926788514954989, -8.994768322000299, -9.061200454819298, -9.126068224236976, -9.189355653328905, -9.251047489851947, -9.311129218131782, -9.369587070394845, -9.426408037532582, -9.4815798792865, -9.535091133842874, -9.586931126826506, -9.637089979683441, -9.68555861744299, -9.732328775849988, -9.777393007858722, -9.820744689480492, -9.862378024977327, -9.902288051394937, -9.940470642428538, -9.976922511615763, -10.011641214851448, -10.044625152219691, -10.075873569139151, -10.105386556818152, -10.133165052016768, -10.159210836113736, -10.183526533476522, -10.206115609133608, -10.226982365748656, -10.2461319398968, -10.263570297643966, -10.27930422943074, -10.293341344262984, -10.305690063211918, -10.31635961222714, -10.325360014266613, -10.332702080748282, -10.338397402328647, -10.342458339014197, -10.34489800961226, -10.34573028052846, -10.344969753918493, -10.342631755202705, -10.338732319952378, -10.333288180157382, -10.326316749885335, -10.317836110343055, -10.307864994351664, -10.296422770247228, -10.283529425219443, -10.26920554810139, -10.253472311623915, -10.236351454148753, -10.217865260895032, -10.198036544674254, -10.17688862614946, -10.15444531363461, -10.130730882450868, -10.105770053856764, -10.079587973569812, -10.052210189897455, -10.02366263149575, -9.993971584774501, -9.963163670968031, -9.93126582289107, -9.898305261399686, -9.86430947157741, -9.829306178667146, -9.793323323769677, -9.756389039329914, -9.718531624432263, -9.679779519926786, -9.640161283408018, -9.599705564068548, -9.558441077449636, -9.516396580111353, -9.473600844244842, -9.430082632249466, -9.385870671297702, -9.340993627910766, -9.29548008256797, -9.24935850437294, -9.202657225799813, -9.155404417542508, -9.107628063490283, -9.059355935852572, -9.010615570456242, -8.961434242238193, -8.911838940956192, -8.86185634714073, -8.811512808310493, -8.760834315473982, -8.709846479939516, -8.658574510455768, -8.607043190704692, -8.555276857168487, -8.503299377391981, -8.451134128661536, -8.398803977121258, -8.346331257347012, -8.293737752398362, -8.241044674368224, -8.188272645449604, -8.13544167953846, -8.082571164391203, -8.029679844355053, -7.9767858036888795, -7.923906450491755, -7.871058501255976, -7.818257966060714, -7.7655201344220455, -7.712859561814483, -7.660290056878594, -7.6078246693287745, -7.555475678574574, -7.503254583068439, -7.4511720903920695, -7.399238108093019, -7.347461735282465, -7.295851255004445, -7.244414127386212, -7.193156983578656, -7.142085620495041, -7.091204996355662, -7.040519227045238, -6.990031583289241, -6.939744488654514, -6.889659518378907, -6.8397773990338875, -6.790098009023257, -6.740620379920504, -6.6913426986464, -6.642262310487838, -6.593375722957994, -6.544678610497276, -6.496165820013618, -6.447831377259997, -6.39966849404622, -6.351669576281285, -6.303826232841837, -6.2561292852614505, -6.208568778234713, -6.161133990929321, -6.113813449098615, -6.0665949379862205, -6.019465516013725, -5.972411529241536, -5.925418626592335, -5.878471775825825, -5.831555280252663, -5.7846527961748455, -5.737747351039019, -5.6908213622884976, -5.643856656899113, -5.596834491583287, -5.549735573646058, -5.502540082476148, -5.455227691654493, -5.40777759166201, -5.360168513167784, -5.312378750878208, -5.264386187927086, -5.2161683207860285, -5.167702284674023, -5.1189648794444444, -5.069932595927254, -5.020581642703672, -4.970887973290062, -4.920827313707345, -4.8703751904117745, -4.819506958562505, -4.768197830600948, -4.716422905116556, -4.6641571959732735, -4.61137566167058, -4.558053234912709, -4.50416485235933, -4.449685484530726, -4.394590165840208, -4.3388540247263165, -4.28245231385714, -4.225360440378891, -4.16755399618074, -4.109008788147777, -4.0497008683738525, -3.9896065643059693, -3.9287025087918632, -3.866965670002368, -3.804373381200133, -3.740903370326329, -3.6765337893770025, -3.611243243540793, -3.545010820069881, -3.4778161168561006, -3.40963927068435, -3.3404609851355844, -3.270262558111895, -3.199025908956392, -3.1267336051408874, -3.053368888494635, -2.978915700947703, -2.9033587097628804, -2.8266833322303846, -2.7488757598000113, -2.669922981625776, -2.5898128074985363, -2.5085338901425227, -2.4260757468522085, -2.342428780446422, -2.2575842995171604, -2.171534537951091, -2.0842726737023045, -1.9957928467954744, -1.906090176539199, -1.8151607779299195, -1.7230017772274842, -1.6296113266840813, -1.5349886184089783, -1.4391338973522037, -1.3420484733910423, -1.24373473250397, -1.144196147017411, -1.0434372849114877, -0.9414638181717343, -0.838282530174549, -0.7339013220950003, -0.6283292183264301, -0.5215763709021658, -0.41365406291050655, -0.3045747108950356, -0.1943518662331991, -0.08300021548698666, 0.029464420279539327, 0.14302508722019924, 0.25766370146755135, 0.3733610529371285, 0.4900968101765919, 0.6078495262604091, 0.7265966457297051, 0.8463145125759866, 0.9669783792664914, 1.0885624168079528, 1.2110397258446128, 1.3343823487853694, 1.4585612829539754, 1.583546494755262, 1.7093069348494034, 1.835810554325292, 1.9630243218631453, 2.090914241875538, 2.219445373615101, 2.34858185123623, 2.4782869047971854, 2.6085228821881135, 2.7392512719695548, 2.8704327271051713, 3.0020270895715004, 3.133993415826708, 3.2662900031194466, 3.3988744166180713, 3.53170351733967, 3.66473349085752, 3.7979198767648183, 3.931217598871703, 4.064580996111949, 4.197963854134772, 4.331319437556655, 4.464600522847288, 4.597759431823088, 4.730748065721046, 4.86351793982508, 4.996020218616403, 5.128205751418833, 5.260025108509433, 5.3914286176642685, 5.522366401108609, 5.65278841284036, 5.782644476295081, 5.911884322320461, 6.040457627427765, 6.168314052287326, 6.295403280434818, 6.421675057154756, 6.547079228507284, 6.671565780464135, 6.795084878119327, 6.9175869049400145, 7.039022502022695, 7.159342607319804, 7.278498494801697, 7.396441813518818, 7.513124626528885, 7.628499449653879, 7.742519290031576, 7.8551376844265075, 7.966308737265197, 8.075987158360707, 8.184128300291611, 8.290688195400712, 8.395623592379014, 8.498891992400681, 8.600451684774983, 8.700261782081544, 8.798282254755511, 8.894473965089635, 8.988798700620645, 9.081219206867713, 9.171699219391266, 9.260203495140903, 9.34669784306162, 9.431149153928208, 9.513525429378124, 9.593795810113859, 9.671930603246393, 9.747901308751963, 9.821680645015165, 9.893242573431987, 9.962562322047196, 10.029616408201301, 10.094382660162976, 10.156840237723788, 10.216969651732825, 10.274752782549744, 10.33017289739556, 10.383214666581562, 10.433864178597473, 10.48210895404111, 10.527937958372647, 10.57134161347766, 10.612311808024073, 10.650841906599188, 10.686926757614064, 10.720562699963477, 10.751747568430844, 10.78048069782859, 10.806762925865455, 10.830596594733443, 10.851985551408205, 10.870935146657773, 10.887452232755718, 10.901545159895951, 10.91322377130758, 10.922499397069318, 10.929384846624243, 10.933894399996719, 10.936043797714618, 10.935850229441066, 10.933332321321153, 10.928510122050232, 10.921405087671591, 10.912040065112459, 10.900439274468523, 10.886628290048241, 10.870634020189433, 10.852484685861787, 10.832209798070078, 10.809840134073971, 10.785407712441529, 10.758945766954538, 10.73048871938497, 10.700072151162914, 10.66773277395747, 10.63350839919309, 10.597437906524956, 10.559561211297957, 10.519919231014917, 10.478553850840655, 10.435507888169434, 10.390825056284386, 10.344549927138306, 10.29672789328623, 10.247405129001036, 10.196628550604173, 10.144445776044455, 10.090905083758708, 10.036055370848718, 9.979946110609887, 9.922627309447439, 9.864149463217032, 9.804563513026999, 9.743920800540291, 9.682273022814663, 9.61967218672025, 9.556170562974263, 9.491820639832925, 9.42667507648131, 9.360786656162208, 9.294208239085407, 9.22699271515926, 9.159192956586699, 9.090861770368068, 9.022051850753524, 8.95281573168783, 8.883205739290625, 8.81327394441538, 8.743072115330262, 8.672651670564306, 8.602063631962231, 8.531358577991192, 8.460586597342793, 8.389797242873492, 8.31903948592642, 8.248361671077463, 8.177811471348246, 8.107435843928323, 8.037280986448712, 7.9673922938484045, 7.897814315875275, 7.828590715262258, 7.759764226619268, 7.691376616080868, 7.623468641749072, 7.5560800149701866, 7.489249362483946, 7.423014189482531, 7.357410843616417, 7.292474479983277, 7.228239027135354, 7.1647371541400116, 7.102000238727275, 7.0400583365573715, 6.978940151640332, 6.918673007938847, 6.859282822184577, 6.800794077937142, 6.743229800913996, 6.68661153561834, 6.630959323291161, 6.576291681212377, 6.522625583374912, 6.469976442554433, 6.418358093796188, 6.367782779339304, 6.31826113499759, 6.269802178014638, 6.222413296409805, 6.17610023983028, 6.130867111923216, 6.086716364240486, 6.043648791687345, 6.0016635295249, 5.960758051934876, 5.920928172153827, 5.882168044182504, 5.844470166074709, 5.807825384808474, 5.772222902741094, 5.737650285647962, 5.704093472343827, 5.671536785883562, 5.6399629463381755, 5.609353085140203, 5.579686760991327, 5.550941977323491, 5.523095201303375, 5.496121384368668, 5.469993984283102, 5.4446849886957525, 5.420164940188773, 5.396402962796191, 5.37336678997507, 5.3510227940089115, 5.329336016821781, 5.30827020218028, 5.287787829259118, 5.26785014754472, 5.2484172130499385, 5.229447925811702, 5.210900068642074, 5.192730347101995, 5.17489443066572, 5.157346995042734, 5.140041765622765, 5.122931562008336, 5.10596834359817, 5.089103256183654, 5.072286679519485, 5.055468275828595, 5.038597039200407, 5.021621345840529, 5.0044890051290425, 4.987147311443584, 4.969543096702615, 4.9516227835833995, 4.933332439368401, 4.914617830373074, 4.895424476907289, 4.875697708721963, 4.855382720891809, 4.834424630084545, 4.812768531166325, 4.790359554092671, 4.767142921033679, 4.743064003681909, 4.718068380690924, 4.692101895192162, 4.66511071233753, 4.637041376814851, 4.607840870283129, 4.577456668674436, 4.545836799309146, 4.5129298977711745, 4.478685264489892, 4.4430529209754335, 4.405983665654244, 4.36742912925174, 4.327341829669365, 4.285675226303332, 4.24238377375282, 4.197422974865617, 4.150749433069659, 4.102320903939326, 4.05209634594584, 4.000035970341659, 3.946101290129342, 3.8902551680659654, 3.832461863654893, 3.772687079077378, 3.71089800401729, 3.6470633593330235, 3.5811534395315645, 3.513140154000544, 3.4429970669550864, 3.370699436057243, 3.29622424966683, 3.219550262683585, 3.1406580309416343, 3.059529944118472, 2.9761502571217964, 2.890505119918813, 2.8025826057738885, 2.7123727378617035, 2.6198675142244667, 2.5250609310430416, 2.4279490041933367, 2.3285297890606613, 2.2268033985863003, 2.1227720195220074, 2.016439926869658, 1.9078134964848836, 1.7969012158250526, 1.6837136928235923, 1.568263662874276, 1.450565993910725, 1.3306376895680794, 1.2084978904154515, 1.084167873249493, 0.957671048441135, 0.8290329553292759, 0.6982812556569626, 0.5654457250473572, 0.43055824251854546, 0.29365277803803824, 0.15476537811958613, 0.013934149466708678, -0.12880075933083823, -0.27339717803967556, -0.4198109369547032, -0.5679958909887011, -0.7179039456662692, -0.8694850850069475, -1.0226874012805889, -1.177457126616284, -1.3337386664443802, -1.4914746347493921, -1.6506058911098629, -1.8110715794995116, -1.972809168822312, -2.1357544951524496, -2.2998418056484415, -2.4650038041090614, -2.631171698137088, -2.798275247875295, -2.9662428162775343, -3.135001420876215, -3.3044767870059584, -3.4745934024417426, -3.6452745734083765, -3.81644248191673, -3.9880182443807684, -4.159921971468075, -4.332072829135237, -4.504389100798229, -4.676788250586616, -4.849186987629303, -5.021501331318337, -5.193646677496183, -5.36553786551087, -5.5370892460822985, -5.708214749922152, -5.878827957048844, -6.048842166738094, -6.218170468048917, -6.386725810864032, -6.554421077382977, -6.721169154005559, -6.886883003542663, -7.051475737690883, -7.214860689706944, -7.376951487217433, -7.53766212509899, -7.696907038363763, -7.854601174984654, -8.010660068594701, -8.164999910994766, -8.317537624403581, -8.468190933384241, -8.616878436381157, -8.763519676801652, -8.908035213576497, -9.050346691133855, -9.190376908721426, -9.328049889011861, -9.463290945926932, -9.596026751616355, -9.726185402527692, -9.85369648450435, -9.978491136849224, -10.100502115292329, -10.219663853801432, -10.33591252517552, -10.449186100361784, -10.55942440643775, -10.666569183201101, -10.770564138310826, -10.871355000924336, -10.968889573776442, -11.06311778364712, -11.1539917301664, -11.241465732905873, -11.325496376707742, -11.40604255520373, -11.483065512477527, -11.556528882826072, -11.626398728576397, -11.692643575916412, -11.755234448699612, -11.81414490018536, -11.869351042678115, -11.920831575030743, -11.968567807978783, -12.012543687274487, -12.052745814591157, -12.089163466170362, -12.1217886091864, -12.15061591580448, -12.175642774910964, -12.196869301496102, -12.21429834367169, -12.227935487308233, -12.23778905827816, -12.243870122293862, -12.246192482331399, -12.244772673632877, -12.239629956282675, -12.230786305354844, -12.218266398631224, -12.202097601892001, -12.182309951782615, -12.15893613626318, -12.132011472648724, -12.101573883250854, -12.067663868633568, -12.030324478498246, -11.98960128021492, -11.945542325019304, -11.898198111897067, -11.847621549179163, -11.793867913874058, -11.736994808764988, -11.677062117302384, -11.614131956323805, -11.548268626635767, -11.479538561493962, -11.408010273020334, -11.333754296597625, -11.25684313328384, -11.177351190291192, -11.095354719575866, -11.01093175458699, -10.92416204522489, -10.835126991060706, -10.74390957287104, -10.65059428254319, -10.555267051408146, -10.45801517706018, -10.358927248723473, -10.258093071227792, -10.155603587656678, -10.05155080073312, -9.946027693009059, -9.839128145926402, -9.730946857818525, -9.621579260922486, -9.511121437473317, -9.399670034952871, -9.287322180566802, -9.174175395024173, -9.06032750569515, -8.945876559223114, -8.830920733668261, -8.715558250260548, -8.599887284840468, -8.48400587906672, -8.36801185147039, -8.252002708435683, -8.136075555187645, -8.020327006867607, -7.904853099777321, -7.789749202872942, -7.675109929590075, -7.561029050081119, -7.447599403946139, -7.3349128135382715, -7.223059997924567, -7.11213048758283, -7.002212539914661, -6.893393055654519, -6.785757496254121, -6.679389802320847, -6.574372313188264, -6.470785687696118, -6.368708826256327, -6.268218794280683, -6.1693907470450045, -6.07229785606347, -5.977011237045797, -5.88359987950876, -5.792130578112314, -5.702667865789349, -5.615273948736656, -5.530008643333332, -5.446929315051358, -5.366090819421446, -5.287545445115754, -5.21134285920728, -5.13753005466408, -5.066151300134576, -4.997248092078453, -4.930859109295625, -4.867020169903874, -4.805764190813655, -4.747121149746523, -4.691118049841552, -4.637778886891797, -4.587124619250819, -4.539173140446831, -4.493939254539883, -4.451434654254989, -4.411667901921836, -4.374644413249209, -4.340366443959809, -4.308833079308677, -4.2800402265058475, -4.2539806100613795, -4.2306437700682, -4.210016063435735, -4.192080668084552, -4.176817590109626, -4.164203673917198, -4.154212615337467, -4.146814977712665, -4.141978210957384, -4.139666673585298, -4.1398416576936405, -4.1424614168942036, -4.147481197176742, -4.154853270688072, -4.164526972407357, -4.1764487396953935, -4.190562154693, -4.206807989540907, -4.225124254390895, -4.245446248175236, -4.267706612098888, -4.2918353858162295, -4.31776006625158, -4.345405669020107, -4.374694792403274, -4.405547683830409, -4.4378823088154915, -4.471614422295899, -4.506657642317348, -4.542923526006987, -4.580321647774252, -4.618759679676819, -4.658143473886788, -4.69837714719004, -4.739363167449615, -4.781002441961843, -4.823194407632048, -4.865837122894565, -4.908827361300027, -4.952060706691028, -4.995431649885444, -5.038833686785085, -5.082159417825674, -5.125300648682573, -5.168148492145254, -5.210593471072026, -5.25252562233524, -5.2938346016658935, -5.334409789305427, -5.374140396371308, -5.412915571842073, -5.45062451006647, -5.487156558700541, -5.522401326975663, -5.5562487941999015, -5.588589418394416, -5.619314244966119, -5.648315015317388, -5.675484275293268, -5.700715483366334, -5.723903118459248, -5.744942787304947, -5.763731331244422, -5.780166932362148, -5.794149218859424, -5.805579369566189, -5.814360217492217, -5.820396352319115, -5.823594221735092, -5.823862231515073, -5.821110844249599, -5.815252676626628, -5.806202595171439, -5.793877810350731, -5.778197968948188, -5.759085244619953, -5.736464426539731, -5.710263006044626, -5.680411261194252, -5.64684233915722, -5.6094923363407165, -5.5683003761805985, -5.5232086845112285, -5.474162662436128, -5.42111095662249, -5.364005526944633, -5.302801711403535, -5.237458288251825, -5.16793753525583, -5.094205286028574, -5.016230983370076, -4.933987729553711, -4.847452333499936, -4.756605354781256, -4.661431144405003, -4.561917882323138, -4.458057611621113, -4.349846269340639, -4.237283713894041, -4.1203737490308585, -3.999124144320276, -3.8735466521160222, -3.743657020973415, -3.6094750054913023, -3.471024372554849, -3.3283329039582323, -3.18143239538952, -3.03035865176329]}, {\"hovertemplate\": \"x: %{x} <br>y: %{y}\", \"name\": \"Predicted\", \"type\": \"scatter\", \"x\": [-2.0, -1.98998998998999, -1.97997997997998, -1.96996996996997, -1.95995995995996, -1.94994994994995, -1.93993993993994, -1.92992992992993, -1.91991991991992, -1.90990990990991, -1.8998998998999, -1.88988988988989, -1.87987987987988, -1.86986986986987, -1.85985985985986, -1.84984984984985, -1.83983983983984, -1.82982982982983, -1.8198198198198199, -1.8098098098098099, -1.7997997997997999, -1.7897897897897899, -1.7797797797797799, -1.7697697697697699, -1.7597597597597598, -1.7497497497497498, -1.7397397397397398, -1.7297297297297298, -1.7197197197197198, -1.7097097097097098, -1.6996996996996998, -1.6896896896896898, -1.6796796796796798, -1.6696696696696698, -1.6596596596596598, -1.6496496496496498, -1.6396396396396398, -1.6296296296296298, -1.6196196196196198, -1.6096096096096097, -1.5995995995995997, -1.5895895895895895, -1.5795795795795795, -1.5695695695695695, -1.5595595595595595, -1.5495495495495495, -1.5395395395395395, -1.5295295295295295, -1.5195195195195195, -1.5095095095095095, -1.4994994994994995, -1.4894894894894894, -1.4794794794794794, -1.4694694694694694, -1.4594594594594594, -1.4494494494494494, -1.4394394394394394, -1.4294294294294294, -1.4194194194194194, -1.4094094094094094, -1.3993993993993994, -1.3893893893893894, -1.3793793793793794, -1.3693693693693694, -1.3593593593593594, -1.3493493493493494, -1.3393393393393394, -1.3293293293293293, -1.3193193193193193, -1.3093093093093093, -1.2992992992992993, -1.2892892892892893, -1.2792792792792793, -1.2692692692692693, -1.2592592592592593, -1.2492492492492493, -1.2392392392392393, -1.2292292292292293, -1.2192192192192193, -1.2092092092092093, -1.1991991991991993, -1.189189189189189, -1.179179179179179, -1.169169169169169, -1.159159159159159, -1.149149149149149, -1.139139139139139, -1.129129129129129, -1.119119119119119, -1.109109109109109, -1.099099099099099, -1.089089089089089, -1.079079079079079, -1.069069069069069, -1.059059059059059, -1.049049049049049, -1.039039039039039, -1.029029029029029, -1.019019019019019, -1.009009009009009, -0.9989989989989989, -0.9889889889889889, -0.9789789789789789, -0.9689689689689689, -0.9589589589589589, -0.9489489489489489, -0.9389389389389389, -0.9289289289289289, -0.9189189189189189, -0.9089089089089089, -0.8988988988988988, -0.8888888888888888, -0.8788788788788788, -0.8688688688688688, -0.8588588588588588, -0.8488488488488488, -0.8388388388388388, -0.8288288288288288, -0.8188188188188188, -0.8088088088088088, -0.7987987987987988, -0.7887887887887888, -0.7787787787787788, -0.7687687687687688, -0.7587587587587588, -0.7487487487487487, -0.7387387387387387, -0.7287287287287287, -0.7187187187187187, -0.7087087087087087, -0.6986986986986987, -0.6886886886886887, -0.6786786786786787, -0.6686686686686687, -0.6586586586586587, -0.6486486486486487, -0.6386386386386387, -0.6286286286286287, -0.6186186186186187, -0.6086086086086087, -0.5985985985985987, -0.5885885885885886, -0.5785785785785786, -0.5685685685685686, -0.5585585585585586, -0.5485485485485486, -0.5385385385385386, -0.5285285285285286, -0.5185185185185186, -0.5085085085085086, -0.4984984984984986, -0.4884884884884886, -0.4784784784784786, -0.46846846846846857, -0.45845845845845856, -0.44844844844844856, -0.43843843843843855, -0.42842842842842854, -0.41841841841841854, -0.40840840840840853, -0.3983983983983985, -0.3883883883883883, -0.3783783783783783, -0.3683683683683683, -0.3583583583583583, -0.34834834834834827, -0.33833833833833826, -0.32832832832832826, -0.31831831831831825, -0.30830830830830824, -0.29829829829829824, -0.28828828828828823, -0.2782782782782782, -0.2682682682682682, -0.2582582582582582, -0.2482482482482482, -0.2382382382382382, -0.2282282282282282, -0.21821821821821819, -0.20820820820820818, -0.19819819819819817, -0.18818818818818817, -0.17817817817817816, -0.16816816816816815, -0.15815815815815815, -0.14814814814814814, -0.13813813813813813, -0.12812812812812813, -0.11811811811811812, -0.10810810810810811, -0.09809809809809811, -0.0880880880880881, -0.0780780780780781, -0.06806806806806809, -0.05805805805805808, -0.048048048048048075, -0.03803803803803807, -0.028028028028028062, -0.018018018018018056, -0.00800800800800805, 0.002002002002002179, 0.012012012012012185, 0.022022022022022192, 0.0320320320320322, 0.042042042042042205, 0.05205205205205221, 0.06206206206206222, 0.07207207207207222, 0.08208208208208223, 0.09209209209209224, 0.10210210210210224, 0.11211211211211225, 0.12212212212212226, 0.13213213213213226, 0.14214214214214227, 0.15215215215215228, 0.16216216216216228, 0.1721721721721723, 0.1821821821821823, 0.1921921921921923, 0.2022022022022023, 0.21221221221221231, 0.22222222222222232, 0.23223223223223233, 0.24224224224224233, 0.25225225225225234, 0.26226226226226235, 0.27227227227227235, 0.28228228228228236, 0.29229229229229237, 0.3023023023023024, 0.3123123123123124, 0.3223223223223224, 0.3323323323323324, 0.3423423423423424, 0.3523523523523524, 0.3623623623623624, 0.3723723723723724, 0.3823823823823824, 0.39239239239239243, 0.40240240240240244, 0.41241241241241244, 0.42242242242242245, 0.43243243243243246, 0.44244244244244246, 0.45245245245245247, 0.4624624624624625, 0.4724724724724725, 0.4824824824824825, 0.4924924924924925, 0.5025025025025025, 0.5125125125125125, 0.5225225225225225, 0.5325325325325325, 0.5425425425425425, 0.5525525525525525, 0.5625625625625625, 0.5725725725725725, 0.5825825825825826, 0.5925925925925926, 0.6026026026026026, 0.6126126126126126, 0.6226226226226226, 0.6326326326326326, 0.6426426426426426, 0.6526526526526526, 0.6626626626626626, 0.6726726726726726, 0.6826826826826826, 0.6926926926926926, 0.7027027027027026, 0.7127127127127126, 0.7227227227227226, 0.7327327327327327, 0.7427427427427427, 0.7527527527527527, 0.7627627627627627, 0.7727727727727727, 0.7827827827827827, 0.7927927927927927, 0.8028028028028027, 0.8128128128128127, 0.8228228228228227, 0.8328328328328327, 0.8428428428428427, 0.8528528528528527, 0.8628628628628627, 0.8728728728728727, 0.8828828828828827, 0.8928928928928928, 0.9029029029029028, 0.9129129129129128, 0.9229229229229228, 0.9329329329329328, 0.9429429429429428, 0.9529529529529528, 0.9629629629629628, 0.9729729729729728, 0.9829829829829828, 0.9929929929929928, 1.0030030030030028, 1.0130130130130128, 1.0230230230230228, 1.0330330330330328, 1.0430430430430429, 1.0530530530530529, 1.0630630630630629, 1.0730730730730729, 1.0830830830830829, 1.0930930930930929, 1.1031031031031029, 1.113113113113113, 1.123123123123123, 1.133133133133133, 1.143143143143143, 1.153153153153153, 1.163163163163163, 1.173173173173173, 1.183183183183183, 1.193193193193193, 1.203203203203203, 1.2132132132132134, 1.2232232232232234, 1.2332332332332334, 1.2432432432432434, 1.2532532532532534, 1.2632632632632634, 1.2732732732732734, 1.2832832832832834, 1.2932932932932935, 1.3033033033033035, 1.3133133133133135, 1.3233233233233235, 1.3333333333333335, 1.3433433433433435, 1.3533533533533535, 1.3633633633633635, 1.3733733733733735, 1.3833833833833835, 1.3933933933933935, 1.4034034034034035, 1.4134134134134135, 1.4234234234234235, 1.4334334334334335, 1.4434434434434436, 1.4534534534534536, 1.4634634634634636, 1.4734734734734736, 1.4834834834834836, 1.4934934934934936, 1.5035035035035036, 1.5135135135135136, 1.5235235235235236, 1.5335335335335336, 1.5435435435435436, 1.5535535535535536, 1.5635635635635636, 1.5735735735735736, 1.5835835835835836, 1.5935935935935936, 1.6036036036036037, 1.6136136136136137, 1.6236236236236237, 1.6336336336336337, 1.6436436436436437, 1.6536536536536537, 1.6636636636636637, 1.6736736736736737, 1.6836836836836837, 1.6936936936936937, 1.7037037037037037, 1.7137137137137137, 1.7237237237237237, 1.7337337337337337, 1.7437437437437437, 1.7537537537537538, 1.7637637637637638, 1.7737737737737738, 1.7837837837837838, 1.7937937937937938, 1.8038038038038038, 1.8138138138138138, 1.8238238238238238, 1.8338338338338338, 1.8438438438438438, 1.8538538538538538, 1.8638638638638638, 1.8738738738738738, 1.8838838838838838, 1.8938938938938938, 1.9039039039039038, 1.9139139139139139, 1.9239239239239239, 1.9339339339339339, 1.9439439439439439, 1.9539539539539539, 1.9639639639639639, 1.973973973973974, 1.983983983983984, 1.993993993993994, 2.0040040040040044, 2.0140140140140144, 2.0240240240240244, 2.0340340340340344, 2.0440440440440444, 2.0540540540540544, 2.0640640640640644, 2.0740740740740744, 2.0840840840840844, 2.0940940940940944, 2.1041041041041044, 2.1141141141141144, 2.1241241241241244, 2.1341341341341344, 2.1441441441441444, 2.1541541541541545, 2.1641641641641645, 2.1741741741741745, 2.1841841841841845, 2.1941941941941945, 2.2042042042042045, 2.2142142142142145, 2.2242242242242245, 2.2342342342342345, 2.2442442442442445, 2.2542542542542545, 2.2642642642642645, 2.2742742742742745, 2.2842842842842845, 2.2942942942942945, 2.3043043043043046, 2.3143143143143146, 2.3243243243243246, 2.3343343343343346, 2.3443443443443446, 2.3543543543543546, 2.3643643643643646, 2.3743743743743746, 2.3843843843843846, 2.3943943943943946, 2.4044044044044046, 2.4144144144144146, 2.4244244244244246, 2.4344344344344346, 2.4444444444444446, 2.4544544544544546, 2.4644644644644647, 2.4744744744744747, 2.4844844844844847, 2.4944944944944947, 2.5045045045045047, 2.5145145145145147, 2.5245245245245247, 2.5345345345345347, 2.5445445445445447, 2.5545545545545547, 2.5645645645645647, 2.5745745745745747, 2.5845845845845847, 2.5945945945945947, 2.6046046046046047, 2.6146146146146148, 2.6246246246246248, 2.6346346346346348, 2.6446446446446448, 2.6546546546546548, 2.664664664664665, 2.674674674674675, 2.684684684684685, 2.694694694694695, 2.704704704704705, 2.714714714714715, 2.724724724724725, 2.734734734734735, 2.744744744744745, 2.754754754754755, 2.764764764764765, 2.774774774774775, 2.784784784784785, 2.794794794794795, 2.804804804804805, 2.814814814814815, 2.824824824824825, 2.834834834834835, 2.844844844844845, 2.854854854854855, 2.864864864864865, 2.874874874874875, 2.884884884884885, 2.894894894894895, 2.904904904904905, 2.914914914914915, 2.924924924924925, 2.934934934934935, 2.944944944944945, 2.954954954954955, 2.964964964964965, 2.974974974974975, 2.984984984984985, 2.994994994994995, 3.005005005005005, 3.015015015015015, 3.025025025025025, 3.035035035035035, 3.045045045045045, 3.055055055055055, 3.065065065065065, 3.075075075075075, 3.085085085085085, 3.095095095095095, 3.105105105105105, 3.115115115115115, 3.125125125125125, 3.135135135135135, 3.145145145145145, 3.155155155155155, 3.165165165165165, 3.175175175175175, 3.185185185185185, 3.195195195195195, 3.205205205205205, 3.215215215215215, 3.225225225225225, 3.235235235235235, 3.245245245245245, 3.255255255255255, 3.265265265265265, 3.275275275275275, 3.285285285285285, 3.295295295295295, 3.305305305305305, 3.315315315315315, 3.325325325325325, 3.335335335335335, 3.3453453453453452, 3.3553553553553552, 3.3653653653653652, 3.3753753753753752, 3.3853853853853852, 3.3953953953953953, 3.4054054054054053, 3.4154154154154153, 3.4254254254254253, 3.4354354354354353, 3.4454454454454453, 3.4554554554554553, 3.4654654654654653, 3.4754754754754753, 3.4854854854854853, 3.4954954954954953, 3.5055055055055053, 3.5155155155155153, 3.5255255255255253, 3.5355355355355353, 3.5455455455455454, 3.5555555555555554, 3.5655655655655654, 3.5755755755755754, 3.5855855855855854, 3.5955955955955954, 3.6056056056056054, 3.6156156156156154, 3.6256256256256254, 3.6356356356356354, 3.6456456456456454, 3.6556556556556554, 3.6656656656656654, 3.6756756756756754, 3.6856856856856854, 3.6956956956956954, 3.7057057057057055, 3.7157157157157155, 3.7257257257257255, 3.7357357357357355, 3.7457457457457455, 3.7557557557557555, 3.7657657657657655, 3.7757757757757755, 3.7857857857857855, 3.7957957957957955, 3.8058058058058055, 3.8158158158158155, 3.8258258258258255, 3.8358358358358355, 3.8458458458458455, 3.8558558558558556, 3.8658658658658656, 3.8758758758758756, 3.8858858858858856, 3.8958958958958956, 3.9059059059059056, 3.9159159159159156, 3.9259259259259256, 3.9359359359359356, 3.9459459459459456, 3.9559559559559556, 3.9659659659659656, 3.9759759759759756, 3.9859859859859856, 3.9959959959959956, 4.006006006006006, 4.016016016016016, 4.026026026026026, 4.036036036036036, 4.046046046046046, 4.056056056056056, 4.066066066066066, 4.076076076076076, 4.086086086086086, 4.096096096096096, 4.106106106106106, 4.116116116116116, 4.126126126126126, 4.136136136136136, 4.146146146146146, 4.156156156156156, 4.166166166166166, 4.176176176176176, 4.186186186186186, 4.196196196196196, 4.206206206206206, 4.216216216216216, 4.226226226226226, 4.236236236236236, 4.246246246246246, 4.256256256256256, 4.266266266266266, 4.276276276276276, 4.286286286286286, 4.296296296296296, 4.306306306306306, 4.316316316316316, 4.326326326326326, 4.336336336336336, 4.346346346346346, 4.356356356356356, 4.366366366366366, 4.376376376376376, 4.386386386386386, 4.396396396396396, 4.406406406406406, 4.416416416416417, 4.426426426426427, 4.436436436436437, 4.446446446446447, 4.456456456456457, 4.466466466466467, 4.476476476476477, 4.486486486486487, 4.496496496496497, 4.506506506506507, 4.516516516516517, 4.526526526526527, 4.536536536536537, 4.546546546546547, 4.556556556556557, 4.566566566566567, 4.576576576576577, 4.586586586586587, 4.596596596596597, 4.606606606606607, 4.616616616616617, 4.626626626626627, 4.636636636636637, 4.646646646646647, 4.656656656656657, 4.666666666666667, 4.676676676676677, 4.686686686686687, 4.696696696696697, 4.706706706706707, 4.716716716716717, 4.726726726726727, 4.736736736736737, 4.746746746746747, 4.756756756756757, 4.766766766766767, 4.776776776776777, 4.786786786786787, 4.796796796796797, 4.806806806806807, 4.816816816816817, 4.826826826826827, 4.836836836836837, 4.846846846846847, 4.856856856856857, 4.866866866866867, 4.876876876876877, 4.886886886886887, 4.896896896896897, 4.906906906906907, 4.916916916916917, 4.926926926926927, 4.936936936936937, 4.946946946946947, 4.956956956956957, 4.966966966966967, 4.976976976976977, 4.986986986986987, 4.996996996996997, 5.007007007007007, 5.017017017017017, 5.027027027027027, 5.037037037037037, 5.047047047047047, 5.057057057057057, 5.067067067067067, 5.077077077077077, 5.087087087087087, 5.097097097097097, 5.107107107107107, 5.117117117117117, 5.127127127127127, 5.137137137137137, 5.147147147147147, 5.157157157157157, 5.167167167167167, 5.177177177177177, 5.187187187187187, 5.197197197197197, 5.207207207207207, 5.217217217217217, 5.227227227227227, 5.237237237237237, 5.247247247247247, 5.257257257257257, 5.267267267267267, 5.277277277277277, 5.287287287287287, 5.297297297297297, 5.307307307307307, 5.317317317317317, 5.327327327327327, 5.337337337337337, 5.347347347347347, 5.357357357357357, 5.367367367367367, 5.377377377377377, 5.387387387387387, 5.397397397397397, 5.407407407407407, 5.4174174174174174, 5.4274274274274275, 5.4374374374374375, 5.4474474474474475, 5.4574574574574575, 5.4674674674674675, 5.4774774774774775, 5.4874874874874875, 5.4974974974974975, 5.5075075075075075, 5.5175175175175175, 5.5275275275275275, 5.5375375375375375, 5.5475475475475475, 5.5575575575575575, 5.5675675675675675, 5.5775775775775776, 5.587587587587588, 5.597597597597598, 5.607607607607608, 5.617617617617618, 5.627627627627628, 5.637637637637638, 5.647647647647648, 5.657657657657658, 5.667667667667668, 5.677677677677678, 5.687687687687688, 5.697697697697698, 5.707707707707708, 5.717717717717718, 5.727727727727728, 5.737737737737738, 5.747747747747748, 5.757757757757758, 5.767767767767768, 5.777777777777778, 5.787787787787788, 5.797797797797798, 5.807807807807808, 5.817817817817818, 5.827827827827828, 5.837837837837838, 5.847847847847848, 5.857857857857858, 5.867867867867868, 5.877877877877878, 5.887887887887888, 5.897897897897898, 5.907907907907908, 5.917917917917918, 5.927927927927928, 5.937937937937938, 5.947947947947948, 5.957957957957958, 5.967967967967968, 5.977977977977978, 5.987987987987988, 5.997997997997998, 6.008008008008009, 6.018018018018019, 6.028028028028029, 6.038038038038039, 6.048048048048049, 6.058058058058059, 6.068068068068069, 6.078078078078079, 6.088088088088089, 6.098098098098099, 6.108108108108109, 6.118118118118119, 6.128128128128129, 6.138138138138139, 6.148148148148149, 6.158158158158159, 6.168168168168169, 6.178178178178179, 6.188188188188189, 6.198198198198199, 6.208208208208209, 6.218218218218219, 6.228228228228229, 6.238238238238239, 6.248248248248249, 6.258258258258259, 6.268268268268269, 6.278278278278279, 6.288288288288289, 6.298298298298299, 6.308308308308309, 6.318318318318319, 6.328328328328329, 6.338338338338339, 6.348348348348349, 6.358358358358359, 6.368368368368369, 6.378378378378379, 6.388388388388389, 6.398398398398399, 6.408408408408409, 6.418418418418419, 6.428428428428429, 6.438438438438439, 6.448448448448449, 6.458458458458459, 6.468468468468469, 6.478478478478479, 6.488488488488489, 6.498498498498499, 6.508508508508509, 6.518518518518519, 6.528528528528529, 6.538538538538539, 6.548548548548549, 6.558558558558559, 6.568568568568569, 6.578578578578579, 6.588588588588589, 6.598598598598599, 6.608608608608609, 6.618618618618619, 6.628628628628629, 6.638638638638639, 6.648648648648649, 6.658658658658659, 6.668668668668669, 6.678678678678679, 6.688688688688689, 6.698698698698699, 6.708708708708709, 6.718718718718719, 6.728728728728729, 6.738738738738739, 6.748748748748749, 6.758758758758759, 6.768768768768769, 6.778778778778779, 6.788788788788789, 6.798798798798799, 6.808808808808809, 6.818818818818819, 6.828828828828829, 6.838838838838839, 6.848848848848849, 6.858858858858859, 6.868868868868869, 6.878878878878879, 6.888888888888889, 6.898898898898899, 6.908908908908909, 6.918918918918919, 6.928928928928929, 6.938938938938939, 6.948948948948949, 6.958958958958959, 6.968968968968969, 6.978978978978979, 6.988988988988989, 6.998998998998999, 7.009009009009009, 7.019019019019019, 7.029029029029029, 7.039039039039039, 7.049049049049049, 7.059059059059059, 7.069069069069069, 7.079079079079079, 7.089089089089089, 7.099099099099099, 7.109109109109109, 7.119119119119119, 7.129129129129129, 7.1391391391391394, 7.1491491491491495, 7.1591591591591595, 7.1691691691691695, 7.1791791791791795, 7.1891891891891895, 7.1991991991991995, 7.2092092092092095, 7.2192192192192195, 7.2292292292292295, 7.2392392392392395, 7.2492492492492495, 7.2592592592592595, 7.2692692692692695, 7.2792792792792795, 7.2892892892892895, 7.2992992992992995, 7.3093093093093096, 7.31931931931932, 7.32932932932933, 7.33933933933934, 7.34934934934935, 7.35935935935936, 7.36936936936937, 7.37937937937938, 7.38938938938939, 7.3993993993994, 7.40940940940941, 7.41941941941942, 7.42942942942943, 7.43943943943944, 7.44944944944945, 7.45945945945946, 7.46946946946947, 7.47947947947948, 7.48948948948949, 7.4994994994995, 7.50950950950951, 7.51951951951952, 7.52952952952953, 7.53953953953954, 7.54954954954955, 7.55955955955956, 7.56956956956957, 7.57957957957958, 7.58958958958959, 7.5995995995996, 7.60960960960961, 7.61961961961962, 7.62962962962963, 7.63963963963964, 7.64964964964965, 7.65965965965966, 7.66966966966967, 7.67967967967968, 7.68968968968969, 7.6996996996997, 7.70970970970971, 7.71971971971972, 7.72972972972973, 7.73973973973974, 7.74974974974975, 7.75975975975976, 7.76976976976977, 7.77977977977978, 7.78978978978979, 7.7997997997998, 7.80980980980981, 7.81981981981982, 7.82982982982983, 7.83983983983984, 7.84984984984985, 7.85985985985986, 7.86986986986987, 7.87987987987988, 7.88988988988989, 7.8998998998999, 7.90990990990991, 7.91991991991992, 7.92992992992993, 7.93993993993994, 7.94994994994995, 7.95995995995996, 7.96996996996997, 7.97997997997998, 7.98998998998999, 8.0], \"y\": [4.305573463439941, 4.247528553009033, 4.187112808227539, 4.1243696212768555, 4.059372901916504, 3.992199420928955, 3.9229507446289062, 3.8517332077026367, 3.7786808013916016, 3.70391845703125, 3.6275925636291504, 3.5498428344726562, 3.470822811126709, 3.3906679153442383, 3.309525966644287, 3.2275142669677734, 3.1447601318359375, 3.0613608360290527, 2.977415084838867, 2.8929853439331055, 2.808138370513916, 2.7229042053222656, 2.6373143196105957, 2.5513668060302734, 2.465066432952881, 2.378383159637451, 2.2913031578063965, 2.203779458999634, 2.1157829761505127, 2.0272583961486816, 1.9381728172302246, 1.848482370376587, 1.7581536769866943, 1.667142391204834, 1.575432538986206, 1.4829978942871094, 1.38983154296875, 1.2959182262420654, 1.2012665271759033, 1.105881690979004, 1.0097782611846924, 0.9129626750946045, 0.8154616355895996, 0.7172982692718506, 0.618483304977417, 0.5190410614013672, 0.41898679733276367, 0.3183436393737793, 0.21711421012878418, 0.1153099536895752, 0.012932777404785156, -0.09000754356384277, -0.1935257911682129, -0.29761266708374023, -0.4022855758666992, -0.5075392723083496, -0.6133923530578613, -0.7198333740234375, -0.8268823623657227, -0.9345223903656006, -1.0427604913711548, -1.1515768766403198, -1.260971188545227, -1.3709100484848022, -1.4813843965530396, -1.5923515558242798, -1.7037941217422485, -1.8156605958938599, -1.9279237985610962, -2.0405306816101074, -2.153451442718506, -2.2666234970092773, -2.3800196647644043, -2.4935832023620605, -2.6072745323181152, -2.721048355102539, -2.8348770141601562, -2.9487128257751465, -3.0625267028808594, -3.1762852668762207, -3.2899727821350098, -3.403557777404785, -3.517021656036377, -3.6303482055664062, -3.7435359954833984, -3.856562614440918, -3.9694275856018066, -4.0821213722229, -4.1946539878845215, -4.307008743286133, -4.419192790985107, -4.5312018394470215, -4.6430344581604, -4.754687309265137, -4.866150379180908, -4.977415561676025, -5.088464260101318, -5.199289798736572, -5.309850215911865, -5.4201340675354, -5.530086517333984, -5.639682292938232, -5.748857498168945, -5.857573986053467, -5.965756893157959, -6.073358535766602, -6.180299758911133, -6.286515235900879, -6.391934394836426, -6.49648380279541, -6.600094795227051, -6.702701091766357, -6.804237365722656, -6.904644012451172, -7.00386905670166, -7.101863384246826, -7.198590278625488, -7.294013500213623, -7.388113498687744, -7.4808669090271, -7.572269916534424, -7.662312984466553, -7.751003742218018, -7.838346004486084, -7.924353122711182, -8.009039878845215, -8.092419624328613, -8.174511909484863, -8.255330085754395, -8.334893226623535, -8.413209915161133, -8.490291595458984, -8.566141128540039, -8.640759468078613, -8.714143753051758, -8.786282539367676, -8.857160568237305, -8.926755905151367, -8.99504566192627, -9.061994552612305, -9.127573013305664, -9.191740036010742, -9.2544527053833, -9.315669059753418, -9.375346183776855, -9.43343448638916, -9.489892959594727, -9.544673919677734, -9.597738265991211, -9.649043083190918, -9.698555946350098, -9.74624252319336, -9.792073249816895, -9.836024284362793, -9.87807559967041, -9.91821575164795, -9.95643424987793, -9.992724418640137, -10.02708625793457, -10.059526443481445, -10.090052604675293, -10.118677139282227, -10.145413398742676, -10.17028522491455, -10.193312644958496, -10.214517593383789, -10.233929634094238, -10.251574516296387, -10.267481803894043, -10.28167724609375, -10.294198036193848, -10.305066108703613, -10.314314842224121, -10.32197093963623, -10.328064918518066, -10.332619667053223, -10.335663795471191, -10.337221145629883, -10.337311744689941, -10.335960388183594, -10.333182334899902, -10.328999519348145, -10.323427200317383, -10.316479682922363, -10.308167457580566, -10.298505783081055, -10.287503242492676, -10.27517032623291, -10.261513710021973, -10.246541023254395, -10.230257987976074, -10.21267318725586, -10.193791389465332, -10.173617362976074, -10.1521577835083, -10.129420280456543, -10.1054105758667, -10.080140113830566, -10.053614616394043, -10.025848388671875, -9.996850967407227, -9.96664047241211, -9.935232162475586, -9.902645111083984, -9.86889934539795, -9.83402156829834, -9.7980375289917, -9.760974884033203, -9.722867012023926, -9.683746337890625, -9.643651008605957, -9.602618217468262, -9.560688972473145, -9.517906188964844, -9.474312782287598, -9.429953575134277, -9.384873390197754, -9.339120864868164, -9.29273796081543, -9.245773315429688, -9.198267936706543, -9.150270462036133, -9.101818084716797, -9.052953720092773, -9.00371265411377, -8.954133987426758, -8.904250144958496, -8.854087829589844, -8.803679466247559, -8.753046989440918, -8.7022123336792, -8.651195526123047, -8.600013732910156, -8.548681259155273, -8.497209548950195, -8.445611000061035, -8.393892288208008, -8.342062950134277, -8.290128707885742, -8.2380952835083, -8.1859712600708, -8.133757591247559, -8.081463813781738, -8.029095649719238, -7.976660251617432, -7.924167156219482, -7.8716254234313965, -7.819045543670654, -7.766439914703369, -7.713821887969971, -7.661208629608154, -7.60861349105835, -7.556053638458252, -7.503549098968506, -7.451117038726807, -7.39877462387085, -7.3465495109558105, -7.2944512367248535, -7.242506980895996, -7.190729141235352, -7.139141082763672, -7.0877532958984375, -7.036588668823242, -6.98565149307251, -6.934961318969727, -6.884520053863525, -6.834339618682861, -6.784424304962158, -6.734773635864258, -6.685390949249268, -6.636272430419922, -6.587411403656006, -6.538800239562988, -6.49043083190918, -6.442290306091309, -6.394366264343262, -6.346640586853027, -6.299097061157227, -6.251718044281006, -6.2044830322265625, -6.157371997833252, -6.110365390777588, -6.06343936920166, -6.016574859619141, -5.969748497009277, -5.922941207885742, -5.876131534576416, -5.829298496246338, -5.782424449920654, -5.735490798950195, -5.688478469848633, -5.6413726806640625, -5.594155788421631, -5.546815395355225, -5.499334812164307, -5.45170259475708, -5.403904914855957, -5.355930328369141, -5.307765960693359, -5.259399890899658, -5.210820198059082, -5.162012100219727, -5.112966060638428, -5.063665390014648, -5.014094829559326, -4.964238166809082, -4.914079189300537, -4.863596439361572, -4.812768459320068, -4.761573791503906, -4.709985256195068, -4.657978534698486, -4.605523586273193, -4.5525898933410645, -4.4991455078125, -4.445156097412109, -4.390588283538818, -4.335404396057129, -4.27957010269165, -4.223046779632568, -4.165797710418701, -4.107787132263184, -4.04897928237915, -3.9893407821655273, -3.9288382530212402, -3.867441177368164, -3.8051204681396484, -3.7418503761291504, -3.6776084899902344, -3.6123743057250977, -3.546128273010254, -3.47885799407959, -3.410551071166992, -3.3411970138549805, -3.270789623260498, -3.1993236541748047, -3.1267943382263184, -3.0532007217407227, -2.9785385131835938, -2.902806282043457, -2.8260021209716797, -2.748122215270996, -2.6691622734069824, -2.5891165733337402, -2.5079760551452637, -2.4257330894470215, -2.3423714637756348, -2.257880687713623, -2.172244071960449, -2.0854454040527344, -1.9974631071090698, -1.9082838296890259, -1.8178857564926147, -1.7262519598007202, -1.6333626508712769, -1.5392049551010132, -1.4437636137008667, -1.3470290899276733, -1.2489932775497437, -1.1496541500091553, -1.0490106344223022, -0.9470661878585815, -0.8438328504562378, -0.7393200397491455, -0.6335462331771851, -0.5265308618545532, -0.4182993173599243, -0.30887603759765625, -0.1982908844947815, -0.08657461404800415, 0.02624279260635376, 0.14012885093688965, 0.25505131483078003, 0.3709815442562103, 0.48788946866989136, 0.6057477593421936, 0.7245317697525024, 0.8442166447639465, 0.9647826552391052, 1.0862107276916504, 1.2084836959838867, 1.33158540725708, 1.4555020332336426, 1.5802178382873535, 1.7057204246520996, 1.8319907188415527, 1.9590152502059937, 2.086771011352539, 2.215235710144043, 2.34438419342041, 2.4741878509521484, 2.6046066284179688, 2.735602378845215, 2.867131233215332, 2.9991455078125, 3.131591320037842, 3.2644104957580566, 3.397545337677002, 3.530932903289795, 3.6645092964172363, 3.798208713531494, 3.9319682121276855, 4.065722942352295, 4.199411392211914, 4.3329758644104, 4.4663591384887695, 4.599510669708252, 4.73238468170166, 4.864936828613281, 4.997130870819092, 5.128934860229492, 5.260318756103516, 5.391258239746094, 5.521731853485107, 5.65172004699707, 5.781200408935547, 5.910156726837158, 6.038565635681152, 6.166401386260986, 6.293630599975586, 6.420222282409668, 6.546121597290039, 6.671277046203613, 6.795623779296875, 6.919084548950195, 7.041579246520996, 7.163010597229004, 7.2832794189453125, 7.402278900146484, 7.519903659820557, 7.636045932769775, 7.750603675842285, 7.863482475280762, 7.974600315093994, 8.083888053894043, 8.191290855407715, 8.296777725219727, 8.400330543518066, 8.501948356628418, 8.601651191711426, 8.699469566345215, 8.795446395874023, 8.889630317687988, 8.982074737548828, 9.072829246520996, 9.16193962097168, 9.249439239501953, 9.335350036621094, 9.41967487335205, 9.502399444580078, 9.583491325378418, 9.662894248962402, 9.740538597106934, 9.816335678100586, 9.890182495117188, 9.961969375610352, 10.031578063964844, 10.098891258239746, 10.163792610168457, 10.226171493530273, 10.285929679870605, 10.342979431152344, 10.397252082824707, 10.44869327545166, 10.49726676940918, 10.542954444885254, 10.58575439453125, 10.625683784484863, 10.662769317626953, 10.697054862976074, 10.728591918945312, 10.757444381713867, 10.783677101135254, 10.807361602783203, 10.828573226928711, 10.847384452819824, 10.863869667053223, 10.878096580505371, 10.890130996704102, 10.900033950805664, 10.907859802246094, 10.913653373718262, 10.91745662689209, 10.919305801391602, 10.919221878051758, 10.91722297668457, 10.913321495056152, 10.907516479492188, 10.899808883666992, 10.890183448791504, 10.87862491607666, 10.865117073059082, 10.849630355834961, 10.832136154174805, 10.812606811523438, 10.791008949279785, 10.767313957214355, 10.741490364074707, 10.713517189025879, 10.683369636535645, 10.65103530883789, 10.616508483886719, 10.579791069030762, 10.540898323059082, 10.499853134155273, 10.45669174194336, 10.411468505859375, 10.364240646362305, 10.31509017944336, 10.264101028442383, 10.211377143859863, 10.157024383544922, 10.101163864135742, 10.043917655944824, 9.985413551330566, 9.92578125, 9.86514663696289, 9.803632736206055, 9.741353034973145, 9.678415298461914, 9.614912033081055, 9.55092716217041, 9.486527442932129, 9.42176628112793, 9.356679916381836, 9.291296005249023, 9.225627899169922, 9.159669876098633, 9.09341812133789, 9.02685260772705, 8.959952354431152, 8.89269733428955, 8.825061798095703, 8.757026672363281, 8.688579559326172, 8.619711875915527, 8.55042839050293, 8.480746269226074, 8.410690307617188, 8.340302467346191, 8.269633293151855, 8.198748588562012, 8.127723693847656, 8.056640625, 7.985594272613525, 7.914677143096924, 7.843994140625, 7.7736406326293945, 7.703714370727539, 7.634311199188232, 7.565515995025635, 7.497409343719482, 7.430058479309082, 7.363525390625, 7.297856330871582, 7.233090400695801, 7.169257164001465, 7.1063737869262695, 7.044450759887695, 6.983490943908691, 6.9234938621521, 6.8644514083862305, 6.806352615356445, 6.749190807342529, 6.692953109741211, 6.637631893157959, 6.583220481872559, 6.529714107513428, 6.477114677429199, 6.4254255294799805, 6.374655246734619, 6.3248162269592285, 6.275922775268555, 6.227993488311768, 6.181049823760986, 6.135113716125488, 6.090208053588867, 6.046353816986084, 6.003576278686523, 5.9618916511535645, 5.9213175773620605, 5.881869792938232, 5.843557357788086, 5.806386470794678, 5.770358085632324, 5.735468864440918, 5.701708793640137, 5.669065475463867, 5.637521743774414, 5.607051849365234, 5.57763147354126, 5.549228668212891, 5.52180814743042, 5.495332717895508, 5.469761371612549, 5.4450507164001465, 5.421156406402588, 5.398030757904053, 5.375626564025879, 5.353893756866455, 5.332784175872803, 5.3122477531433105, 5.292233943939209, 5.272695064544678, 5.253579616546631, 5.2348408699035645, 5.216428756713867, 5.19829797744751, 5.1803998947143555, 5.162689685821533, 5.145121097564697, 5.127651691436768, 5.110235214233398, 5.092831611633301, 5.0753960609436035, 5.057887554168701, 5.040264129638672, 5.022486686706543, 5.00451135635376, 4.986299514770508, 4.96781063079834, 4.949002265930176, 4.92983341217041, 4.910264015197754, 4.890249252319336, 4.869749069213867, 4.848717212677002, 4.82711124420166, 4.80488395690918, 4.781989574432373, 4.758378505706787, 4.734002590179443, 4.708808898925781, 4.682747840881348, 4.655760765075684, 4.6277971267700195, 4.598795413970947, 4.568699359893799, 4.537447452545166, 4.504978656768799, 4.471229076385498, 4.436136245727539, 4.399638652801514, 4.361666679382324, 4.322160720825195, 4.281052589416504, 4.238282680511475, 4.19378662109375, 4.14750862121582, 4.099388122558594, 4.049374580383301, 3.9974136352539062, 3.9434638023376465, 3.8874802589416504, 3.8294296264648438, 3.7692766189575195, 3.7069997787475586, 3.642575263977051, 3.575990676879883, 3.507232666015625, 3.4362993240356445, 3.3631834983825684, 3.2878923416137695, 3.2104225158691406, 3.130784511566162, 3.048975944519043, 2.9650073051452637, 2.878873348236084, 2.7905807495117188, 2.7001161575317383, 2.6074838638305664, 2.512660026550293, 2.415645122528076, 2.3164076805114746, 2.214944362640381, 2.1112208366394043, 2.005232810974121, 1.8969475030899048, 1.7863664627075195, 1.6734657287597656, 1.558250904083252, 1.440726399421692, 1.320894718170166, 1.1987847089767456, 1.0744075775146484, 0.9478136897087097, 0.8190264105796814, 0.6881083846092224, 0.5550925731658936, 0.42005085945129395, 0.28302001953125, 0.1440749168395996, 0.0032500028610229492, -0.13938134908676147, -0.28379011154174805, -0.42990922927856445, -0.5777127742767334, -0.7271497249603271, -0.8781934976577759, -1.030800461769104, -1.1849476099014282, -1.340593934059143, -1.4977151155471802, -1.6562682390213013, -1.8162211179733276, -1.977519154548645, -2.1401219367980957, -2.3039603233337402, -2.4689764976501465, -2.635082721710205, -2.8022055625915527, -2.9702415466308594, -3.139101982116699, -3.3086719512939453, -3.4788546562194824, -3.6495323181152344, -3.820613384246826, -3.9919819831848145, -4.1635589599609375, -4.335244655609131, -4.506972312927246, -4.678669452667236, -4.850271224975586, -5.021735668182373, -5.193008899688721, -5.36405611038208, -5.534827709197998, -5.70529317855835, -5.875389575958252, -6.0450758934021, -6.214273929595947, -6.382915019989014, -6.550889492034912, -6.7181010246276855, -6.884405612945557, -7.049672603607178, -7.213731288909912, -7.376438617706299, -7.537611484527588, -7.697113513946533, -7.854783535003662, -8.010525703430176, -8.16423225402832, -8.315874099731445, -8.465412139892578, -8.612882614135742, -8.758313179016113, -8.901779174804688, -9.043316841125488, -9.182987213134766, -9.32077693939209, -9.456661224365234, -9.590531349182129, -9.722238540649414, -9.851570129394531, -9.978282928466797, -10.102103233337402, -10.2227783203125, -10.340063095092773, -10.453782081604004, -10.563812255859375, -10.670095443725586, -10.772671699523926, -10.871613502502441, -10.967065811157227, -11.059181213378906, -11.14813232421875, -11.234062194824219, -11.317086219787598, -11.397266387939453, -11.474610328674316, -11.549062728881836, -11.620513916015625, -11.688810348510742, -11.753766059875488, -11.815176010131836, -11.87283992767334, -11.926568984985352, -11.976203918457031, -12.021621704101562, -12.062742233276367, -12.099523544311523, -12.131969451904297, -12.160118103027344, -12.184039115905762, -12.203826904296875, -12.219587326049805, -12.231438636779785, -12.239500045776367, -12.243886947631836, -12.244706153869629, -12.242056846618652, -12.236018180847168, -12.226658821105957, -12.214027404785156, -12.198163032531738, -12.179080963134766, -12.156790733337402, -12.131280899047852, -12.102537155151367, -12.070528030395508, -12.035224914550781, -11.99659538269043, -11.954599380493164, -11.909205436706543, -11.860387802124023, -11.808126449584961, -11.75241470336914, -11.693258285522461, -11.630680084228516, -11.564722061157227, -11.495441436767578, -11.422921180725098, -11.347259521484375, -11.26857852935791, -11.18701457977295, -11.1027250289917, -11.015878677368164, -10.926654815673828, -10.835240364074707, -10.741825103759766, -10.646600723266602, -10.549751281738281, -10.45144271850586, -10.351850509643555, -10.251108169555664, -10.149354934692383, -10.046686172485352, -9.943202018737793, -9.838955879211426, -9.734006881713867, -9.628366470336914, -9.522058486938477, -9.41506576538086, -9.30738353729248, -9.198978424072266, -9.089831352233887, -8.979898452758789, -8.869169235229492, -8.75760555267334, -8.645206451416016, -8.531976699829102, -8.41791820526123, -8.303075790405273, -8.187485694885254, -8.071229934692383, -7.954381465911865, -7.837067127227783, -7.719393253326416, -7.601526737213135, -7.483609676361084, -7.3658318519592285, -7.248375415802002, -7.131442070007324, -7.015233993530273, -6.899966239929199, -6.78585147857666, -6.673096656799316, -6.561911582946777, -6.452496528625488, -6.345043182373047, -6.239731311798096, -6.136723518371582, -6.036177158355713, -5.938228130340576, -5.842997074127197, -5.750584125518799, -5.661074161529541, -5.574538230895996, -5.491023540496826, -5.410566806793213, -5.333186149597168, -5.258882522583008, -5.18765115737915, -5.119463920593262, -5.054296493530273, -4.992091178894043, -4.932806968688965, -4.876377105712891, -4.8227410316467285, -4.771823406219482, -4.7235493659973145, -4.6778483390808105, -4.634632587432861, -4.593830108642578, -4.555353164672852, -4.519132137298584, -4.485083103179932, -4.45313835144043, -4.423220634460449, -4.395270347595215, -4.369218826293945, -4.3450140953063965, -4.322600364685059, -4.301934719085693, -4.282975196838379, -4.26569128036499, -4.250054359436035, -4.236049652099609, -4.223664283752441, -4.2129011154174805, -4.203764915466309, -4.196270942687988, -4.1904449462890625, -4.1863226890563965, -4.183948516845703, -4.183376312255859, -4.18466854095459, -4.1878981590271, -4.193145751953125, -4.200499057769775, -4.210055828094482, -4.221911430358887, -4.236172199249268, -4.252941608428955, -4.2723236083984375, -4.294411659240723, -4.319296836853027, -4.347051620483398, -4.377734661102295, -4.41138219833374, -4.447997093200684, -4.487559795379639, -4.530003547668457, -4.575231075286865, -4.623092174530029, -4.673404693603516, -4.725926876068115, -4.780385494232178, -4.836460590362549, -4.893804550170898, -4.952032089233398, -5.010750770568848, -5.069540977478027, -5.128002166748047, -5.1857171058654785, -5.242310047149658, -5.29741096496582, -5.35069465637207, -5.401859283447266, -5.45065450668335, -5.496861457824707, -5.5403151512146, -5.580872535705566, -5.618447780609131, -5.6529693603515625, -5.68441104888916, -5.712759017944336, -5.738030433654785, -5.760248184204102, -5.779454708099365, -5.7956929206848145, -5.809014320373535, -5.819465637207031, -5.827094554901123, -5.83194637298584, -5.834052085876465, -5.8334455490112305, -5.830142021179199, -5.824151039123535, -5.815476417541504, -5.804108619689941, -5.790027618408203, -5.7732086181640625, -5.753617763519287, -5.73121452331543, -5.705955982208252, -5.67779016494751, -5.646669864654541, -5.612544059753418, -5.575370788574219, -5.535101890563965, -5.491709232330322, -5.445167064666748, -5.3954668045043945, -5.342609405517578, -5.286625385284424, -5.2275519371032715, -5.165463447570801, -5.1004414558410645, -5.0326128005981445, -4.962113380432129, -4.889125347137451, -4.813834190368652, -4.73647403717041, -4.657284736633301, -4.576545715332031, -4.494532585144043, -4.411560535430908, -4.32793664932251, -4.243987560272217, -4.160028457641602, -4.07638692855835, -3.9933695793151855, -3.911283493041992, -3.830404758453369, -3.7510108947753906, -3.673333168029785, -3.5976009368896484, -3.5239973068237305]}],\n",
              "                        {\"hovermode\": \"x\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Regression Results\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('df3c1d73-b183-45ff-95d0-6501fb044c05');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioPZVusBP8rE"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}
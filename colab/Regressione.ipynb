{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regressione.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNXbkx7Tt9UqbqZNLc709cU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visiont3lab/deep-learning-course/blob/main/colab/Regressione.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nESzafMjgIou"
      },
      "source": [
        "## Importa Libreria"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N35_WMlfOgf"
      },
      "source": [
        "from torch import nn\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch import optim\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torchsummary import summary\r\n",
        "#!pip install torchsummary\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import TensorDataset,Dataset\r\n",
        "from torchvision import datasets\r\n",
        "from torchvision import transforms\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import numpy as np\r\n",
        "import plotly.graph_objects as go\r\n",
        "# Loss function pytorch: https://neptune.ai/blog/pytorch-loss-functions\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLFs2Z3Wfew4"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kajs9lR3fRK7",
        "outputId": "e23fc308-dae0-442b-f2e6-7732cd6ce614"
      },
      "source": [
        "# Dati Numpy\r\n",
        "R_X = np.linspace(-2,8,1000)\r\n",
        "R_Y = np.exp(0.2*R_X)*np.sin(3*R_X) - 10*np.cos(R_X)\r\n",
        "\r\n",
        "# Normalization\r\n",
        "R_mean = np.mean(R_X)\r\n",
        "R_std = np.std(R_X)\r\n",
        "\r\n",
        "# Dati Pytorch Tensor\r\n",
        "R_Xt = torch.from_numpy(R_X).type(torch.float32).reshape(-1,1)#.unsqueeze(1)\r\n",
        "R_Yt = torch.from_numpy(R_Y).type(torch.float32).unsqueeze(1)\r\n",
        "print(f\"X Tensor data shape: \", R_Xt.shape)\r\n",
        "print(f\"Y Tensor data shape: \", R_Yt.shape)\r\n",
        "\r\n",
        "# Training and Test Set\r\n",
        "R_X_train, R_X_test, R_Y_train, R_Y_test = train_test_split(R_X,R_Y,test_size=0.3,shuffle=True,random_state=4)\r\n",
        "print(f\"X Train shape: {R_X_train.shape} , X Test shape: {R_X_test.shape}\")\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X Tensor data shape:  torch.Size([1000, 1])\n",
            "Y Tensor data shape:  torch.Size([1000, 1])\n",
            "X Train shape: (700,) , X Test shape: (300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "TGgqiGibfWNw",
        "outputId": "ecbe70b2-fbeb-4438-8ce6-fdb3e676a4aa"
      },
      "source": [
        "# Visualization\r\n",
        "fig = go.Figure()\r\n",
        "fig.add_traces( go.Scatter(x=R_X, y=R_Y,hovertemplate='x: %{x} <br>y: %{y}',mode=\"markers\", name=\"Real data\") )\r\n",
        "fig.add_traces( go.Scatter(x=R_X_train, y=R_Y_train,hovertemplate='x: %{x} <br>y: %{y}',mode=\"markers\", name=\"Train data\") )\r\n",
        "fig.add_traces( go.Scatter(x=R_X_test, y=R_Y_test,hovertemplate='x: %{x} <br>y: %{y}',mode=\"markers\", name=\"Test data\") )\r\n",
        "\r\n",
        "fig.update_layout(title=\"Funzione di stimare\")\r\n",
        "fig.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"da03f025-6b9d-4a7e-b1bf-8d315ba1a210\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"da03f025-6b9d-4a7e-b1bf-8d315ba1a210\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'da03f025-6b9d-4a7e-b1bf-8d315ba1a210',\n",
              "                        [{\"hovertemplate\": \"x: %{x} <br>y: %{y}\", \"mode\": \"markers\", \"name\": \"Real data\", \"type\": \"scatter\", \"x\": [-2.0, -1.98998998998999, -1.97997997997998, -1.96996996996997, -1.95995995995996, -1.94994994994995, -1.93993993993994, -1.92992992992993, -1.91991991991992, -1.90990990990991, -1.8998998998999, -1.88988988988989, -1.87987987987988, -1.86986986986987, -1.85985985985986, -1.84984984984985, -1.83983983983984, -1.82982982982983, -1.8198198198198199, -1.8098098098098099, -1.7997997997997999, -1.7897897897897899, -1.7797797797797799, -1.7697697697697699, -1.7597597597597598, -1.7497497497497498, -1.7397397397397398, -1.7297297297297298, -1.7197197197197198, -1.7097097097097098, -1.6996996996996998, -1.6896896896896898, -1.6796796796796798, -1.6696696696696698, -1.6596596596596598, -1.6496496496496498, -1.6396396396396398, -1.6296296296296298, -1.6196196196196198, -1.6096096096096097, -1.5995995995995997, -1.5895895895895895, -1.5795795795795795, -1.5695695695695695, -1.5595595595595595, -1.5495495495495495, -1.5395395395395395, -1.5295295295295295, -1.5195195195195195, -1.5095095095095095, -1.4994994994994995, -1.4894894894894894, -1.4794794794794794, -1.4694694694694694, -1.4594594594594594, -1.4494494494494494, -1.4394394394394394, -1.4294294294294294, -1.4194194194194194, -1.4094094094094094, -1.3993993993993994, -1.3893893893893894, -1.3793793793793794, -1.3693693693693694, -1.3593593593593594, -1.3493493493493494, -1.3393393393393394, -1.3293293293293293, -1.3193193193193193, -1.3093093093093093, -1.2992992992992993, -1.2892892892892893, -1.2792792792792793, -1.2692692692692693, -1.2592592592592593, -1.2492492492492493, -1.2392392392392393, -1.2292292292292293, -1.2192192192192193, -1.2092092092092093, -1.1991991991991993, -1.189189189189189, -1.179179179179179, -1.169169169169169, -1.159159159159159, -1.149149149149149, -1.139139139139139, -1.129129129129129, -1.119119119119119, -1.109109109109109, -1.099099099099099, -1.089089089089089, -1.079079079079079, -1.069069069069069, -1.059059059059059, -1.049049049049049, -1.039039039039039, -1.029029029029029, -1.019019019019019, -1.009009009009009, -0.9989989989989989, -0.9889889889889889, -0.9789789789789789, -0.9689689689689689, -0.9589589589589589, -0.9489489489489489, -0.9389389389389389, -0.9289289289289289, -0.9189189189189189, -0.9089089089089089, -0.8988988988988988, -0.8888888888888888, -0.8788788788788788, -0.8688688688688688, -0.8588588588588588, -0.8488488488488488, -0.8388388388388388, -0.8288288288288288, -0.8188188188188188, -0.8088088088088088, -0.7987987987987988, -0.7887887887887888, -0.7787787787787788, -0.7687687687687688, -0.7587587587587588, -0.7487487487487487, -0.7387387387387387, -0.7287287287287287, -0.7187187187187187, -0.7087087087087087, -0.6986986986986987, -0.6886886886886887, -0.6786786786786787, -0.6686686686686687, -0.6586586586586587, -0.6486486486486487, -0.6386386386386387, -0.6286286286286287, -0.6186186186186187, -0.6086086086086087, -0.5985985985985987, -0.5885885885885886, -0.5785785785785786, -0.5685685685685686, -0.5585585585585586, -0.5485485485485486, -0.5385385385385386, -0.5285285285285286, -0.5185185185185186, -0.5085085085085086, -0.4984984984984986, -0.4884884884884886, -0.4784784784784786, -0.46846846846846857, -0.45845845845845856, -0.44844844844844856, -0.43843843843843855, -0.42842842842842854, -0.41841841841841854, -0.40840840840840853, -0.3983983983983985, -0.3883883883883883, -0.3783783783783783, -0.3683683683683683, -0.3583583583583583, -0.34834834834834827, -0.33833833833833826, -0.32832832832832826, -0.31831831831831825, -0.30830830830830824, -0.29829829829829824, -0.28828828828828823, -0.2782782782782782, -0.2682682682682682, -0.2582582582582582, -0.2482482482482482, -0.2382382382382382, -0.2282282282282282, -0.21821821821821819, -0.20820820820820818, -0.19819819819819817, -0.18818818818818817, -0.17817817817817816, -0.16816816816816815, -0.15815815815815815, -0.14814814814814814, -0.13813813813813813, -0.12812812812812813, -0.11811811811811812, -0.10810810810810811, -0.09809809809809811, -0.0880880880880881, -0.0780780780780781, -0.06806806806806809, -0.05805805805805808, -0.048048048048048075, -0.03803803803803807, -0.028028028028028062, -0.018018018018018056, -0.00800800800800805, 0.002002002002002179, 0.012012012012012185, 0.022022022022022192, 0.0320320320320322, 0.042042042042042205, 0.05205205205205221, 0.06206206206206222, 0.07207207207207222, 0.08208208208208223, 0.09209209209209224, 0.10210210210210224, 0.11211211211211225, 0.12212212212212226, 0.13213213213213226, 0.14214214214214227, 0.15215215215215228, 0.16216216216216228, 0.1721721721721723, 0.1821821821821823, 0.1921921921921923, 0.2022022022022023, 0.21221221221221231, 0.22222222222222232, 0.23223223223223233, 0.24224224224224233, 0.25225225225225234, 0.26226226226226235, 0.27227227227227235, 0.28228228228228236, 0.29229229229229237, 0.3023023023023024, 0.3123123123123124, 0.3223223223223224, 0.3323323323323324, 0.3423423423423424, 0.3523523523523524, 0.3623623623623624, 0.3723723723723724, 0.3823823823823824, 0.39239239239239243, 0.40240240240240244, 0.41241241241241244, 0.42242242242242245, 0.43243243243243246, 0.44244244244244246, 0.45245245245245247, 0.4624624624624625, 0.4724724724724725, 0.4824824824824825, 0.4924924924924925, 0.5025025025025025, 0.5125125125125125, 0.5225225225225225, 0.5325325325325325, 0.5425425425425425, 0.5525525525525525, 0.5625625625625625, 0.5725725725725725, 0.5825825825825826, 0.5925925925925926, 0.6026026026026026, 0.6126126126126126, 0.6226226226226226, 0.6326326326326326, 0.6426426426426426, 0.6526526526526526, 0.6626626626626626, 0.6726726726726726, 0.6826826826826826, 0.6926926926926926, 0.7027027027027026, 0.7127127127127126, 0.7227227227227226, 0.7327327327327327, 0.7427427427427427, 0.7527527527527527, 0.7627627627627627, 0.7727727727727727, 0.7827827827827827, 0.7927927927927927, 0.8028028028028027, 0.8128128128128127, 0.8228228228228227, 0.8328328328328327, 0.8428428428428427, 0.8528528528528527, 0.8628628628628627, 0.8728728728728727, 0.8828828828828827, 0.8928928928928928, 0.9029029029029028, 0.9129129129129128, 0.9229229229229228, 0.9329329329329328, 0.9429429429429428, 0.9529529529529528, 0.9629629629629628, 0.9729729729729728, 0.9829829829829828, 0.9929929929929928, 1.0030030030030028, 1.0130130130130128, 1.0230230230230228, 1.0330330330330328, 1.0430430430430429, 1.0530530530530529, 1.0630630630630629, 1.0730730730730729, 1.0830830830830829, 1.0930930930930929, 1.1031031031031029, 1.113113113113113, 1.123123123123123, 1.133133133133133, 1.143143143143143, 1.153153153153153, 1.163163163163163, 1.173173173173173, 1.183183183183183, 1.193193193193193, 1.203203203203203, 1.2132132132132134, 1.2232232232232234, 1.2332332332332334, 1.2432432432432434, 1.2532532532532534, 1.2632632632632634, 1.2732732732732734, 1.2832832832832834, 1.2932932932932935, 1.3033033033033035, 1.3133133133133135, 1.3233233233233235, 1.3333333333333335, 1.3433433433433435, 1.3533533533533535, 1.3633633633633635, 1.3733733733733735, 1.3833833833833835, 1.3933933933933935, 1.4034034034034035, 1.4134134134134135, 1.4234234234234235, 1.4334334334334335, 1.4434434434434436, 1.4534534534534536, 1.4634634634634636, 1.4734734734734736, 1.4834834834834836, 1.4934934934934936, 1.5035035035035036, 1.5135135135135136, 1.5235235235235236, 1.5335335335335336, 1.5435435435435436, 1.5535535535535536, 1.5635635635635636, 1.5735735735735736, 1.5835835835835836, 1.5935935935935936, 1.6036036036036037, 1.6136136136136137, 1.6236236236236237, 1.6336336336336337, 1.6436436436436437, 1.6536536536536537, 1.6636636636636637, 1.6736736736736737, 1.6836836836836837, 1.6936936936936937, 1.7037037037037037, 1.7137137137137137, 1.7237237237237237, 1.7337337337337337, 1.7437437437437437, 1.7537537537537538, 1.7637637637637638, 1.7737737737737738, 1.7837837837837838, 1.7937937937937938, 1.8038038038038038, 1.8138138138138138, 1.8238238238238238, 1.8338338338338338, 1.8438438438438438, 1.8538538538538538, 1.8638638638638638, 1.8738738738738738, 1.8838838838838838, 1.8938938938938938, 1.9039039039039038, 1.9139139139139139, 1.9239239239239239, 1.9339339339339339, 1.9439439439439439, 1.9539539539539539, 1.9639639639639639, 1.973973973973974, 1.983983983983984, 1.993993993993994, 2.0040040040040044, 2.0140140140140144, 2.0240240240240244, 2.0340340340340344, 2.0440440440440444, 2.0540540540540544, 2.0640640640640644, 2.0740740740740744, 2.0840840840840844, 2.0940940940940944, 2.1041041041041044, 2.1141141141141144, 2.1241241241241244, 2.1341341341341344, 2.1441441441441444, 2.1541541541541545, 2.1641641641641645, 2.1741741741741745, 2.1841841841841845, 2.1941941941941945, 2.2042042042042045, 2.2142142142142145, 2.2242242242242245, 2.2342342342342345, 2.2442442442442445, 2.2542542542542545, 2.2642642642642645, 2.2742742742742745, 2.2842842842842845, 2.2942942942942945, 2.3043043043043046, 2.3143143143143146, 2.3243243243243246, 2.3343343343343346, 2.3443443443443446, 2.3543543543543546, 2.3643643643643646, 2.3743743743743746, 2.3843843843843846, 2.3943943943943946, 2.4044044044044046, 2.4144144144144146, 2.4244244244244246, 2.4344344344344346, 2.4444444444444446, 2.4544544544544546, 2.4644644644644647, 2.4744744744744747, 2.4844844844844847, 2.4944944944944947, 2.5045045045045047, 2.5145145145145147, 2.5245245245245247, 2.5345345345345347, 2.5445445445445447, 2.5545545545545547, 2.5645645645645647, 2.5745745745745747, 2.5845845845845847, 2.5945945945945947, 2.6046046046046047, 2.6146146146146148, 2.6246246246246248, 2.6346346346346348, 2.6446446446446448, 2.6546546546546548, 2.664664664664665, 2.674674674674675, 2.684684684684685, 2.694694694694695, 2.704704704704705, 2.714714714714715, 2.724724724724725, 2.734734734734735, 2.744744744744745, 2.754754754754755, 2.764764764764765, 2.774774774774775, 2.784784784784785, 2.794794794794795, 2.804804804804805, 2.814814814814815, 2.824824824824825, 2.834834834834835, 2.844844844844845, 2.854854854854855, 2.864864864864865, 2.874874874874875, 2.884884884884885, 2.894894894894895, 2.904904904904905, 2.914914914914915, 2.924924924924925, 2.934934934934935, 2.944944944944945, 2.954954954954955, 2.964964964964965, 2.974974974974975, 2.984984984984985, 2.994994994994995, 3.005005005005005, 3.015015015015015, 3.025025025025025, 3.035035035035035, 3.045045045045045, 3.055055055055055, 3.065065065065065, 3.075075075075075, 3.085085085085085, 3.095095095095095, 3.105105105105105, 3.115115115115115, 3.125125125125125, 3.135135135135135, 3.145145145145145, 3.155155155155155, 3.165165165165165, 3.175175175175175, 3.185185185185185, 3.195195195195195, 3.205205205205205, 3.215215215215215, 3.225225225225225, 3.235235235235235, 3.245245245245245, 3.255255255255255, 3.265265265265265, 3.275275275275275, 3.285285285285285, 3.295295295295295, 3.305305305305305, 3.315315315315315, 3.325325325325325, 3.335335335335335, 3.3453453453453452, 3.3553553553553552, 3.3653653653653652, 3.3753753753753752, 3.3853853853853852, 3.3953953953953953, 3.4054054054054053, 3.4154154154154153, 3.4254254254254253, 3.4354354354354353, 3.4454454454454453, 3.4554554554554553, 3.4654654654654653, 3.4754754754754753, 3.4854854854854853, 3.4954954954954953, 3.5055055055055053, 3.5155155155155153, 3.5255255255255253, 3.5355355355355353, 3.5455455455455454, 3.5555555555555554, 3.5655655655655654, 3.5755755755755754, 3.5855855855855854, 3.5955955955955954, 3.6056056056056054, 3.6156156156156154, 3.6256256256256254, 3.6356356356356354, 3.6456456456456454, 3.6556556556556554, 3.6656656656656654, 3.6756756756756754, 3.6856856856856854, 3.6956956956956954, 3.7057057057057055, 3.7157157157157155, 3.7257257257257255, 3.7357357357357355, 3.7457457457457455, 3.7557557557557555, 3.7657657657657655, 3.7757757757757755, 3.7857857857857855, 3.7957957957957955, 3.8058058058058055, 3.8158158158158155, 3.8258258258258255, 3.8358358358358355, 3.8458458458458455, 3.8558558558558556, 3.8658658658658656, 3.8758758758758756, 3.8858858858858856, 3.8958958958958956, 3.9059059059059056, 3.9159159159159156, 3.9259259259259256, 3.9359359359359356, 3.9459459459459456, 3.9559559559559556, 3.9659659659659656, 3.9759759759759756, 3.9859859859859856, 3.9959959959959956, 4.006006006006006, 4.016016016016016, 4.026026026026026, 4.036036036036036, 4.046046046046046, 4.056056056056056, 4.066066066066066, 4.076076076076076, 4.086086086086086, 4.096096096096096, 4.106106106106106, 4.116116116116116, 4.126126126126126, 4.136136136136136, 4.146146146146146, 4.156156156156156, 4.166166166166166, 4.176176176176176, 4.186186186186186, 4.196196196196196, 4.206206206206206, 4.216216216216216, 4.226226226226226, 4.236236236236236, 4.246246246246246, 4.256256256256256, 4.266266266266266, 4.276276276276276, 4.286286286286286, 4.296296296296296, 4.306306306306306, 4.316316316316316, 4.326326326326326, 4.336336336336336, 4.346346346346346, 4.356356356356356, 4.366366366366366, 4.376376376376376, 4.386386386386386, 4.396396396396396, 4.406406406406406, 4.416416416416417, 4.426426426426427, 4.436436436436437, 4.446446446446447, 4.456456456456457, 4.466466466466467, 4.476476476476477, 4.486486486486487, 4.496496496496497, 4.506506506506507, 4.516516516516517, 4.526526526526527, 4.536536536536537, 4.546546546546547, 4.556556556556557, 4.566566566566567, 4.576576576576577, 4.586586586586587, 4.596596596596597, 4.606606606606607, 4.616616616616617, 4.626626626626627, 4.636636636636637, 4.646646646646647, 4.656656656656657, 4.666666666666667, 4.676676676676677, 4.686686686686687, 4.696696696696697, 4.706706706706707, 4.716716716716717, 4.726726726726727, 4.736736736736737, 4.746746746746747, 4.756756756756757, 4.766766766766767, 4.776776776776777, 4.786786786786787, 4.796796796796797, 4.806806806806807, 4.816816816816817, 4.826826826826827, 4.836836836836837, 4.846846846846847, 4.856856856856857, 4.866866866866867, 4.876876876876877, 4.886886886886887, 4.896896896896897, 4.906906906906907, 4.916916916916917, 4.926926926926927, 4.936936936936937, 4.946946946946947, 4.956956956956957, 4.966966966966967, 4.976976976976977, 4.986986986986987, 4.996996996996997, 5.007007007007007, 5.017017017017017, 5.027027027027027, 5.037037037037037, 5.047047047047047, 5.057057057057057, 5.067067067067067, 5.077077077077077, 5.087087087087087, 5.097097097097097, 5.107107107107107, 5.117117117117117, 5.127127127127127, 5.137137137137137, 5.147147147147147, 5.157157157157157, 5.167167167167167, 5.177177177177177, 5.187187187187187, 5.197197197197197, 5.207207207207207, 5.217217217217217, 5.227227227227227, 5.237237237237237, 5.247247247247247, 5.257257257257257, 5.267267267267267, 5.277277277277277, 5.287287287287287, 5.297297297297297, 5.307307307307307, 5.317317317317317, 5.327327327327327, 5.337337337337337, 5.347347347347347, 5.357357357357357, 5.367367367367367, 5.377377377377377, 5.387387387387387, 5.397397397397397, 5.407407407407407, 5.4174174174174174, 5.4274274274274275, 5.4374374374374375, 5.4474474474474475, 5.4574574574574575, 5.4674674674674675, 5.4774774774774775, 5.4874874874874875, 5.4974974974974975, 5.5075075075075075, 5.5175175175175175, 5.5275275275275275, 5.5375375375375375, 5.5475475475475475, 5.5575575575575575, 5.5675675675675675, 5.5775775775775776, 5.587587587587588, 5.597597597597598, 5.607607607607608, 5.617617617617618, 5.627627627627628, 5.637637637637638, 5.647647647647648, 5.657657657657658, 5.667667667667668, 5.677677677677678, 5.687687687687688, 5.697697697697698, 5.707707707707708, 5.717717717717718, 5.727727727727728, 5.737737737737738, 5.747747747747748, 5.757757757757758, 5.767767767767768, 5.777777777777778, 5.787787787787788, 5.797797797797798, 5.807807807807808, 5.817817817817818, 5.827827827827828, 5.837837837837838, 5.847847847847848, 5.857857857857858, 5.867867867867868, 5.877877877877878, 5.887887887887888, 5.897897897897898, 5.907907907907908, 5.917917917917918, 5.927927927927928, 5.937937937937938, 5.947947947947948, 5.957957957957958, 5.967967967967968, 5.977977977977978, 5.987987987987988, 5.997997997997998, 6.008008008008009, 6.018018018018019, 6.028028028028029, 6.038038038038039, 6.048048048048049, 6.058058058058059, 6.068068068068069, 6.078078078078079, 6.088088088088089, 6.098098098098099, 6.108108108108109, 6.118118118118119, 6.128128128128129, 6.138138138138139, 6.148148148148149, 6.158158158158159, 6.168168168168169, 6.178178178178179, 6.188188188188189, 6.198198198198199, 6.208208208208209, 6.218218218218219, 6.228228228228229, 6.238238238238239, 6.248248248248249, 6.258258258258259, 6.268268268268269, 6.278278278278279, 6.288288288288289, 6.298298298298299, 6.308308308308309, 6.318318318318319, 6.328328328328329, 6.338338338338339, 6.348348348348349, 6.358358358358359, 6.368368368368369, 6.378378378378379, 6.388388388388389, 6.398398398398399, 6.408408408408409, 6.418418418418419, 6.428428428428429, 6.438438438438439, 6.448448448448449, 6.458458458458459, 6.468468468468469, 6.478478478478479, 6.488488488488489, 6.498498498498499, 6.508508508508509, 6.518518518518519, 6.528528528528529, 6.538538538538539, 6.548548548548549, 6.558558558558559, 6.568568568568569, 6.578578578578579, 6.588588588588589, 6.598598598598599, 6.608608608608609, 6.618618618618619, 6.628628628628629, 6.638638638638639, 6.648648648648649, 6.658658658658659, 6.668668668668669, 6.678678678678679, 6.688688688688689, 6.698698698698699, 6.708708708708709, 6.718718718718719, 6.728728728728729, 6.738738738738739, 6.748748748748749, 6.758758758758759, 6.768768768768769, 6.778778778778779, 6.788788788788789, 6.798798798798799, 6.808808808808809, 6.818818818818819, 6.828828828828829, 6.838838838838839, 6.848848848848849, 6.858858858858859, 6.868868868868869, 6.878878878878879, 6.888888888888889, 6.898898898898899, 6.908908908908909, 6.918918918918919, 6.928928928928929, 6.938938938938939, 6.948948948948949, 6.958958958958959, 6.968968968968969, 6.978978978978979, 6.988988988988989, 6.998998998998999, 7.009009009009009, 7.019019019019019, 7.029029029029029, 7.039039039039039, 7.049049049049049, 7.059059059059059, 7.069069069069069, 7.079079079079079, 7.089089089089089, 7.099099099099099, 7.109109109109109, 7.119119119119119, 7.129129129129129, 7.1391391391391394, 7.1491491491491495, 7.1591591591591595, 7.1691691691691695, 7.1791791791791795, 7.1891891891891895, 7.1991991991991995, 7.2092092092092095, 7.2192192192192195, 7.2292292292292295, 7.2392392392392395, 7.2492492492492495, 7.2592592592592595, 7.2692692692692695, 7.2792792792792795, 7.2892892892892895, 7.2992992992992995, 7.3093093093093096, 7.31931931931932, 7.32932932932933, 7.33933933933934, 7.34934934934935, 7.35935935935936, 7.36936936936937, 7.37937937937938, 7.38938938938939, 7.3993993993994, 7.40940940940941, 7.41941941941942, 7.42942942942943, 7.43943943943944, 7.44944944944945, 7.45945945945946, 7.46946946946947, 7.47947947947948, 7.48948948948949, 7.4994994994995, 7.50950950950951, 7.51951951951952, 7.52952952952953, 7.53953953953954, 7.54954954954955, 7.55955955955956, 7.56956956956957, 7.57957957957958, 7.58958958958959, 7.5995995995996, 7.60960960960961, 7.61961961961962, 7.62962962962963, 7.63963963963964, 7.64964964964965, 7.65965965965966, 7.66966966966967, 7.67967967967968, 7.68968968968969, 7.6996996996997, 7.70970970970971, 7.71971971971972, 7.72972972972973, 7.73973973973974, 7.74974974974975, 7.75975975975976, 7.76976976976977, 7.77977977977978, 7.78978978978979, 7.7997997997998, 7.80980980980981, 7.81981981981982, 7.82982982982983, 7.83983983983984, 7.84984984984985, 7.85985985985986, 7.86986986986987, 7.87987987987988, 7.88988988988989, 7.8998998998999, 7.90990990990991, 7.91991991991992, 7.92992992992993, 7.93993993993994, 7.94994994994995, 7.95995995995996, 7.96996996996997, 7.97997997997998, 7.98998998998999, 8.0], \"y\": [4.348766175087199, 4.277192966093969, 4.205102954463674, 4.132487146126188, 4.059336629423321, 3.9856425908123354, 3.911396330578623, 3.8365892785425917, 3.7612130097457555, 3.6852592601009544, 3.6087199419916254, 3.5315871598050066, 3.4538532253841696, 3.375510673383772, 3.296552276514456, 3.2169710606608466, 3.1367603198581673, 3.055913631112543, 2.974424869050148, 2.892288220380445, 2.8094981981588774, 2.7260496558344807, 2.641937801068032, 2.5571582093064924, 2.4717068370996693, 2.385580035145178, 2.2987745610480106, 2.211287591781181, 2.123116735834155, 2.0342600450359956, 1.9447160260403917, 1.8544836514599983, 1.763562370637775, 1.6719521200432987, 1.579653333282303, 1.4866669507080115, 1.392994428623144, 1.2986377480617897, 1.2035994231406981, 1.1078825089698654, 1.0114906091126674, 0.9144278825861474, 0.8166990503924698, 0.7183094015728864, 0.6192647987760354, 0.5195716833327357, 0.41923707982988656, 0.31826860017649905, 0.21667444715532413, 0.11446341745397193, 0.011644904169876624, -0.09177110121609311, -0.19577400740211237, -0.3003526223682337, -0.40549515302337114, -0.5111892053997501, -0.6174217853976534, -0.7241793000828051, -0.8314475595382479, -0.9392117792720592, -1.0474565831817677, -1.1561660070758175, -1.2653235027519267, -1.3749119426316847, -1.484913624950209, -1.595310279499198, -1.706083073921177, -1.8172126205522456, -1.9286789838101188, -2.0404616881237443, -2.15253972640026, -2.2648915690245763, -2.377495173386338, -2.4903279939285246, -2.6033669927114618, -2.716588650485514, -2.8299689782652266, -2.9434835293972257, -3.0571074121136768, -3.1708153025626418, -3.284581458306205, -3.3983797322767635, -3.5121835871814295, -3.6259661103440575, -3.7397000289739224, -3.8533577258496927, -3.9669112554068846, -4.080332360216582, -4.193592487842789, -4.3066628080653935, -4.419514230455329, -4.532117422288148, -4.644442826781849, -4.756460681644461, -4.868141037916503, -4.9794537790931885, -5.0903686405108175, -5.200855228981597, -5.310883042660767, -5.420421491129657, -5.529439915678046, -5.637907609768911, -5.745793839668446, -5.8530678652239985, -5.959698960772354, -6.065656436160612, -6.170909657861748, -6.275428070166713, -6.379181216434895, -6.482138760384516, -6.584270507404513, -6.685546425869305, -6.7859366684377775, -6.88541159331777, -6.983941785477266, -7.081498077783479, -7.178051572051032, -7.2735736599803715, -7.368036043967623, -7.461410757767144, -7.553670186988011, -7.644787089405842, -7.734734615071362, -7.823486326197288, -7.9110162168051685, -7.997298732114024, -8.082308787652714, -8.166021788078188, -8.24841364568196, -8.329460798567293, -8.409140228479904, -8.487429478275137, -8.564306669004896, -8.639750516607812, -8.71374034818652, -8.78625611785608, -8.857278422148063, -8.926788514954989, -8.994768322000299, -9.061200454819298, -9.126068224236976, -9.189355653328905, -9.251047489851947, -9.311129218131782, -9.369587070394845, -9.426408037532582, -9.4815798792865, -9.535091133842874, -9.586931126826506, -9.637089979683441, -9.68555861744299, -9.732328775849988, -9.777393007858722, -9.820744689480492, -9.862378024977327, -9.902288051394937, -9.940470642428538, -9.976922511615763, -10.011641214851448, -10.044625152219691, -10.075873569139151, -10.105386556818152, -10.133165052016768, -10.159210836113736, -10.183526533476522, -10.206115609133608, -10.226982365748656, -10.2461319398968, -10.263570297643966, -10.27930422943074, -10.293341344262984, -10.305690063211918, -10.31635961222714, -10.325360014266613, -10.332702080748282, -10.338397402328647, -10.342458339014197, -10.34489800961226, -10.34573028052846, -10.344969753918493, -10.342631755202705, -10.338732319952378, -10.333288180157382, -10.326316749885335, -10.317836110343055, -10.307864994351664, -10.296422770247228, -10.283529425219443, -10.26920554810139, -10.253472311623915, -10.236351454148753, -10.217865260895032, -10.198036544674254, -10.17688862614946, -10.15444531363461, -10.130730882450868, -10.105770053856764, -10.079587973569812, -10.052210189897455, -10.02366263149575, -9.993971584774501, -9.963163670968031, -9.93126582289107, -9.898305261399686, -9.86430947157741, -9.829306178667146, -9.793323323769677, -9.756389039329914, -9.718531624432263, -9.679779519926786, -9.640161283408018, -9.599705564068548, -9.558441077449636, -9.516396580111353, -9.473600844244842, -9.430082632249466, -9.385870671297702, -9.340993627910766, -9.29548008256797, -9.24935850437294, -9.202657225799813, -9.155404417542508, -9.107628063490283, -9.059355935852572, -9.010615570456242, -8.961434242238193, -8.911838940956192, -8.86185634714073, -8.811512808310493, -8.760834315473982, -8.709846479939516, -8.658574510455768, -8.607043190704692, -8.555276857168487, -8.503299377391981, -8.451134128661536, -8.398803977121258, -8.346331257347012, -8.293737752398362, -8.241044674368224, -8.188272645449604, -8.13544167953846, -8.082571164391203, -8.029679844355053, -7.9767858036888795, -7.923906450491755, -7.871058501255976, -7.818257966060714, -7.7655201344220455, -7.712859561814483, -7.660290056878594, -7.6078246693287745, -7.555475678574574, -7.503254583068439, -7.4511720903920695, -7.399238108093019, -7.347461735282465, -7.295851255004445, -7.244414127386212, -7.193156983578656, -7.142085620495041, -7.091204996355662, -7.040519227045238, -6.990031583289241, -6.939744488654514, -6.889659518378907, -6.8397773990338875, -6.790098009023257, -6.740620379920504, -6.6913426986464, -6.642262310487838, -6.593375722957994, -6.544678610497276, -6.496165820013618, -6.447831377259997, -6.39966849404622, -6.351669576281285, -6.303826232841837, -6.2561292852614505, -6.208568778234713, -6.161133990929321, -6.113813449098615, -6.0665949379862205, -6.019465516013725, -5.972411529241536, -5.925418626592335, -5.878471775825825, -5.831555280252663, -5.7846527961748455, -5.737747351039019, -5.6908213622884976, -5.643856656899113, -5.596834491583287, -5.549735573646058, -5.502540082476148, -5.455227691654493, -5.40777759166201, -5.360168513167784, -5.312378750878208, -5.264386187927086, -5.2161683207860285, -5.167702284674023, -5.1189648794444444, -5.069932595927254, -5.020581642703672, -4.970887973290062, -4.920827313707345, -4.8703751904117745, -4.819506958562505, -4.768197830600948, -4.716422905116556, -4.6641571959732735, -4.61137566167058, -4.558053234912709, -4.50416485235933, -4.449685484530726, -4.394590165840208, -4.3388540247263165, -4.28245231385714, -4.225360440378891, -4.16755399618074, -4.109008788147777, -4.0497008683738525, -3.9896065643059693, -3.9287025087918632, -3.866965670002368, -3.804373381200133, -3.740903370326329, -3.6765337893770025, -3.611243243540793, -3.545010820069881, -3.4778161168561006, -3.40963927068435, -3.3404609851355844, -3.270262558111895, -3.199025908956392, -3.1267336051408874, -3.053368888494635, -2.978915700947703, -2.9033587097628804, -2.8266833322303846, -2.7488757598000113, -2.669922981625776, -2.5898128074985363, -2.5085338901425227, -2.4260757468522085, -2.342428780446422, -2.2575842995171604, -2.171534537951091, -2.0842726737023045, -1.9957928467954744, -1.906090176539199, -1.8151607779299195, -1.7230017772274842, -1.6296113266840813, -1.5349886184089783, -1.4391338973522037, -1.3420484733910423, -1.24373473250397, -1.144196147017411, -1.0434372849114877, -0.9414638181717343, -0.838282530174549, -0.7339013220950003, -0.6283292183264301, -0.5215763709021658, -0.41365406291050655, -0.3045747108950356, -0.1943518662331991, -0.08300021548698666, 0.029464420279539327, 0.14302508722019924, 0.25766370146755135, 0.3733610529371285, 0.4900968101765919, 0.6078495262604091, 0.7265966457297051, 0.8463145125759866, 0.9669783792664914, 1.0885624168079528, 1.2110397258446128, 1.3343823487853694, 1.4585612829539754, 1.583546494755262, 1.7093069348494034, 1.835810554325292, 1.9630243218631453, 2.090914241875538, 2.219445373615101, 2.34858185123623, 2.4782869047971854, 2.6085228821881135, 2.7392512719695548, 2.8704327271051713, 3.0020270895715004, 3.133993415826708, 3.2662900031194466, 3.3988744166180713, 3.53170351733967, 3.66473349085752, 3.7979198767648183, 3.931217598871703, 4.064580996111949, 4.197963854134772, 4.331319437556655, 4.464600522847288, 4.597759431823088, 4.730748065721046, 4.86351793982508, 4.996020218616403, 5.128205751418833, 5.260025108509433, 5.3914286176642685, 5.522366401108609, 5.65278841284036, 5.782644476295081, 5.911884322320461, 6.040457627427765, 6.168314052287326, 6.295403280434818, 6.421675057154756, 6.547079228507284, 6.671565780464135, 6.795084878119327, 6.9175869049400145, 7.039022502022695, 7.159342607319804, 7.278498494801697, 7.396441813518818, 7.513124626528885, 7.628499449653879, 7.742519290031576, 7.8551376844265075, 7.966308737265197, 8.075987158360707, 8.184128300291611, 8.290688195400712, 8.395623592379014, 8.498891992400681, 8.600451684774983, 8.700261782081544, 8.798282254755511, 8.894473965089635, 8.988798700620645, 9.081219206867713, 9.171699219391266, 9.260203495140903, 9.34669784306162, 9.431149153928208, 9.513525429378124, 9.593795810113859, 9.671930603246393, 9.747901308751963, 9.821680645015165, 9.893242573431987, 9.962562322047196, 10.029616408201301, 10.094382660162976, 10.156840237723788, 10.216969651732825, 10.274752782549744, 10.33017289739556, 10.383214666581562, 10.433864178597473, 10.48210895404111, 10.527937958372647, 10.57134161347766, 10.612311808024073, 10.650841906599188, 10.686926757614064, 10.720562699963477, 10.751747568430844, 10.78048069782859, 10.806762925865455, 10.830596594733443, 10.851985551408205, 10.870935146657773, 10.887452232755718, 10.901545159895951, 10.91322377130758, 10.922499397069318, 10.929384846624243, 10.933894399996719, 10.936043797714618, 10.935850229441066, 10.933332321321153, 10.928510122050232, 10.921405087671591, 10.912040065112459, 10.900439274468523, 10.886628290048241, 10.870634020189433, 10.852484685861787, 10.832209798070078, 10.809840134073971, 10.785407712441529, 10.758945766954538, 10.73048871938497, 10.700072151162914, 10.66773277395747, 10.63350839919309, 10.597437906524956, 10.559561211297957, 10.519919231014917, 10.478553850840655, 10.435507888169434, 10.390825056284386, 10.344549927138306, 10.29672789328623, 10.247405129001036, 10.196628550604173, 10.144445776044455, 10.090905083758708, 10.036055370848718, 9.979946110609887, 9.922627309447439, 9.864149463217032, 9.804563513026999, 9.743920800540291, 9.682273022814663, 9.61967218672025, 9.556170562974263, 9.491820639832925, 9.42667507648131, 9.360786656162208, 9.294208239085407, 9.22699271515926, 9.159192956586699, 9.090861770368068, 9.022051850753524, 8.95281573168783, 8.883205739290625, 8.81327394441538, 8.743072115330262, 8.672651670564306, 8.602063631962231, 8.531358577991192, 8.460586597342793, 8.389797242873492, 8.31903948592642, 8.248361671077463, 8.177811471348246, 8.107435843928323, 8.037280986448712, 7.9673922938484045, 7.897814315875275, 7.828590715262258, 7.759764226619268, 7.691376616080868, 7.623468641749072, 7.5560800149701866, 7.489249362483946, 7.423014189482531, 7.357410843616417, 7.292474479983277, 7.228239027135354, 7.1647371541400116, 7.102000238727275, 7.0400583365573715, 6.978940151640332, 6.918673007938847, 6.859282822184577, 6.800794077937142, 6.743229800913996, 6.68661153561834, 6.630959323291161, 6.576291681212377, 6.522625583374912, 6.469976442554433, 6.418358093796188, 6.367782779339304, 6.31826113499759, 6.269802178014638, 6.222413296409805, 6.17610023983028, 6.130867111923216, 6.086716364240486, 6.043648791687345, 6.0016635295249, 5.960758051934876, 5.920928172153827, 5.882168044182504, 5.844470166074709, 5.807825384808474, 5.772222902741094, 5.737650285647962, 5.704093472343827, 5.671536785883562, 5.6399629463381755, 5.609353085140203, 5.579686760991327, 5.550941977323491, 5.523095201303375, 5.496121384368668, 5.469993984283102, 5.4446849886957525, 5.420164940188773, 5.396402962796191, 5.37336678997507, 5.3510227940089115, 5.329336016821781, 5.30827020218028, 5.287787829259118, 5.26785014754472, 5.2484172130499385, 5.229447925811702, 5.210900068642074, 5.192730347101995, 5.17489443066572, 5.157346995042734, 5.140041765622765, 5.122931562008336, 5.10596834359817, 5.089103256183654, 5.072286679519485, 5.055468275828595, 5.038597039200407, 5.021621345840529, 5.0044890051290425, 4.987147311443584, 4.969543096702615, 4.9516227835833995, 4.933332439368401, 4.914617830373074, 4.895424476907289, 4.875697708721963, 4.855382720891809, 4.834424630084545, 4.812768531166325, 4.790359554092671, 4.767142921033679, 4.743064003681909, 4.718068380690924, 4.692101895192162, 4.66511071233753, 4.637041376814851, 4.607840870283129, 4.577456668674436, 4.545836799309146, 4.5129298977711745, 4.478685264489892, 4.4430529209754335, 4.405983665654244, 4.36742912925174, 4.327341829669365, 4.285675226303332, 4.24238377375282, 4.197422974865617, 4.150749433069659, 4.102320903939326, 4.05209634594584, 4.000035970341659, 3.946101290129342, 3.8902551680659654, 3.832461863654893, 3.772687079077378, 3.71089800401729, 3.6470633593330235, 3.5811534395315645, 3.513140154000544, 3.4429970669550864, 3.370699436057243, 3.29622424966683, 3.219550262683585, 3.1406580309416343, 3.059529944118472, 2.9761502571217964, 2.890505119918813, 2.8025826057738885, 2.7123727378617035, 2.6198675142244667, 2.5250609310430416, 2.4279490041933367, 2.3285297890606613, 2.2268033985863003, 2.1227720195220074, 2.016439926869658, 1.9078134964848836, 1.7969012158250526, 1.6837136928235923, 1.568263662874276, 1.450565993910725, 1.3306376895680794, 1.2084978904154515, 1.084167873249493, 0.957671048441135, 0.8290329553292759, 0.6982812556569626, 0.5654457250473572, 0.43055824251854546, 0.29365277803803824, 0.15476537811958613, 0.013934149466708678, -0.12880075933083823, -0.27339717803967556, -0.4198109369547032, -0.5679958909887011, -0.7179039456662692, -0.8694850850069475, -1.0226874012805889, -1.177457126616284, -1.3337386664443802, -1.4914746347493921, -1.6506058911098629, -1.8110715794995116, -1.972809168822312, -2.1357544951524496, -2.2998418056484415, -2.4650038041090614, -2.631171698137088, -2.798275247875295, -2.9662428162775343, -3.135001420876215, -3.3044767870059584, -3.4745934024417426, -3.6452745734083765, -3.81644248191673, -3.9880182443807684, -4.159921971468075, -4.332072829135237, -4.504389100798229, -4.676788250586616, -4.849186987629303, -5.021501331318337, -5.193646677496183, -5.36553786551087, -5.5370892460822985, -5.708214749922152, -5.878827957048844, -6.048842166738094, -6.218170468048917, -6.386725810864032, -6.554421077382977, -6.721169154005559, -6.886883003542663, -7.051475737690883, -7.214860689706944, -7.376951487217433, -7.53766212509899, -7.696907038363763, -7.854601174984654, -8.010660068594701, -8.164999910994766, -8.317537624403581, -8.468190933384241, -8.616878436381157, -8.763519676801652, -8.908035213576497, -9.050346691133855, -9.190376908721426, -9.328049889011861, -9.463290945926932, -9.596026751616355, -9.726185402527692, -9.85369648450435, -9.978491136849224, -10.100502115292329, -10.219663853801432, -10.33591252517552, -10.449186100361784, -10.55942440643775, -10.666569183201101, -10.770564138310826, -10.871355000924336, -10.968889573776442, -11.06311778364712, -11.1539917301664, -11.241465732905873, -11.325496376707742, -11.40604255520373, -11.483065512477527, -11.556528882826072, -11.626398728576397, -11.692643575916412, -11.755234448699612, -11.81414490018536, -11.869351042678115, -11.920831575030743, -11.968567807978783, -12.012543687274487, -12.052745814591157, -12.089163466170362, -12.1217886091864, -12.15061591580448, -12.175642774910964, -12.196869301496102, -12.21429834367169, -12.227935487308233, -12.23778905827816, -12.243870122293862, -12.246192482331399, -12.244772673632877, -12.239629956282675, -12.230786305354844, -12.218266398631224, -12.202097601892001, -12.182309951782615, -12.15893613626318, -12.132011472648724, -12.101573883250854, -12.067663868633568, -12.030324478498246, -11.98960128021492, -11.945542325019304, -11.898198111897067, -11.847621549179163, -11.793867913874058, -11.736994808764988, -11.677062117302384, -11.614131956323805, -11.548268626635767, -11.479538561493962, -11.408010273020334, -11.333754296597625, -11.25684313328384, -11.177351190291192, -11.095354719575866, -11.01093175458699, -10.92416204522489, -10.835126991060706, -10.74390957287104, -10.65059428254319, -10.555267051408146, -10.45801517706018, -10.358927248723473, -10.258093071227792, -10.155603587656678, -10.05155080073312, -9.946027693009059, -9.839128145926402, -9.730946857818525, -9.621579260922486, -9.511121437473317, -9.399670034952871, -9.287322180566802, -9.174175395024173, -9.06032750569515, -8.945876559223114, -8.830920733668261, -8.715558250260548, -8.599887284840468, -8.48400587906672, -8.36801185147039, -8.252002708435683, -8.136075555187645, -8.020327006867607, -7.904853099777321, -7.789749202872942, -7.675109929590075, -7.561029050081119, -7.447599403946139, -7.3349128135382715, -7.223059997924567, -7.11213048758283, -7.002212539914661, -6.893393055654519, -6.785757496254121, -6.679389802320847, -6.574372313188264, -6.470785687696118, -6.368708826256327, -6.268218794280683, -6.1693907470450045, -6.07229785606347, -5.977011237045797, -5.88359987950876, -5.792130578112314, -5.702667865789349, -5.615273948736656, -5.530008643333332, -5.446929315051358, -5.366090819421446, -5.287545445115754, -5.21134285920728, -5.13753005466408, -5.066151300134576, -4.997248092078453, -4.930859109295625, -4.867020169903874, -4.805764190813655, -4.747121149746523, -4.691118049841552, -4.637778886891797, -4.587124619250819, -4.539173140446831, -4.493939254539883, -4.451434654254989, -4.411667901921836, -4.374644413249209, -4.340366443959809, -4.308833079308677, -4.2800402265058475, -4.2539806100613795, -4.2306437700682, -4.210016063435735, -4.192080668084552, -4.176817590109626, -4.164203673917198, -4.154212615337467, -4.146814977712665, -4.141978210957384, -4.139666673585298, -4.1398416576936405, -4.1424614168942036, -4.147481197176742, -4.154853270688072, -4.164526972407357, -4.1764487396953935, -4.190562154693, -4.206807989540907, -4.225124254390895, -4.245446248175236, -4.267706612098888, -4.2918353858162295, -4.31776006625158, -4.345405669020107, -4.374694792403274, -4.405547683830409, -4.4378823088154915, -4.471614422295899, -4.506657642317348, -4.542923526006987, -4.580321647774252, -4.618759679676819, -4.658143473886788, -4.69837714719004, -4.739363167449615, -4.781002441961843, -4.823194407632048, -4.865837122894565, -4.908827361300027, -4.952060706691028, -4.995431649885444, -5.038833686785085, -5.082159417825674, -5.125300648682573, -5.168148492145254, -5.210593471072026, -5.25252562233524, -5.2938346016658935, -5.334409789305427, -5.374140396371308, -5.412915571842073, -5.45062451006647, -5.487156558700541, -5.522401326975663, -5.5562487941999015, -5.588589418394416, -5.619314244966119, -5.648315015317388, -5.675484275293268, -5.700715483366334, -5.723903118459248, -5.744942787304947, -5.763731331244422, -5.780166932362148, -5.794149218859424, -5.805579369566189, -5.814360217492217, -5.820396352319115, -5.823594221735092, -5.823862231515073, -5.821110844249599, -5.815252676626628, -5.806202595171439, -5.793877810350731, -5.778197968948188, -5.759085244619953, -5.736464426539731, -5.710263006044626, -5.680411261194252, -5.64684233915722, -5.6094923363407165, -5.5683003761805985, -5.5232086845112285, -5.474162662436128, -5.42111095662249, -5.364005526944633, -5.302801711403535, -5.237458288251825, -5.16793753525583, -5.094205286028574, -5.016230983370076, -4.933987729553711, -4.847452333499936, -4.756605354781256, -4.661431144405003, -4.561917882323138, -4.458057611621113, -4.349846269340639, -4.237283713894041, -4.1203737490308585, -3.999124144320276, -3.8735466521160222, -3.743657020973415, -3.6094750054913023, -3.471024372554849, -3.3283329039582323, -3.18143239538952, -3.03035865176329]}, {\"hovertemplate\": \"x: %{x} <br>y: %{y}\", \"mode\": \"markers\", \"name\": \"Train data\", \"type\": \"scatter\", \"x\": [5.157157157157157, 7.2092092092092095, 0.9529529529529528, -1.169169169169169, 7.42942942942943, 4.786786786786787, 2.0440440440440444, 6.398398398398399, 3.7057057057057055, 7.029029029029029, 0.7827827827827827, -0.32832832832832826, -0.6086086086086087, 4.856856856856857, -0.15815815815815815, 7.2892892892892895, 1.3633633633633635, 6.768768768768769, 2.6346346346346348, 0.1921921921921923, 6.628628628628629, 7.1591591591591595, 7.57957957957958, 6.108108108108109, 2.0940940940940944, 7.80980980980981, 2.1341341341341344, 1.1031031031031029, 6.608608608608609, 5.027027027027027, 7.67967967967968, 5.4474474474474475, 3.6856856856856854, 4.736736736736737, 1.2632632632632634, 0.7727727727727727, 4.826826826826827, 6.988988988988989, 0.13213213213213226, 4.926926926926927, 3.4554554554554553, -1.6896896896896898, 1.4534534534534536, 7.84984984984985, 2.904904904904905, 6.378378378378379, 5.827827827827828, 0.0320320320320322, 4.816816816816817, 6.328328328328329, 6.798798798798799, 3.5055055055055053, -0.6486486486486487, 6.198198198198199, 6.418418418418419, 7.1891891891891895, 5.397397397397397, -0.8288288288288288, 3.035035035035035, 7.78978978978979, -0.9889889889889889, 0.3123123123123124, 4.836836836836837, 7.77977977977978, 6.668668668668669, 5.657657657657658, 4.616616616616617, 1.9539539539539539, 6.358358358358359, 0.12212212212212226, -1.91991991991992, 2.3143143143143146, -0.9189189189189189, 6.638638638638639, 3.9059059059059056, 1.5935935935935936, -0.2382382382382382, 4.166166166166166, 1.6436436436436437, 4.206206206206206, 5.037037037037037, 0.4824824824824825, -1.5495495495495495, -0.40840840840840853, 0.6626626626626626, 1.203203203203203, 5.697697697697698, -1.1991991991991993, 2.774774774774775, 1.4234234234234235, 6.788788788788789, 4.306306306306306, 0.6826826826826826, 4.006006006006006, 2.1141141141141144, 4.516516516516517, 5.5275275275275275, 0.6726726726726726, -1.029029029029029, 5.287287287287287, 3.075075075075075, 0.4924924924924925, -1.8198198198198199, 2.3643643643643646, 3.8258258258258255, -1.3893893893893894, -0.33833833833833826, -1.7497497497497498, 6.738738738738739, 3.305305305305305, 4.666666666666667, -1.2892892892892893, 0.7527527527527527, 7.2992992992992995, 4.716716716716717, 5.4574574574574575, -0.30830830830830824, -1.2492492492492493, 3.5855855855855854, 5.247247247247247, 1.0530530530530529, 4.056056056056056, 1.7637637637637638, 1.8338338338338338, 1.4834834834834836, 4.346346346346346, 7.86986986986987, 1.5035035035035036, 2.954954954954955, 4.016016016016016, 5.187187187187187, 6.578578578578579, -1.3193193193193193, 4.596596596596597, 0.7627627627627627, 2.3443443443443446, 1.7837837837837838, 4.876876876876877, -0.2482482482482482, 0.16216216216216228, 5.767767767767768, 7.53953953953954, 7.74974974974975, 4.756756756756757, 3.5955955955955954, 6.898898898898899, 7.009009009009009, 7.55955955955956, -0.2282282282282282, 7.93993993993994, 0.022022022022022192, 2.3043043043043046, 6.958958958958959, 6.468468468468469, -1.96996996996997, 0.6326326326326326, 2.3243243243243246, 6.888888888888889, 4.586586586586587, 4.176176176176176, 0.6126126126126126, 0.5125125125125125, 1.973973973973974, 2.4744744744744747, -1.069069069069069, 3.8458458458458455, 0.26226226226226235, 2.4444444444444446, -1.7597597597597598, 7.91991991991992, 2.3343343343343346, 3.5755755755755754, 5.747747747747748, 0.002002002002002179, 5.227227227227227, -1.84984984984985, 0.5725725725725725, 5.817817817817818, -1.85985985985986, 1.3333333333333335, 0.29229229229229237, 4.976976976976977, 2.764764764764765, 6.598598598598599, 7.76976976976977, 3.085085085085085, 7.059059059059059, 3.3753753753753752, 7.44944944944945, 4.116116116116116, 7.33933933933934, -1.009009009009009, 2.5945945945945947, 6.948948948948949, 6.878878878878879, 7.37937937937938, 6.128128128128129, 5.777777777777778, 4.406406406406406, 2.714714714714715, 1.163163163163163, 6.138138138138139, 0.7127127127127126, -1.4294294294294294, 6.748748748748749, 6.528528528528529, -0.7687687687687688, 3.9459459459459456, -0.5585585585585586, 7.099099099099099, 6.038038038038039, 6.278278278278279, -0.41841841841841854, 3.245245245245245, 1.173173173173173, -0.31831831831831825, 4.446446446446447, 1.8238238238238238, 0.7427427427427427, -1.93993993993994, -1.95995995995996, 2.1741741741741745, 0.10210210210210224, 8.0, -1.98998998998999, 2.974974974974975, 3.6756756756756754, 7.40940940940941, -1.3593593593593594, 2.6146146146146148, 1.113113113113113, 5.327327327327327, -0.9989989989989989, 7.63963963963964, 5.897897897897898, 6.368368368368369, 1.3833833833833835, 5.4174174174174174, 1.8938938938938938, -0.9689689689689689, -0.03803803803803807, 1.3433433433433435, 6.318318318318319, -0.6586586586586587, 1.4134134134134135, 2.924924924924925, -0.0780780780780781, -0.5785785785785786, 2.1041041041041044, 0.9829829829829828, 4.546546546546547, 1.0130130130130128, 2.6446446446446448, 3.5155155155155153, -0.7187187187187187, -0.3983983983983985, 1.5635635635635636, 5.407407407407407, 5.107107107107107, 3.9559559559559556, -0.9489489489489489, 5.867867867867868, 4.726726726726727, 7.34934934934935, 0.3523523523523524, 4.146146146146146, -1.149149149149149, 1.6536536536536537, 6.588588588588589, 4.506506506506507, 7.43943943943944, 4.256256256256256, -1.83983983983984, 6.068068068068069, -1.5395395395395395, 4.046046046046046, 3.7257257257257255, 1.4434434434434436, -0.7087087087087087, 0.6926926926926926, 2.4044044044044046, 6.208208208208209, -1.059059059059059, 2.684684684684685, 4.696696696696697, 6.758758758758759, 3.135135135135135, 7.81981981981982, -0.048048048048048075, 6.448448448448449, 3.7357357357357355, 1.3233233233233235, 0.42242242242242245, -1.2092092092092093, 5.677677677677678, 4.576576576576577, -1.5695695695695695, -1.7997997997997999, 0.05205205205205221, 7.2192192192192195, 0.06206206206206222, 6.538538538538539, -1.089089089089089, 0.9229229229229228, -0.4984984984984986, 5.967967967967968, -1.179179179179179, -1.129129129129129, 6.218218218218219, -0.5885885885885886, 4.956956956956957, 0.5625625625625625, 4.656656656656657, 1.3933933933933935, 7.019019019019019, 7.38938938938939, 7.1791791791791795, -1.6096096096096097, 4.466466466466467, 7.039039039039039, -1.159159159159159, 0.3023023023023024, 3.4654654654654653, 5.217217217217217, 7.68968968968969, 5.337337337337337, 1.143143143143143, 1.9239239239239239, 6.118118118118119, 2.5145145145145147, 4.196196196196196, 3.5555555555555554, 2.834834834834835, -0.2782782782782782, -0.028028028028028062, 5.017017017017017, 6.148148148148149, 2.4944944944944947, 1.6136136136136137, 0.012012012012012185, -0.5985985985985987, 6.508508508508509, 1.123123123123123, -1.5195195195195195, 1.6836836836836837, 6.658658658658659, -1.039039039039039, 5.077077077077077, 0.41241241241241244, -1.7197197197197198, 0.3223223223223224, 2.4244244244244246, 6.858858858858859, 6.438438438438439, -1.3993993993993994, -1.87987987987988, 6.308308308308309, 7.71971971971972, -1.7897897897897899, 4.686686686686687, 4.376376376376376, -0.2682682682682682, -1.6196196196196198, 5.5775775775775776, 2.694694694694695, 0.7227227227227226, 1.4934934934934936, 5.347347347347347, -1.3093093093093093, 5.607607607607608, 0.45245245245245247, 7.4994994994995, 1.2932932932932935, 7.85985985985986, 1.8638638638638638, 7.5995995995996, 1.0230230230230228, 6.648648648648649, 5.837837837837838, 5.887887887887888, -0.6986986986986987, 3.8358358358358355, 3.065065065065065, 1.4634634634634636, -0.13813813813813813, 1.5535535535535536, 5.5675675675675675, 2.3843843843843846, 4.796796796796797, 5.237237237237237, 4.226226226226226, -0.6286286286286287, 3.285285285285285, 1.2332332332332334, 5.5175175175175175, 4.036036036036036, 2.5345345345345347, 6.688688688688689, 2.1541541541541545, 6.388388388388389, 4.136136136136136, 0.042042042042042205, 7.32932932932933, 3.295295295295295, 5.797797797797798, 1.2732732732732734, 3.325325325325325, 1.6736736736736737, 2.0040040040040044, 7.82982982982983, -1.5895895895895895, 1.5135135135135136, 7.60960960960961, -0.00800800800800805, 6.098098098098099, 6.498498498498499, 3.8158158158158155, 0.23223223223223233, 2.804804804804805, 5.997997997997998, 3.145145145145145, 6.028028028028029, 5.937937937937938, 1.6936936936936937, 5.257257257257257, -0.17817817817817816, 3.4254254254254253, -0.14814814814814814, 3.315315315315315, -0.6786786786786787, 3.4954954954954953, 4.896896896896897, 0.3423423423423424, 2.854854854854855, 5.947947947947948, -1.8098098098098099, 2.0740740740740744, 7.62962962962963, 3.6056056056056054, 6.348348348348349, 4.076076076076076, 2.0340340340340344, 7.1691691691691695, 2.5045045045045047, 6.778778778778779, -0.8388388388388388, -0.5185185185185186, 5.4774774774774775, 3.5655655655655654, 6.908908908908909, 0.4624624624624625, 0.1721721721721723, -0.8888888888888888, 7.6996996996997, 5.917917917917918, -1.6296296296296298, 1.6636636636636637, -0.8188188188188188, 0.3323323323323324, 5.927927927927928, -1.7697697697697699, 0.5825825825825826, 1.0930930930930929, 4.026026026026026, 3.4054054054054053, 1.7737737737737738, 2.1841841841841845, 3.4754754754754753, 7.3993993993994, 0.07207207207207222, -0.5685685685685686, 4.906906906906907, -1.189189189189189, 3.6956956956956954, 7.129129129129129, -0.7887887887887888, 2.5445445445445447, 2.944944944944945, -1.7397397397397398, 3.005005005005005, 5.5475475475475475, 1.9639639639639639, 5.717717717717718, 6.568568568568569, 3.165165165165165, 3.8958958958958956, 7.35935935935936, 7.1391391391391394, 4.476476476476477, 1.183183183183183, 2.3543543543543546, 0.1821821821821823, 1.3533533533533535, 6.618618618618619, 0.9629629629629628, 4.676676676676677, 5.977977977977978, 7.31931931931932, 5.617617617617618, 0.8428428428428427, -1.7797797797797799, 0.27227227227227235, 5.207207207207207, -1.97997997997998, 2.4544544544544546, 3.4854854854854853, -1.7097097097097098, 2.984984984984985, 2.5745745745745747, -1.94994994994995, 6.298298298298299, 6.058058058058059, 0.8128128128128127, -1.6796796796796798, 2.2142142142142145, 6.458458458458459, 4.276276276276276, -0.43843843843843855, 7.75975975975976, 3.195195195195195, 7.3093093093093096, 7.83983983983984, 3.8658658658658656, 0.5925925925925926, 0.9729729729729728, 4.556556556556557, 3.025025025025025, 2.704704704704705, 3.5355355355355353, 6.968968968968969, -1.8998998998999, 4.916916916916917, -1.92992992992993, 0.08208208208208223, 4.606606606606607, 1.7537537537537538, 3.9859859859859856, 6.338338338338339, 2.1441441441441444, 0.9129129129129128, 7.88988988988989, -1.5095095095095095, 4.866866866866867, 4.096096096096096, 5.277277277277277, 2.4844844844844847, -1.4794794794794794, 1.0830830830830829, 1.153153153153153, 2.884884884884885, 3.9159159159159156, 5.4274274274274275, 1.9139139139139139, 7.48948948948949, 3.015015015015015, -0.8088088088088088, 7.94994994994995, 1.9439439439439439, 7.73973973973974, 4.536536536536537, 3.275275275275275, 4.396396396396396, -1.139139139139139, -0.28828828828828823, 6.288288288288289, -1.7297297297297298, 5.857857857857858, 5.4674674674674675, 3.105105105105105, 5.4874874874874875, 4.626626626626627, 4.936936936936937, 2.914914914914915, 3.7957957957957955, 5.367367367367367, 0.5225225225225225, 3.5255255255255253, 6.928928928928929, 5.727727727727728, -0.6886886886886887, 4.996996996996997, 4.266266266266266, 5.087087087087087, 3.6556556556556554, -0.7487487487487487, -1.90990990990991, -1.2192192192192193, 6.088088088088089, 3.4354354354354353, 2.844844844844845, 7.049049049049049, 3.8758758758758756, -1.4594594594594594, 1.8838838838838838, 4.706706706706707, 1.5235235235235236, 2.874874874874875, 3.6656656656656654, 6.938938938938939, 0.5025025025025025, 2.4344344344344346, 6.408408408408409, -0.34834834834834827, 3.055055055055055, 0.6426426426426426, -1.4894894894894894, -0.9289289289289289, -1.4094094094094094, 6.048048048048049, 6.478478478478479, 2.784784784784785, 3.9759759759759756, 7.36936936936937, 2.994994994994995, -0.18818818818818817, 7.97997997997998, 2.664664664664665, 5.117117117117117, 5.4974974974974975, 5.4374374374374375, -0.018018018018018056, 6.828828828828829, 7.65965965965966, 0.7927927927927927, 1.3133133133133135, 7.2692692692692695, -0.09809809809809811, 3.3453453453453452, 0.22222222222222232, -0.8588588588588588, -0.4884884884884886, 2.6046046046046047, 4.886886886886887, -0.12812812812812813, -1.2692692692692693, 5.297297297297297, 0.40240240240240244, -0.5485485485485486, 3.6256256256256254, 0.5525525525525525, -1.049049049049049, 4.106106106106106, 5.197197197197197, 1.7137137137137137, 1.7037037037037037, 3.155155155155155, 2.1241241241241244, 2.934934934934935, 4.386386386386386, 6.008008008008009, -0.3883883883883883, 7.41941941941942, 2.2642642642642645, 0.43243243243243246, 4.806806806806807, 2.1641641641641645, 1.4334334334334335, 7.54954954954955, 1.2232232232232234, 3.6156156156156154, 4.486486486486487, -1.6996996996996998, 1.7337337337337337, 7.98998998998999, 5.067067067067067, -1.4394394394394394, 2.864864864864865, 3.095095095095095, 2.2242242242242245, 5.137137137137137, -0.5085085085085086, 3.335335335335335, -0.16816816816816815, -2.0, 1.8738738738738738, 3.6456456456456454, -0.7387387387387387, 0.9429429429429428, -1.5595595595595595, 7.46946946946947, 4.766766766766767, 6.728728728728729, 1.133133133133133, 7.51951951951952, 6.238238238238239, 4.066066066066066, -0.9089089089089089, 6.718718718718719, 5.647647647647648, -1.4194194194194194, 1.9339339339339339, 6.188188188188189, 2.5645645645645647, 3.9959959959959956, 1.6036036036036037, 6.978978978978979, 5.097097097097097, 2.3943943943943946, -0.2582582582582582, -0.7787787787787788], \"y\": [-3.6452745734083765, -4.506657642317348, -5.455227691654493, -3.6259661103440575, -5.412915571842073, 1.7969012158250526, 4.331319437556655, -8.715558250260548, 6.367782779339304, -4.1398416576936405, -6.2561292852614505, -10.2461319398968, -9.061200454819298, 0.957671048441135, -10.317836110343055, -4.823194407632048, -3.1267336051408874, -4.997248092078453, 10.433864178597473, -9.24935850437294, -6.1693907470450045, -4.345405669020107, -5.805579369566189, -11.548268626635767, 4.996020218616403, -5.16793753525583, 5.522366401108609, -4.716422905116556, -6.368708826256327, -1.4914746347493921, -5.759085244619953, -8.468190933384241, 6.469976442554433, 2.3285297890606613, -3.804373381200133, -6.303826232841837, 1.3306376895680794, -4.154212615337467, -9.516396580111353, 0.013934149466708678, 7.897814315875275, 1.8544836514599983, -2.4260757468522085, -4.847452333499936, 10.886628290048241, -8.945876559223114, -12.1217886091864, -9.898305261399686, 1.450565993910725, -9.511121437473317, -4.805764190813655, 7.5560800149701866, -8.78625611785608, -10.835126991060706, -8.48400587906672, -4.4378823088154915, -7.696907038363763, -7.2735736599803715, 10.519919231014917, -5.302801711403535, -5.637907609768911, -8.658574510455768, 1.2084978904154515, -5.364005526944633, -5.792130578112314, -11.06311778364712, 3.370699436057243, 3.133993415826708, -9.174175395024173, -9.558441077449636, 3.7612130097457555, 7.742519290031576, -6.379181216434895, -6.07229785606347, 5.579686760991327, -1.144196147017411, -10.342458339014197, 5.038597039200407, -0.6283292183264301, 4.969543096702615, -1.6506058911098629, -7.7655201344220455, 0.5195716833327357, -10.044625152219691, -6.8397773990338875, -4.16755399618074, -11.40604255520373, -3.284581458306205, 10.887452232755718, -2.669922981625776, -4.867020169903874, 4.767142921033679, -6.740620379920504, 5.329336016821781, 5.260025108509433, 4.000035970341659, -9.596026751616355, -6.790098009023257, -5.200855228981597, -5.878827957048844, 10.344549927138306, -7.712859561814483, 2.974424869050148, 8.290688195400712, 5.844470166074709, -1.1561660070758175, -10.226982365748656, 2.385580035145178, -5.21134285920728, 8.95281573168783, 2.9761502571217964, -2.2648915690245763, -6.39966849404622, -4.865837122894565, 2.5250609310430416, -8.616878436381157, -10.27930422943074, -2.716588650485514, 7.0400583365573715, -5.193646677496183, -4.970887973290062, 5.229447925811702, 0.7265966457297051, 1.583546494755262, -2.171534537951091, 4.66511071233753, -4.661431144405003, -1.9957928467954744, 10.785407712441529, 5.30827020218028, -4.159921971468075, -6.679389802320847, -1.9286789838101188, 3.513140154000544, -6.351669576281285, 8.075987158360707, 0.9669783792664914, 0.6982812556569626, -10.338397402328647, -9.385870671297702, -11.869351042678115, -5.744942787304947, -5.5232086845112285, 2.1227720195220074, 6.978940151640332, -4.340366443959809, -4.141978210957384, -5.780166932362148, -10.34489800961226, -3.8735466521160222, -9.93126582289107, 7.628499449653879, -4.192080668084552, -7.904853099777321, 4.132487146126188, -6.990031583289241, 7.8551376844265075, -4.374644413249209, 3.5811534395315645, 5.021621345840529, -7.091204996355662, -7.6078246693287745, 3.3988744166180713, 9.34669784306162, -4.756460681644461, 5.772222902741094, -8.911838940956192, 9.081219206867713, 2.4717068370996693, -4.1203737490308585, 7.966308737265197, 7.102000238727275, -11.755234448699612, -9.993971584774501, -4.849186987629303, 3.2169710606608466, -7.295851255004445, -12.089163466170362, 3.296552276514456, -3.3404609851355844, -8.760834315473982, -0.7179039456662692, 10.870935146657773, -6.470785687696118, -5.42111095662249, 10.29672789328623, -4.154853270688072, 8.460586597342793, -5.487156558700541, 5.122931562008336, -5.038833686785085, -5.420421491129657, 10.216969651732825, -4.210016063435735, -4.411667901921836, -5.210593471072026, -11.408010273020334, -11.920831575030743, 4.478685264489892, 10.751747568430844, -4.394590165840208, -11.333754296597625, -6.593375722957994, -0.7241793000828051, -5.13753005466408, -7.223059997924567, -7.823486326197288, 5.469993984283102, -9.369587070394845, -4.206807989540907, -11.945542325019304, -10.05155080073312, -10.011641214851448, 9.360786656162208, -4.3388540247263165, -10.263570297643966, 4.327341829669365, 1.4585612829539754, -6.447831377259997, 3.911396330578623, 4.059336629423321, 6.040457627427765, -9.640161283408018, -3.03035865176329, 4.277192966093969, 10.73048871938497, 6.522625583374912, -5.334409789305427, -1.484913624950209, 10.33017289739556, -4.6641571959732735, -6.554421077382977, -5.529439915678046, -5.815252676626628, -12.243870122293862, -9.06032750569515, -2.978915700947703, -8.010660068594701, 2.34858185123623, -5.8530678652239985, -10.105770053856764, -3.270262558111895, -9.621579260922486, -8.71374034818652, -2.7488757598000113, 10.852484685861787, -10.198036544674254, -9.251047489851947, 5.128205751418833, -5.312378750878208, 3.832461863654893, -5.167702284674023, 10.48210895404111, 7.489249362483946, -8.24841364568196, -10.075873569139151, -1.4391338973522037, -7.854601174984654, -2.798275247875295, 5.4446849886957525, -6.065656436160612, -12.21429834367169, 2.4279490041933367, -5.082159417825674, -8.451134128661536, 5.072286679519485, -3.8533577258496927, -0.5215763709021658, -6.574372313188264, 4.05209634594584, -5.45062451006647, 4.875697708721963, 3.1367603198581673, -11.793867913874058, 0.41923707982988656, 5.2484172130499385, 6.269802178014638, -2.5085338901425227, -8.329460798567293, -6.6913426986464, 8.700261782081544, -10.74390957287104, -4.868141037916503, 10.650841906599188, 2.7123727378617035, -5.066151300134576, 10.036055370848718, -5.094205286028574, -10.130730882450868, -8.136075555187645, 6.222413296409805, -3.40963927068435, -8.082571164391203, -3.1708153025626418, -11.241465732905873, 3.6470633593330235, 0.7183094015728864, 2.8094981981588774, -9.829306178667146, -4.542923526006987, -9.793323323769677, -7.11213048758283, -4.532117422288148, -5.596834491583287, -9.68555861744299, -12.182309951782615, -3.5121835871814295, -4.080332360216582, -10.65059428254319, -9.189355653328905, -0.4198109369547032, -7.347461735282465, 3.059529944118472, -2.9033587097628804, -4.139666673585298, -5.25252562233524, -4.405547683830409, 1.1078825089698654, 4.24238377375282, -4.1424614168942036, -3.7397000289739224, -8.709846479939516, 7.828590715262258, -4.676788250586616, -5.736464426539731, -6.721169154005559, -4.50416485235933, 2.7392512719695548, -11.479538561493962, 9.671930603246393, 4.987147311443584, 7.228239027135354, 10.936043797714618, -10.31635961222714, -10.079587973569812, -1.3337386664443802, -11.25684313328384, 9.513525429378124, -0.9414638181717343, -9.963163670968031, -9.126068224236976, -7.447599403946139, -4.61137566167058, 0.21667444715532413, -0.1943518662331991, -5.88359987950876, -5.0903686405108175, -2.2998418056484415, -8.13544167953846, 2.123116735834155, -8.607043190704692, 8.894473965089635, -4.493939254539883, -8.252002708435683, -1.0474565831817677, 3.4538532253841696, -9.730946857818525, -5.64684233915722, 2.7260496558344807, 2.8025826057738885, 4.577456668674436, -10.325360014266613, 1.2035994231406981, -10.219663853801432, 10.686926757614064, -6.544678610497276, -2.0842726737023045, -6.886883003542663, -2.0404616881237443, -10.55942440643775, -7.923906450491755, -5.648315015317388, -3.611243243540793, -4.756605354781256, 1.9630243218631453, -5.820396352319115, -5.1189648794444444, -5.977011237045797, -12.15061591580448, -12.23778905827816, -8.409140228479904, 5.807825384808474, 10.390825056284386, -2.342428780446422, -10.296422770247228, -1.5349886184089783, -10.100502115292329, 8.498891992400681, 1.6837136928235923, -5.021501331318337, 4.933332439368401, -8.926788514954989, 9.090861770368068, -3.9896065643059693, -9.463290945926932, 5.26785014754472, 9.821680645015165, -5.615273948736656, 5.782644476295081, -8.830920733668261, 5.089103256183654, -9.86430947157741, -4.995431649885444, 9.022051850753524, -12.012543687274487, -3.740903370326329, 8.81327394441538, -0.3045747108950356, 3.7979198767648183, -5.016230983370076, 0.9144278825861474, -1.906090176539199, -5.823594221735092, -10.02366263149575, -11.614131956323805, -7.561029050081119, 5.882168044182504, -9.059355935852572, 10.922499397069318, -12.101573883250854, 9.979946110609887, -11.98960128021492, -12.230786305354844, -0.08300021548698666, -5.36553786551087, -10.333288180157382, 8.107435843928323, -10.307864994351664, 8.883205739290625, -8.564306669004896, 7.623468641749072, 0.43055824251854546, -8.503299377391981, 10.933332321321153, -12.218266398631224, 2.892288220380445, 4.730748065721046, -5.821110844249599, 6.918673007938847, -9.287322180566802, 5.192730347101995, 4.197963854134772, -4.374694792403274, 9.593795810113859, -4.930859109295625, -7.178051572051032, -9.586931126826506, -8.908035213576497, 7.1647371541400116, -4.308833079308677, -7.871058501255976, -9.340993627910766, -6.685546425869305, -5.710263006044626, -12.244772673632877, 1.2986377480617897, -0.41365406291050655, -7.368036043967623, -8.555276857168487, -12.239629956282675, 2.5571582093064924, -7.244414127386212, -4.768197830600948, 5.287787829259118, 8.248361671077463, 0.8463145125759866, 6.168314052287326, 7.759764226619268, -5.2938346016658935, -9.756389039329914, -9.311129218131782, 0.29365277803803824, -3.3983797322767635, 6.418358093796188, -4.267706612098888, -7.644787089405842, 9.893242573431987, 10.809840134073971, 2.2987745610480106, 10.63350839919309, -9.85369648450435, 3.2662900031194466, -11.556528882826072, -6.785757496254121, 9.864149463217032, 5.609353085140203, -5.125300648682573, -4.2918353858162295, 4.197422974865617, -4.28245231385714, 8.184128300291611, -9.29548008256797, -3.199025908956392, -6.268218794280683, -5.40777759166201, 2.890505119918813, -12.15893613626318, -4.952060706691028, -10.666569183201101, -5.972411529241536, 2.641937801068032, -8.86185634714073, -4.504389100798229, 4.205102954463674, 9.171699219391266, 7.691376616080868, 2.0342600450359956, 10.700072151162914, 10.094382660162976, 3.9856425908123354, -9.839128145926402, -11.847621549179163, -6.113813449098615, 1.763562370637775, 6.547079228507284, -8.020327006867607, 4.834424630084545, -9.940470642428538, -5.474162662436128, 9.682273022814663, -4.908827361300027, -4.933987729553711, 5.704093472343827, -7.193156983578656, -5.360168513167784, 3.772687079077378, 10.559561211297957, 10.720562699963477, 7.357410843616417, -4.176817590109626, 3.6087199419916254, 0.15476537811958613, 3.8365892785425917, -9.718531624432263, 3.4429970669550864, 0.6078495262604091, 5.37336678997507, -9.399670034952871, 5.65278841284036, -5.643856656899113, -4.458057611621113, 0.11446341745397193, 0.8290329553292759, 5.157346995042734, -5.708214749922152, 9.431149153928208, -0.19577400740211237, -4.819506958562505, -4.449685484530726, 10.912040065112459, 5.550941977323491, -8.164999910994766, 2.6085228821881135, -5.619314244966119, 10.597437906524956, -7.461410757767144, -3.743657020973415, 3.0020270895715004, -5.5683003761805985, 3.8902551680659654, 9.159192956586699, 4.5129298977711745, -3.9669112554068846, -10.305690063211918, -9.946027693009059, 2.211287591781181, -12.196869301496102, -8.763519676801652, 10.196628550604173, -9.050346691133855, 3.29622424966683, -0.12880075933083823, 10.870634020189433, 5.960758051934876, -7.214860689706944, -7.555475678574574, 7.423014189482531, -4.2539806100613795, -11.626398728576397, -8.487429478275137, -1.0226874012805889, 4.855382720891809, -2.4650038041090614, 6.630959323291161, -7.997298732114024, 3.6852592601009544, -3.0571074121136768, -11.677062117302384, 8.037280986448712, 10.935850229441066, -4.147481197176742, 5.671536785883562, -0.40549515302337114, 2.219445373615101, 2.6198675142244667, -1.8151607779299195, 10.921405087671591, 6.576291681212377, -4.2306437700682, -7.660290056878594, 8.988798700620645, -8.599887284840468, -10.206115609133608, 10.435507888169434, -6.939744488654514, -0.09177110121609311, -6.275428070166713, -0.9392117792720592, -11.898198111897067, -7.789749202872942, 10.901545159895951, 5.396402962796191, -5.168148492145254, 10.66773277395747, -10.338732319952378, -3.3283329039582323, 10.57134161347766, -2.9662428162775343, -9.190376908721426, -8.317537624403581, -10.052210189897455, -4.637778886891797, -5.793877810350731, -6.208568778234713, -3.4778161168561006, -4.739363167449615, -10.236351454148753, 8.672651670564306, -9.107628063490283, -6.983941785477266, -9.732328775849988, 10.274752782549744, 0.5654457250473572, -10.283529425219443, -2.4903279939285246, -6.048842166738094, -8.188272645449604, -9.426408037532582, 6.800794077937142, -7.399238108093019, -4.9794537790931885, 5.140041765622765, -4.332072829135237, 0.14302508722019924, 0.029464420279539327, 9.922627309447439, 5.3914286176642685, 10.832209798070078, 4.545836799309146, -12.067663868633568, -10.105386556818152, -5.374140396371308, 7.159342607319804, -8.029679844355053, 1.568263662874276, 5.911884322320461, -2.5898128074985363, -5.763731331244422, -4.0497008683738525, 6.859282822184577, 4.150749433069659, 1.9447160260403917, 0.3733610529371285, -3.18143239538952, -2.1357544951524496, -0.6174217853976534, 10.928510122050232, 10.247405129001036, 6.671565780464135, -3.3044767870059584, -9.637089979683441, 8.743072115330262, -10.326316749885335, 4.348766175087199, 2.090914241875538, 6.68661153561834, -8.082308787652714, -5.502540082476148, 0.6192647987760354, -5.5562487941999015, 2.016439926869658, -5.287545445115754, -4.558053234912709, -5.700715483366334, -10.45801517706018, 5.210900068642074, -6.482138760384516, -5.366090819421446, -10.968889573776442, -0.8314475595382479, 2.8704327271051713, -10.92416204522489, 10.029616408201301, 5.3510227940089115, -1.0434372849114877, -4.164203673917198, -2.631171698137088, 8.600451684774983, -10.332702080748282, -7.734734615071362]}, {\"hovertemplate\": \"x: %{x} <br>y: %{y}\", \"mode\": \"markers\", \"name\": \"Test data\", \"type\": \"scatter\", \"x\": [4.986986986986987, 3.7757757757757755, 5.637637637637638, 5.907907907907908, 3.205205205205205, 6.258258258258259, 7.58958958958959, 7.96996996996997, 6.178178178178179, -0.4784784784784786, 3.175175175175175, 1.8438438438438438, 6.818818818818819, 0.4724724724724725, -0.29829829829829824, 7.089089089089089, 3.3653653653653652, 4.366366366366366, 1.0030030030030028, 6.078078078078079, 5.847847847847848, 5.5575575575575575, 0.24224224224224233, 2.2842842842842845, -1.119119119119119, -0.46846846846846857, 4.246246246246246, -0.5285285285285286, 6.708708708708709, 3.9659659659659656, 3.8858858858858856, -1.2792792792792793, 5.787787787787788, 1.5835835835835836, 1.2832832832832834, 2.6546546546546548, 3.265265265265265, 6.558558558558559, 0.3623623623623624, 3.3953953953953953, 5.597597597597598, -1.5295295295295295, 0.7327327327327327, 7.50950950950951, 7.079079079079079, 4.636636636636637, 7.8998998998999, -1.5995995995995997, -1.079079079079079, 5.007007007007007, 2.2442442442442445, 6.518518518518519, 4.746746746746747, -1.2392392392392393, 1.8538538538538538, -0.0880880880880881, -1.3693693693693694, 6.158158158158159, 5.307307307307307, 7.87987987987988, 3.7557557557557555, -0.8988988988988988, -1.109109109109109, 4.526526526526527, 7.2392392392392395, 4.216216216216216, -0.45845845845845856, 7.52952952952953, -0.6686686686686687, 7.119119119119119, -1.82982982982983, 2.794794794794795, 3.7157157157157155, 7.7997997997998, 2.814814814814815, -0.21821821821821819, 1.8138138138138138, 5.5075075075075075, 5.5375375375375375, 2.2542542542542545, 0.44244244244244246, 0.7027027027027026, 1.9039039039039038, 5.047047047047047, -0.8488488488488488, 0.8328328328328327, 1.2532532532532534, 0.8928928928928928, 3.5455455455455454, -0.6186186186186187, 4.946946946946947, 5.377377377377377, -0.3683683683683683, 7.61961961961962, -1.3393393393393394, 2.674674674674675, 1.6236236236236237, 5.317317317317317, 1.8038038038038038, 2.894894894894895, 0.39239239239239243, 1.0430430430430429, 7.64964964964965, -0.9789789789789789, 6.678678678678679, 0.3723723723723724, 0.5325325325325325, 0.15215215215215228, 0.6226226226226226, 4.336336336336336, 6.228228228228229, -1.2992992992992993, 5.357357357357357, -1.6496496496496498, 7.56956956956957, 3.215215215215215, -1.019019019019019, 7.2792792792792795, 3.4154154154154153, 2.1941941941941945, -0.20820820820820818, 4.156156156156156, -0.3583583583583583, 1.2132132132132134, 5.737737737737738, 6.998998998998999, 5.057057057057057, 7.1991991991991995, 3.8058058058058055, 1.4034034034034035, -0.9589589589589589, -0.42842842842842854, -0.44844844844844856, -1.4494494494494494, 3.225225225225225, -1.2592592592592593, 6.918918918918919, 5.707707707707708, 0.3823823823823824, -1.3293293293293293, 1.6336336336336337, 7.47947947947948, 0.14214214214214227, 2.6246246246246248, 4.436436436436437, 5.387387387387387, 5.177177177177177, 2.754754754754755, 4.456456456456457, 3.4454454454454453, 0.21221221221221231, -0.5385385385385386, 5.667667667667668, 4.236236236236236, 7.2292292292292295, 5.957957957957958, -1.3493493493493494, 3.255255255255255, 0.9029029029029028, 2.0540540540540544, 0.8528528528528527, 4.316316316316316, 3.7457457457457455, 6.168168168168169, 7.069069069069069, 0.8028028028028027, 0.5425425425425425, 4.296296296296296, 2.5845845845845847, 1.5735735735735736, 3.9359359359359356, 2.724724724724725, 6.808808808808809, 1.7237237237237237, 2.0640640640640644, 2.2942942942942945, 0.9929929929929928, 0.9329329329329328, 3.7657657657657655, 0.11211211211211225, -1.2292292292292293, 3.125125125125125, 2.4144144144144146, 3.045045045045045, 5.127127127127127, -0.3783783783783783, 5.167167167167167, 1.3033033033033035, 4.846846846846847, 6.488488488488489, 1.0330330330330328, 4.776776776776777, 1.193193193193193, 2.0240240240240244, 5.877877877877878, -0.7287287287287287, 7.2492492492492495, 4.326326326326326, 6.698698698698699, -1.88988988988989, 4.086086086086086, 3.115115115115115, 4.416416416416417, -0.8788788788788788, 3.3553553553553552, 1.7437437437437437, 5.587587587587588, 2.5545545545545547, 5.807807807807808, -0.11811811811811812, 3.9259259259259256, 6.248248248248249, 2.964964964964965, 4.646646646646647, -1.6396396396396398, 7.92992992992993, 4.286286286286286, -0.7587587587587588, 1.5335335335335336, 0.6526526526526526, 6.018018018018019, 0.6026026026026026, 0.25225225225225234, 1.5435435435435436, 1.2432432432432434, 1.7937937937937938, -1.3793793793793794, 3.6356356356356354, 7.90990990990991, 6.848848848848849, -1.6696696696696698, 1.993993993993994, 1.0630630630630629, 6.868868868868869, -1.86986986986987, 5.987987987987988, 0.28228228228228236, 2.3743743743743746, -0.06806806806806809, 4.356356356356356, 1.983983983983984, 5.147147147147147, 4.126126126126126, 5.687687687687688, -0.10810810810810811, 4.426426426426427, 6.838838838838839, 2.2042042042042045, 2.4644644644644647, -1.6596596596596598, 0.09209209209209224, 2.5245245245245247, 3.185185185185185, 7.66966966966967, 7.2592592592592595, 4.966966966966967, 2.0840840840840844, -0.19819819819819817, -0.6386386386386387, 4.186186186186186, 4.496496496496497, 7.109109109109109, 0.8628628628628627, 1.4734734734734736, 6.548548548548549, 2.0140140140140144, 7.70970970970971, -1.5795795795795795, 2.824824824824825, -0.05805805805805808, 7.72972972972973, 5.627627627627628, 5.757757757757758, -0.7987987987987988, 3.235235235235235, -0.9389389389389389, -0.8688688688688688, 2.2742742742742745, 3.8558558558558556, 1.0730730730730729, 5.267267267267267, 7.45945945945946, 0.8728728728728727, 2.734734734734735, 6.268268268268269, -1.099099099099099, 6.428428428428429, 0.2022022022022023, 2.744744744744745, 3.3853853853853852, 0.8828828828828827, 2.2342342342342345, 7.1491491491491495, 3.7857857857857855, 1.3733733733733735, -1.4694694694694694, 0.8228228228228227, 4.566566566566567, 7.95995995995996, -1.4994994994994995], \"y\": [-0.8694850850069475, 6.043648791687345, -10.871355000924336, -12.246192482331399, 9.61967218672025, -10.258093071227792, -5.814360217492217, -3.471024372554849, -11.01093175458699, -9.777393007858722, 9.804563513026999, 1.7093069348494034, -4.691118049841552, -7.818257966060714, -10.293341344262984, -4.190562154693, 8.531358577991192, 4.607840870283129, -5.2161683207860285, -11.736994808764988, -12.175642774910964, -9.978491136849224, -9.010615570456242, 7.396441813518818, -4.193592487842789, -9.820744689480492, 4.895424476907289, -9.535091133842874, -5.446929315051358, 5.420164940188773, 5.6399629463381755, -2.377495173386338, -11.968567807978783, -1.24373473250397, -3.6765337893770025, 10.527937958372647, 9.22699271515926, -6.893393055654519, -8.398803977121258, 8.31903948592642, -10.449186100361784, 0.31826860017649905, -6.496165820013618, -5.675484275293268, -4.1764487396953935, 3.219550262683585, -4.349846269340639, 1.0114906091126674, -4.644442826781849, -1.177457126616284, 6.9175869049400145, -7.3349128135382715, 2.2268033985863003, -2.8299689782652266, 1.835810554325292, -10.217865260895032, -1.3749119426316847, -11.177351190291192, -6.218170468048917, -4.561917882323138, 6.130867111923216, -6.584270507404513, -4.3066628080653935, 3.946101290129342, -4.618759679676819, 4.9516227835833995, -9.862378024977327, -5.723903118459248, -8.639750516607812, -4.245446248175236, 3.055913631112543, 10.91322377130758, 6.31826113499759, -5.237458288251825, 10.929384846624243, -10.34573028052846, 1.3343823487853694, -9.328049889011861, -9.726185402527692, 7.039022502022695, -7.9767858036888795, -6.642262310487838, 2.4782869047971854, -1.8110715794995116, -7.081498077783479, -6.019465516013725, -3.866965670002368, -5.737747351039019, 7.292474479983277, -8.994768322000299, -0.27339717803967556, -7.376951487217433, -10.159210836113736, -5.823862231515073, -1.706083073921177, 10.612311808024073, -0.838282530174549, -6.386725810864032, 1.2110397258446128, 10.900439274468523, -8.241044674368224, -5.020581642703672, -5.806202595171439, -5.745793839668446, -5.702667865789349, -8.346331257347012, -7.503254583068439, -9.430082632249466, -7.040519227045238, 4.692101895192162, -10.555267051408146, -2.15253972640026, -7.051475737690883, 1.4866669507080115, -5.794149218859424, 9.556170562974263, -5.310883042660767, -4.781002441961843, 8.177811471348246, 6.295403280434818, -10.344969753918493, 5.055468275828595, -10.183526533476522, -4.109008788147777, -11.692643575916412, -4.146814977712665, -1.972809168822312, -4.471614422295899, 5.920928172153827, -2.8266833322303846, -5.959698960772354, -9.976922511615763, -9.902288051394937, -0.5111892053997501, 9.491820639832925, -2.6033669927114618, -4.2800402265058475, -11.483065512477527, -8.293737752398362, -1.8172126205522456, -0.7339013220950003, -5.588589418394416, -9.473600844244842, 10.383214666581562, 4.36742912925174, -7.53766212509899, -3.9880182443807684, 10.851985551408205, 4.285675226303332, 7.9673922938484045, -9.155404417542508, -9.4815798792865, -11.1539917301664, 4.914617830373074, -4.580321647774252, -12.202097601892001, -1.595310279499198, 9.294208239085407, -5.6908213622884976, 4.464600522847288, -5.925418626592335, 4.743064003681909, 6.17610023983028, -11.095354719575866, -4.164526972407357, -6.161133990929321, -7.4511720903920695, 4.790359554092671, 10.156840237723788, -1.3420484733910423, 5.496121384368668, 10.78048069782859, -4.747121149746523, 0.25766370146755135, 4.597759431823088, 7.513124626528885, -5.264386187927086, -5.549735573646058, 6.086716364240486, -9.599705564068548, -2.9434835293972257, 10.090905083758708, 8.798282254755511, 10.478553850840655, -3.135001420876215, -10.133165052016768, -3.81644248191673, -3.545010820069881, 1.084167873249493, -7.675109929590075, -5.069932595927254, 1.9078134964848836, -4.225360440378891, 4.064580996111949, -12.227935487308233, -8.166021788078188, -4.658143473886788, 4.718068380690924, -5.530008643333332, 3.5315871598050066, 5.17489443066572, 10.144445776044455, 4.4430529209754335, -6.7859366684377775, 8.602063631962231, 0.4900968101765919, -10.33591252517552, 9.962562322047196, -12.052745814591157, -10.26920554810139, 5.523095201303375, -10.358927248723473, 10.758945766954538, 3.1406580309416343, 1.392994428623144, -3.999124144320276, 4.812768531166325, -7.9110162168051685, -1.7230017772274842, -6.889659518378907, -12.030324478498246, -7.142085620495041, -8.961434242238193, -1.6296113266840813, -3.9287025087918632, 1.0885624168079528, -1.2653235027519267, 6.743229800913996, -4.237283713894041, -4.539173140446831, 1.6719521200432987, 3.66473349085752, -4.920827313707345, -4.451434654254989, 3.375510673383772, -12.132011472648724, -8.811512808310493, 8.395623592379014, -10.17688862614946, 4.637041376814851, 3.53170351733967, -3.4745934024417426, 5.10596834359817, -11.325496376707742, -10.253472311623915, 4.405983665654244, -4.587124619250819, 6.421675057154756, 9.260203495140903, 1.579653333282303, -9.679779519926786, 9.747901308751963, 9.743920800540291, -5.778197968948188, -4.69837714719004, -0.5679958909887011, 4.86351793982508, -10.342631755202705, -8.857278422148063, 5.0044890051290425, 4.102320903939326, -4.225124254390895, -5.878471775825825, -2.2575842995171604, -7.002212539914661, 3.931217598871703, -5.680411261194252, 0.8166990503924698, 10.933894399996719, -10.15444531363461, -5.6094923363407165, -10.770564138310826, -11.81414490018536, -7.553670186988011, 9.42667507648131, -6.170909657861748, -6.88541159331777, 7.278498494801697, 5.737650285647962, -4.8703751904117745, -5.5370892460822985, -5.522401326975663, -5.831555280252663, 10.806762925865455, -10.155603587656678, -4.419514230455329, -8.36801185147039, -9.202657225799813, 10.830596594733443, 8.389797242873492, -5.7846527961748455, 6.795084878119327, -4.31776006625158, 6.0016635295249, -3.053368888494635, -0.3003526223682337, -6.0665949379862205, 3.71089800401729, -3.6094750054913023, 0.011644904169876624]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Funzione di stimare\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('da03f025-6b9d-4a7e-b1bf-8d315ba1a210');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qZnYsQIfmCm"
      },
      "source": [
        "# Tensor Dataset Che converte i dati da numpy a Pytorch\r\n",
        "class CustomTensorDataset(Dataset):\r\n",
        "    def __init__(self, x,y,mean,std):\r\n",
        "        x = (x - mean)/std\r\n",
        "        self.x = torch.from_numpy(x).type(torch.float32).unsqueeze(1)\r\n",
        "        self.y = torch.from_numpy(y).type(torch.float32).unsqueeze(1)\r\n",
        "    def __getitem__(self, index):\r\n",
        "        x = self.x[index]\r\n",
        "        y = self.y[index]\r\n",
        "        return x, y\r\n",
        "    def __len__(self):\r\n",
        "        return self.x.shape[0]\r\n",
        "\r\n",
        "# Dataset generator creation\r\n",
        "R_train_ds = CustomTensorDataset(R_X_train,R_Y_train,R_mean,R_std)\r\n",
        "R_test_ds = CustomTensorDataset(R_X_test,R_Y_test,R_mean,R_std)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIi1CUhQfnzh"
      },
      "source": [
        "## Training\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HQp0vM5fYf-"
      },
      "source": [
        "# Validation: Metric Regression\r\n",
        "def metrics_func_regression(target, output):\r\n",
        "  # Comptue mean squaer error (Migliora quanto piu' ci avviciniamo a zero)\r\n",
        "  mse = torch.sum((output - target) ** 2)\r\n",
        "  return mse\r\n",
        "\r\n",
        "# Validation: Metric cassification\r\n",
        "def metrics_func_classification(target, output):\r\n",
        "  # Compute number of correct prediction\r\n",
        "  pred = output.argmax(dim=-1,keepdim=True)\r\n",
        "  corrects =pred.eq(target.reshape(pred.shape)).sum().item()\r\n",
        "  return -corrects # minus for coeherence with best result is the most negative one\r\n",
        "\r\n",
        "# Training: Loss calculation and backward step\r\n",
        "def loss_batch(loss_func,metric_func, xb,yb,yb_h, opt=None):\r\n",
        "  # obtain loss\r\n",
        "  loss = loss_func(yb_h, yb)\r\n",
        "  # obtain permormance metric \r\n",
        "  metric_b = metric_func(yb,yb_h)\r\n",
        "  if opt is not None:\r\n",
        "    loss.backward()\r\n",
        "    opt.step()\r\n",
        "    opt.zero_grad()\r\n",
        "  return loss.item(), metric_b\r\n",
        "\r\n",
        "# Trainig: Function 1 epoch\r\n",
        "def loss_epoch(model, loss_func,metric_func, dataset_dl, opt, device):\r\n",
        "  loss = 0.0\r\n",
        "  metric = 0.0\r\n",
        "  len_data = len(dataset_dl.dataset)\r\n",
        "  # Get batch data\r\n",
        "  for xb,yb in dataset_dl:    \r\n",
        "    # Send to cuda the data (batch size)\r\n",
        "    xb = xb.to(device)\r\n",
        "    yb = yb.to(device)\r\n",
        "    # obtain model output \r\n",
        "    yb_h = model.forward(xb)\r\n",
        "    # Loss and Metric Calculation\r\n",
        "    loss_b, metric_b = loss_batch(loss_func,metric_func, xb,yb,yb_h,opt)\r\n",
        "    loss += loss_b\r\n",
        "    if metric_b is not None:\r\n",
        "      metric+=metric_b \r\n",
        "  loss /=len_data\r\n",
        "  metric /=len_data\r\n",
        "  return loss, metric\r\n",
        "\r\n",
        "# Training: Iterate on epochs\r\n",
        "def train_val(epochs, model, loss_func, metric_func, opt, train_dl,test_dl,device, path2weigths=\"./weights.pt\"):\r\n",
        "  lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.999) #  lr = lr * gamma ** last_epoch\r\n",
        "  best_val_metric = 1000000\r\n",
        "  for epoch in range(epochs):\r\n",
        "    model.train()\r\n",
        "    train_loss,train_metric = loss_epoch(model, loss_func, metric_func,train_dl, opt,device)\r\n",
        "    lr_scheduler.step()\r\n",
        "    model.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "      val_loss, val_metric = loss_epoch(model, loss_func, metric_func, test_dl,opt=None,device=device)\r\n",
        "      print(\"epoch: %d, train_loss: %.6f, val loss: %.6f,  train_metric: %.3f test_metric: %.3f lr: %.5f)\" % (epoch,train_loss, val_loss,train_metric,val_metric,opt.param_groups[0]['lr']))\r\n",
        "      if (val_metric <= best_val_metric):        \r\n",
        "        # Save Models (It save last weights)\r\n",
        "        torch.save(model.state_dict(),path2weigths)\r\n",
        "        best_val_metric = val_metric\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QakBtSSMfaNd",
        "outputId": "1a874db8-8da1-4af7-d8d8-93b68a093483"
      },
      "source": [
        "class RegressionNet(nn.Module):\r\n",
        "    def __init__(self,num_inputs):\r\n",
        "        super(RegressionNet,self).__init__()\r\n",
        "        self.fc1 = nn.Linear(num_inputs,100)\r\n",
        "        self.fc2 = nn.Linear(100,50)\r\n",
        "        self.fc3 = nn.Linear(50,1)\r\n",
        "    def forward(self,x):\r\n",
        "        # torch.sigmoid, torch.tanh, torch.relu\r\n",
        "        x = torch.tanh(self.fc1(x)) \r\n",
        "        x = torch.tanh(self.fc2(x))\r\n",
        "        x = self.fc3(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "# Setup GPU Device\r\n",
        "device = torch.device(\"cpu\")\r\n",
        "if torch.cuda.is_available():\r\n",
        "    device = torch.device(\"cuda:0\")\r\n",
        "\r\n",
        "# Regression\r\n",
        "R_model = RegressionNet(num_inputs=1).to(device)\r\n",
        "R_loss_func = nn.MSELoss(reduction=\"sum\") \r\n",
        "R_opt = optim.Adam(R_model.parameters(),lr=0.005)\r\n",
        "R_train_dl = DataLoader(R_train_ds,batch_size=100,shuffle=True)\r\n",
        "R_test_dl = DataLoader(R_test_ds,batch_size=50,shuffle=True)\r\n",
        "\r\n",
        "# Regression\r\n",
        "train_val(2000,R_model,R_loss_func,metrics_func_regression,R_opt, R_train_dl,R_test_dl,device,path2weigths=\"./weights_regression.pt\")\r\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, train_loss: 47.123795, val loss: 46.170017,  train_metric: 47.124 test_metric: 46.170 lr: 0.00500)\n",
            "epoch: 1, train_loss: 44.748514, val loss: 44.297085,  train_metric: 44.749 test_metric: 44.297 lr: 0.00499)\n",
            "epoch: 2, train_loss: 42.754157, val loss: 41.411383,  train_metric: 42.754 test_metric: 41.411 lr: 0.00499)\n",
            "epoch: 3, train_loss: 39.174116, val loss: 36.679178,  train_metric: 39.174 test_metric: 36.679 lr: 0.00498)\n",
            "epoch: 4, train_loss: 34.326625, val loss: 30.778174,  train_metric: 34.327 test_metric: 30.778 lr: 0.00498)\n",
            "epoch: 5, train_loss: 28.257298, val loss: 24.004098,  train_metric: 28.257 test_metric: 24.004 lr: 0.00497)\n",
            "epoch: 6, train_loss: 22.149866, val loss: 18.480194,  train_metric: 22.150 test_metric: 18.480 lr: 0.00497)\n",
            "epoch: 7, train_loss: 17.934423, val loss: 14.489844,  train_metric: 17.934 test_metric: 14.490 lr: 0.00496)\n",
            "epoch: 8, train_loss: 15.173620, val loss: 12.198866,  train_metric: 15.174 test_metric: 12.199 lr: 0.00496)\n",
            "epoch: 9, train_loss: 13.156706, val loss: 10.428604,  train_metric: 13.157 test_metric: 10.429 lr: 0.00495)\n",
            "epoch: 10, train_loss: 11.546088, val loss: 9.335410,  train_metric: 11.546 test_metric: 9.335 lr: 0.00495)\n",
            "epoch: 11, train_loss: 10.596225, val loss: 8.749639,  train_metric: 10.596 test_metric: 8.750 lr: 0.00494)\n",
            "epoch: 12, train_loss: 10.114870, val loss: 8.395141,  train_metric: 10.115 test_metric: 8.395 lr: 0.00494)\n",
            "epoch: 13, train_loss: 9.774037, val loss: 8.224794,  train_metric: 9.774 test_metric: 8.225 lr: 0.00493)\n",
            "epoch: 14, train_loss: 9.568529, val loss: 7.829792,  train_metric: 9.569 test_metric: 7.830 lr: 0.00493)\n",
            "epoch: 15, train_loss: 9.225174, val loss: 7.595232,  train_metric: 9.225 test_metric: 7.595 lr: 0.00492)\n",
            "epoch: 16, train_loss: 8.970172, val loss: 7.363236,  train_metric: 8.970 test_metric: 7.363 lr: 0.00492)\n",
            "epoch: 17, train_loss: 8.684455, val loss: 7.134290,  train_metric: 8.684 test_metric: 7.134 lr: 0.00491)\n",
            "epoch: 18, train_loss: 8.365783, val loss: 6.941070,  train_metric: 8.366 test_metric: 6.941 lr: 0.00491)\n",
            "epoch: 19, train_loss: 8.073668, val loss: 6.637891,  train_metric: 8.074 test_metric: 6.638 lr: 0.00490)\n",
            "epoch: 20, train_loss: 7.755816, val loss: 6.396682,  train_metric: 7.756 test_metric: 6.397 lr: 0.00490)\n",
            "epoch: 21, train_loss: 7.459980, val loss: 6.129802,  train_metric: 7.460 test_metric: 6.130 lr: 0.00489)\n",
            "epoch: 22, train_loss: 7.108350, val loss: 5.742435,  train_metric: 7.108 test_metric: 5.742 lr: 0.00489)\n",
            "epoch: 23, train_loss: 6.727864, val loss: 5.423395,  train_metric: 6.728 test_metric: 5.423 lr: 0.00488)\n",
            "epoch: 24, train_loss: 6.381482, val loss: 5.243465,  train_metric: 6.381 test_metric: 5.243 lr: 0.00488)\n",
            "epoch: 25, train_loss: 5.914474, val loss: 4.841574,  train_metric: 5.914 test_metric: 4.842 lr: 0.00487)\n",
            "epoch: 26, train_loss: 5.498317, val loss: 4.497499,  train_metric: 5.498 test_metric: 4.497 lr: 0.00487)\n",
            "epoch: 27, train_loss: 5.088124, val loss: 4.161277,  train_metric: 5.088 test_metric: 4.161 lr: 0.00486)\n",
            "epoch: 28, train_loss: 4.729924, val loss: 3.846344,  train_metric: 4.730 test_metric: 3.846 lr: 0.00486)\n",
            "epoch: 29, train_loss: 4.336821, val loss: 3.548797,  train_metric: 4.337 test_metric: 3.549 lr: 0.00485)\n",
            "epoch: 30, train_loss: 4.002162, val loss: 3.285836,  train_metric: 4.002 test_metric: 3.286 lr: 0.00485)\n",
            "epoch: 31, train_loss: 3.674243, val loss: 2.989988,  train_metric: 3.674 test_metric: 2.990 lr: 0.00484)\n",
            "epoch: 32, train_loss: 3.359225, val loss: 2.767632,  train_metric: 3.359 test_metric: 2.768 lr: 0.00484)\n",
            "epoch: 33, train_loss: 3.055774, val loss: 2.519414,  train_metric: 3.056 test_metric: 2.519 lr: 0.00483)\n",
            "epoch: 34, train_loss: 2.804976, val loss: 2.347163,  train_metric: 2.805 test_metric: 2.347 lr: 0.00483)\n",
            "epoch: 35, train_loss: 2.568612, val loss: 2.134254,  train_metric: 2.569 test_metric: 2.134 lr: 0.00482)\n",
            "epoch: 36, train_loss: 2.360533, val loss: 1.973502,  train_metric: 2.361 test_metric: 1.974 lr: 0.00482)\n",
            "epoch: 37, train_loss: 2.167810, val loss: 1.815690,  train_metric: 2.168 test_metric: 1.816 lr: 0.00481)\n",
            "epoch: 38, train_loss: 1.989358, val loss: 1.673581,  train_metric: 1.989 test_metric: 1.674 lr: 0.00481)\n",
            "epoch: 39, train_loss: 1.824478, val loss: 1.546588,  train_metric: 1.824 test_metric: 1.547 lr: 0.00480)\n",
            "epoch: 40, train_loss: 1.682372, val loss: 1.442822,  train_metric: 1.682 test_metric: 1.443 lr: 0.00480)\n",
            "epoch: 41, train_loss: 1.552944, val loss: 1.342584,  train_metric: 1.553 test_metric: 1.343 lr: 0.00479)\n",
            "epoch: 42, train_loss: 1.439829, val loss: 1.238918,  train_metric: 1.440 test_metric: 1.239 lr: 0.00479)\n",
            "epoch: 43, train_loss: 1.332212, val loss: 1.156397,  train_metric: 1.332 test_metric: 1.156 lr: 0.00478)\n",
            "epoch: 44, train_loss: 1.239517, val loss: 1.071481,  train_metric: 1.240 test_metric: 1.071 lr: 0.00478)\n",
            "epoch: 45, train_loss: 1.138923, val loss: 1.022930,  train_metric: 1.139 test_metric: 1.023 lr: 0.00478)\n",
            "epoch: 46, train_loss: 1.068753, val loss: 0.941666,  train_metric: 1.069 test_metric: 0.942 lr: 0.00477)\n",
            "epoch: 47, train_loss: 1.000500, val loss: 0.881525,  train_metric: 1.001 test_metric: 0.882 lr: 0.00477)\n",
            "epoch: 48, train_loss: 0.929549, val loss: 0.810124,  train_metric: 0.930 test_metric: 0.810 lr: 0.00476)\n",
            "epoch: 49, train_loss: 0.874510, val loss: 0.789778,  train_metric: 0.875 test_metric: 0.790 lr: 0.00476)\n",
            "epoch: 50, train_loss: 0.814973, val loss: 0.737536,  train_metric: 0.815 test_metric: 0.738 lr: 0.00475)\n",
            "epoch: 51, train_loss: 0.760202, val loss: 0.671611,  train_metric: 0.760 test_metric: 0.672 lr: 0.00475)\n",
            "epoch: 52, train_loss: 0.707936, val loss: 0.632905,  train_metric: 0.708 test_metric: 0.633 lr: 0.00474)\n",
            "epoch: 53, train_loss: 0.664626, val loss: 0.592598,  train_metric: 0.665 test_metric: 0.593 lr: 0.00474)\n",
            "epoch: 54, train_loss: 0.632625, val loss: 0.567188,  train_metric: 0.633 test_metric: 0.567 lr: 0.00473)\n",
            "epoch: 55, train_loss: 0.587138, val loss: 0.530392,  train_metric: 0.587 test_metric: 0.530 lr: 0.00473)\n",
            "epoch: 56, train_loss: 0.551459, val loss: 0.537003,  train_metric: 0.551 test_metric: 0.537 lr: 0.00472)\n",
            "epoch: 57, train_loss: 0.538283, val loss: 0.467328,  train_metric: 0.538 test_metric: 0.467 lr: 0.00472)\n",
            "epoch: 58, train_loss: 0.497248, val loss: 0.462828,  train_metric: 0.497 test_metric: 0.463 lr: 0.00471)\n",
            "epoch: 59, train_loss: 0.461926, val loss: 0.413384,  train_metric: 0.462 test_metric: 0.413 lr: 0.00471)\n",
            "epoch: 60, train_loss: 0.430414, val loss: 0.399976,  train_metric: 0.430 test_metric: 0.400 lr: 0.00470)\n",
            "epoch: 61, train_loss: 0.406202, val loss: 0.382271,  train_metric: 0.406 test_metric: 0.382 lr: 0.00470)\n",
            "epoch: 62, train_loss: 0.386283, val loss: 0.353012,  train_metric: 0.386 test_metric: 0.353 lr: 0.00469)\n",
            "epoch: 63, train_loss: 0.364702, val loss: 0.332348,  train_metric: 0.365 test_metric: 0.332 lr: 0.00469)\n",
            "epoch: 64, train_loss: 0.346640, val loss: 0.312751,  train_metric: 0.347 test_metric: 0.313 lr: 0.00469)\n",
            "epoch: 65, train_loss: 0.322789, val loss: 0.295875,  train_metric: 0.323 test_metric: 0.296 lr: 0.00468)\n",
            "epoch: 66, train_loss: 0.304369, val loss: 0.290258,  train_metric: 0.304 test_metric: 0.290 lr: 0.00468)\n",
            "epoch: 67, train_loss: 0.292745, val loss: 0.275576,  train_metric: 0.293 test_metric: 0.276 lr: 0.00467)\n",
            "epoch: 68, train_loss: 0.276319, val loss: 0.257146,  train_metric: 0.276 test_metric: 0.257 lr: 0.00467)\n",
            "epoch: 69, train_loss: 0.260564, val loss: 0.241675,  train_metric: 0.261 test_metric: 0.242 lr: 0.00466)\n",
            "epoch: 70, train_loss: 0.247172, val loss: 0.229370,  train_metric: 0.247 test_metric: 0.229 lr: 0.00466)\n",
            "epoch: 71, train_loss: 0.231690, val loss: 0.226130,  train_metric: 0.232 test_metric: 0.226 lr: 0.00465)\n",
            "epoch: 72, train_loss: 0.221337, val loss: 0.209851,  train_metric: 0.221 test_metric: 0.210 lr: 0.00465)\n",
            "epoch: 73, train_loss: 0.211562, val loss: 0.202009,  train_metric: 0.212 test_metric: 0.202 lr: 0.00464)\n",
            "epoch: 74, train_loss: 0.202224, val loss: 0.192216,  train_metric: 0.202 test_metric: 0.192 lr: 0.00464)\n",
            "epoch: 75, train_loss: 0.190863, val loss: 0.183950,  train_metric: 0.191 test_metric: 0.184 lr: 0.00463)\n",
            "epoch: 76, train_loss: 0.184327, val loss: 0.174987,  train_metric: 0.184 test_metric: 0.175 lr: 0.00463)\n",
            "epoch: 77, train_loss: 0.177104, val loss: 0.171647,  train_metric: 0.177 test_metric: 0.172 lr: 0.00462)\n",
            "epoch: 78, train_loss: 0.169319, val loss: 0.162562,  train_metric: 0.169 test_metric: 0.163 lr: 0.00462)\n",
            "epoch: 79, train_loss: 0.162640, val loss: 0.155440,  train_metric: 0.163 test_metric: 0.155 lr: 0.00462)\n",
            "epoch: 80, train_loss: 0.155420, val loss: 0.151036,  train_metric: 0.155 test_metric: 0.151 lr: 0.00461)\n",
            "epoch: 81, train_loss: 0.149780, val loss: 0.144913,  train_metric: 0.150 test_metric: 0.145 lr: 0.00461)\n",
            "epoch: 82, train_loss: 0.144030, val loss: 0.140577,  train_metric: 0.144 test_metric: 0.141 lr: 0.00460)\n",
            "epoch: 83, train_loss: 0.139338, val loss: 0.137322,  train_metric: 0.139 test_metric: 0.137 lr: 0.00460)\n",
            "epoch: 84, train_loss: 0.135245, val loss: 0.133944,  train_metric: 0.135 test_metric: 0.134 lr: 0.00459)\n",
            "epoch: 85, train_loss: 0.130918, val loss: 0.127927,  train_metric: 0.131 test_metric: 0.128 lr: 0.00459)\n",
            "epoch: 86, train_loss: 0.126243, val loss: 0.125114,  train_metric: 0.126 test_metric: 0.125 lr: 0.00458)\n",
            "epoch: 87, train_loss: 0.123135, val loss: 0.120772,  train_metric: 0.123 test_metric: 0.121 lr: 0.00458)\n",
            "epoch: 88, train_loss: 0.119258, val loss: 0.117151,  train_metric: 0.119 test_metric: 0.117 lr: 0.00457)\n",
            "epoch: 89, train_loss: 0.115655, val loss: 0.114948,  train_metric: 0.116 test_metric: 0.115 lr: 0.00457)\n",
            "epoch: 90, train_loss: 0.112787, val loss: 0.112206,  train_metric: 0.113 test_metric: 0.112 lr: 0.00456)\n",
            "epoch: 91, train_loss: 0.109983, val loss: 0.110177,  train_metric: 0.110 test_metric: 0.110 lr: 0.00456)\n",
            "epoch: 92, train_loss: 0.108208, val loss: 0.110286,  train_metric: 0.108 test_metric: 0.110 lr: 0.00456)\n",
            "epoch: 93, train_loss: 0.104980, val loss: 0.105181,  train_metric: 0.105 test_metric: 0.105 lr: 0.00455)\n",
            "epoch: 94, train_loss: 0.102506, val loss: 0.104337,  train_metric: 0.103 test_metric: 0.104 lr: 0.00455)\n",
            "epoch: 95, train_loss: 0.101538, val loss: 0.100545,  train_metric: 0.102 test_metric: 0.101 lr: 0.00454)\n",
            "epoch: 96, train_loss: 0.098684, val loss: 0.099828,  train_metric: 0.099 test_metric: 0.100 lr: 0.00454)\n",
            "epoch: 97, train_loss: 0.097005, val loss: 0.098022,  train_metric: 0.097 test_metric: 0.098 lr: 0.00453)\n",
            "epoch: 98, train_loss: 0.094431, val loss: 0.095897,  train_metric: 0.094 test_metric: 0.096 lr: 0.00453)\n",
            "epoch: 99, train_loss: 0.092923, val loss: 0.094545,  train_metric: 0.093 test_metric: 0.095 lr: 0.00452)\n",
            "epoch: 100, train_loss: 0.091371, val loss: 0.092459,  train_metric: 0.091 test_metric: 0.092 lr: 0.00452)\n",
            "epoch: 101, train_loss: 0.089426, val loss: 0.090510,  train_metric: 0.089 test_metric: 0.091 lr: 0.00451)\n",
            "epoch: 102, train_loss: 0.088193, val loss: 0.090107,  train_metric: 0.088 test_metric: 0.090 lr: 0.00451)\n",
            "epoch: 103, train_loss: 0.087567, val loss: 0.088398,  train_metric: 0.088 test_metric: 0.088 lr: 0.00451)\n",
            "epoch: 104, train_loss: 0.087907, val loss: 0.091262,  train_metric: 0.088 test_metric: 0.091 lr: 0.00450)\n",
            "epoch: 105, train_loss: 0.087102, val loss: 0.086056,  train_metric: 0.087 test_metric: 0.086 lr: 0.00450)\n",
            "epoch: 106, train_loss: 0.084182, val loss: 0.085915,  train_metric: 0.084 test_metric: 0.086 lr: 0.00449)\n",
            "epoch: 107, train_loss: 0.083165, val loss: 0.085595,  train_metric: 0.083 test_metric: 0.086 lr: 0.00449)\n",
            "epoch: 108, train_loss: 0.083320, val loss: 0.084424,  train_metric: 0.083 test_metric: 0.084 lr: 0.00448)\n",
            "epoch: 109, train_loss: 0.081996, val loss: 0.082564,  train_metric: 0.082 test_metric: 0.083 lr: 0.00448)\n",
            "epoch: 110, train_loss: 0.080680, val loss: 0.083086,  train_metric: 0.081 test_metric: 0.083 lr: 0.00447)\n",
            "epoch: 111, train_loss: 0.080152, val loss: 0.083322,  train_metric: 0.080 test_metric: 0.083 lr: 0.00447)\n",
            "epoch: 112, train_loss: 0.079711, val loss: 0.080842,  train_metric: 0.080 test_metric: 0.081 lr: 0.00447)\n",
            "epoch: 113, train_loss: 0.079024, val loss: 0.079610,  train_metric: 0.079 test_metric: 0.080 lr: 0.00446)\n",
            "epoch: 114, train_loss: 0.078106, val loss: 0.080076,  train_metric: 0.078 test_metric: 0.080 lr: 0.00446)\n",
            "epoch: 115, train_loss: 0.078190, val loss: 0.078909,  train_metric: 0.078 test_metric: 0.079 lr: 0.00445)\n",
            "epoch: 116, train_loss: 0.077136, val loss: 0.080013,  train_metric: 0.077 test_metric: 0.080 lr: 0.00445)\n",
            "epoch: 117, train_loss: 0.077142, val loss: 0.078570,  train_metric: 0.077 test_metric: 0.079 lr: 0.00444)\n",
            "epoch: 118, train_loss: 0.075929, val loss: 0.078283,  train_metric: 0.076 test_metric: 0.078 lr: 0.00444)\n",
            "epoch: 119, train_loss: 0.075567, val loss: 0.077048,  train_metric: 0.076 test_metric: 0.077 lr: 0.00443)\n",
            "epoch: 120, train_loss: 0.075361, val loss: 0.076955,  train_metric: 0.075 test_metric: 0.077 lr: 0.00443)\n",
            "epoch: 121, train_loss: 0.077026, val loss: 0.079707,  train_metric: 0.077 test_metric: 0.080 lr: 0.00443)\n",
            "epoch: 122, train_loss: 0.076228, val loss: 0.077872,  train_metric: 0.076 test_metric: 0.078 lr: 0.00442)\n",
            "epoch: 123, train_loss: 0.075809, val loss: 0.077056,  train_metric: 0.076 test_metric: 0.077 lr: 0.00442)\n",
            "epoch: 124, train_loss: 0.074037, val loss: 0.075382,  train_metric: 0.074 test_metric: 0.075 lr: 0.00441)\n",
            "epoch: 125, train_loss: 0.073408, val loss: 0.076976,  train_metric: 0.073 test_metric: 0.077 lr: 0.00441)\n",
            "epoch: 126, train_loss: 0.074919, val loss: 0.074324,  train_metric: 0.075 test_metric: 0.074 lr: 0.00440)\n",
            "epoch: 127, train_loss: 0.074756, val loss: 0.079175,  train_metric: 0.075 test_metric: 0.079 lr: 0.00440)\n",
            "epoch: 128, train_loss: 0.077971, val loss: 0.074116,  train_metric: 0.078 test_metric: 0.074 lr: 0.00439)\n",
            "epoch: 129, train_loss: 0.074971, val loss: 0.076371,  train_metric: 0.075 test_metric: 0.076 lr: 0.00439)\n",
            "epoch: 130, train_loss: 0.072957, val loss: 0.073161,  train_metric: 0.073 test_metric: 0.073 lr: 0.00439)\n",
            "epoch: 131, train_loss: 0.071277, val loss: 0.074249,  train_metric: 0.071 test_metric: 0.074 lr: 0.00438)\n",
            "epoch: 132, train_loss: 0.071453, val loss: 0.073398,  train_metric: 0.071 test_metric: 0.073 lr: 0.00438)\n",
            "epoch: 133, train_loss: 0.071501, val loss: 0.072385,  train_metric: 0.072 test_metric: 0.072 lr: 0.00437)\n",
            "epoch: 134, train_loss: 0.070455, val loss: 0.072387,  train_metric: 0.070 test_metric: 0.072 lr: 0.00437)\n",
            "epoch: 135, train_loss: 0.070446, val loss: 0.072881,  train_metric: 0.070 test_metric: 0.073 lr: 0.00436)\n",
            "epoch: 136, train_loss: 0.070989, val loss: 0.071797,  train_metric: 0.071 test_metric: 0.072 lr: 0.00436)\n",
            "epoch: 137, train_loss: 0.069469, val loss: 0.071541,  train_metric: 0.069 test_metric: 0.072 lr: 0.00436)\n",
            "epoch: 138, train_loss: 0.069580, val loss: 0.072240,  train_metric: 0.070 test_metric: 0.072 lr: 0.00435)\n",
            "epoch: 139, train_loss: 0.069445, val loss: 0.071463,  train_metric: 0.069 test_metric: 0.071 lr: 0.00435)\n",
            "epoch: 140, train_loss: 0.069355, val loss: 0.071041,  train_metric: 0.069 test_metric: 0.071 lr: 0.00434)\n",
            "epoch: 141, train_loss: 0.068823, val loss: 0.071244,  train_metric: 0.069 test_metric: 0.071 lr: 0.00434)\n",
            "epoch: 142, train_loss: 0.069419, val loss: 0.070067,  train_metric: 0.069 test_metric: 0.070 lr: 0.00433)\n",
            "epoch: 143, train_loss: 0.068547, val loss: 0.071250,  train_metric: 0.069 test_metric: 0.071 lr: 0.00433)\n",
            "epoch: 144, train_loss: 0.068708, val loss: 0.070904,  train_metric: 0.069 test_metric: 0.071 lr: 0.00432)\n",
            "epoch: 145, train_loss: 0.068811, val loss: 0.070545,  train_metric: 0.069 test_metric: 0.071 lr: 0.00432)\n",
            "epoch: 146, train_loss: 0.067912, val loss: 0.069592,  train_metric: 0.068 test_metric: 0.070 lr: 0.00432)\n",
            "epoch: 147, train_loss: 0.067937, val loss: 0.069900,  train_metric: 0.068 test_metric: 0.070 lr: 0.00431)\n",
            "epoch: 148, train_loss: 0.067969, val loss: 0.069832,  train_metric: 0.068 test_metric: 0.070 lr: 0.00431)\n",
            "epoch: 149, train_loss: 0.067941, val loss: 0.070497,  train_metric: 0.068 test_metric: 0.070 lr: 0.00430)\n",
            "epoch: 150, train_loss: 0.067477, val loss: 0.069153,  train_metric: 0.067 test_metric: 0.069 lr: 0.00430)\n",
            "epoch: 151, train_loss: 0.067438, val loss: 0.071343,  train_metric: 0.067 test_metric: 0.071 lr: 0.00429)\n",
            "epoch: 152, train_loss: 0.067469, val loss: 0.068674,  train_metric: 0.067 test_metric: 0.069 lr: 0.00429)\n",
            "epoch: 153, train_loss: 0.067765, val loss: 0.069050,  train_metric: 0.068 test_metric: 0.069 lr: 0.00429)\n",
            "epoch: 154, train_loss: 0.067361, val loss: 0.069251,  train_metric: 0.067 test_metric: 0.069 lr: 0.00428)\n",
            "epoch: 155, train_loss: 0.068026, val loss: 0.068719,  train_metric: 0.068 test_metric: 0.069 lr: 0.00428)\n",
            "epoch: 156, train_loss: 0.067036, val loss: 0.069108,  train_metric: 0.067 test_metric: 0.069 lr: 0.00427)\n",
            "epoch: 157, train_loss: 0.066823, val loss: 0.068039,  train_metric: 0.067 test_metric: 0.068 lr: 0.00427)\n",
            "epoch: 158, train_loss: 0.066363, val loss: 0.067840,  train_metric: 0.066 test_metric: 0.068 lr: 0.00426)\n",
            "epoch: 159, train_loss: 0.066687, val loss: 0.068306,  train_metric: 0.067 test_metric: 0.068 lr: 0.00426)\n",
            "epoch: 160, train_loss: 0.068115, val loss: 0.067981,  train_metric: 0.068 test_metric: 0.068 lr: 0.00426)\n",
            "epoch: 161, train_loss: 0.066229, val loss: 0.068042,  train_metric: 0.066 test_metric: 0.068 lr: 0.00425)\n",
            "epoch: 162, train_loss: 0.066045, val loss: 0.067862,  train_metric: 0.066 test_metric: 0.068 lr: 0.00425)\n",
            "epoch: 163, train_loss: 0.066409, val loss: 0.067565,  train_metric: 0.066 test_metric: 0.068 lr: 0.00424)\n",
            "epoch: 164, train_loss: 0.066704, val loss: 0.067587,  train_metric: 0.067 test_metric: 0.068 lr: 0.00424)\n",
            "epoch: 165, train_loss: 0.066055, val loss: 0.068051,  train_metric: 0.066 test_metric: 0.068 lr: 0.00423)\n",
            "epoch: 166, train_loss: 0.066148, val loss: 0.067995,  train_metric: 0.066 test_metric: 0.068 lr: 0.00423)\n",
            "epoch: 167, train_loss: 0.066068, val loss: 0.067102,  train_metric: 0.066 test_metric: 0.067 lr: 0.00423)\n",
            "epoch: 168, train_loss: 0.065579, val loss: 0.066995,  train_metric: 0.066 test_metric: 0.067 lr: 0.00422)\n",
            "epoch: 169, train_loss: 0.065637, val loss: 0.066452,  train_metric: 0.066 test_metric: 0.066 lr: 0.00422)\n",
            "epoch: 170, train_loss: 0.066660, val loss: 0.067617,  train_metric: 0.067 test_metric: 0.068 lr: 0.00421)\n",
            "epoch: 171, train_loss: 0.066692, val loss: 0.067134,  train_metric: 0.067 test_metric: 0.067 lr: 0.00421)\n",
            "epoch: 172, train_loss: 0.065793, val loss: 0.067404,  train_metric: 0.066 test_metric: 0.067 lr: 0.00421)\n",
            "epoch: 173, train_loss: 0.065450, val loss: 0.067958,  train_metric: 0.065 test_metric: 0.068 lr: 0.00420)\n",
            "epoch: 174, train_loss: 0.065334, val loss: 0.066757,  train_metric: 0.065 test_metric: 0.067 lr: 0.00420)\n",
            "epoch: 175, train_loss: 0.064525, val loss: 0.067101,  train_metric: 0.065 test_metric: 0.067 lr: 0.00419)\n",
            "epoch: 176, train_loss: 0.065771, val loss: 0.066069,  train_metric: 0.066 test_metric: 0.066 lr: 0.00419)\n",
            "epoch: 177, train_loss: 0.065441, val loss: 0.065966,  train_metric: 0.065 test_metric: 0.066 lr: 0.00418)\n",
            "epoch: 178, train_loss: 0.064961, val loss: 0.067328,  train_metric: 0.065 test_metric: 0.067 lr: 0.00418)\n",
            "epoch: 179, train_loss: 0.065093, val loss: 0.066868,  train_metric: 0.065 test_metric: 0.067 lr: 0.00418)\n",
            "epoch: 180, train_loss: 0.064657, val loss: 0.066683,  train_metric: 0.065 test_metric: 0.067 lr: 0.00417)\n",
            "epoch: 181, train_loss: 0.064364, val loss: 0.066083,  train_metric: 0.064 test_metric: 0.066 lr: 0.00417)\n",
            "epoch: 182, train_loss: 0.064316, val loss: 0.066104,  train_metric: 0.064 test_metric: 0.066 lr: 0.00416)\n",
            "epoch: 183, train_loss: 0.064207, val loss: 0.066076,  train_metric: 0.064 test_metric: 0.066 lr: 0.00416)\n",
            "epoch: 184, train_loss: 0.064295, val loss: 0.066542,  train_metric: 0.064 test_metric: 0.067 lr: 0.00416)\n",
            "epoch: 185, train_loss: 0.064441, val loss: 0.065526,  train_metric: 0.064 test_metric: 0.066 lr: 0.00415)\n",
            "epoch: 186, train_loss: 0.064518, val loss: 0.065456,  train_metric: 0.065 test_metric: 0.065 lr: 0.00415)\n",
            "epoch: 187, train_loss: 0.063890, val loss: 0.066017,  train_metric: 0.064 test_metric: 0.066 lr: 0.00414)\n",
            "epoch: 188, train_loss: 0.064361, val loss: 0.065414,  train_metric: 0.064 test_metric: 0.065 lr: 0.00414)\n",
            "epoch: 189, train_loss: 0.063839, val loss: 0.065808,  train_metric: 0.064 test_metric: 0.066 lr: 0.00413)\n",
            "epoch: 190, train_loss: 0.064870, val loss: 0.066581,  train_metric: 0.065 test_metric: 0.067 lr: 0.00413)\n",
            "epoch: 191, train_loss: 0.064723, val loss: 0.066468,  train_metric: 0.065 test_metric: 0.066 lr: 0.00413)\n",
            "epoch: 192, train_loss: 0.065061, val loss: 0.065907,  train_metric: 0.065 test_metric: 0.066 lr: 0.00412)\n",
            "epoch: 193, train_loss: 0.064445, val loss: 0.065182,  train_metric: 0.064 test_metric: 0.065 lr: 0.00412)\n",
            "epoch: 194, train_loss: 0.064143, val loss: 0.064924,  train_metric: 0.064 test_metric: 0.065 lr: 0.00411)\n",
            "epoch: 195, train_loss: 0.064248, val loss: 0.064923,  train_metric: 0.064 test_metric: 0.065 lr: 0.00411)\n",
            "epoch: 196, train_loss: 0.064274, val loss: 0.065618,  train_metric: 0.064 test_metric: 0.066 lr: 0.00411)\n",
            "epoch: 197, train_loss: 0.064037, val loss: 0.065502,  train_metric: 0.064 test_metric: 0.066 lr: 0.00410)\n",
            "epoch: 198, train_loss: 0.063728, val loss: 0.065902,  train_metric: 0.064 test_metric: 0.066 lr: 0.00410)\n",
            "epoch: 199, train_loss: 0.063224, val loss: 0.064464,  train_metric: 0.063 test_metric: 0.064 lr: 0.00409)\n",
            "epoch: 200, train_loss: 0.063004, val loss: 0.064761,  train_metric: 0.063 test_metric: 0.065 lr: 0.00409)\n",
            "epoch: 201, train_loss: 0.063177, val loss: 0.064022,  train_metric: 0.063 test_metric: 0.064 lr: 0.00409)\n",
            "epoch: 202, train_loss: 0.063406, val loss: 0.065275,  train_metric: 0.063 test_metric: 0.065 lr: 0.00408)\n",
            "epoch: 203, train_loss: 0.063390, val loss: 0.064716,  train_metric: 0.063 test_metric: 0.065 lr: 0.00408)\n",
            "epoch: 204, train_loss: 0.063690, val loss: 0.064307,  train_metric: 0.064 test_metric: 0.064 lr: 0.00407)\n",
            "epoch: 205, train_loss: 0.063336, val loss: 0.064906,  train_metric: 0.063 test_metric: 0.065 lr: 0.00407)\n",
            "epoch: 206, train_loss: 0.063862, val loss: 0.065824,  train_metric: 0.064 test_metric: 0.066 lr: 0.00406)\n",
            "epoch: 207, train_loss: 0.064020, val loss: 0.065382,  train_metric: 0.064 test_metric: 0.065 lr: 0.00406)\n",
            "epoch: 208, train_loss: 0.063738, val loss: 0.065063,  train_metric: 0.064 test_metric: 0.065 lr: 0.00406)\n",
            "epoch: 209, train_loss: 0.063179, val loss: 0.064094,  train_metric: 0.063 test_metric: 0.064 lr: 0.00405)\n",
            "epoch: 210, train_loss: 0.062780, val loss: 0.064282,  train_metric: 0.063 test_metric: 0.064 lr: 0.00405)\n",
            "epoch: 211, train_loss: 0.063071, val loss: 0.065619,  train_metric: 0.063 test_metric: 0.066 lr: 0.00404)\n",
            "epoch: 212, train_loss: 0.063472, val loss: 0.063638,  train_metric: 0.063 test_metric: 0.064 lr: 0.00404)\n",
            "epoch: 213, train_loss: 0.062993, val loss: 0.064027,  train_metric: 0.063 test_metric: 0.064 lr: 0.00404)\n",
            "epoch: 214, train_loss: 0.063241, val loss: 0.064221,  train_metric: 0.063 test_metric: 0.064 lr: 0.00403)\n",
            "epoch: 215, train_loss: 0.062904, val loss: 0.064994,  train_metric: 0.063 test_metric: 0.065 lr: 0.00403)\n",
            "epoch: 216, train_loss: 0.063021, val loss: 0.063799,  train_metric: 0.063 test_metric: 0.064 lr: 0.00402)\n",
            "epoch: 217, train_loss: 0.063231, val loss: 0.064555,  train_metric: 0.063 test_metric: 0.065 lr: 0.00402)\n",
            "epoch: 218, train_loss: 0.063385, val loss: 0.066234,  train_metric: 0.063 test_metric: 0.066 lr: 0.00402)\n",
            "epoch: 219, train_loss: 0.063213, val loss: 0.063952,  train_metric: 0.063 test_metric: 0.064 lr: 0.00401)\n",
            "epoch: 220, train_loss: 0.063077, val loss: 0.064881,  train_metric: 0.063 test_metric: 0.065 lr: 0.00401)\n",
            "epoch: 221, train_loss: 0.063454, val loss: 0.064022,  train_metric: 0.063 test_metric: 0.064 lr: 0.00400)\n",
            "epoch: 222, train_loss: 0.063926, val loss: 0.066436,  train_metric: 0.064 test_metric: 0.066 lr: 0.00400)\n",
            "epoch: 223, train_loss: 0.064075, val loss: 0.064895,  train_metric: 0.064 test_metric: 0.065 lr: 0.00400)\n",
            "epoch: 224, train_loss: 0.063729, val loss: 0.064655,  train_metric: 0.064 test_metric: 0.065 lr: 0.00399)\n",
            "epoch: 225, train_loss: 0.063121, val loss: 0.064385,  train_metric: 0.063 test_metric: 0.064 lr: 0.00399)\n",
            "epoch: 226, train_loss: 0.062505, val loss: 0.063179,  train_metric: 0.063 test_metric: 0.063 lr: 0.00398)\n",
            "epoch: 227, train_loss: 0.062119, val loss: 0.064281,  train_metric: 0.062 test_metric: 0.064 lr: 0.00398)\n",
            "epoch: 228, train_loss: 0.062303, val loss: 0.063404,  train_metric: 0.062 test_metric: 0.063 lr: 0.00398)\n",
            "epoch: 229, train_loss: 0.062182, val loss: 0.063313,  train_metric: 0.062 test_metric: 0.063 lr: 0.00397)\n",
            "epoch: 230, train_loss: 0.062147, val loss: 0.062944,  train_metric: 0.062 test_metric: 0.063 lr: 0.00397)\n",
            "epoch: 231, train_loss: 0.061897, val loss: 0.063784,  train_metric: 0.062 test_metric: 0.064 lr: 0.00396)\n",
            "epoch: 232, train_loss: 0.062357, val loss: 0.063035,  train_metric: 0.062 test_metric: 0.063 lr: 0.00396)\n",
            "epoch: 233, train_loss: 0.061566, val loss: 0.063103,  train_metric: 0.062 test_metric: 0.063 lr: 0.00396)\n",
            "epoch: 234, train_loss: 0.062003, val loss: 0.064594,  train_metric: 0.062 test_metric: 0.065 lr: 0.00395)\n",
            "epoch: 235, train_loss: 0.062121, val loss: 0.063162,  train_metric: 0.062 test_metric: 0.063 lr: 0.00395)\n",
            "epoch: 236, train_loss: 0.061926, val loss: 0.062626,  train_metric: 0.062 test_metric: 0.063 lr: 0.00394)\n",
            "epoch: 237, train_loss: 0.062016, val loss: 0.064067,  train_metric: 0.062 test_metric: 0.064 lr: 0.00394)\n",
            "epoch: 238, train_loss: 0.061854, val loss: 0.063122,  train_metric: 0.062 test_metric: 0.063 lr: 0.00394)\n",
            "epoch: 239, train_loss: 0.061359, val loss: 0.062683,  train_metric: 0.061 test_metric: 0.063 lr: 0.00393)\n",
            "epoch: 240, train_loss: 0.061606, val loss: 0.062900,  train_metric: 0.062 test_metric: 0.063 lr: 0.00393)\n",
            "epoch: 241, train_loss: 0.061368, val loss: 0.062850,  train_metric: 0.061 test_metric: 0.063 lr: 0.00392)\n",
            "epoch: 242, train_loss: 0.061498, val loss: 0.062444,  train_metric: 0.061 test_metric: 0.062 lr: 0.00392)\n",
            "epoch: 243, train_loss: 0.061620, val loss: 0.062729,  train_metric: 0.062 test_metric: 0.063 lr: 0.00392)\n",
            "epoch: 244, train_loss: 0.061530, val loss: 0.063444,  train_metric: 0.062 test_metric: 0.063 lr: 0.00391)\n",
            "epoch: 245, train_loss: 0.061752, val loss: 0.063204,  train_metric: 0.062 test_metric: 0.063 lr: 0.00391)\n",
            "epoch: 246, train_loss: 0.062067, val loss: 0.063850,  train_metric: 0.062 test_metric: 0.064 lr: 0.00391)\n",
            "epoch: 247, train_loss: 0.062385, val loss: 0.063270,  train_metric: 0.062 test_metric: 0.063 lr: 0.00390)\n",
            "epoch: 248, train_loss: 0.062398, val loss: 0.064033,  train_metric: 0.062 test_metric: 0.064 lr: 0.00390)\n",
            "epoch: 249, train_loss: 0.062269, val loss: 0.063664,  train_metric: 0.062 test_metric: 0.064 lr: 0.00389)\n",
            "epoch: 250, train_loss: 0.062646, val loss: 0.064074,  train_metric: 0.063 test_metric: 0.064 lr: 0.00389)\n",
            "epoch: 251, train_loss: 0.062208, val loss: 0.064073,  train_metric: 0.062 test_metric: 0.064 lr: 0.00389)\n",
            "epoch: 252, train_loss: 0.062755, val loss: 0.063065,  train_metric: 0.063 test_metric: 0.063 lr: 0.00388)\n",
            "epoch: 253, train_loss: 0.061984, val loss: 0.062596,  train_metric: 0.062 test_metric: 0.063 lr: 0.00388)\n",
            "epoch: 254, train_loss: 0.061626, val loss: 0.063386,  train_metric: 0.062 test_metric: 0.063 lr: 0.00387)\n",
            "epoch: 255, train_loss: 0.061259, val loss: 0.063078,  train_metric: 0.061 test_metric: 0.063 lr: 0.00387)\n",
            "epoch: 256, train_loss: 0.061007, val loss: 0.062280,  train_metric: 0.061 test_metric: 0.062 lr: 0.00387)\n",
            "epoch: 257, train_loss: 0.061058, val loss: 0.062073,  train_metric: 0.061 test_metric: 0.062 lr: 0.00386)\n",
            "epoch: 258, train_loss: 0.061095, val loss: 0.062471,  train_metric: 0.061 test_metric: 0.062 lr: 0.00386)\n",
            "epoch: 259, train_loss: 0.061212, val loss: 0.061834,  train_metric: 0.061 test_metric: 0.062 lr: 0.00385)\n",
            "epoch: 260, train_loss: 0.061223, val loss: 0.062314,  train_metric: 0.061 test_metric: 0.062 lr: 0.00385)\n",
            "epoch: 261, train_loss: 0.061352, val loss: 0.062903,  train_metric: 0.061 test_metric: 0.063 lr: 0.00385)\n",
            "epoch: 262, train_loss: 0.061896, val loss: 0.063641,  train_metric: 0.062 test_metric: 0.064 lr: 0.00384)\n",
            "epoch: 263, train_loss: 0.061728, val loss: 0.063450,  train_metric: 0.062 test_metric: 0.063 lr: 0.00384)\n",
            "epoch: 264, train_loss: 0.061390, val loss: 0.063217,  train_metric: 0.061 test_metric: 0.063 lr: 0.00384)\n",
            "epoch: 265, train_loss: 0.061192, val loss: 0.062428,  train_metric: 0.061 test_metric: 0.062 lr: 0.00383)\n",
            "epoch: 266, train_loss: 0.061866, val loss: 0.062753,  train_metric: 0.062 test_metric: 0.063 lr: 0.00383)\n",
            "epoch: 267, train_loss: 0.061574, val loss: 0.062810,  train_metric: 0.062 test_metric: 0.063 lr: 0.00382)\n",
            "epoch: 268, train_loss: 0.061221, val loss: 0.061837,  train_metric: 0.061 test_metric: 0.062 lr: 0.00382)\n",
            "epoch: 269, train_loss: 0.060896, val loss: 0.061975,  train_metric: 0.061 test_metric: 0.062 lr: 0.00382)\n",
            "epoch: 270, train_loss: 0.060620, val loss: 0.062874,  train_metric: 0.061 test_metric: 0.063 lr: 0.00381)\n",
            "epoch: 271, train_loss: 0.061236, val loss: 0.062430,  train_metric: 0.061 test_metric: 0.062 lr: 0.00381)\n",
            "epoch: 272, train_loss: 0.061161, val loss: 0.061607,  train_metric: 0.061 test_metric: 0.062 lr: 0.00380)\n",
            "epoch: 273, train_loss: 0.062199, val loss: 0.062613,  train_metric: 0.062 test_metric: 0.063 lr: 0.00380)\n",
            "epoch: 274, train_loss: 0.060910, val loss: 0.062063,  train_metric: 0.061 test_metric: 0.062 lr: 0.00380)\n",
            "epoch: 275, train_loss: 0.061723, val loss: 0.062597,  train_metric: 0.062 test_metric: 0.063 lr: 0.00379)\n",
            "epoch: 276, train_loss: 0.061428, val loss: 0.063281,  train_metric: 0.061 test_metric: 0.063 lr: 0.00379)\n",
            "epoch: 277, train_loss: 0.061792, val loss: 0.062676,  train_metric: 0.062 test_metric: 0.063 lr: 0.00379)\n",
            "epoch: 278, train_loss: 0.060727, val loss: 0.062282,  train_metric: 0.061 test_metric: 0.062 lr: 0.00378)\n",
            "epoch: 279, train_loss: 0.060798, val loss: 0.063025,  train_metric: 0.061 test_metric: 0.063 lr: 0.00378)\n",
            "epoch: 280, train_loss: 0.061445, val loss: 0.063511,  train_metric: 0.061 test_metric: 0.064 lr: 0.00377)\n",
            "epoch: 281, train_loss: 0.061180, val loss: 0.062624,  train_metric: 0.061 test_metric: 0.063 lr: 0.00377)\n",
            "epoch: 282, train_loss: 0.060723, val loss: 0.062274,  train_metric: 0.061 test_metric: 0.062 lr: 0.00377)\n",
            "epoch: 283, train_loss: 0.061121, val loss: 0.061761,  train_metric: 0.061 test_metric: 0.062 lr: 0.00376)\n",
            "epoch: 284, train_loss: 0.060711, val loss: 0.062031,  train_metric: 0.061 test_metric: 0.062 lr: 0.00376)\n",
            "epoch: 285, train_loss: 0.060535, val loss: 0.061847,  train_metric: 0.061 test_metric: 0.062 lr: 0.00376)\n",
            "epoch: 286, train_loss: 0.060568, val loss: 0.062397,  train_metric: 0.061 test_metric: 0.062 lr: 0.00375)\n",
            "epoch: 287, train_loss: 0.060530, val loss: 0.062118,  train_metric: 0.061 test_metric: 0.062 lr: 0.00375)\n",
            "epoch: 288, train_loss: 0.060534, val loss: 0.062060,  train_metric: 0.061 test_metric: 0.062 lr: 0.00374)\n",
            "epoch: 289, train_loss: 0.060808, val loss: 0.062152,  train_metric: 0.061 test_metric: 0.062 lr: 0.00374)\n",
            "epoch: 290, train_loss: 0.061070, val loss: 0.061762,  train_metric: 0.061 test_metric: 0.062 lr: 0.00374)\n",
            "epoch: 291, train_loss: 0.060934, val loss: 0.062042,  train_metric: 0.061 test_metric: 0.062 lr: 0.00373)\n",
            "epoch: 292, train_loss: 0.060879, val loss: 0.062314,  train_metric: 0.061 test_metric: 0.062 lr: 0.00373)\n",
            "epoch: 293, train_loss: 0.060915, val loss: 0.062197,  train_metric: 0.061 test_metric: 0.062 lr: 0.00373)\n",
            "epoch: 294, train_loss: 0.061462, val loss: 0.063653,  train_metric: 0.061 test_metric: 0.064 lr: 0.00372)\n",
            "epoch: 295, train_loss: 0.061440, val loss: 0.062535,  train_metric: 0.061 test_metric: 0.063 lr: 0.00372)\n",
            "epoch: 296, train_loss: 0.061352, val loss: 0.062762,  train_metric: 0.061 test_metric: 0.063 lr: 0.00371)\n",
            "epoch: 297, train_loss: 0.061575, val loss: 0.063160,  train_metric: 0.062 test_metric: 0.063 lr: 0.00371)\n",
            "epoch: 298, train_loss: 0.062734, val loss: 0.065476,  train_metric: 0.063 test_metric: 0.065 lr: 0.00371)\n",
            "epoch: 299, train_loss: 0.062814, val loss: 0.065569,  train_metric: 0.063 test_metric: 0.066 lr: 0.00370)\n",
            "epoch: 300, train_loss: 0.062916, val loss: 0.063165,  train_metric: 0.063 test_metric: 0.063 lr: 0.00370)\n",
            "epoch: 301, train_loss: 0.061017, val loss: 0.062205,  train_metric: 0.061 test_metric: 0.062 lr: 0.00370)\n",
            "epoch: 302, train_loss: 0.061283, val loss: 0.062575,  train_metric: 0.061 test_metric: 0.063 lr: 0.00369)\n",
            "epoch: 303, train_loss: 0.060332, val loss: 0.061533,  train_metric: 0.060 test_metric: 0.062 lr: 0.00369)\n",
            "epoch: 304, train_loss: 0.060329, val loss: 0.061275,  train_metric: 0.060 test_metric: 0.061 lr: 0.00369)\n",
            "epoch: 305, train_loss: 0.060590, val loss: 0.061660,  train_metric: 0.061 test_metric: 0.062 lr: 0.00368)\n",
            "epoch: 306, train_loss: 0.060410, val loss: 0.061452,  train_metric: 0.060 test_metric: 0.061 lr: 0.00368)\n",
            "epoch: 307, train_loss: 0.060272, val loss: 0.062853,  train_metric: 0.060 test_metric: 0.063 lr: 0.00367)\n",
            "epoch: 308, train_loss: 0.061755, val loss: 0.061216,  train_metric: 0.062 test_metric: 0.061 lr: 0.00367)\n",
            "epoch: 309, train_loss: 0.060411, val loss: 0.061711,  train_metric: 0.060 test_metric: 0.062 lr: 0.00367)\n",
            "epoch: 310, train_loss: 0.061139, val loss: 0.062437,  train_metric: 0.061 test_metric: 0.062 lr: 0.00366)\n",
            "epoch: 311, train_loss: 0.061260, val loss: 0.063064,  train_metric: 0.061 test_metric: 0.063 lr: 0.00366)\n",
            "epoch: 312, train_loss: 0.061785, val loss: 0.063386,  train_metric: 0.062 test_metric: 0.063 lr: 0.00366)\n",
            "epoch: 313, train_loss: 0.061890, val loss: 0.063141,  train_metric: 0.062 test_metric: 0.063 lr: 0.00365)\n",
            "epoch: 314, train_loss: 0.060913, val loss: 0.062189,  train_metric: 0.061 test_metric: 0.062 lr: 0.00365)\n",
            "epoch: 315, train_loss: 0.060209, val loss: 0.062450,  train_metric: 0.060 test_metric: 0.062 lr: 0.00364)\n",
            "epoch: 316, train_loss: 0.060844, val loss: 0.062257,  train_metric: 0.061 test_metric: 0.062 lr: 0.00364)\n",
            "epoch: 317, train_loss: 0.062081, val loss: 0.062010,  train_metric: 0.062 test_metric: 0.062 lr: 0.00364)\n",
            "epoch: 318, train_loss: 0.060793, val loss: 0.061634,  train_metric: 0.061 test_metric: 0.062 lr: 0.00363)\n",
            "epoch: 319, train_loss: 0.060220, val loss: 0.061603,  train_metric: 0.060 test_metric: 0.062 lr: 0.00363)\n",
            "epoch: 320, train_loss: 0.061100, val loss: 0.060766,  train_metric: 0.061 test_metric: 0.061 lr: 0.00363)\n",
            "epoch: 321, train_loss: 0.060367, val loss: 0.063827,  train_metric: 0.060 test_metric: 0.064 lr: 0.00362)\n",
            "epoch: 322, train_loss: 0.061579, val loss: 0.064216,  train_metric: 0.062 test_metric: 0.064 lr: 0.00362)\n",
            "epoch: 323, train_loss: 0.061789, val loss: 0.062282,  train_metric: 0.062 test_metric: 0.062 lr: 0.00362)\n",
            "epoch: 324, train_loss: 0.060356, val loss: 0.061510,  train_metric: 0.060 test_metric: 0.062 lr: 0.00361)\n",
            "epoch: 325, train_loss: 0.059836, val loss: 0.060832,  train_metric: 0.060 test_metric: 0.061 lr: 0.00361)\n",
            "epoch: 326, train_loss: 0.059671, val loss: 0.061332,  train_metric: 0.060 test_metric: 0.061 lr: 0.00360)\n",
            "epoch: 327, train_loss: 0.059344, val loss: 0.061167,  train_metric: 0.059 test_metric: 0.061 lr: 0.00360)\n",
            "epoch: 328, train_loss: 0.059997, val loss: 0.061478,  train_metric: 0.060 test_metric: 0.061 lr: 0.00360)\n",
            "epoch: 329, train_loss: 0.060059, val loss: 0.060430,  train_metric: 0.060 test_metric: 0.060 lr: 0.00359)\n",
            "epoch: 330, train_loss: 0.059864, val loss: 0.060746,  train_metric: 0.060 test_metric: 0.061 lr: 0.00359)\n",
            "epoch: 331, train_loss: 0.059581, val loss: 0.060370,  train_metric: 0.060 test_metric: 0.060 lr: 0.00359)\n",
            "epoch: 332, train_loss: 0.060596, val loss: 0.062045,  train_metric: 0.061 test_metric: 0.062 lr: 0.00358)\n",
            "epoch: 333, train_loss: 0.060032, val loss: 0.060760,  train_metric: 0.060 test_metric: 0.061 lr: 0.00358)\n",
            "epoch: 334, train_loss: 0.059257, val loss: 0.060447,  train_metric: 0.059 test_metric: 0.060 lr: 0.00358)\n",
            "epoch: 335, train_loss: 0.059127, val loss: 0.060373,  train_metric: 0.059 test_metric: 0.060 lr: 0.00357)\n",
            "epoch: 336, train_loss: 0.058014, val loss: 0.062482,  train_metric: 0.058 test_metric: 0.062 lr: 0.00357)\n",
            "epoch: 337, train_loss: 0.059898, val loss: 0.059525,  train_metric: 0.060 test_metric: 0.060 lr: 0.00357)\n",
            "epoch: 338, train_loss: 0.058457, val loss: 0.060275,  train_metric: 0.058 test_metric: 0.060 lr: 0.00356)\n",
            "epoch: 339, train_loss: 0.057493, val loss: 0.059595,  train_metric: 0.057 test_metric: 0.060 lr: 0.00356)\n",
            "epoch: 340, train_loss: 0.060941, val loss: 0.060613,  train_metric: 0.061 test_metric: 0.061 lr: 0.00355)\n",
            "epoch: 341, train_loss: 0.065597, val loss: 0.059744,  train_metric: 0.066 test_metric: 0.060 lr: 0.00355)\n",
            "epoch: 342, train_loss: 0.059527, val loss: 0.063363,  train_metric: 0.060 test_metric: 0.063 lr: 0.00355)\n",
            "epoch: 343, train_loss: 0.067564, val loss: 0.061162,  train_metric: 0.068 test_metric: 0.061 lr: 0.00354)\n",
            "epoch: 344, train_loss: 0.062130, val loss: 0.060076,  train_metric: 0.062 test_metric: 0.060 lr: 0.00354)\n",
            "epoch: 345, train_loss: 0.062126, val loss: 0.063102,  train_metric: 0.062 test_metric: 0.063 lr: 0.00354)\n",
            "epoch: 346, train_loss: 0.059497, val loss: 0.060208,  train_metric: 0.059 test_metric: 0.060 lr: 0.00353)\n",
            "epoch: 347, train_loss: 0.059419, val loss: 0.060020,  train_metric: 0.059 test_metric: 0.060 lr: 0.00353)\n",
            "epoch: 348, train_loss: 0.058300, val loss: 0.059674,  train_metric: 0.058 test_metric: 0.060 lr: 0.00353)\n",
            "epoch: 349, train_loss: 0.058528, val loss: 0.060102,  train_metric: 0.059 test_metric: 0.060 lr: 0.00352)\n",
            "epoch: 350, train_loss: 0.061902, val loss: 0.059957,  train_metric: 0.062 test_metric: 0.060 lr: 0.00352)\n",
            "epoch: 351, train_loss: 0.057254, val loss: 0.062948,  train_metric: 0.057 test_metric: 0.063 lr: 0.00352)\n",
            "epoch: 352, train_loss: 0.061027, val loss: 0.059968,  train_metric: 0.061 test_metric: 0.060 lr: 0.00351)\n",
            "epoch: 353, train_loss: 0.059634, val loss: 0.061988,  train_metric: 0.060 test_metric: 0.062 lr: 0.00351)\n",
            "epoch: 354, train_loss: 0.055872, val loss: 0.058540,  train_metric: 0.056 test_metric: 0.059 lr: 0.00351)\n",
            "epoch: 355, train_loss: 0.059706, val loss: 0.058474,  train_metric: 0.060 test_metric: 0.058 lr: 0.00350)\n",
            "epoch: 356, train_loss: 0.057143, val loss: 0.061097,  train_metric: 0.057 test_metric: 0.061 lr: 0.00350)\n",
            "epoch: 357, train_loss: 0.061993, val loss: 0.058612,  train_metric: 0.062 test_metric: 0.059 lr: 0.00349)\n",
            "epoch: 358, train_loss: 0.058045, val loss: 0.063256,  train_metric: 0.058 test_metric: 0.063 lr: 0.00349)\n",
            "epoch: 359, train_loss: 0.058527, val loss: 0.059071,  train_metric: 0.059 test_metric: 0.059 lr: 0.00349)\n",
            "epoch: 360, train_loss: 0.064076, val loss: 0.059676,  train_metric: 0.064 test_metric: 0.060 lr: 0.00348)\n",
            "epoch: 361, train_loss: 0.059522, val loss: 0.073406,  train_metric: 0.060 test_metric: 0.073 lr: 0.00348)\n",
            "epoch: 362, train_loss: 0.063252, val loss: 0.059216,  train_metric: 0.063 test_metric: 0.059 lr: 0.00348)\n",
            "epoch: 363, train_loss: 0.059076, val loss: 0.060210,  train_metric: 0.059 test_metric: 0.060 lr: 0.00347)\n",
            "epoch: 364, train_loss: 0.056289, val loss: 0.058314,  train_metric: 0.056 test_metric: 0.058 lr: 0.00347)\n",
            "epoch: 365, train_loss: 0.055838, val loss: 0.059168,  train_metric: 0.056 test_metric: 0.059 lr: 0.00347)\n",
            "epoch: 366, train_loss: 0.056705, val loss: 0.058489,  train_metric: 0.057 test_metric: 0.058 lr: 0.00346)\n",
            "epoch: 367, train_loss: 0.054538, val loss: 0.058976,  train_metric: 0.055 test_metric: 0.059 lr: 0.00346)\n",
            "epoch: 368, train_loss: 0.056045, val loss: 0.058082,  train_metric: 0.056 test_metric: 0.058 lr: 0.00346)\n",
            "epoch: 369, train_loss: 0.055558, val loss: 0.056339,  train_metric: 0.056 test_metric: 0.056 lr: 0.00345)\n",
            "epoch: 370, train_loss: 0.055685, val loss: 0.058643,  train_metric: 0.056 test_metric: 0.059 lr: 0.00345)\n",
            "epoch: 371, train_loss: 0.054540, val loss: 0.059391,  train_metric: 0.055 test_metric: 0.059 lr: 0.00345)\n",
            "epoch: 372, train_loss: 0.053901, val loss: 0.056172,  train_metric: 0.054 test_metric: 0.056 lr: 0.00344)\n",
            "epoch: 373, train_loss: 0.055170, val loss: 0.056349,  train_metric: 0.055 test_metric: 0.056 lr: 0.00344)\n",
            "epoch: 374, train_loss: 0.056008, val loss: 0.059904,  train_metric: 0.056 test_metric: 0.060 lr: 0.00344)\n",
            "epoch: 375, train_loss: 0.055893, val loss: 0.058381,  train_metric: 0.056 test_metric: 0.058 lr: 0.00343)\n",
            "epoch: 376, train_loss: 0.054821, val loss: 0.055331,  train_metric: 0.055 test_metric: 0.055 lr: 0.00343)\n",
            "epoch: 377, train_loss: 0.051859, val loss: 0.058222,  train_metric: 0.052 test_metric: 0.058 lr: 0.00343)\n",
            "epoch: 378, train_loss: 0.053351, val loss: 0.055715,  train_metric: 0.053 test_metric: 0.056 lr: 0.00342)\n",
            "epoch: 379, train_loss: 0.052851, val loss: 0.055657,  train_metric: 0.053 test_metric: 0.056 lr: 0.00342)\n",
            "epoch: 380, train_loss: 0.052614, val loss: 0.055917,  train_metric: 0.053 test_metric: 0.056 lr: 0.00342)\n",
            "epoch: 381, train_loss: 0.051625, val loss: 0.055210,  train_metric: 0.052 test_metric: 0.055 lr: 0.00341)\n",
            "epoch: 382, train_loss: 0.051143, val loss: 0.054665,  train_metric: 0.051 test_metric: 0.055 lr: 0.00341)\n",
            "epoch: 383, train_loss: 0.053928, val loss: 0.055164,  train_metric: 0.054 test_metric: 0.055 lr: 0.00341)\n",
            "epoch: 384, train_loss: 0.051612, val loss: 0.055708,  train_metric: 0.052 test_metric: 0.056 lr: 0.00340)\n",
            "epoch: 385, train_loss: 0.051216, val loss: 0.053658,  train_metric: 0.051 test_metric: 0.054 lr: 0.00340)\n",
            "epoch: 386, train_loss: 0.052979, val loss: 0.054595,  train_metric: 0.053 test_metric: 0.055 lr: 0.00339)\n",
            "epoch: 387, train_loss: 0.050962, val loss: 0.057164,  train_metric: 0.051 test_metric: 0.057 lr: 0.00339)\n",
            "epoch: 388, train_loss: 0.051687, val loss: 0.053412,  train_metric: 0.052 test_metric: 0.053 lr: 0.00339)\n",
            "epoch: 389, train_loss: 0.051742, val loss: 0.054125,  train_metric: 0.052 test_metric: 0.054 lr: 0.00338)\n",
            "epoch: 390, train_loss: 0.051583, val loss: 0.056902,  train_metric: 0.052 test_metric: 0.057 lr: 0.00338)\n",
            "epoch: 391, train_loss: 0.051578, val loss: 0.054181,  train_metric: 0.052 test_metric: 0.054 lr: 0.00338)\n",
            "epoch: 392, train_loss: 0.053910, val loss: 0.055068,  train_metric: 0.054 test_metric: 0.055 lr: 0.00337)\n",
            "epoch: 393, train_loss: 0.056345, val loss: 0.059107,  train_metric: 0.056 test_metric: 0.059 lr: 0.00337)\n",
            "epoch: 394, train_loss: 0.054058, val loss: 0.054625,  train_metric: 0.054 test_metric: 0.055 lr: 0.00337)\n",
            "epoch: 395, train_loss: 0.051810, val loss: 0.053622,  train_metric: 0.052 test_metric: 0.054 lr: 0.00336)\n",
            "epoch: 396, train_loss: 0.051159, val loss: 0.053421,  train_metric: 0.051 test_metric: 0.053 lr: 0.00336)\n",
            "epoch: 397, train_loss: 0.049823, val loss: 0.052197,  train_metric: 0.050 test_metric: 0.052 lr: 0.00336)\n",
            "epoch: 398, train_loss: 0.049690, val loss: 0.053514,  train_metric: 0.050 test_metric: 0.054 lr: 0.00335)\n",
            "epoch: 399, train_loss: 0.049514, val loss: 0.053001,  train_metric: 0.050 test_metric: 0.053 lr: 0.00335)\n",
            "epoch: 400, train_loss: 0.052466, val loss: 0.054472,  train_metric: 0.052 test_metric: 0.054 lr: 0.00335)\n",
            "epoch: 401, train_loss: 0.056291, val loss: 0.062425,  train_metric: 0.056 test_metric: 0.062 lr: 0.00334)\n",
            "epoch: 402, train_loss: 0.057155, val loss: 0.060495,  train_metric: 0.057 test_metric: 0.060 lr: 0.00334)\n",
            "epoch: 403, train_loss: 0.054305, val loss: 0.054584,  train_metric: 0.054 test_metric: 0.055 lr: 0.00334)\n",
            "epoch: 404, train_loss: 0.055833, val loss: 0.058574,  train_metric: 0.056 test_metric: 0.059 lr: 0.00333)\n",
            "epoch: 405, train_loss: 0.051391, val loss: 0.055003,  train_metric: 0.051 test_metric: 0.055 lr: 0.00333)\n",
            "epoch: 406, train_loss: 0.051372, val loss: 0.052607,  train_metric: 0.051 test_metric: 0.053 lr: 0.00333)\n",
            "epoch: 407, train_loss: 0.050807, val loss: 0.053551,  train_metric: 0.051 test_metric: 0.054 lr: 0.00332)\n",
            "epoch: 408, train_loss: 0.050387, val loss: 0.053943,  train_metric: 0.050 test_metric: 0.054 lr: 0.00332)\n",
            "epoch: 409, train_loss: 0.049230, val loss: 0.051890,  train_metric: 0.049 test_metric: 0.052 lr: 0.00332)\n",
            "epoch: 410, train_loss: 0.050255, val loss: 0.056769,  train_metric: 0.050 test_metric: 0.057 lr: 0.00331)\n",
            "epoch: 411, train_loss: 0.051712, val loss: 0.053739,  train_metric: 0.052 test_metric: 0.054 lr: 0.00331)\n",
            "epoch: 412, train_loss: 0.049523, val loss: 0.053494,  train_metric: 0.050 test_metric: 0.053 lr: 0.00331)\n",
            "epoch: 413, train_loss: 0.049682, val loss: 0.053149,  train_metric: 0.050 test_metric: 0.053 lr: 0.00330)\n",
            "epoch: 414, train_loss: 0.048751, val loss: 0.051032,  train_metric: 0.049 test_metric: 0.051 lr: 0.00330)\n",
            "epoch: 415, train_loss: 0.048114, val loss: 0.053049,  train_metric: 0.048 test_metric: 0.053 lr: 0.00330)\n",
            "epoch: 416, train_loss: 0.048352, val loss: 0.052518,  train_metric: 0.048 test_metric: 0.053 lr: 0.00329)\n",
            "epoch: 417, train_loss: 0.048533, val loss: 0.052008,  train_metric: 0.049 test_metric: 0.052 lr: 0.00329)\n",
            "epoch: 418, train_loss: 0.047988, val loss: 0.051737,  train_metric: 0.048 test_metric: 0.052 lr: 0.00329)\n",
            "epoch: 419, train_loss: 0.048472, val loss: 0.052540,  train_metric: 0.048 test_metric: 0.053 lr: 0.00328)\n",
            "epoch: 420, train_loss: 0.048681, val loss: 0.050712,  train_metric: 0.049 test_metric: 0.051 lr: 0.00328)\n",
            "epoch: 421, train_loss: 0.046588, val loss: 0.051085,  train_metric: 0.047 test_metric: 0.051 lr: 0.00328)\n",
            "epoch: 422, train_loss: 0.047146, val loss: 0.050819,  train_metric: 0.047 test_metric: 0.051 lr: 0.00327)\n",
            "epoch: 423, train_loss: 0.046773, val loss: 0.049973,  train_metric: 0.047 test_metric: 0.050 lr: 0.00327)\n",
            "epoch: 424, train_loss: 0.047771, val loss: 0.052100,  train_metric: 0.048 test_metric: 0.052 lr: 0.00327)\n",
            "epoch: 425, train_loss: 0.048172, val loss: 0.049587,  train_metric: 0.048 test_metric: 0.050 lr: 0.00326)\n",
            "epoch: 426, train_loss: 0.047378, val loss: 0.052955,  train_metric: 0.047 test_metric: 0.053 lr: 0.00326)\n",
            "epoch: 427, train_loss: 0.045952, val loss: 0.050323,  train_metric: 0.046 test_metric: 0.050 lr: 0.00326)\n",
            "epoch: 428, train_loss: 0.047842, val loss: 0.050425,  train_metric: 0.048 test_metric: 0.050 lr: 0.00326)\n",
            "epoch: 429, train_loss: 0.046861, val loss: 0.052104,  train_metric: 0.047 test_metric: 0.052 lr: 0.00325)\n",
            "epoch: 430, train_loss: 0.046363, val loss: 0.049304,  train_metric: 0.046 test_metric: 0.049 lr: 0.00325)\n",
            "epoch: 431, train_loss: 0.046998, val loss: 0.051668,  train_metric: 0.047 test_metric: 0.052 lr: 0.00325)\n",
            "epoch: 432, train_loss: 0.046349, val loss: 0.050126,  train_metric: 0.046 test_metric: 0.050 lr: 0.00324)\n",
            "epoch: 433, train_loss: 0.045622, val loss: 0.050147,  train_metric: 0.046 test_metric: 0.050 lr: 0.00324)\n",
            "epoch: 434, train_loss: 0.045986, val loss: 0.049244,  train_metric: 0.046 test_metric: 0.049 lr: 0.00324)\n",
            "epoch: 435, train_loss: 0.045891, val loss: 0.050065,  train_metric: 0.046 test_metric: 0.050 lr: 0.00323)\n",
            "epoch: 436, train_loss: 0.045897, val loss: 0.048916,  train_metric: 0.046 test_metric: 0.049 lr: 0.00323)\n",
            "epoch: 437, train_loss: 0.045382, val loss: 0.050582,  train_metric: 0.045 test_metric: 0.051 lr: 0.00323)\n",
            "epoch: 438, train_loss: 0.045365, val loss: 0.048904,  train_metric: 0.045 test_metric: 0.049 lr: 0.00322)\n",
            "epoch: 439, train_loss: 0.045215, val loss: 0.049818,  train_metric: 0.045 test_metric: 0.050 lr: 0.00322)\n",
            "epoch: 440, train_loss: 0.044752, val loss: 0.049866,  train_metric: 0.045 test_metric: 0.050 lr: 0.00322)\n",
            "epoch: 441, train_loss: 0.045247, val loss: 0.048476,  train_metric: 0.045 test_metric: 0.048 lr: 0.00321)\n",
            "epoch: 442, train_loss: 0.044588, val loss: 0.049490,  train_metric: 0.045 test_metric: 0.049 lr: 0.00321)\n",
            "epoch: 443, train_loss: 0.045681, val loss: 0.049842,  train_metric: 0.046 test_metric: 0.050 lr: 0.00321)\n",
            "epoch: 444, train_loss: 0.045865, val loss: 0.048169,  train_metric: 0.046 test_metric: 0.048 lr: 0.00320)\n",
            "epoch: 445, train_loss: 0.044490, val loss: 0.050915,  train_metric: 0.044 test_metric: 0.051 lr: 0.00320)\n",
            "epoch: 446, train_loss: 0.044788, val loss: 0.048622,  train_metric: 0.045 test_metric: 0.049 lr: 0.00320)\n",
            "epoch: 447, train_loss: 0.044317, val loss: 0.049038,  train_metric: 0.044 test_metric: 0.049 lr: 0.00319)\n",
            "epoch: 448, train_loss: 0.045278, val loss: 0.048441,  train_metric: 0.045 test_metric: 0.048 lr: 0.00319)\n",
            "epoch: 449, train_loss: 0.044224, val loss: 0.048087,  train_metric: 0.044 test_metric: 0.048 lr: 0.00319)\n",
            "epoch: 450, train_loss: 0.045280, val loss: 0.050599,  train_metric: 0.045 test_metric: 0.051 lr: 0.00318)\n",
            "epoch: 451, train_loss: 0.044766, val loss: 0.048954,  train_metric: 0.045 test_metric: 0.049 lr: 0.00318)\n",
            "epoch: 452, train_loss: 0.044648, val loss: 0.048696,  train_metric: 0.045 test_metric: 0.049 lr: 0.00318)\n",
            "epoch: 453, train_loss: 0.043842, val loss: 0.048456,  train_metric: 0.044 test_metric: 0.048 lr: 0.00317)\n",
            "epoch: 454, train_loss: 0.043742, val loss: 0.048667,  train_metric: 0.044 test_metric: 0.049 lr: 0.00317)\n",
            "epoch: 455, train_loss: 0.044353, val loss: 0.049009,  train_metric: 0.044 test_metric: 0.049 lr: 0.00317)\n",
            "epoch: 456, train_loss: 0.045379, val loss: 0.051052,  train_metric: 0.045 test_metric: 0.051 lr: 0.00317)\n",
            "epoch: 457, train_loss: 0.045460, val loss: 0.049542,  train_metric: 0.045 test_metric: 0.050 lr: 0.00316)\n",
            "epoch: 458, train_loss: 0.045660, val loss: 0.051264,  train_metric: 0.046 test_metric: 0.051 lr: 0.00316)\n",
            "epoch: 459, train_loss: 0.046759, val loss: 0.049912,  train_metric: 0.047 test_metric: 0.050 lr: 0.00316)\n",
            "epoch: 460, train_loss: 0.046495, val loss: 0.051577,  train_metric: 0.046 test_metric: 0.052 lr: 0.00315)\n",
            "epoch: 461, train_loss: 0.044128, val loss: 0.047774,  train_metric: 0.044 test_metric: 0.048 lr: 0.00315)\n",
            "epoch: 462, train_loss: 0.043849, val loss: 0.049424,  train_metric: 0.044 test_metric: 0.049 lr: 0.00315)\n",
            "epoch: 463, train_loss: 0.043973, val loss: 0.048807,  train_metric: 0.044 test_metric: 0.049 lr: 0.00314)\n",
            "epoch: 464, train_loss: 0.043757, val loss: 0.047962,  train_metric: 0.044 test_metric: 0.048 lr: 0.00314)\n",
            "epoch: 465, train_loss: 0.044333, val loss: 0.048239,  train_metric: 0.044 test_metric: 0.048 lr: 0.00314)\n",
            "epoch: 466, train_loss: 0.045324, val loss: 0.052238,  train_metric: 0.045 test_metric: 0.052 lr: 0.00313)\n",
            "epoch: 467, train_loss: 0.045394, val loss: 0.051315,  train_metric: 0.045 test_metric: 0.051 lr: 0.00313)\n",
            "epoch: 468, train_loss: 0.045584, val loss: 0.050599,  train_metric: 0.046 test_metric: 0.051 lr: 0.00313)\n",
            "epoch: 469, train_loss: 0.045096, val loss: 0.048318,  train_metric: 0.045 test_metric: 0.048 lr: 0.00312)\n",
            "epoch: 470, train_loss: 0.043454, val loss: 0.048136,  train_metric: 0.043 test_metric: 0.048 lr: 0.00312)\n",
            "epoch: 471, train_loss: 0.042921, val loss: 0.048121,  train_metric: 0.043 test_metric: 0.048 lr: 0.00312)\n",
            "epoch: 472, train_loss: 0.043004, val loss: 0.048628,  train_metric: 0.043 test_metric: 0.049 lr: 0.00311)\n",
            "epoch: 473, train_loss: 0.044052, val loss: 0.047662,  train_metric: 0.044 test_metric: 0.048 lr: 0.00311)\n",
            "epoch: 474, train_loss: 0.045566, val loss: 0.050413,  train_metric: 0.046 test_metric: 0.050 lr: 0.00311)\n",
            "epoch: 475, train_loss: 0.045135, val loss: 0.051507,  train_metric: 0.045 test_metric: 0.052 lr: 0.00311)\n",
            "epoch: 476, train_loss: 0.046592, val loss: 0.049458,  train_metric: 0.047 test_metric: 0.049 lr: 0.00310)\n",
            "epoch: 477, train_loss: 0.044775, val loss: 0.049104,  train_metric: 0.045 test_metric: 0.049 lr: 0.00310)\n",
            "epoch: 478, train_loss: 0.044865, val loss: 0.049123,  train_metric: 0.045 test_metric: 0.049 lr: 0.00310)\n",
            "epoch: 479, train_loss: 0.043456, val loss: 0.049339,  train_metric: 0.043 test_metric: 0.049 lr: 0.00309)\n",
            "epoch: 480, train_loss: 0.043355, val loss: 0.048005,  train_metric: 0.043 test_metric: 0.048 lr: 0.00309)\n",
            "epoch: 481, train_loss: 0.042605, val loss: 0.047059,  train_metric: 0.043 test_metric: 0.047 lr: 0.00309)\n",
            "epoch: 482, train_loss: 0.042307, val loss: 0.046883,  train_metric: 0.042 test_metric: 0.047 lr: 0.00308)\n",
            "epoch: 483, train_loss: 0.043826, val loss: 0.048875,  train_metric: 0.044 test_metric: 0.049 lr: 0.00308)\n",
            "epoch: 484, train_loss: 0.043525, val loss: 0.048494,  train_metric: 0.044 test_metric: 0.048 lr: 0.00308)\n",
            "epoch: 485, train_loss: 0.043599, val loss: 0.048942,  train_metric: 0.044 test_metric: 0.049 lr: 0.00307)\n",
            "epoch: 486, train_loss: 0.043104, val loss: 0.046933,  train_metric: 0.043 test_metric: 0.047 lr: 0.00307)\n",
            "epoch: 487, train_loss: 0.042749, val loss: 0.046659,  train_metric: 0.043 test_metric: 0.047 lr: 0.00307)\n",
            "epoch: 488, train_loss: 0.042800, val loss: 0.047623,  train_metric: 0.043 test_metric: 0.048 lr: 0.00307)\n",
            "epoch: 489, train_loss: 0.042260, val loss: 0.046443,  train_metric: 0.042 test_metric: 0.046 lr: 0.00306)\n",
            "epoch: 490, train_loss: 0.042492, val loss: 0.047802,  train_metric: 0.042 test_metric: 0.048 lr: 0.00306)\n",
            "epoch: 491, train_loss: 0.042512, val loss: 0.046716,  train_metric: 0.043 test_metric: 0.047 lr: 0.00306)\n",
            "epoch: 492, train_loss: 0.042346, val loss: 0.048208,  train_metric: 0.042 test_metric: 0.048 lr: 0.00305)\n",
            "epoch: 493, train_loss: 0.043453, val loss: 0.046898,  train_metric: 0.043 test_metric: 0.047 lr: 0.00305)\n",
            "epoch: 494, train_loss: 0.043275, val loss: 0.047404,  train_metric: 0.043 test_metric: 0.047 lr: 0.00305)\n",
            "epoch: 495, train_loss: 0.044135, val loss: 0.047674,  train_metric: 0.044 test_metric: 0.048 lr: 0.00304)\n",
            "epoch: 496, train_loss: 0.042811, val loss: 0.046309,  train_metric: 0.043 test_metric: 0.046 lr: 0.00304)\n",
            "epoch: 497, train_loss: 0.041608, val loss: 0.047365,  train_metric: 0.042 test_metric: 0.047 lr: 0.00304)\n",
            "epoch: 498, train_loss: 0.042094, val loss: 0.046431,  train_metric: 0.042 test_metric: 0.046 lr: 0.00303)\n",
            "epoch: 499, train_loss: 0.041615, val loss: 0.046849,  train_metric: 0.042 test_metric: 0.047 lr: 0.00303)\n",
            "epoch: 500, train_loss: 0.042132, val loss: 0.046402,  train_metric: 0.042 test_metric: 0.046 lr: 0.00303)\n",
            "epoch: 501, train_loss: 0.042876, val loss: 0.046200,  train_metric: 0.043 test_metric: 0.046 lr: 0.00303)\n",
            "epoch: 502, train_loss: 0.042753, val loss: 0.047797,  train_metric: 0.043 test_metric: 0.048 lr: 0.00302)\n",
            "epoch: 503, train_loss: 0.043096, val loss: 0.045576,  train_metric: 0.043 test_metric: 0.046 lr: 0.00302)\n",
            "epoch: 504, train_loss: 0.043758, val loss: 0.047880,  train_metric: 0.044 test_metric: 0.048 lr: 0.00302)\n",
            "epoch: 505, train_loss: 0.044113, val loss: 0.048202,  train_metric: 0.044 test_metric: 0.048 lr: 0.00301)\n",
            "epoch: 506, train_loss: 0.043384, val loss: 0.049142,  train_metric: 0.043 test_metric: 0.049 lr: 0.00301)\n",
            "epoch: 507, train_loss: 0.042754, val loss: 0.047553,  train_metric: 0.043 test_metric: 0.048 lr: 0.00301)\n",
            "epoch: 508, train_loss: 0.042487, val loss: 0.047945,  train_metric: 0.042 test_metric: 0.048 lr: 0.00300)\n",
            "epoch: 509, train_loss: 0.043240, val loss: 0.048126,  train_metric: 0.043 test_metric: 0.048 lr: 0.00300)\n",
            "epoch: 510, train_loss: 0.043191, val loss: 0.047940,  train_metric: 0.043 test_metric: 0.048 lr: 0.00300)\n",
            "epoch: 511, train_loss: 0.042964, val loss: 0.046550,  train_metric: 0.043 test_metric: 0.047 lr: 0.00300)\n",
            "epoch: 512, train_loss: 0.041864, val loss: 0.047639,  train_metric: 0.042 test_metric: 0.048 lr: 0.00299)\n",
            "epoch: 513, train_loss: 0.043146, val loss: 0.047929,  train_metric: 0.043 test_metric: 0.048 lr: 0.00299)\n",
            "epoch: 514, train_loss: 0.042770, val loss: 0.046705,  train_metric: 0.043 test_metric: 0.047 lr: 0.00299)\n",
            "epoch: 515, train_loss: 0.042088, val loss: 0.046830,  train_metric: 0.042 test_metric: 0.047 lr: 0.00298)\n",
            "epoch: 516, train_loss: 0.042540, val loss: 0.047020,  train_metric: 0.043 test_metric: 0.047 lr: 0.00298)\n",
            "epoch: 517, train_loss: 0.043288, val loss: 0.046937,  train_metric: 0.043 test_metric: 0.047 lr: 0.00298)\n",
            "epoch: 518, train_loss: 0.042019, val loss: 0.047297,  train_metric: 0.042 test_metric: 0.047 lr: 0.00297)\n",
            "epoch: 519, train_loss: 0.041332, val loss: 0.045478,  train_metric: 0.041 test_metric: 0.045 lr: 0.00297)\n",
            "epoch: 520, train_loss: 0.041746, val loss: 0.046540,  train_metric: 0.042 test_metric: 0.047 lr: 0.00297)\n",
            "epoch: 521, train_loss: 0.041459, val loss: 0.046605,  train_metric: 0.041 test_metric: 0.047 lr: 0.00297)\n",
            "epoch: 522, train_loss: 0.040812, val loss: 0.045971,  train_metric: 0.041 test_metric: 0.046 lr: 0.00296)\n",
            "epoch: 523, train_loss: 0.042016, val loss: 0.046847,  train_metric: 0.042 test_metric: 0.047 lr: 0.00296)\n",
            "epoch: 524, train_loss: 0.040924, val loss: 0.045280,  train_metric: 0.041 test_metric: 0.045 lr: 0.00296)\n",
            "epoch: 525, train_loss: 0.040310, val loss: 0.045158,  train_metric: 0.040 test_metric: 0.045 lr: 0.00295)\n",
            "epoch: 526, train_loss: 0.040331, val loss: 0.045406,  train_metric: 0.040 test_metric: 0.045 lr: 0.00295)\n",
            "epoch: 527, train_loss: 0.040470, val loss: 0.045141,  train_metric: 0.040 test_metric: 0.045 lr: 0.00295)\n",
            "epoch: 528, train_loss: 0.040157, val loss: 0.045015,  train_metric: 0.040 test_metric: 0.045 lr: 0.00295)\n",
            "epoch: 529, train_loss: 0.040593, val loss: 0.045588,  train_metric: 0.041 test_metric: 0.046 lr: 0.00294)\n",
            "epoch: 530, train_loss: 0.040514, val loss: 0.045765,  train_metric: 0.041 test_metric: 0.046 lr: 0.00294)\n",
            "epoch: 531, train_loss: 0.040772, val loss: 0.045712,  train_metric: 0.041 test_metric: 0.046 lr: 0.00294)\n",
            "epoch: 532, train_loss: 0.040707, val loss: 0.046461,  train_metric: 0.041 test_metric: 0.046 lr: 0.00293)\n",
            "epoch: 533, train_loss: 0.041125, val loss: 0.045636,  train_metric: 0.041 test_metric: 0.046 lr: 0.00293)\n",
            "epoch: 534, train_loss: 0.041354, val loss: 0.045568,  train_metric: 0.041 test_metric: 0.046 lr: 0.00293)\n",
            "epoch: 535, train_loss: 0.042654, val loss: 0.046850,  train_metric: 0.043 test_metric: 0.047 lr: 0.00292)\n",
            "epoch: 536, train_loss: 0.042015, val loss: 0.046605,  train_metric: 0.042 test_metric: 0.047 lr: 0.00292)\n",
            "epoch: 537, train_loss: 0.041483, val loss: 0.044915,  train_metric: 0.041 test_metric: 0.045 lr: 0.00292)\n",
            "epoch: 538, train_loss: 0.042950, val loss: 0.048585,  train_metric: 0.043 test_metric: 0.049 lr: 0.00292)\n",
            "epoch: 539, train_loss: 0.041498, val loss: 0.045052,  train_metric: 0.041 test_metric: 0.045 lr: 0.00291)\n",
            "epoch: 540, train_loss: 0.041424, val loss: 0.045495,  train_metric: 0.041 test_metric: 0.045 lr: 0.00291)\n",
            "epoch: 541, train_loss: 0.040944, val loss: 0.046402,  train_metric: 0.041 test_metric: 0.046 lr: 0.00291)\n",
            "epoch: 542, train_loss: 0.040906, val loss: 0.045429,  train_metric: 0.041 test_metric: 0.045 lr: 0.00290)\n",
            "epoch: 543, train_loss: 0.041107, val loss: 0.046836,  train_metric: 0.041 test_metric: 0.047 lr: 0.00290)\n",
            "epoch: 544, train_loss: 0.041364, val loss: 0.045351,  train_metric: 0.041 test_metric: 0.045 lr: 0.00290)\n",
            "epoch: 545, train_loss: 0.040518, val loss: 0.046221,  train_metric: 0.041 test_metric: 0.046 lr: 0.00290)\n",
            "epoch: 546, train_loss: 0.040617, val loss: 0.045089,  train_metric: 0.041 test_metric: 0.045 lr: 0.00289)\n",
            "epoch: 547, train_loss: 0.040769, val loss: 0.044685,  train_metric: 0.041 test_metric: 0.045 lr: 0.00289)\n",
            "epoch: 548, train_loss: 0.039855, val loss: 0.045488,  train_metric: 0.040 test_metric: 0.045 lr: 0.00289)\n",
            "epoch: 549, train_loss: 0.039880, val loss: 0.046004,  train_metric: 0.040 test_metric: 0.046 lr: 0.00288)\n",
            "epoch: 550, train_loss: 0.039951, val loss: 0.044770,  train_metric: 0.040 test_metric: 0.045 lr: 0.00288)\n",
            "epoch: 551, train_loss: 0.039914, val loss: 0.045222,  train_metric: 0.040 test_metric: 0.045 lr: 0.00288)\n",
            "epoch: 552, train_loss: 0.040278, val loss: 0.046143,  train_metric: 0.040 test_metric: 0.046 lr: 0.00288)\n",
            "epoch: 553, train_loss: 0.040347, val loss: 0.045069,  train_metric: 0.040 test_metric: 0.045 lr: 0.00287)\n",
            "epoch: 554, train_loss: 0.040497, val loss: 0.044691,  train_metric: 0.040 test_metric: 0.045 lr: 0.00287)\n",
            "epoch: 555, train_loss: 0.039796, val loss: 0.044812,  train_metric: 0.040 test_metric: 0.045 lr: 0.00287)\n",
            "epoch: 556, train_loss: 0.040530, val loss: 0.045759,  train_metric: 0.041 test_metric: 0.046 lr: 0.00286)\n",
            "epoch: 557, train_loss: 0.040341, val loss: 0.044881,  train_metric: 0.040 test_metric: 0.045 lr: 0.00286)\n",
            "epoch: 558, train_loss: 0.039951, val loss: 0.045179,  train_metric: 0.040 test_metric: 0.045 lr: 0.00286)\n",
            "epoch: 559, train_loss: 0.040217, val loss: 0.044850,  train_metric: 0.040 test_metric: 0.045 lr: 0.00286)\n",
            "epoch: 560, train_loss: 0.040188, val loss: 0.045403,  train_metric: 0.040 test_metric: 0.045 lr: 0.00285)\n",
            "epoch: 561, train_loss: 0.039930, val loss: 0.044485,  train_metric: 0.040 test_metric: 0.044 lr: 0.00285)\n",
            "epoch: 562, train_loss: 0.039877, val loss: 0.044729,  train_metric: 0.040 test_metric: 0.045 lr: 0.00285)\n",
            "epoch: 563, train_loss: 0.039710, val loss: 0.044097,  train_metric: 0.040 test_metric: 0.044 lr: 0.00284)\n",
            "epoch: 564, train_loss: 0.039440, val loss: 0.045517,  train_metric: 0.039 test_metric: 0.046 lr: 0.00284)\n",
            "epoch: 565, train_loss: 0.040187, val loss: 0.044801,  train_metric: 0.040 test_metric: 0.045 lr: 0.00284)\n",
            "epoch: 566, train_loss: 0.039717, val loss: 0.045361,  train_metric: 0.040 test_metric: 0.045 lr: 0.00284)\n",
            "epoch: 567, train_loss: 0.040479, val loss: 0.044586,  train_metric: 0.040 test_metric: 0.045 lr: 0.00283)\n",
            "epoch: 568, train_loss: 0.041092, val loss: 0.045054,  train_metric: 0.041 test_metric: 0.045 lr: 0.00283)\n",
            "epoch: 569, train_loss: 0.040665, val loss: 0.045780,  train_metric: 0.041 test_metric: 0.046 lr: 0.00283)\n",
            "epoch: 570, train_loss: 0.040236, val loss: 0.044856,  train_metric: 0.040 test_metric: 0.045 lr: 0.00282)\n",
            "epoch: 571, train_loss: 0.040127, val loss: 0.046311,  train_metric: 0.040 test_metric: 0.046 lr: 0.00282)\n",
            "epoch: 572, train_loss: 0.040498, val loss: 0.045598,  train_metric: 0.040 test_metric: 0.046 lr: 0.00282)\n",
            "epoch: 573, train_loss: 0.040813, val loss: 0.044897,  train_metric: 0.041 test_metric: 0.045 lr: 0.00282)\n",
            "epoch: 574, train_loss: 0.039813, val loss: 0.045469,  train_metric: 0.040 test_metric: 0.045 lr: 0.00281)\n",
            "epoch: 575, train_loss: 0.039721, val loss: 0.044924,  train_metric: 0.040 test_metric: 0.045 lr: 0.00281)\n",
            "epoch: 576, train_loss: 0.041014, val loss: 0.045644,  train_metric: 0.041 test_metric: 0.046 lr: 0.00281)\n",
            "epoch: 577, train_loss: 0.042290, val loss: 0.048018,  train_metric: 0.042 test_metric: 0.048 lr: 0.00280)\n",
            "epoch: 578, train_loss: 0.043417, val loss: 0.049187,  train_metric: 0.043 test_metric: 0.049 lr: 0.00280)\n",
            "epoch: 579, train_loss: 0.042665, val loss: 0.044558,  train_metric: 0.043 test_metric: 0.045 lr: 0.00280)\n",
            "epoch: 580, train_loss: 0.040797, val loss: 0.046401,  train_metric: 0.041 test_metric: 0.046 lr: 0.00280)\n",
            "epoch: 581, train_loss: 0.040509, val loss: 0.044751,  train_metric: 0.041 test_metric: 0.045 lr: 0.00279)\n",
            "epoch: 582, train_loss: 0.040101, val loss: 0.044108,  train_metric: 0.040 test_metric: 0.044 lr: 0.00279)\n",
            "epoch: 583, train_loss: 0.039728, val loss: 0.045761,  train_metric: 0.040 test_metric: 0.046 lr: 0.00279)\n",
            "epoch: 584, train_loss: 0.041160, val loss: 0.046715,  train_metric: 0.041 test_metric: 0.047 lr: 0.00278)\n",
            "epoch: 585, train_loss: 0.041057, val loss: 0.044189,  train_metric: 0.041 test_metric: 0.044 lr: 0.00278)\n",
            "epoch: 586, train_loss: 0.039878, val loss: 0.045519,  train_metric: 0.040 test_metric: 0.046 lr: 0.00278)\n",
            "epoch: 587, train_loss: 0.040486, val loss: 0.045517,  train_metric: 0.040 test_metric: 0.046 lr: 0.00278)\n",
            "epoch: 588, train_loss: 0.041905, val loss: 0.047392,  train_metric: 0.042 test_metric: 0.047 lr: 0.00277)\n",
            "epoch: 589, train_loss: 0.042098, val loss: 0.046741,  train_metric: 0.042 test_metric: 0.047 lr: 0.00277)\n",
            "epoch: 590, train_loss: 0.042071, val loss: 0.047806,  train_metric: 0.042 test_metric: 0.048 lr: 0.00277)\n",
            "epoch: 591, train_loss: 0.042776, val loss: 0.047853,  train_metric: 0.043 test_metric: 0.048 lr: 0.00277)\n",
            "epoch: 592, train_loss: 0.041034, val loss: 0.047136,  train_metric: 0.041 test_metric: 0.047 lr: 0.00276)\n",
            "epoch: 593, train_loss: 0.041219, val loss: 0.045131,  train_metric: 0.041 test_metric: 0.045 lr: 0.00276)\n",
            "epoch: 594, train_loss: 0.041607, val loss: 0.045457,  train_metric: 0.042 test_metric: 0.045 lr: 0.00276)\n",
            "epoch: 595, train_loss: 0.040153, val loss: 0.044834,  train_metric: 0.040 test_metric: 0.045 lr: 0.00275)\n",
            "epoch: 596, train_loss: 0.040036, val loss: 0.045985,  train_metric: 0.040 test_metric: 0.046 lr: 0.00275)\n",
            "epoch: 597, train_loss: 0.040791, val loss: 0.045385,  train_metric: 0.041 test_metric: 0.045 lr: 0.00275)\n",
            "epoch: 598, train_loss: 0.040568, val loss: 0.045743,  train_metric: 0.041 test_metric: 0.046 lr: 0.00275)\n",
            "epoch: 599, train_loss: 0.040107, val loss: 0.044241,  train_metric: 0.040 test_metric: 0.044 lr: 0.00274)\n",
            "epoch: 600, train_loss: 0.040520, val loss: 0.047292,  train_metric: 0.041 test_metric: 0.047 lr: 0.00274)\n",
            "epoch: 601, train_loss: 0.041516, val loss: 0.045492,  train_metric: 0.042 test_metric: 0.045 lr: 0.00274)\n",
            "epoch: 602, train_loss: 0.041648, val loss: 0.045980,  train_metric: 0.042 test_metric: 0.046 lr: 0.00274)\n",
            "epoch: 603, train_loss: 0.041006, val loss: 0.045458,  train_metric: 0.041 test_metric: 0.045 lr: 0.00273)\n",
            "epoch: 604, train_loss: 0.039949, val loss: 0.044356,  train_metric: 0.040 test_metric: 0.044 lr: 0.00273)\n",
            "epoch: 605, train_loss: 0.040522, val loss: 0.045391,  train_metric: 0.041 test_metric: 0.045 lr: 0.00273)\n",
            "epoch: 606, train_loss: 0.040548, val loss: 0.045652,  train_metric: 0.041 test_metric: 0.046 lr: 0.00272)\n",
            "epoch: 607, train_loss: 0.040874, val loss: 0.049100,  train_metric: 0.041 test_metric: 0.049 lr: 0.00272)\n",
            "epoch: 608, train_loss: 0.041715, val loss: 0.045407,  train_metric: 0.042 test_metric: 0.045 lr: 0.00272)\n",
            "epoch: 609, train_loss: 0.040080, val loss: 0.045597,  train_metric: 0.040 test_metric: 0.046 lr: 0.00272)\n",
            "epoch: 610, train_loss: 0.039767, val loss: 0.046605,  train_metric: 0.040 test_metric: 0.047 lr: 0.00271)\n",
            "epoch: 611, train_loss: 0.041236, val loss: 0.045287,  train_metric: 0.041 test_metric: 0.045 lr: 0.00271)\n",
            "epoch: 612, train_loss: 0.040101, val loss: 0.044327,  train_metric: 0.040 test_metric: 0.044 lr: 0.00271)\n",
            "epoch: 613, train_loss: 0.040469, val loss: 0.044965,  train_metric: 0.040 test_metric: 0.045 lr: 0.00271)\n",
            "epoch: 614, train_loss: 0.040202, val loss: 0.048213,  train_metric: 0.040 test_metric: 0.048 lr: 0.00270)\n",
            "epoch: 615, train_loss: 0.041091, val loss: 0.046802,  train_metric: 0.041 test_metric: 0.047 lr: 0.00270)\n",
            "epoch: 616, train_loss: 0.040896, val loss: 0.045613,  train_metric: 0.041 test_metric: 0.046 lr: 0.00270)\n",
            "epoch: 617, train_loss: 0.040713, val loss: 0.044363,  train_metric: 0.041 test_metric: 0.044 lr: 0.00269)\n",
            "epoch: 618, train_loss: 0.039269, val loss: 0.044810,  train_metric: 0.039 test_metric: 0.045 lr: 0.00269)\n",
            "epoch: 619, train_loss: 0.039556, val loss: 0.043323,  train_metric: 0.040 test_metric: 0.043 lr: 0.00269)\n",
            "epoch: 620, train_loss: 0.039041, val loss: 0.044143,  train_metric: 0.039 test_metric: 0.044 lr: 0.00269)\n",
            "epoch: 621, train_loss: 0.039148, val loss: 0.043764,  train_metric: 0.039 test_metric: 0.044 lr: 0.00268)\n",
            "epoch: 622, train_loss: 0.039662, val loss: 0.043640,  train_metric: 0.040 test_metric: 0.044 lr: 0.00268)\n",
            "epoch: 623, train_loss: 0.038707, val loss: 0.043482,  train_metric: 0.039 test_metric: 0.043 lr: 0.00268)\n",
            "epoch: 624, train_loss: 0.038687, val loss: 0.043873,  train_metric: 0.039 test_metric: 0.044 lr: 0.00268)\n",
            "epoch: 625, train_loss: 0.038714, val loss: 0.043150,  train_metric: 0.039 test_metric: 0.043 lr: 0.00267)\n",
            "epoch: 626, train_loss: 0.038912, val loss: 0.043662,  train_metric: 0.039 test_metric: 0.044 lr: 0.00267)\n",
            "epoch: 627, train_loss: 0.039109, val loss: 0.044094,  train_metric: 0.039 test_metric: 0.044 lr: 0.00267)\n",
            "epoch: 628, train_loss: 0.038969, val loss: 0.043696,  train_metric: 0.039 test_metric: 0.044 lr: 0.00266)\n",
            "epoch: 629, train_loss: 0.038868, val loss: 0.044089,  train_metric: 0.039 test_metric: 0.044 lr: 0.00266)\n",
            "epoch: 630, train_loss: 0.039023, val loss: 0.043297,  train_metric: 0.039 test_metric: 0.043 lr: 0.00266)\n",
            "epoch: 631, train_loss: 0.038631, val loss: 0.044172,  train_metric: 0.039 test_metric: 0.044 lr: 0.00266)\n",
            "epoch: 632, train_loss: 0.039164, val loss: 0.044953,  train_metric: 0.039 test_metric: 0.045 lr: 0.00265)\n",
            "epoch: 633, train_loss: 0.040672, val loss: 0.045786,  train_metric: 0.041 test_metric: 0.046 lr: 0.00265)\n",
            "epoch: 634, train_loss: 0.042509, val loss: 0.051030,  train_metric: 0.043 test_metric: 0.051 lr: 0.00265)\n",
            "epoch: 635, train_loss: 0.042770, val loss: 0.047756,  train_metric: 0.043 test_metric: 0.048 lr: 0.00265)\n",
            "epoch: 636, train_loss: 0.043142, val loss: 0.053946,  train_metric: 0.043 test_metric: 0.054 lr: 0.00264)\n",
            "epoch: 637, train_loss: 0.046716, val loss: 0.050431,  train_metric: 0.047 test_metric: 0.050 lr: 0.00264)\n",
            "epoch: 638, train_loss: 0.047580, val loss: 0.048762,  train_metric: 0.048 test_metric: 0.049 lr: 0.00264)\n",
            "epoch: 639, train_loss: 0.044875, val loss: 0.048646,  train_metric: 0.045 test_metric: 0.049 lr: 0.00264)\n",
            "epoch: 640, train_loss: 0.043379, val loss: 0.047706,  train_metric: 0.043 test_metric: 0.048 lr: 0.00263)\n",
            "epoch: 641, train_loss: 0.041275, val loss: 0.046065,  train_metric: 0.041 test_metric: 0.046 lr: 0.00263)\n",
            "epoch: 642, train_loss: 0.041807, val loss: 0.046162,  train_metric: 0.042 test_metric: 0.046 lr: 0.00263)\n",
            "epoch: 643, train_loss: 0.041025, val loss: 0.044305,  train_metric: 0.041 test_metric: 0.044 lr: 0.00263)\n",
            "epoch: 644, train_loss: 0.041193, val loss: 0.046411,  train_metric: 0.041 test_metric: 0.046 lr: 0.00262)\n",
            "epoch: 645, train_loss: 0.042229, val loss: 0.044738,  train_metric: 0.042 test_metric: 0.045 lr: 0.00262)\n",
            "epoch: 646, train_loss: 0.039322, val loss: 0.043519,  train_metric: 0.039 test_metric: 0.044 lr: 0.00262)\n",
            "epoch: 647, train_loss: 0.039420, val loss: 0.043355,  train_metric: 0.039 test_metric: 0.043 lr: 0.00261)\n",
            "epoch: 648, train_loss: 0.038943, val loss: 0.044332,  train_metric: 0.039 test_metric: 0.044 lr: 0.00261)\n",
            "epoch: 649, train_loss: 0.039031, val loss: 0.044294,  train_metric: 0.039 test_metric: 0.044 lr: 0.00261)\n",
            "epoch: 650, train_loss: 0.039089, val loss: 0.044227,  train_metric: 0.039 test_metric: 0.044 lr: 0.00261)\n",
            "epoch: 651, train_loss: 0.039447, val loss: 0.044728,  train_metric: 0.039 test_metric: 0.045 lr: 0.00260)\n",
            "epoch: 652, train_loss: 0.039619, val loss: 0.044415,  train_metric: 0.040 test_metric: 0.044 lr: 0.00260)\n",
            "epoch: 653, train_loss: 0.039902, val loss: 0.045687,  train_metric: 0.040 test_metric: 0.046 lr: 0.00260)\n",
            "epoch: 654, train_loss: 0.040415, val loss: 0.044115,  train_metric: 0.040 test_metric: 0.044 lr: 0.00260)\n",
            "epoch: 655, train_loss: 0.040435, val loss: 0.045029,  train_metric: 0.040 test_metric: 0.045 lr: 0.00259)\n",
            "epoch: 656, train_loss: 0.041312, val loss: 0.043912,  train_metric: 0.041 test_metric: 0.044 lr: 0.00259)\n",
            "epoch: 657, train_loss: 0.040143, val loss: 0.045254,  train_metric: 0.040 test_metric: 0.045 lr: 0.00259)\n",
            "epoch: 658, train_loss: 0.039077, val loss: 0.043295,  train_metric: 0.039 test_metric: 0.043 lr: 0.00259)\n",
            "epoch: 659, train_loss: 0.039145, val loss: 0.043907,  train_metric: 0.039 test_metric: 0.044 lr: 0.00258)\n",
            "epoch: 660, train_loss: 0.038762, val loss: 0.043018,  train_metric: 0.039 test_metric: 0.043 lr: 0.00258)\n",
            "epoch: 661, train_loss: 0.038960, val loss: 0.043971,  train_metric: 0.039 test_metric: 0.044 lr: 0.00258)\n",
            "epoch: 662, train_loss: 0.038595, val loss: 0.042957,  train_metric: 0.039 test_metric: 0.043 lr: 0.00258)\n",
            "epoch: 663, train_loss: 0.038363, val loss: 0.043695,  train_metric: 0.038 test_metric: 0.044 lr: 0.00257)\n",
            "epoch: 664, train_loss: 0.039218, val loss: 0.042848,  train_metric: 0.039 test_metric: 0.043 lr: 0.00257)\n",
            "epoch: 665, train_loss: 0.038650, val loss: 0.043548,  train_metric: 0.039 test_metric: 0.044 lr: 0.00257)\n",
            "epoch: 666, train_loss: 0.039377, val loss: 0.043404,  train_metric: 0.039 test_metric: 0.043 lr: 0.00257)\n",
            "epoch: 667, train_loss: 0.038825, val loss: 0.043448,  train_metric: 0.039 test_metric: 0.043 lr: 0.00256)\n",
            "epoch: 668, train_loss: 0.038701, val loss: 0.043716,  train_metric: 0.039 test_metric: 0.044 lr: 0.00256)\n",
            "epoch: 669, train_loss: 0.038973, val loss: 0.043064,  train_metric: 0.039 test_metric: 0.043 lr: 0.00256)\n",
            "epoch: 670, train_loss: 0.039087, val loss: 0.046740,  train_metric: 0.039 test_metric: 0.047 lr: 0.00256)\n",
            "epoch: 671, train_loss: 0.040856, val loss: 0.046127,  train_metric: 0.041 test_metric: 0.046 lr: 0.00255)\n",
            "epoch: 672, train_loss: 0.040877, val loss: 0.047631,  train_metric: 0.041 test_metric: 0.048 lr: 0.00255)\n",
            "epoch: 673, train_loss: 0.050727, val loss: 0.065140,  train_metric: 0.051 test_metric: 0.065 lr: 0.00255)\n",
            "epoch: 674, train_loss: 0.052882, val loss: 0.048621,  train_metric: 0.053 test_metric: 0.049 lr: 0.00254)\n",
            "epoch: 675, train_loss: 0.043607, val loss: 0.050187,  train_metric: 0.044 test_metric: 0.050 lr: 0.00254)\n",
            "epoch: 676, train_loss: 0.043562, val loss: 0.044812,  train_metric: 0.044 test_metric: 0.045 lr: 0.00254)\n",
            "epoch: 677, train_loss: 0.041260, val loss: 0.049048,  train_metric: 0.041 test_metric: 0.049 lr: 0.00254)\n",
            "epoch: 678, train_loss: 0.042269, val loss: 0.047065,  train_metric: 0.042 test_metric: 0.047 lr: 0.00253)\n",
            "epoch: 679, train_loss: 0.043498, val loss: 0.047120,  train_metric: 0.043 test_metric: 0.047 lr: 0.00253)\n",
            "epoch: 680, train_loss: 0.041608, val loss: 0.049296,  train_metric: 0.042 test_metric: 0.049 lr: 0.00253)\n",
            "epoch: 681, train_loss: 0.042524, val loss: 0.047192,  train_metric: 0.043 test_metric: 0.047 lr: 0.00253)\n",
            "epoch: 682, train_loss: 0.040185, val loss: 0.044556,  train_metric: 0.040 test_metric: 0.045 lr: 0.00252)\n",
            "epoch: 683, train_loss: 0.039676, val loss: 0.043215,  train_metric: 0.040 test_metric: 0.043 lr: 0.00252)\n",
            "epoch: 684, train_loss: 0.039447, val loss: 0.043911,  train_metric: 0.039 test_metric: 0.044 lr: 0.00252)\n",
            "epoch: 685, train_loss: 0.039025, val loss: 0.043969,  train_metric: 0.039 test_metric: 0.044 lr: 0.00252)\n",
            "epoch: 686, train_loss: 0.038575, val loss: 0.042751,  train_metric: 0.039 test_metric: 0.043 lr: 0.00251)\n",
            "epoch: 687, train_loss: 0.039739, val loss: 0.044069,  train_metric: 0.040 test_metric: 0.044 lr: 0.00251)\n",
            "epoch: 688, train_loss: 0.038636, val loss: 0.043255,  train_metric: 0.039 test_metric: 0.043 lr: 0.00251)\n",
            "epoch: 689, train_loss: 0.038658, val loss: 0.043063,  train_metric: 0.039 test_metric: 0.043 lr: 0.00251)\n",
            "epoch: 690, train_loss: 0.038136, val loss: 0.043365,  train_metric: 0.038 test_metric: 0.043 lr: 0.00250)\n",
            "epoch: 691, train_loss: 0.038561, val loss: 0.042780,  train_metric: 0.039 test_metric: 0.043 lr: 0.00250)\n",
            "epoch: 692, train_loss: 0.038108, val loss: 0.042503,  train_metric: 0.038 test_metric: 0.043 lr: 0.00250)\n",
            "epoch: 693, train_loss: 0.037895, val loss: 0.042803,  train_metric: 0.038 test_metric: 0.043 lr: 0.00250)\n",
            "epoch: 694, train_loss: 0.037710, val loss: 0.042786,  train_metric: 0.038 test_metric: 0.043 lr: 0.00249)\n",
            "epoch: 695, train_loss: 0.038113, val loss: 0.042693,  train_metric: 0.038 test_metric: 0.043 lr: 0.00249)\n",
            "epoch: 696, train_loss: 0.037947, val loss: 0.043839,  train_metric: 0.038 test_metric: 0.044 lr: 0.00249)\n",
            "epoch: 697, train_loss: 0.038405, val loss: 0.042590,  train_metric: 0.038 test_metric: 0.043 lr: 0.00249)\n",
            "epoch: 698, train_loss: 0.038067, val loss: 0.042667,  train_metric: 0.038 test_metric: 0.043 lr: 0.00248)\n",
            "epoch: 699, train_loss: 0.037719, val loss: 0.043127,  train_metric: 0.038 test_metric: 0.043 lr: 0.00248)\n",
            "epoch: 700, train_loss: 0.038033, val loss: 0.042501,  train_metric: 0.038 test_metric: 0.043 lr: 0.00248)\n",
            "epoch: 701, train_loss: 0.038378, val loss: 0.042762,  train_metric: 0.038 test_metric: 0.043 lr: 0.00248)\n",
            "epoch: 702, train_loss: 0.037934, val loss: 0.042685,  train_metric: 0.038 test_metric: 0.043 lr: 0.00247)\n",
            "epoch: 703, train_loss: 0.038155, val loss: 0.043390,  train_metric: 0.038 test_metric: 0.043 lr: 0.00247)\n",
            "epoch: 704, train_loss: 0.038230, val loss: 0.043435,  train_metric: 0.038 test_metric: 0.043 lr: 0.00247)\n",
            "epoch: 705, train_loss: 0.037829, val loss: 0.042779,  train_metric: 0.038 test_metric: 0.043 lr: 0.00247)\n",
            "epoch: 706, train_loss: 0.039023, val loss: 0.042247,  train_metric: 0.039 test_metric: 0.042 lr: 0.00246)\n",
            "epoch: 707, train_loss: 0.038408, val loss: 0.045221,  train_metric: 0.038 test_metric: 0.045 lr: 0.00246)\n",
            "epoch: 708, train_loss: 0.038625, val loss: 0.042420,  train_metric: 0.039 test_metric: 0.042 lr: 0.00246)\n",
            "epoch: 709, train_loss: 0.037744, val loss: 0.042870,  train_metric: 0.038 test_metric: 0.043 lr: 0.00246)\n",
            "epoch: 710, train_loss: 0.037694, val loss: 0.042920,  train_metric: 0.038 test_metric: 0.043 lr: 0.00245)\n",
            "epoch: 711, train_loss: 0.037857, val loss: 0.043341,  train_metric: 0.038 test_metric: 0.043 lr: 0.00245)\n",
            "epoch: 712, train_loss: 0.038117, val loss: 0.042441,  train_metric: 0.038 test_metric: 0.042 lr: 0.00245)\n",
            "epoch: 713, train_loss: 0.038185, val loss: 0.043102,  train_metric: 0.038 test_metric: 0.043 lr: 0.00245)\n",
            "epoch: 714, train_loss: 0.037977, val loss: 0.043502,  train_metric: 0.038 test_metric: 0.044 lr: 0.00245)\n",
            "epoch: 715, train_loss: 0.038487, val loss: 0.042992,  train_metric: 0.038 test_metric: 0.043 lr: 0.00244)\n",
            "epoch: 716, train_loss: 0.039654, val loss: 0.043075,  train_metric: 0.040 test_metric: 0.043 lr: 0.00244)\n",
            "epoch: 717, train_loss: 0.038698, val loss: 0.044939,  train_metric: 0.039 test_metric: 0.045 lr: 0.00244)\n",
            "epoch: 718, train_loss: 0.039659, val loss: 0.044365,  train_metric: 0.040 test_metric: 0.044 lr: 0.00244)\n",
            "epoch: 719, train_loss: 0.038217, val loss: 0.043159,  train_metric: 0.038 test_metric: 0.043 lr: 0.00243)\n",
            "epoch: 720, train_loss: 0.037811, val loss: 0.042274,  train_metric: 0.038 test_metric: 0.042 lr: 0.00243)\n",
            "epoch: 721, train_loss: 0.038071, val loss: 0.042396,  train_metric: 0.038 test_metric: 0.042 lr: 0.00243)\n",
            "epoch: 722, train_loss: 0.037660, val loss: 0.042876,  train_metric: 0.038 test_metric: 0.043 lr: 0.00243)\n",
            "epoch: 723, train_loss: 0.037795, val loss: 0.042516,  train_metric: 0.038 test_metric: 0.043 lr: 0.00242)\n",
            "epoch: 724, train_loss: 0.037715, val loss: 0.042320,  train_metric: 0.038 test_metric: 0.042 lr: 0.00242)\n",
            "epoch: 725, train_loss: 0.037634, val loss: 0.042901,  train_metric: 0.038 test_metric: 0.043 lr: 0.00242)\n",
            "epoch: 726, train_loss: 0.037625, val loss: 0.042394,  train_metric: 0.038 test_metric: 0.042 lr: 0.00242)\n",
            "epoch: 727, train_loss: 0.037627, val loss: 0.042409,  train_metric: 0.038 test_metric: 0.042 lr: 0.00241)\n",
            "epoch: 728, train_loss: 0.037625, val loss: 0.042695,  train_metric: 0.038 test_metric: 0.043 lr: 0.00241)\n",
            "epoch: 729, train_loss: 0.037417, val loss: 0.041976,  train_metric: 0.037 test_metric: 0.042 lr: 0.00241)\n",
            "epoch: 730, train_loss: 0.037560, val loss: 0.042222,  train_metric: 0.038 test_metric: 0.042 lr: 0.00241)\n",
            "epoch: 731, train_loss: 0.037924, val loss: 0.042833,  train_metric: 0.038 test_metric: 0.043 lr: 0.00240)\n",
            "epoch: 732, train_loss: 0.037860, val loss: 0.042418,  train_metric: 0.038 test_metric: 0.042 lr: 0.00240)\n",
            "epoch: 733, train_loss: 0.037612, val loss: 0.042978,  train_metric: 0.038 test_metric: 0.043 lr: 0.00240)\n",
            "epoch: 734, train_loss: 0.037673, val loss: 0.042660,  train_metric: 0.038 test_metric: 0.043 lr: 0.00240)\n",
            "epoch: 735, train_loss: 0.037519, val loss: 0.042136,  train_metric: 0.038 test_metric: 0.042 lr: 0.00239)\n",
            "epoch: 736, train_loss: 0.037987, val loss: 0.042520,  train_metric: 0.038 test_metric: 0.043 lr: 0.00239)\n",
            "epoch: 737, train_loss: 0.037652, val loss: 0.042563,  train_metric: 0.038 test_metric: 0.043 lr: 0.00239)\n",
            "epoch: 738, train_loss: 0.037911, val loss: 0.042405,  train_metric: 0.038 test_metric: 0.042 lr: 0.00239)\n",
            "epoch: 739, train_loss: 0.038245, val loss: 0.042471,  train_metric: 0.038 test_metric: 0.042 lr: 0.00238)\n",
            "epoch: 740, train_loss: 0.037938, val loss: 0.043243,  train_metric: 0.038 test_metric: 0.043 lr: 0.00238)\n",
            "epoch: 741, train_loss: 0.038288, val loss: 0.043590,  train_metric: 0.038 test_metric: 0.044 lr: 0.00238)\n",
            "epoch: 742, train_loss: 0.038920, val loss: 0.043004,  train_metric: 0.039 test_metric: 0.043 lr: 0.00238)\n",
            "epoch: 743, train_loss: 0.038891, val loss: 0.043876,  train_metric: 0.039 test_metric: 0.044 lr: 0.00238)\n",
            "epoch: 744, train_loss: 0.038128, val loss: 0.043055,  train_metric: 0.038 test_metric: 0.043 lr: 0.00237)\n",
            "epoch: 745, train_loss: 0.037826, val loss: 0.042907,  train_metric: 0.038 test_metric: 0.043 lr: 0.00237)\n",
            "epoch: 746, train_loss: 0.038195, val loss: 0.042218,  train_metric: 0.038 test_metric: 0.042 lr: 0.00237)\n",
            "epoch: 747, train_loss: 0.037487, val loss: 0.041910,  train_metric: 0.037 test_metric: 0.042 lr: 0.00237)\n",
            "epoch: 748, train_loss: 0.037752, val loss: 0.042353,  train_metric: 0.038 test_metric: 0.042 lr: 0.00236)\n",
            "epoch: 749, train_loss: 0.037860, val loss: 0.043154,  train_metric: 0.038 test_metric: 0.043 lr: 0.00236)\n",
            "epoch: 750, train_loss: 0.038383, val loss: 0.042964,  train_metric: 0.038 test_metric: 0.043 lr: 0.00236)\n",
            "epoch: 751, train_loss: 0.038064, val loss: 0.042674,  train_metric: 0.038 test_metric: 0.043 lr: 0.00236)\n",
            "epoch: 752, train_loss: 0.037968, val loss: 0.043363,  train_metric: 0.038 test_metric: 0.043 lr: 0.00235)\n",
            "epoch: 753, train_loss: 0.038237, val loss: 0.042520,  train_metric: 0.038 test_metric: 0.043 lr: 0.00235)\n",
            "epoch: 754, train_loss: 0.037748, val loss: 0.042945,  train_metric: 0.038 test_metric: 0.043 lr: 0.00235)\n",
            "epoch: 755, train_loss: 0.037933, val loss: 0.043263,  train_metric: 0.038 test_metric: 0.043 lr: 0.00235)\n",
            "epoch: 756, train_loss: 0.037525, val loss: 0.041982,  train_metric: 0.038 test_metric: 0.042 lr: 0.00234)\n",
            "epoch: 757, train_loss: 0.037335, val loss: 0.042234,  train_metric: 0.037 test_metric: 0.042 lr: 0.00234)\n",
            "epoch: 758, train_loss: 0.037538, val loss: 0.042542,  train_metric: 0.038 test_metric: 0.043 lr: 0.00234)\n",
            "epoch: 759, train_loss: 0.037004, val loss: 0.041848,  train_metric: 0.037 test_metric: 0.042 lr: 0.00234)\n",
            "epoch: 760, train_loss: 0.038160, val loss: 0.042569,  train_metric: 0.038 test_metric: 0.043 lr: 0.00234)\n",
            "epoch: 761, train_loss: 0.038627, val loss: 0.044641,  train_metric: 0.039 test_metric: 0.045 lr: 0.00233)\n",
            "epoch: 762, train_loss: 0.038888, val loss: 0.042626,  train_metric: 0.039 test_metric: 0.043 lr: 0.00233)\n",
            "epoch: 763, train_loss: 0.038899, val loss: 0.042763,  train_metric: 0.039 test_metric: 0.043 lr: 0.00233)\n",
            "epoch: 764, train_loss: 0.037444, val loss: 0.042234,  train_metric: 0.037 test_metric: 0.042 lr: 0.00233)\n",
            "epoch: 765, train_loss: 0.037535, val loss: 0.042459,  train_metric: 0.038 test_metric: 0.042 lr: 0.00232)\n",
            "epoch: 766, train_loss: 0.037900, val loss: 0.042764,  train_metric: 0.038 test_metric: 0.043 lr: 0.00232)\n",
            "epoch: 767, train_loss: 0.038532, val loss: 0.043028,  train_metric: 0.039 test_metric: 0.043 lr: 0.00232)\n",
            "epoch: 768, train_loss: 0.037852, val loss: 0.042858,  train_metric: 0.038 test_metric: 0.043 lr: 0.00232)\n",
            "epoch: 769, train_loss: 0.038438, val loss: 0.043007,  train_metric: 0.038 test_metric: 0.043 lr: 0.00231)\n",
            "epoch: 770, train_loss: 0.038103, val loss: 0.042728,  train_metric: 0.038 test_metric: 0.043 lr: 0.00231)\n",
            "epoch: 771, train_loss: 0.037720, val loss: 0.042119,  train_metric: 0.038 test_metric: 0.042 lr: 0.00231)\n",
            "epoch: 772, train_loss: 0.037475, val loss: 0.042608,  train_metric: 0.037 test_metric: 0.043 lr: 0.00231)\n",
            "epoch: 773, train_loss: 0.037688, val loss: 0.042288,  train_metric: 0.038 test_metric: 0.042 lr: 0.00230)\n",
            "epoch: 774, train_loss: 0.037546, val loss: 0.042220,  train_metric: 0.038 test_metric: 0.042 lr: 0.00230)\n",
            "epoch: 775, train_loss: 0.037433, val loss: 0.042082,  train_metric: 0.037 test_metric: 0.042 lr: 0.00230)\n",
            "epoch: 776, train_loss: 0.037268, val loss: 0.042168,  train_metric: 0.037 test_metric: 0.042 lr: 0.00230)\n",
            "epoch: 777, train_loss: 0.037147, val loss: 0.042018,  train_metric: 0.037 test_metric: 0.042 lr: 0.00230)\n",
            "epoch: 778, train_loss: 0.037241, val loss: 0.041976,  train_metric: 0.037 test_metric: 0.042 lr: 0.00229)\n",
            "epoch: 779, train_loss: 0.037554, val loss: 0.041846,  train_metric: 0.038 test_metric: 0.042 lr: 0.00229)\n",
            "epoch: 780, train_loss: 0.037670, val loss: 0.042142,  train_metric: 0.038 test_metric: 0.042 lr: 0.00229)\n",
            "epoch: 781, train_loss: 0.037422, val loss: 0.042498,  train_metric: 0.037 test_metric: 0.042 lr: 0.00229)\n",
            "epoch: 782, train_loss: 0.037708, val loss: 0.042208,  train_metric: 0.038 test_metric: 0.042 lr: 0.00228)\n",
            "epoch: 783, train_loss: 0.037479, val loss: 0.042045,  train_metric: 0.037 test_metric: 0.042 lr: 0.00228)\n",
            "epoch: 784, train_loss: 0.037472, val loss: 0.042428,  train_metric: 0.037 test_metric: 0.042 lr: 0.00228)\n",
            "epoch: 785, train_loss: 0.038405, val loss: 0.043742,  train_metric: 0.038 test_metric: 0.044 lr: 0.00228)\n",
            "epoch: 786, train_loss: 0.037906, val loss: 0.042896,  train_metric: 0.038 test_metric: 0.043 lr: 0.00228)\n",
            "epoch: 787, train_loss: 0.038196, val loss: 0.042883,  train_metric: 0.038 test_metric: 0.043 lr: 0.00227)\n",
            "epoch: 788, train_loss: 0.038138, val loss: 0.042205,  train_metric: 0.038 test_metric: 0.042 lr: 0.00227)\n",
            "epoch: 789, train_loss: 0.037501, val loss: 0.042555,  train_metric: 0.038 test_metric: 0.043 lr: 0.00227)\n",
            "epoch: 790, train_loss: 0.037709, val loss: 0.042275,  train_metric: 0.038 test_metric: 0.042 lr: 0.00227)\n",
            "epoch: 791, train_loss: 0.037568, val loss: 0.042489,  train_metric: 0.038 test_metric: 0.042 lr: 0.00226)\n",
            "epoch: 792, train_loss: 0.037489, val loss: 0.042100,  train_metric: 0.037 test_metric: 0.042 lr: 0.00226)\n",
            "epoch: 793, train_loss: 0.037945, val loss: 0.042289,  train_metric: 0.038 test_metric: 0.042 lr: 0.00226)\n",
            "epoch: 794, train_loss: 0.037463, val loss: 0.042132,  train_metric: 0.037 test_metric: 0.042 lr: 0.00226)\n",
            "epoch: 795, train_loss: 0.037698, val loss: 0.041937,  train_metric: 0.038 test_metric: 0.042 lr: 0.00225)\n",
            "epoch: 796, train_loss: 0.037823, val loss: 0.042589,  train_metric: 0.038 test_metric: 0.043 lr: 0.00225)\n",
            "epoch: 797, train_loss: 0.037554, val loss: 0.042601,  train_metric: 0.038 test_metric: 0.043 lr: 0.00225)\n",
            "epoch: 798, train_loss: 0.038056, val loss: 0.041878,  train_metric: 0.038 test_metric: 0.042 lr: 0.00225)\n",
            "epoch: 799, train_loss: 0.037949, val loss: 0.042109,  train_metric: 0.038 test_metric: 0.042 lr: 0.00225)\n",
            "epoch: 800, train_loss: 0.037250, val loss: 0.042321,  train_metric: 0.037 test_metric: 0.042 lr: 0.00224)\n",
            "epoch: 801, train_loss: 0.037475, val loss: 0.042449,  train_metric: 0.037 test_metric: 0.042 lr: 0.00224)\n",
            "epoch: 802, train_loss: 0.037201, val loss: 0.042336,  train_metric: 0.037 test_metric: 0.042 lr: 0.00224)\n",
            "epoch: 803, train_loss: 0.037433, val loss: 0.042329,  train_metric: 0.037 test_metric: 0.042 lr: 0.00224)\n",
            "epoch: 804, train_loss: 0.037496, val loss: 0.041723,  train_metric: 0.037 test_metric: 0.042 lr: 0.00223)\n",
            "epoch: 805, train_loss: 0.037656, val loss: 0.041929,  train_metric: 0.038 test_metric: 0.042 lr: 0.00223)\n",
            "epoch: 806, train_loss: 0.036936, val loss: 0.042746,  train_metric: 0.037 test_metric: 0.043 lr: 0.00223)\n",
            "epoch: 807, train_loss: 0.037232, val loss: 0.041623,  train_metric: 0.037 test_metric: 0.042 lr: 0.00223)\n",
            "epoch: 808, train_loss: 0.036865, val loss: 0.041631,  train_metric: 0.037 test_metric: 0.042 lr: 0.00223)\n",
            "epoch: 809, train_loss: 0.036954, val loss: 0.041717,  train_metric: 0.037 test_metric: 0.042 lr: 0.00222)\n",
            "epoch: 810, train_loss: 0.036703, val loss: 0.041574,  train_metric: 0.037 test_metric: 0.042 lr: 0.00222)\n",
            "epoch: 811, train_loss: 0.036664, val loss: 0.041830,  train_metric: 0.037 test_metric: 0.042 lr: 0.00222)\n",
            "epoch: 812, train_loss: 0.036980, val loss: 0.042319,  train_metric: 0.037 test_metric: 0.042 lr: 0.00222)\n",
            "epoch: 813, train_loss: 0.037111, val loss: 0.041520,  train_metric: 0.037 test_metric: 0.042 lr: 0.00221)\n",
            "epoch: 814, train_loss: 0.036849, val loss: 0.041953,  train_metric: 0.037 test_metric: 0.042 lr: 0.00221)\n",
            "epoch: 815, train_loss: 0.036869, val loss: 0.041655,  train_metric: 0.037 test_metric: 0.042 lr: 0.00221)\n",
            "epoch: 816, train_loss: 0.037317, val loss: 0.042305,  train_metric: 0.037 test_metric: 0.042 lr: 0.00221)\n",
            "epoch: 817, train_loss: 0.037620, val loss: 0.041896,  train_metric: 0.038 test_metric: 0.042 lr: 0.00221)\n",
            "epoch: 818, train_loss: 0.039011, val loss: 0.042265,  train_metric: 0.039 test_metric: 0.042 lr: 0.00220)\n",
            "epoch: 819, train_loss: 0.038543, val loss: 0.046884,  train_metric: 0.039 test_metric: 0.047 lr: 0.00220)\n",
            "epoch: 820, train_loss: 0.039994, val loss: 0.042151,  train_metric: 0.040 test_metric: 0.042 lr: 0.00220)\n",
            "epoch: 821, train_loss: 0.038958, val loss: 0.043531,  train_metric: 0.039 test_metric: 0.044 lr: 0.00220)\n",
            "epoch: 822, train_loss: 0.038936, val loss: 0.043069,  train_metric: 0.039 test_metric: 0.043 lr: 0.00219)\n",
            "epoch: 823, train_loss: 0.041151, val loss: 0.044097,  train_metric: 0.041 test_metric: 0.044 lr: 0.00219)\n",
            "epoch: 824, train_loss: 0.040343, val loss: 0.047965,  train_metric: 0.040 test_metric: 0.048 lr: 0.00219)\n",
            "epoch: 825, train_loss: 0.040625, val loss: 0.043070,  train_metric: 0.041 test_metric: 0.043 lr: 0.00219)\n",
            "epoch: 826, train_loss: 0.038276, val loss: 0.042908,  train_metric: 0.038 test_metric: 0.043 lr: 0.00219)\n",
            "epoch: 827, train_loss: 0.037534, val loss: 0.041947,  train_metric: 0.038 test_metric: 0.042 lr: 0.00218)\n",
            "epoch: 828, train_loss: 0.037394, val loss: 0.041688,  train_metric: 0.037 test_metric: 0.042 lr: 0.00218)\n",
            "epoch: 829, train_loss: 0.036860, val loss: 0.041687,  train_metric: 0.037 test_metric: 0.042 lr: 0.00218)\n",
            "epoch: 830, train_loss: 0.037223, val loss: 0.041874,  train_metric: 0.037 test_metric: 0.042 lr: 0.00218)\n",
            "epoch: 831, train_loss: 0.036999, val loss: 0.041594,  train_metric: 0.037 test_metric: 0.042 lr: 0.00217)\n",
            "epoch: 832, train_loss: 0.037362, val loss: 0.042042,  train_metric: 0.037 test_metric: 0.042 lr: 0.00217)\n",
            "epoch: 833, train_loss: 0.038205, val loss: 0.041525,  train_metric: 0.038 test_metric: 0.042 lr: 0.00217)\n",
            "epoch: 834, train_loss: 0.037778, val loss: 0.043126,  train_metric: 0.038 test_metric: 0.043 lr: 0.00217)\n",
            "epoch: 835, train_loss: 0.037557, val loss: 0.042314,  train_metric: 0.038 test_metric: 0.042 lr: 0.00217)\n",
            "epoch: 836, train_loss: 0.037852, val loss: 0.042453,  train_metric: 0.038 test_metric: 0.042 lr: 0.00216)\n",
            "epoch: 837, train_loss: 0.036983, val loss: 0.041892,  train_metric: 0.037 test_metric: 0.042 lr: 0.00216)\n",
            "epoch: 838, train_loss: 0.037316, val loss: 0.041579,  train_metric: 0.037 test_metric: 0.042 lr: 0.00216)\n",
            "epoch: 839, train_loss: 0.037580, val loss: 0.042166,  train_metric: 0.038 test_metric: 0.042 lr: 0.00216)\n",
            "epoch: 840, train_loss: 0.037339, val loss: 0.041811,  train_metric: 0.037 test_metric: 0.042 lr: 0.00216)\n",
            "epoch: 841, train_loss: 0.037386, val loss: 0.042442,  train_metric: 0.037 test_metric: 0.042 lr: 0.00215)\n",
            "epoch: 842, train_loss: 0.037249, val loss: 0.041996,  train_metric: 0.037 test_metric: 0.042 lr: 0.00215)\n",
            "epoch: 843, train_loss: 0.037506, val loss: 0.041930,  train_metric: 0.038 test_metric: 0.042 lr: 0.00215)\n",
            "epoch: 844, train_loss: 0.037635, val loss: 0.043552,  train_metric: 0.038 test_metric: 0.044 lr: 0.00215)\n",
            "epoch: 845, train_loss: 0.037834, val loss: 0.041837,  train_metric: 0.038 test_metric: 0.042 lr: 0.00214)\n",
            "epoch: 846, train_loss: 0.037258, val loss: 0.042477,  train_metric: 0.037 test_metric: 0.042 lr: 0.00214)\n",
            "epoch: 847, train_loss: 0.037124, val loss: 0.042025,  train_metric: 0.037 test_metric: 0.042 lr: 0.00214)\n",
            "epoch: 848, train_loss: 0.037058, val loss: 0.041969,  train_metric: 0.037 test_metric: 0.042 lr: 0.00214)\n",
            "epoch: 849, train_loss: 0.037136, val loss: 0.041328,  train_metric: 0.037 test_metric: 0.041 lr: 0.00214)\n",
            "epoch: 850, train_loss: 0.037266, val loss: 0.042326,  train_metric: 0.037 test_metric: 0.042 lr: 0.00213)\n",
            "epoch: 851, train_loss: 0.037186, val loss: 0.042062,  train_metric: 0.037 test_metric: 0.042 lr: 0.00213)\n",
            "epoch: 852, train_loss: 0.038348, val loss: 0.041715,  train_metric: 0.038 test_metric: 0.042 lr: 0.00213)\n",
            "epoch: 853, train_loss: 0.038066, val loss: 0.042589,  train_metric: 0.038 test_metric: 0.043 lr: 0.00213)\n",
            "epoch: 854, train_loss: 0.038522, val loss: 0.043419,  train_metric: 0.039 test_metric: 0.043 lr: 0.00213)\n",
            "epoch: 855, train_loss: 0.038192, val loss: 0.043134,  train_metric: 0.038 test_metric: 0.043 lr: 0.00212)\n",
            "epoch: 856, train_loss: 0.037599, val loss: 0.041967,  train_metric: 0.038 test_metric: 0.042 lr: 0.00212)\n",
            "epoch: 857, train_loss: 0.037087, val loss: 0.041518,  train_metric: 0.037 test_metric: 0.042 lr: 0.00212)\n",
            "epoch: 858, train_loss: 0.036940, val loss: 0.041893,  train_metric: 0.037 test_metric: 0.042 lr: 0.00212)\n",
            "epoch: 859, train_loss: 0.037520, val loss: 0.042256,  train_metric: 0.038 test_metric: 0.042 lr: 0.00211)\n",
            "epoch: 860, train_loss: 0.037267, val loss: 0.041848,  train_metric: 0.037 test_metric: 0.042 lr: 0.00211)\n",
            "epoch: 861, train_loss: 0.036700, val loss: 0.041448,  train_metric: 0.037 test_metric: 0.041 lr: 0.00211)\n",
            "epoch: 862, train_loss: 0.037131, val loss: 0.041596,  train_metric: 0.037 test_metric: 0.042 lr: 0.00211)\n",
            "epoch: 863, train_loss: 0.037414, val loss: 0.041706,  train_metric: 0.037 test_metric: 0.042 lr: 0.00211)\n",
            "epoch: 864, train_loss: 0.036805, val loss: 0.041308,  train_metric: 0.037 test_metric: 0.041 lr: 0.00210)\n",
            "epoch: 865, train_loss: 0.036993, val loss: 0.042405,  train_metric: 0.037 test_metric: 0.042 lr: 0.00210)\n",
            "epoch: 866, train_loss: 0.037085, val loss: 0.041793,  train_metric: 0.037 test_metric: 0.042 lr: 0.00210)\n",
            "epoch: 867, train_loss: 0.037892, val loss: 0.042908,  train_metric: 0.038 test_metric: 0.043 lr: 0.00210)\n",
            "epoch: 868, train_loss: 0.037676, val loss: 0.042233,  train_metric: 0.038 test_metric: 0.042 lr: 0.00210)\n",
            "epoch: 869, train_loss: 0.039152, val loss: 0.042898,  train_metric: 0.039 test_metric: 0.043 lr: 0.00209)\n",
            "epoch: 870, train_loss: 0.038380, val loss: 0.042775,  train_metric: 0.038 test_metric: 0.043 lr: 0.00209)\n",
            "epoch: 871, train_loss: 0.038072, val loss: 0.044411,  train_metric: 0.038 test_metric: 0.044 lr: 0.00209)\n",
            "epoch: 872, train_loss: 0.039745, val loss: 0.046848,  train_metric: 0.040 test_metric: 0.047 lr: 0.00209)\n",
            "epoch: 873, train_loss: 0.039958, val loss: 0.043642,  train_metric: 0.040 test_metric: 0.044 lr: 0.00209)\n",
            "epoch: 874, train_loss: 0.038101, val loss: 0.041839,  train_metric: 0.038 test_metric: 0.042 lr: 0.00208)\n",
            "epoch: 875, train_loss: 0.037785, val loss: 0.042916,  train_metric: 0.038 test_metric: 0.043 lr: 0.00208)\n",
            "epoch: 876, train_loss: 0.037476, val loss: 0.041812,  train_metric: 0.037 test_metric: 0.042 lr: 0.00208)\n",
            "epoch: 877, train_loss: 0.037132, val loss: 0.042526,  train_metric: 0.037 test_metric: 0.043 lr: 0.00208)\n",
            "epoch: 878, train_loss: 0.036947, val loss: 0.041923,  train_metric: 0.037 test_metric: 0.042 lr: 0.00208)\n",
            "epoch: 879, train_loss: 0.037005, val loss: 0.041912,  train_metric: 0.037 test_metric: 0.042 lr: 0.00207)\n",
            "epoch: 880, train_loss: 0.037262, val loss: 0.042224,  train_metric: 0.037 test_metric: 0.042 lr: 0.00207)\n",
            "epoch: 881, train_loss: 0.036790, val loss: 0.041299,  train_metric: 0.037 test_metric: 0.041 lr: 0.00207)\n",
            "epoch: 882, train_loss: 0.036535, val loss: 0.041703,  train_metric: 0.037 test_metric: 0.042 lr: 0.00207)\n",
            "epoch: 883, train_loss: 0.036659, val loss: 0.041489,  train_metric: 0.037 test_metric: 0.041 lr: 0.00206)\n",
            "epoch: 884, train_loss: 0.036898, val loss: 0.041770,  train_metric: 0.037 test_metric: 0.042 lr: 0.00206)\n",
            "epoch: 885, train_loss: 0.036636, val loss: 0.041495,  train_metric: 0.037 test_metric: 0.041 lr: 0.00206)\n",
            "epoch: 886, train_loss: 0.036821, val loss: 0.042114,  train_metric: 0.037 test_metric: 0.042 lr: 0.00206)\n",
            "epoch: 887, train_loss: 0.036671, val loss: 0.041333,  train_metric: 0.037 test_metric: 0.041 lr: 0.00206)\n",
            "epoch: 888, train_loss: 0.036678, val loss: 0.041893,  train_metric: 0.037 test_metric: 0.042 lr: 0.00205)\n",
            "epoch: 889, train_loss: 0.036817, val loss: 0.041308,  train_metric: 0.037 test_metric: 0.041 lr: 0.00205)\n",
            "epoch: 890, train_loss: 0.036519, val loss: 0.041663,  train_metric: 0.037 test_metric: 0.042 lr: 0.00205)\n",
            "epoch: 891, train_loss: 0.036642, val loss: 0.041442,  train_metric: 0.037 test_metric: 0.041 lr: 0.00205)\n",
            "epoch: 892, train_loss: 0.036702, val loss: 0.042087,  train_metric: 0.037 test_metric: 0.042 lr: 0.00205)\n",
            "epoch: 893, train_loss: 0.037139, val loss: 0.041992,  train_metric: 0.037 test_metric: 0.042 lr: 0.00204)\n",
            "epoch: 894, train_loss: 0.037007, val loss: 0.042191,  train_metric: 0.037 test_metric: 0.042 lr: 0.00204)\n",
            "epoch: 895, train_loss: 0.037025, val loss: 0.041661,  train_metric: 0.037 test_metric: 0.042 lr: 0.00204)\n",
            "epoch: 896, train_loss: 0.036835, val loss: 0.042183,  train_metric: 0.037 test_metric: 0.042 lr: 0.00204)\n",
            "epoch: 897, train_loss: 0.037164, val loss: 0.041898,  train_metric: 0.037 test_metric: 0.042 lr: 0.00204)\n",
            "epoch: 898, train_loss: 0.036770, val loss: 0.041473,  train_metric: 0.037 test_metric: 0.041 lr: 0.00203)\n",
            "epoch: 899, train_loss: 0.036864, val loss: 0.041514,  train_metric: 0.037 test_metric: 0.042 lr: 0.00203)\n",
            "epoch: 900, train_loss: 0.036715, val loss: 0.042272,  train_metric: 0.037 test_metric: 0.042 lr: 0.00203)\n",
            "epoch: 901, train_loss: 0.037078, val loss: 0.042063,  train_metric: 0.037 test_metric: 0.042 lr: 0.00203)\n",
            "epoch: 902, train_loss: 0.037418, val loss: 0.041549,  train_metric: 0.037 test_metric: 0.042 lr: 0.00203)\n",
            "epoch: 903, train_loss: 0.037596, val loss: 0.042449,  train_metric: 0.038 test_metric: 0.042 lr: 0.00202)\n",
            "epoch: 904, train_loss: 0.037705, val loss: 0.042014,  train_metric: 0.038 test_metric: 0.042 lr: 0.00202)\n",
            "epoch: 905, train_loss: 0.037681, val loss: 0.041577,  train_metric: 0.038 test_metric: 0.042 lr: 0.00202)\n",
            "epoch: 906, train_loss: 0.037185, val loss: 0.042738,  train_metric: 0.037 test_metric: 0.043 lr: 0.00202)\n",
            "epoch: 907, train_loss: 0.037769, val loss: 0.043380,  train_metric: 0.038 test_metric: 0.043 lr: 0.00202)\n",
            "epoch: 908, train_loss: 0.038810, val loss: 0.041923,  train_metric: 0.039 test_metric: 0.042 lr: 0.00201)\n",
            "epoch: 909, train_loss: 0.038053, val loss: 0.043708,  train_metric: 0.038 test_metric: 0.044 lr: 0.00201)\n",
            "epoch: 910, train_loss: 0.037942, val loss: 0.041302,  train_metric: 0.038 test_metric: 0.041 lr: 0.00201)\n",
            "epoch: 911, train_loss: 0.036699, val loss: 0.043119,  train_metric: 0.037 test_metric: 0.043 lr: 0.00201)\n",
            "epoch: 912, train_loss: 0.037873, val loss: 0.042104,  train_metric: 0.038 test_metric: 0.042 lr: 0.00201)\n",
            "epoch: 913, train_loss: 0.038884, val loss: 0.042608,  train_metric: 0.039 test_metric: 0.043 lr: 0.00200)\n",
            "epoch: 914, train_loss: 0.038206, val loss: 0.045249,  train_metric: 0.038 test_metric: 0.045 lr: 0.00200)\n",
            "epoch: 915, train_loss: 0.037653, val loss: 0.041662,  train_metric: 0.038 test_metric: 0.042 lr: 0.00200)\n",
            "epoch: 916, train_loss: 0.036648, val loss: 0.041977,  train_metric: 0.037 test_metric: 0.042 lr: 0.00200)\n",
            "epoch: 917, train_loss: 0.036873, val loss: 0.041275,  train_metric: 0.037 test_metric: 0.041 lr: 0.00200)\n",
            "epoch: 918, train_loss: 0.037698, val loss: 0.042463,  train_metric: 0.038 test_metric: 0.042 lr: 0.00199)\n",
            "epoch: 919, train_loss: 0.037380, val loss: 0.042301,  train_metric: 0.037 test_metric: 0.042 lr: 0.00199)\n",
            "epoch: 920, train_loss: 0.037170, val loss: 0.042164,  train_metric: 0.037 test_metric: 0.042 lr: 0.00199)\n",
            "epoch: 921, train_loss: 0.037166, val loss: 0.041642,  train_metric: 0.037 test_metric: 0.042 lr: 0.00199)\n",
            "epoch: 922, train_loss: 0.036404, val loss: 0.041629,  train_metric: 0.036 test_metric: 0.042 lr: 0.00199)\n",
            "epoch: 923, train_loss: 0.037092, val loss: 0.041839,  train_metric: 0.037 test_metric: 0.042 lr: 0.00198)\n",
            "epoch: 924, train_loss: 0.036915, val loss: 0.041877,  train_metric: 0.037 test_metric: 0.042 lr: 0.00198)\n",
            "epoch: 925, train_loss: 0.037171, val loss: 0.041944,  train_metric: 0.037 test_metric: 0.042 lr: 0.00198)\n",
            "epoch: 926, train_loss: 0.036773, val loss: 0.041450,  train_metric: 0.037 test_metric: 0.041 lr: 0.00198)\n",
            "epoch: 927, train_loss: 0.036746, val loss: 0.041592,  train_metric: 0.037 test_metric: 0.042 lr: 0.00198)\n",
            "epoch: 928, train_loss: 0.036375, val loss: 0.041025,  train_metric: 0.036 test_metric: 0.041 lr: 0.00197)\n",
            "epoch: 929, train_loss: 0.036895, val loss: 0.041744,  train_metric: 0.037 test_metric: 0.042 lr: 0.00197)\n",
            "epoch: 930, train_loss: 0.036610, val loss: 0.041218,  train_metric: 0.037 test_metric: 0.041 lr: 0.00197)\n",
            "epoch: 931, train_loss: 0.036334, val loss: 0.041366,  train_metric: 0.036 test_metric: 0.041 lr: 0.00197)\n",
            "epoch: 932, train_loss: 0.036776, val loss: 0.041414,  train_metric: 0.037 test_metric: 0.041 lr: 0.00197)\n",
            "epoch: 933, train_loss: 0.036198, val loss: 0.041393,  train_metric: 0.036 test_metric: 0.041 lr: 0.00196)\n",
            "epoch: 934, train_loss: 0.036708, val loss: 0.041330,  train_metric: 0.037 test_metric: 0.041 lr: 0.00196)\n",
            "epoch: 935, train_loss: 0.037053, val loss: 0.041039,  train_metric: 0.037 test_metric: 0.041 lr: 0.00196)\n",
            "epoch: 936, train_loss: 0.036452, val loss: 0.041945,  train_metric: 0.036 test_metric: 0.042 lr: 0.00196)\n",
            "epoch: 937, train_loss: 0.037097, val loss: 0.041617,  train_metric: 0.037 test_metric: 0.042 lr: 0.00196)\n",
            "epoch: 938, train_loss: 0.037437, val loss: 0.042423,  train_metric: 0.037 test_metric: 0.042 lr: 0.00195)\n",
            "epoch: 939, train_loss: 0.036878, val loss: 0.041430,  train_metric: 0.037 test_metric: 0.041 lr: 0.00195)\n",
            "epoch: 940, train_loss: 0.037416, val loss: 0.042858,  train_metric: 0.037 test_metric: 0.043 lr: 0.00195)\n",
            "epoch: 941, train_loss: 0.038240, val loss: 0.043536,  train_metric: 0.038 test_metric: 0.044 lr: 0.00195)\n",
            "epoch: 942, train_loss: 0.038267, val loss: 0.041750,  train_metric: 0.038 test_metric: 0.042 lr: 0.00195)\n",
            "epoch: 943, train_loss: 0.037197, val loss: 0.041366,  train_metric: 0.037 test_metric: 0.041 lr: 0.00194)\n",
            "epoch: 944, train_loss: 0.037121, val loss: 0.041897,  train_metric: 0.037 test_metric: 0.042 lr: 0.00194)\n",
            "epoch: 945, train_loss: 0.036893, val loss: 0.041859,  train_metric: 0.037 test_metric: 0.042 lr: 0.00194)\n",
            "epoch: 946, train_loss: 0.037186, val loss: 0.041642,  train_metric: 0.037 test_metric: 0.042 lr: 0.00194)\n",
            "epoch: 947, train_loss: 0.038581, val loss: 0.041900,  train_metric: 0.039 test_metric: 0.042 lr: 0.00194)\n",
            "epoch: 948, train_loss: 0.037621, val loss: 0.042491,  train_metric: 0.038 test_metric: 0.042 lr: 0.00193)\n",
            "epoch: 949, train_loss: 0.037215, val loss: 0.041497,  train_metric: 0.037 test_metric: 0.041 lr: 0.00193)\n",
            "epoch: 950, train_loss: 0.037228, val loss: 0.042149,  train_metric: 0.037 test_metric: 0.042 lr: 0.00193)\n",
            "epoch: 951, train_loss: 0.038189, val loss: 0.042593,  train_metric: 0.038 test_metric: 0.043 lr: 0.00193)\n",
            "epoch: 952, train_loss: 0.038283, val loss: 0.042825,  train_metric: 0.038 test_metric: 0.043 lr: 0.00193)\n",
            "epoch: 953, train_loss: 0.038092, val loss: 0.042737,  train_metric: 0.038 test_metric: 0.043 lr: 0.00193)\n",
            "epoch: 954, train_loss: 0.038229, val loss: 0.042705,  train_metric: 0.038 test_metric: 0.043 lr: 0.00192)\n",
            "epoch: 955, train_loss: 0.037721, val loss: 0.042171,  train_metric: 0.038 test_metric: 0.042 lr: 0.00192)\n",
            "epoch: 956, train_loss: 0.036790, val loss: 0.041710,  train_metric: 0.037 test_metric: 0.042 lr: 0.00192)\n",
            "epoch: 957, train_loss: 0.037017, val loss: 0.041173,  train_metric: 0.037 test_metric: 0.041 lr: 0.00192)\n",
            "epoch: 958, train_loss: 0.036490, val loss: 0.041548,  train_metric: 0.036 test_metric: 0.042 lr: 0.00192)\n",
            "epoch: 959, train_loss: 0.036917, val loss: 0.041718,  train_metric: 0.037 test_metric: 0.042 lr: 0.00191)\n",
            "epoch: 960, train_loss: 0.036693, val loss: 0.041474,  train_metric: 0.037 test_metric: 0.041 lr: 0.00191)\n",
            "epoch: 961, train_loss: 0.036470, val loss: 0.041294,  train_metric: 0.036 test_metric: 0.041 lr: 0.00191)\n",
            "epoch: 962, train_loss: 0.037101, val loss: 0.042489,  train_metric: 0.037 test_metric: 0.042 lr: 0.00191)\n",
            "epoch: 963, train_loss: 0.037085, val loss: 0.041201,  train_metric: 0.037 test_metric: 0.041 lr: 0.00191)\n",
            "epoch: 964, train_loss: 0.036891, val loss: 0.042140,  train_metric: 0.037 test_metric: 0.042 lr: 0.00190)\n",
            "epoch: 965, train_loss: 0.037867, val loss: 0.041606,  train_metric: 0.038 test_metric: 0.042 lr: 0.00190)\n",
            "epoch: 966, train_loss: 0.040012, val loss: 0.042835,  train_metric: 0.040 test_metric: 0.043 lr: 0.00190)\n",
            "epoch: 967, train_loss: 0.037772, val loss: 0.042190,  train_metric: 0.038 test_metric: 0.042 lr: 0.00190)\n",
            "epoch: 968, train_loss: 0.036805, val loss: 0.041177,  train_metric: 0.037 test_metric: 0.041 lr: 0.00190)\n",
            "epoch: 969, train_loss: 0.036800, val loss: 0.041654,  train_metric: 0.037 test_metric: 0.042 lr: 0.00189)\n",
            "epoch: 970, train_loss: 0.036992, val loss: 0.041817,  train_metric: 0.037 test_metric: 0.042 lr: 0.00189)\n",
            "epoch: 971, train_loss: 0.036614, val loss: 0.041516,  train_metric: 0.037 test_metric: 0.042 lr: 0.00189)\n",
            "epoch: 972, train_loss: 0.036499, val loss: 0.041456,  train_metric: 0.036 test_metric: 0.041 lr: 0.00189)\n",
            "epoch: 973, train_loss: 0.036637, val loss: 0.040903,  train_metric: 0.037 test_metric: 0.041 lr: 0.00189)\n",
            "epoch: 974, train_loss: 0.037500, val loss: 0.042256,  train_metric: 0.038 test_metric: 0.042 lr: 0.00189)\n",
            "epoch: 975, train_loss: 0.036681, val loss: 0.041677,  train_metric: 0.037 test_metric: 0.042 lr: 0.00188)\n",
            "epoch: 976, train_loss: 0.036803, val loss: 0.041307,  train_metric: 0.037 test_metric: 0.041 lr: 0.00188)\n",
            "epoch: 977, train_loss: 0.037085, val loss: 0.042295,  train_metric: 0.037 test_metric: 0.042 lr: 0.00188)\n",
            "epoch: 978, train_loss: 0.037788, val loss: 0.042132,  train_metric: 0.038 test_metric: 0.042 lr: 0.00188)\n",
            "epoch: 979, train_loss: 0.037873, val loss: 0.042508,  train_metric: 0.038 test_metric: 0.043 lr: 0.00188)\n",
            "epoch: 980, train_loss: 0.037656, val loss: 0.041263,  train_metric: 0.038 test_metric: 0.041 lr: 0.00187)\n",
            "epoch: 981, train_loss: 0.036927, val loss: 0.041950,  train_metric: 0.037 test_metric: 0.042 lr: 0.00187)\n",
            "epoch: 982, train_loss: 0.036844, val loss: 0.041929,  train_metric: 0.037 test_metric: 0.042 lr: 0.00187)\n",
            "epoch: 983, train_loss: 0.037088, val loss: 0.041548,  train_metric: 0.037 test_metric: 0.042 lr: 0.00187)\n",
            "epoch: 984, train_loss: 0.037859, val loss: 0.042006,  train_metric: 0.038 test_metric: 0.042 lr: 0.00187)\n",
            "epoch: 985, train_loss: 0.037034, val loss: 0.042229,  train_metric: 0.037 test_metric: 0.042 lr: 0.00186)\n",
            "epoch: 986, train_loss: 0.037858, val loss: 0.043944,  train_metric: 0.038 test_metric: 0.044 lr: 0.00186)\n",
            "epoch: 987, train_loss: 0.038422, val loss: 0.043432,  train_metric: 0.038 test_metric: 0.043 lr: 0.00186)\n",
            "epoch: 988, train_loss: 0.039878, val loss: 0.049394,  train_metric: 0.040 test_metric: 0.049 lr: 0.00186)\n",
            "epoch: 989, train_loss: 0.042886, val loss: 0.049896,  train_metric: 0.043 test_metric: 0.050 lr: 0.00186)\n",
            "epoch: 990, train_loss: 0.040882, val loss: 0.043942,  train_metric: 0.041 test_metric: 0.044 lr: 0.00186)\n",
            "epoch: 991, train_loss: 0.038383, val loss: 0.042563,  train_metric: 0.038 test_metric: 0.043 lr: 0.00185)\n",
            "epoch: 992, train_loss: 0.037785, val loss: 0.042047,  train_metric: 0.038 test_metric: 0.042 lr: 0.00185)\n",
            "epoch: 993, train_loss: 0.036981, val loss: 0.042123,  train_metric: 0.037 test_metric: 0.042 lr: 0.00185)\n",
            "epoch: 994, train_loss: 0.037285, val loss: 0.042228,  train_metric: 0.037 test_metric: 0.042 lr: 0.00185)\n",
            "epoch: 995, train_loss: 0.037152, val loss: 0.041863,  train_metric: 0.037 test_metric: 0.042 lr: 0.00185)\n",
            "epoch: 996, train_loss: 0.036995, val loss: 0.041798,  train_metric: 0.037 test_metric: 0.042 lr: 0.00184)\n",
            "epoch: 997, train_loss: 0.036762, val loss: 0.041266,  train_metric: 0.037 test_metric: 0.041 lr: 0.00184)\n",
            "epoch: 998, train_loss: 0.036448, val loss: 0.040944,  train_metric: 0.036 test_metric: 0.041 lr: 0.00184)\n",
            "epoch: 999, train_loss: 0.036339, val loss: 0.041226,  train_metric: 0.036 test_metric: 0.041 lr: 0.00184)\n",
            "epoch: 1000, train_loss: 0.036053, val loss: 0.040758,  train_metric: 0.036 test_metric: 0.041 lr: 0.00184)\n",
            "epoch: 1001, train_loss: 0.036037, val loss: 0.041029,  train_metric: 0.036 test_metric: 0.041 lr: 0.00183)\n",
            "epoch: 1002, train_loss: 0.036160, val loss: 0.041152,  train_metric: 0.036 test_metric: 0.041 lr: 0.00183)\n",
            "epoch: 1003, train_loss: 0.035988, val loss: 0.040695,  train_metric: 0.036 test_metric: 0.041 lr: 0.00183)\n",
            "epoch: 1004, train_loss: 0.035891, val loss: 0.040878,  train_metric: 0.036 test_metric: 0.041 lr: 0.00183)\n",
            "epoch: 1005, train_loss: 0.036081, val loss: 0.041044,  train_metric: 0.036 test_metric: 0.041 lr: 0.00183)\n",
            "epoch: 1006, train_loss: 0.036380, val loss: 0.040965,  train_metric: 0.036 test_metric: 0.041 lr: 0.00183)\n",
            "epoch: 1007, train_loss: 0.036733, val loss: 0.041471,  train_metric: 0.037 test_metric: 0.041 lr: 0.00182)\n",
            "epoch: 1008, train_loss: 0.036801, val loss: 0.040828,  train_metric: 0.037 test_metric: 0.041 lr: 0.00182)\n",
            "epoch: 1009, train_loss: 0.036396, val loss: 0.041488,  train_metric: 0.036 test_metric: 0.041 lr: 0.00182)\n",
            "epoch: 1010, train_loss: 0.036543, val loss: 0.040838,  train_metric: 0.037 test_metric: 0.041 lr: 0.00182)\n",
            "epoch: 1011, train_loss: 0.036793, val loss: 0.041148,  train_metric: 0.037 test_metric: 0.041 lr: 0.00182)\n",
            "epoch: 1012, train_loss: 0.036577, val loss: 0.041847,  train_metric: 0.037 test_metric: 0.042 lr: 0.00181)\n",
            "epoch: 1013, train_loss: 0.037676, val loss: 0.041205,  train_metric: 0.038 test_metric: 0.041 lr: 0.00181)\n",
            "epoch: 1014, train_loss: 0.036549, val loss: 0.041251,  train_metric: 0.037 test_metric: 0.041 lr: 0.00181)\n",
            "epoch: 1015, train_loss: 0.036428, val loss: 0.040929,  train_metric: 0.036 test_metric: 0.041 lr: 0.00181)\n",
            "epoch: 1016, train_loss: 0.036253, val loss: 0.041152,  train_metric: 0.036 test_metric: 0.041 lr: 0.00181)\n",
            "epoch: 1017, train_loss: 0.036536, val loss: 0.042361,  train_metric: 0.037 test_metric: 0.042 lr: 0.00181)\n",
            "epoch: 1018, train_loss: 0.037056, val loss: 0.040765,  train_metric: 0.037 test_metric: 0.041 lr: 0.00180)\n",
            "epoch: 1019, train_loss: 0.036387, val loss: 0.040979,  train_metric: 0.036 test_metric: 0.041 lr: 0.00180)\n",
            "epoch: 1020, train_loss: 0.036283, val loss: 0.041004,  train_metric: 0.036 test_metric: 0.041 lr: 0.00180)\n",
            "epoch: 1021, train_loss: 0.036314, val loss: 0.040764,  train_metric: 0.036 test_metric: 0.041 lr: 0.00180)\n",
            "epoch: 1022, train_loss: 0.036168, val loss: 0.040887,  train_metric: 0.036 test_metric: 0.041 lr: 0.00180)\n",
            "epoch: 1023, train_loss: 0.036155, val loss: 0.041068,  train_metric: 0.036 test_metric: 0.041 lr: 0.00179)\n",
            "epoch: 1024, train_loss: 0.036076, val loss: 0.040989,  train_metric: 0.036 test_metric: 0.041 lr: 0.00179)\n",
            "epoch: 1025, train_loss: 0.036510, val loss: 0.040906,  train_metric: 0.037 test_metric: 0.041 lr: 0.00179)\n",
            "epoch: 1026, train_loss: 0.036747, val loss: 0.041436,  train_metric: 0.037 test_metric: 0.041 lr: 0.00179)\n",
            "epoch: 1027, train_loss: 0.036358, val loss: 0.041149,  train_metric: 0.036 test_metric: 0.041 lr: 0.00179)\n",
            "epoch: 1028, train_loss: 0.036254, val loss: 0.041233,  train_metric: 0.036 test_metric: 0.041 lr: 0.00179)\n",
            "epoch: 1029, train_loss: 0.036374, val loss: 0.040873,  train_metric: 0.036 test_metric: 0.041 lr: 0.00178)\n",
            "epoch: 1030, train_loss: 0.036191, val loss: 0.041197,  train_metric: 0.036 test_metric: 0.041 lr: 0.00178)\n",
            "epoch: 1031, train_loss: 0.036352, val loss: 0.041321,  train_metric: 0.036 test_metric: 0.041 lr: 0.00178)\n",
            "epoch: 1032, train_loss: 0.036729, val loss: 0.041749,  train_metric: 0.037 test_metric: 0.042 lr: 0.00178)\n",
            "epoch: 1033, train_loss: 0.036806, val loss: 0.041886,  train_metric: 0.037 test_metric: 0.042 lr: 0.00178)\n",
            "epoch: 1034, train_loss: 0.036656, val loss: 0.040885,  train_metric: 0.037 test_metric: 0.041 lr: 0.00178)\n",
            "epoch: 1035, train_loss: 0.036453, val loss: 0.040957,  train_metric: 0.036 test_metric: 0.041 lr: 0.00177)\n",
            "epoch: 1036, train_loss: 0.037035, val loss: 0.041494,  train_metric: 0.037 test_metric: 0.041 lr: 0.00177)\n",
            "epoch: 1037, train_loss: 0.036511, val loss: 0.041991,  train_metric: 0.037 test_metric: 0.042 lr: 0.00177)\n",
            "epoch: 1038, train_loss: 0.036319, val loss: 0.040966,  train_metric: 0.036 test_metric: 0.041 lr: 0.00177)\n",
            "epoch: 1039, train_loss: 0.036007, val loss: 0.041254,  train_metric: 0.036 test_metric: 0.041 lr: 0.00177)\n",
            "epoch: 1040, train_loss: 0.037219, val loss: 0.041565,  train_metric: 0.037 test_metric: 0.042 lr: 0.00176)\n",
            "epoch: 1041, train_loss: 0.038083, val loss: 0.040849,  train_metric: 0.038 test_metric: 0.041 lr: 0.00176)\n",
            "epoch: 1042, train_loss: 0.036515, val loss: 0.042789,  train_metric: 0.037 test_metric: 0.043 lr: 0.00176)\n",
            "epoch: 1043, train_loss: 0.036685, val loss: 0.040994,  train_metric: 0.037 test_metric: 0.041 lr: 0.00176)\n",
            "epoch: 1044, train_loss: 0.037311, val loss: 0.041237,  train_metric: 0.037 test_metric: 0.041 lr: 0.00176)\n",
            "epoch: 1045, train_loss: 0.036529, val loss: 0.041612,  train_metric: 0.037 test_metric: 0.042 lr: 0.00176)\n",
            "epoch: 1046, train_loss: 0.036423, val loss: 0.041207,  train_metric: 0.036 test_metric: 0.041 lr: 0.00175)\n",
            "epoch: 1047, train_loss: 0.036241, val loss: 0.040811,  train_metric: 0.036 test_metric: 0.041 lr: 0.00175)\n",
            "epoch: 1048, train_loss: 0.036085, val loss: 0.041175,  train_metric: 0.036 test_metric: 0.041 lr: 0.00175)\n",
            "epoch: 1049, train_loss: 0.036156, val loss: 0.040749,  train_metric: 0.036 test_metric: 0.041 lr: 0.00175)\n",
            "epoch: 1050, train_loss: 0.036036, val loss: 0.041201,  train_metric: 0.036 test_metric: 0.041 lr: 0.00175)\n",
            "epoch: 1051, train_loss: 0.036324, val loss: 0.041148,  train_metric: 0.036 test_metric: 0.041 lr: 0.00175)\n",
            "epoch: 1052, train_loss: 0.037296, val loss: 0.041777,  train_metric: 0.037 test_metric: 0.042 lr: 0.00174)\n",
            "epoch: 1053, train_loss: 0.037407, val loss: 0.043053,  train_metric: 0.037 test_metric: 0.043 lr: 0.00174)\n",
            "epoch: 1054, train_loss: 0.037588, val loss: 0.042100,  train_metric: 0.038 test_metric: 0.042 lr: 0.00174)\n",
            "epoch: 1055, train_loss: 0.037701, val loss: 0.042155,  train_metric: 0.038 test_metric: 0.042 lr: 0.00174)\n",
            "epoch: 1056, train_loss: 0.038868, val loss: 0.045198,  train_metric: 0.039 test_metric: 0.045 lr: 0.00174)\n",
            "epoch: 1057, train_loss: 0.039519, val loss: 0.042702,  train_metric: 0.040 test_metric: 0.043 lr: 0.00173)\n",
            "epoch: 1058, train_loss: 0.039454, val loss: 0.044404,  train_metric: 0.039 test_metric: 0.044 lr: 0.00173)\n",
            "epoch: 1059, train_loss: 0.039415, val loss: 0.044168,  train_metric: 0.039 test_metric: 0.044 lr: 0.00173)\n",
            "epoch: 1060, train_loss: 0.038908, val loss: 0.043731,  train_metric: 0.039 test_metric: 0.044 lr: 0.00173)\n",
            "epoch: 1061, train_loss: 0.037968, val loss: 0.043245,  train_metric: 0.038 test_metric: 0.043 lr: 0.00173)\n",
            "epoch: 1062, train_loss: 0.038364, val loss: 0.043163,  train_metric: 0.038 test_metric: 0.043 lr: 0.00173)\n",
            "epoch: 1063, train_loss: 0.037482, val loss: 0.041492,  train_metric: 0.037 test_metric: 0.041 lr: 0.00172)\n",
            "epoch: 1064, train_loss: 0.036662, val loss: 0.041914,  train_metric: 0.037 test_metric: 0.042 lr: 0.00172)\n",
            "epoch: 1065, train_loss: 0.036491, val loss: 0.040745,  train_metric: 0.036 test_metric: 0.041 lr: 0.00172)\n",
            "epoch: 1066, train_loss: 0.036455, val loss: 0.041555,  train_metric: 0.036 test_metric: 0.042 lr: 0.00172)\n",
            "epoch: 1067, train_loss: 0.036331, val loss: 0.040761,  train_metric: 0.036 test_metric: 0.041 lr: 0.00172)\n",
            "epoch: 1068, train_loss: 0.036355, val loss: 0.041244,  train_metric: 0.036 test_metric: 0.041 lr: 0.00172)\n",
            "epoch: 1069, train_loss: 0.036266, val loss: 0.040875,  train_metric: 0.036 test_metric: 0.041 lr: 0.00171)\n",
            "epoch: 1070, train_loss: 0.036869, val loss: 0.040737,  train_metric: 0.037 test_metric: 0.041 lr: 0.00171)\n",
            "epoch: 1071, train_loss: 0.036277, val loss: 0.041600,  train_metric: 0.036 test_metric: 0.042 lr: 0.00171)\n",
            "epoch: 1072, train_loss: 0.036735, val loss: 0.041225,  train_metric: 0.037 test_metric: 0.041 lr: 0.00171)\n",
            "epoch: 1073, train_loss: 0.037529, val loss: 0.040834,  train_metric: 0.038 test_metric: 0.041 lr: 0.00171)\n",
            "epoch: 1074, train_loss: 0.036948, val loss: 0.042104,  train_metric: 0.037 test_metric: 0.042 lr: 0.00171)\n",
            "epoch: 1075, train_loss: 0.036889, val loss: 0.042082,  train_metric: 0.037 test_metric: 0.042 lr: 0.00170)\n",
            "epoch: 1076, train_loss: 0.037157, val loss: 0.042438,  train_metric: 0.037 test_metric: 0.042 lr: 0.00170)\n",
            "epoch: 1077, train_loss: 0.036979, val loss: 0.040966,  train_metric: 0.037 test_metric: 0.041 lr: 0.00170)\n",
            "epoch: 1078, train_loss: 0.036563, val loss: 0.041831,  train_metric: 0.037 test_metric: 0.042 lr: 0.00170)\n",
            "epoch: 1079, train_loss: 0.037009, val loss: 0.041081,  train_metric: 0.037 test_metric: 0.041 lr: 0.00170)\n",
            "epoch: 1080, train_loss: 0.036341, val loss: 0.041209,  train_metric: 0.036 test_metric: 0.041 lr: 0.00170)\n",
            "epoch: 1081, train_loss: 0.036381, val loss: 0.041099,  train_metric: 0.036 test_metric: 0.041 lr: 0.00169)\n",
            "epoch: 1082, train_loss: 0.036315, val loss: 0.041159,  train_metric: 0.036 test_metric: 0.041 lr: 0.00169)\n",
            "epoch: 1083, train_loss: 0.036101, val loss: 0.041443,  train_metric: 0.036 test_metric: 0.041 lr: 0.00169)\n",
            "epoch: 1084, train_loss: 0.036566, val loss: 0.040926,  train_metric: 0.037 test_metric: 0.041 lr: 0.00169)\n",
            "epoch: 1085, train_loss: 0.035883, val loss: 0.040799,  train_metric: 0.036 test_metric: 0.041 lr: 0.00169)\n",
            "epoch: 1086, train_loss: 0.035924, val loss: 0.040678,  train_metric: 0.036 test_metric: 0.041 lr: 0.00169)\n",
            "epoch: 1087, train_loss: 0.035771, val loss: 0.040960,  train_metric: 0.036 test_metric: 0.041 lr: 0.00168)\n",
            "epoch: 1088, train_loss: 0.036079, val loss: 0.040752,  train_metric: 0.036 test_metric: 0.041 lr: 0.00168)\n",
            "epoch: 1089, train_loss: 0.035733, val loss: 0.040714,  train_metric: 0.036 test_metric: 0.041 lr: 0.00168)\n",
            "epoch: 1090, train_loss: 0.035864, val loss: 0.040808,  train_metric: 0.036 test_metric: 0.041 lr: 0.00168)\n",
            "epoch: 1091, train_loss: 0.036765, val loss: 0.040878,  train_metric: 0.037 test_metric: 0.041 lr: 0.00168)\n",
            "epoch: 1092, train_loss: 0.035945, val loss: 0.041133,  train_metric: 0.036 test_metric: 0.041 lr: 0.00168)\n",
            "epoch: 1093, train_loss: 0.035923, val loss: 0.041047,  train_metric: 0.036 test_metric: 0.041 lr: 0.00167)\n",
            "epoch: 1094, train_loss: 0.036045, val loss: 0.041374,  train_metric: 0.036 test_metric: 0.041 lr: 0.00167)\n",
            "epoch: 1095, train_loss: 0.036173, val loss: 0.041038,  train_metric: 0.036 test_metric: 0.041 lr: 0.00167)\n",
            "epoch: 1096, train_loss: 0.036153, val loss: 0.040713,  train_metric: 0.036 test_metric: 0.041 lr: 0.00167)\n",
            "epoch: 1097, train_loss: 0.035817, val loss: 0.041081,  train_metric: 0.036 test_metric: 0.041 lr: 0.00167)\n",
            "epoch: 1098, train_loss: 0.035961, val loss: 0.041268,  train_metric: 0.036 test_metric: 0.041 lr: 0.00167)\n",
            "epoch: 1099, train_loss: 0.035955, val loss: 0.040911,  train_metric: 0.036 test_metric: 0.041 lr: 0.00166)\n",
            "epoch: 1100, train_loss: 0.036055, val loss: 0.041223,  train_metric: 0.036 test_metric: 0.041 lr: 0.00166)\n",
            "epoch: 1101, train_loss: 0.036077, val loss: 0.041124,  train_metric: 0.036 test_metric: 0.041 lr: 0.00166)\n",
            "epoch: 1102, train_loss: 0.035954, val loss: 0.040593,  train_metric: 0.036 test_metric: 0.041 lr: 0.00166)\n",
            "epoch: 1103, train_loss: 0.036190, val loss: 0.040689,  train_metric: 0.036 test_metric: 0.041 lr: 0.00166)\n",
            "epoch: 1104, train_loss: 0.036091, val loss: 0.041010,  train_metric: 0.036 test_metric: 0.041 lr: 0.00166)\n",
            "epoch: 1105, train_loss: 0.035815, val loss: 0.040696,  train_metric: 0.036 test_metric: 0.041 lr: 0.00165)\n",
            "epoch: 1106, train_loss: 0.036268, val loss: 0.040876,  train_metric: 0.036 test_metric: 0.041 lr: 0.00165)\n",
            "epoch: 1107, train_loss: 0.037022, val loss: 0.041568,  train_metric: 0.037 test_metric: 0.042 lr: 0.00165)\n",
            "epoch: 1108, train_loss: 0.035954, val loss: 0.041419,  train_metric: 0.036 test_metric: 0.041 lr: 0.00165)\n",
            "epoch: 1109, train_loss: 0.036359, val loss: 0.041398,  train_metric: 0.036 test_metric: 0.041 lr: 0.00165)\n",
            "epoch: 1110, train_loss: 0.037206, val loss: 0.041298,  train_metric: 0.037 test_metric: 0.041 lr: 0.00165)\n",
            "epoch: 1111, train_loss: 0.037078, val loss: 0.041848,  train_metric: 0.037 test_metric: 0.042 lr: 0.00164)\n",
            "epoch: 1112, train_loss: 0.036792, val loss: 0.041562,  train_metric: 0.037 test_metric: 0.042 lr: 0.00164)\n",
            "epoch: 1113, train_loss: 0.036314, val loss: 0.040846,  train_metric: 0.036 test_metric: 0.041 lr: 0.00164)\n",
            "epoch: 1114, train_loss: 0.036136, val loss: 0.040699,  train_metric: 0.036 test_metric: 0.041 lr: 0.00164)\n",
            "epoch: 1115, train_loss: 0.035992, val loss: 0.041156,  train_metric: 0.036 test_metric: 0.041 lr: 0.00164)\n",
            "epoch: 1116, train_loss: 0.036094, val loss: 0.041519,  train_metric: 0.036 test_metric: 0.042 lr: 0.00164)\n",
            "epoch: 1117, train_loss: 0.036277, val loss: 0.040890,  train_metric: 0.036 test_metric: 0.041 lr: 0.00163)\n",
            "epoch: 1118, train_loss: 0.036083, val loss: 0.041003,  train_metric: 0.036 test_metric: 0.041 lr: 0.00163)\n",
            "epoch: 1119, train_loss: 0.036400, val loss: 0.040984,  train_metric: 0.036 test_metric: 0.041 lr: 0.00163)\n",
            "epoch: 1120, train_loss: 0.035844, val loss: 0.040625,  train_metric: 0.036 test_metric: 0.041 lr: 0.00163)\n",
            "epoch: 1121, train_loss: 0.036027, val loss: 0.040576,  train_metric: 0.036 test_metric: 0.041 lr: 0.00163)\n",
            "epoch: 1122, train_loss: 0.035642, val loss: 0.040778,  train_metric: 0.036 test_metric: 0.041 lr: 0.00163)\n",
            "epoch: 1123, train_loss: 0.035784, val loss: 0.040907,  train_metric: 0.036 test_metric: 0.041 lr: 0.00162)\n",
            "epoch: 1124, train_loss: 0.035934, val loss: 0.040837,  train_metric: 0.036 test_metric: 0.041 lr: 0.00162)\n",
            "epoch: 1125, train_loss: 0.035823, val loss: 0.040565,  train_metric: 0.036 test_metric: 0.041 lr: 0.00162)\n",
            "epoch: 1126, train_loss: 0.035975, val loss: 0.040691,  train_metric: 0.036 test_metric: 0.041 lr: 0.00162)\n",
            "epoch: 1127, train_loss: 0.035627, val loss: 0.040836,  train_metric: 0.036 test_metric: 0.041 lr: 0.00162)\n",
            "epoch: 1128, train_loss: 0.035923, val loss: 0.040731,  train_metric: 0.036 test_metric: 0.041 lr: 0.00162)\n",
            "epoch: 1129, train_loss: 0.035782, val loss: 0.040879,  train_metric: 0.036 test_metric: 0.041 lr: 0.00161)\n",
            "epoch: 1130, train_loss: 0.035793, val loss: 0.040968,  train_metric: 0.036 test_metric: 0.041 lr: 0.00161)\n",
            "epoch: 1131, train_loss: 0.035707, val loss: 0.040633,  train_metric: 0.036 test_metric: 0.041 lr: 0.00161)\n",
            "epoch: 1132, train_loss: 0.036472, val loss: 0.040986,  train_metric: 0.036 test_metric: 0.041 lr: 0.00161)\n",
            "epoch: 1133, train_loss: 0.036018, val loss: 0.040604,  train_metric: 0.036 test_metric: 0.041 lr: 0.00161)\n",
            "epoch: 1134, train_loss: 0.036017, val loss: 0.040997,  train_metric: 0.036 test_metric: 0.041 lr: 0.00161)\n",
            "epoch: 1135, train_loss: 0.036321, val loss: 0.041095,  train_metric: 0.036 test_metric: 0.041 lr: 0.00160)\n",
            "epoch: 1136, train_loss: 0.036530, val loss: 0.040903,  train_metric: 0.037 test_metric: 0.041 lr: 0.00160)\n",
            "epoch: 1137, train_loss: 0.036095, val loss: 0.041042,  train_metric: 0.036 test_metric: 0.041 lr: 0.00160)\n",
            "epoch: 1138, train_loss: 0.036306, val loss: 0.041072,  train_metric: 0.036 test_metric: 0.041 lr: 0.00160)\n",
            "epoch: 1139, train_loss: 0.036014, val loss: 0.040669,  train_metric: 0.036 test_metric: 0.041 lr: 0.00160)\n",
            "epoch: 1140, train_loss: 0.035881, val loss: 0.041330,  train_metric: 0.036 test_metric: 0.041 lr: 0.00160)\n",
            "epoch: 1141, train_loss: 0.035987, val loss: 0.040450,  train_metric: 0.036 test_metric: 0.040 lr: 0.00159)\n",
            "epoch: 1142, train_loss: 0.036165, val loss: 0.040941,  train_metric: 0.036 test_metric: 0.041 lr: 0.00159)\n",
            "epoch: 1143, train_loss: 0.035854, val loss: 0.040752,  train_metric: 0.036 test_metric: 0.041 lr: 0.00159)\n",
            "epoch: 1144, train_loss: 0.036102, val loss: 0.040866,  train_metric: 0.036 test_metric: 0.041 lr: 0.00159)\n",
            "epoch: 1145, train_loss: 0.035844, val loss: 0.040521,  train_metric: 0.036 test_metric: 0.041 lr: 0.00159)\n",
            "epoch: 1146, train_loss: 0.035874, val loss: 0.040800,  train_metric: 0.036 test_metric: 0.041 lr: 0.00159)\n",
            "epoch: 1147, train_loss: 0.035988, val loss: 0.040453,  train_metric: 0.036 test_metric: 0.040 lr: 0.00159)\n",
            "epoch: 1148, train_loss: 0.036300, val loss: 0.040915,  train_metric: 0.036 test_metric: 0.041 lr: 0.00158)\n",
            "epoch: 1149, train_loss: 0.036049, val loss: 0.040837,  train_metric: 0.036 test_metric: 0.041 lr: 0.00158)\n",
            "epoch: 1150, train_loss: 0.036176, val loss: 0.040681,  train_metric: 0.036 test_metric: 0.041 lr: 0.00158)\n",
            "epoch: 1151, train_loss: 0.036497, val loss: 0.040879,  train_metric: 0.036 test_metric: 0.041 lr: 0.00158)\n",
            "epoch: 1152, train_loss: 0.035902, val loss: 0.040695,  train_metric: 0.036 test_metric: 0.041 lr: 0.00158)\n",
            "epoch: 1153, train_loss: 0.036239, val loss: 0.041120,  train_metric: 0.036 test_metric: 0.041 lr: 0.00158)\n",
            "epoch: 1154, train_loss: 0.035974, val loss: 0.040567,  train_metric: 0.036 test_metric: 0.041 lr: 0.00157)\n",
            "epoch: 1155, train_loss: 0.036197, val loss: 0.040871,  train_metric: 0.036 test_metric: 0.041 lr: 0.00157)\n",
            "epoch: 1156, train_loss: 0.036261, val loss: 0.041593,  train_metric: 0.036 test_metric: 0.042 lr: 0.00157)\n",
            "epoch: 1157, train_loss: 0.037003, val loss: 0.041192,  train_metric: 0.037 test_metric: 0.041 lr: 0.00157)\n",
            "epoch: 1158, train_loss: 0.036756, val loss: 0.043150,  train_metric: 0.037 test_metric: 0.043 lr: 0.00157)\n",
            "epoch: 1159, train_loss: 0.036888, val loss: 0.041307,  train_metric: 0.037 test_metric: 0.041 lr: 0.00157)\n",
            "epoch: 1160, train_loss: 0.036434, val loss: 0.041551,  train_metric: 0.036 test_metric: 0.042 lr: 0.00156)\n",
            "epoch: 1161, train_loss: 0.036848, val loss: 0.041207,  train_metric: 0.037 test_metric: 0.041 lr: 0.00156)\n",
            "epoch: 1162, train_loss: 0.036932, val loss: 0.041135,  train_metric: 0.037 test_metric: 0.041 lr: 0.00156)\n",
            "epoch: 1163, train_loss: 0.037072, val loss: 0.042069,  train_metric: 0.037 test_metric: 0.042 lr: 0.00156)\n",
            "epoch: 1164, train_loss: 0.037499, val loss: 0.042868,  train_metric: 0.037 test_metric: 0.043 lr: 0.00156)\n",
            "epoch: 1165, train_loss: 0.038012, val loss: 0.042008,  train_metric: 0.038 test_metric: 0.042 lr: 0.00156)\n",
            "epoch: 1166, train_loss: 0.037484, val loss: 0.041722,  train_metric: 0.037 test_metric: 0.042 lr: 0.00156)\n",
            "epoch: 1167, train_loss: 0.037046, val loss: 0.042554,  train_metric: 0.037 test_metric: 0.043 lr: 0.00155)\n",
            "epoch: 1168, train_loss: 0.037849, val loss: 0.042426,  train_metric: 0.038 test_metric: 0.042 lr: 0.00155)\n",
            "epoch: 1169, train_loss: 0.037050, val loss: 0.040987,  train_metric: 0.037 test_metric: 0.041 lr: 0.00155)\n",
            "epoch: 1170, train_loss: 0.036939, val loss: 0.042146,  train_metric: 0.037 test_metric: 0.042 lr: 0.00155)\n",
            "epoch: 1171, train_loss: 0.036898, val loss: 0.041079,  train_metric: 0.037 test_metric: 0.041 lr: 0.00155)\n",
            "epoch: 1172, train_loss: 0.036809, val loss: 0.043829,  train_metric: 0.037 test_metric: 0.044 lr: 0.00155)\n",
            "epoch: 1173, train_loss: 0.037545, val loss: 0.043346,  train_metric: 0.038 test_metric: 0.043 lr: 0.00154)\n",
            "epoch: 1174, train_loss: 0.038067, val loss: 0.042709,  train_metric: 0.038 test_metric: 0.043 lr: 0.00154)\n",
            "epoch: 1175, train_loss: 0.037287, val loss: 0.042945,  train_metric: 0.037 test_metric: 0.043 lr: 0.00154)\n",
            "epoch: 1176, train_loss: 0.036880, val loss: 0.041028,  train_metric: 0.037 test_metric: 0.041 lr: 0.00154)\n",
            "epoch: 1177, train_loss: 0.036282, val loss: 0.041644,  train_metric: 0.036 test_metric: 0.042 lr: 0.00154)\n",
            "epoch: 1178, train_loss: 0.036310, val loss: 0.041387,  train_metric: 0.036 test_metric: 0.041 lr: 0.00154)\n",
            "epoch: 1179, train_loss: 0.036161, val loss: 0.041430,  train_metric: 0.036 test_metric: 0.041 lr: 0.00154)\n",
            "epoch: 1180, train_loss: 0.035928, val loss: 0.040846,  train_metric: 0.036 test_metric: 0.041 lr: 0.00153)\n",
            "epoch: 1181, train_loss: 0.035972, val loss: 0.040749,  train_metric: 0.036 test_metric: 0.041 lr: 0.00153)\n",
            "epoch: 1182, train_loss: 0.035821, val loss: 0.040757,  train_metric: 0.036 test_metric: 0.041 lr: 0.00153)\n",
            "epoch: 1183, train_loss: 0.035698, val loss: 0.040552,  train_metric: 0.036 test_metric: 0.041 lr: 0.00153)\n",
            "epoch: 1184, train_loss: 0.035561, val loss: 0.040741,  train_metric: 0.036 test_metric: 0.041 lr: 0.00153)\n",
            "epoch: 1185, train_loss: 0.035643, val loss: 0.040562,  train_metric: 0.036 test_metric: 0.041 lr: 0.00153)\n",
            "epoch: 1186, train_loss: 0.035936, val loss: 0.040613,  train_metric: 0.036 test_metric: 0.041 lr: 0.00152)\n",
            "epoch: 1187, train_loss: 0.035640, val loss: 0.040388,  train_metric: 0.036 test_metric: 0.040 lr: 0.00152)\n",
            "epoch: 1188, train_loss: 0.035516, val loss: 0.040511,  train_metric: 0.036 test_metric: 0.041 lr: 0.00152)\n",
            "epoch: 1189, train_loss: 0.036023, val loss: 0.040476,  train_metric: 0.036 test_metric: 0.040 lr: 0.00152)\n",
            "epoch: 1190, train_loss: 0.035854, val loss: 0.040526,  train_metric: 0.036 test_metric: 0.041 lr: 0.00152)\n",
            "epoch: 1191, train_loss: 0.035695, val loss: 0.040531,  train_metric: 0.036 test_metric: 0.041 lr: 0.00152)\n",
            "epoch: 1192, train_loss: 0.035433, val loss: 0.040361,  train_metric: 0.035 test_metric: 0.040 lr: 0.00152)\n",
            "epoch: 1193, train_loss: 0.036096, val loss: 0.040542,  train_metric: 0.036 test_metric: 0.041 lr: 0.00151)\n",
            "epoch: 1194, train_loss: 0.036213, val loss: 0.040876,  train_metric: 0.036 test_metric: 0.041 lr: 0.00151)\n",
            "epoch: 1195, train_loss: 0.036124, val loss: 0.040394,  train_metric: 0.036 test_metric: 0.040 lr: 0.00151)\n",
            "epoch: 1196, train_loss: 0.035789, val loss: 0.040988,  train_metric: 0.036 test_metric: 0.041 lr: 0.00151)\n",
            "epoch: 1197, train_loss: 0.036135, val loss: 0.040413,  train_metric: 0.036 test_metric: 0.040 lr: 0.00151)\n",
            "epoch: 1198, train_loss: 0.035736, val loss: 0.040636,  train_metric: 0.036 test_metric: 0.041 lr: 0.00151)\n",
            "epoch: 1199, train_loss: 0.035748, val loss: 0.040839,  train_metric: 0.036 test_metric: 0.041 lr: 0.00151)\n",
            "epoch: 1200, train_loss: 0.035823, val loss: 0.040722,  train_metric: 0.036 test_metric: 0.041 lr: 0.00150)\n",
            "epoch: 1201, train_loss: 0.035906, val loss: 0.040513,  train_metric: 0.036 test_metric: 0.041 lr: 0.00150)\n",
            "epoch: 1202, train_loss: 0.035667, val loss: 0.040773,  train_metric: 0.036 test_metric: 0.041 lr: 0.00150)\n",
            "epoch: 1203, train_loss: 0.035763, val loss: 0.040843,  train_metric: 0.036 test_metric: 0.041 lr: 0.00150)\n",
            "epoch: 1204, train_loss: 0.035881, val loss: 0.041019,  train_metric: 0.036 test_metric: 0.041 lr: 0.00150)\n",
            "epoch: 1205, train_loss: 0.035683, val loss: 0.040649,  train_metric: 0.036 test_metric: 0.041 lr: 0.00150)\n",
            "epoch: 1206, train_loss: 0.036061, val loss: 0.040838,  train_metric: 0.036 test_metric: 0.041 lr: 0.00149)\n",
            "epoch: 1207, train_loss: 0.035719, val loss: 0.040304,  train_metric: 0.036 test_metric: 0.040 lr: 0.00149)\n",
            "epoch: 1208, train_loss: 0.035830, val loss: 0.040839,  train_metric: 0.036 test_metric: 0.041 lr: 0.00149)\n",
            "epoch: 1209, train_loss: 0.035647, val loss: 0.040372,  train_metric: 0.036 test_metric: 0.040 lr: 0.00149)\n",
            "epoch: 1210, train_loss: 0.036046, val loss: 0.040891,  train_metric: 0.036 test_metric: 0.041 lr: 0.00149)\n",
            "epoch: 1211, train_loss: 0.036217, val loss: 0.040548,  train_metric: 0.036 test_metric: 0.041 lr: 0.00149)\n",
            "epoch: 1212, train_loss: 0.035815, val loss: 0.040722,  train_metric: 0.036 test_metric: 0.041 lr: 0.00149)\n",
            "epoch: 1213, train_loss: 0.036133, val loss: 0.040437,  train_metric: 0.036 test_metric: 0.040 lr: 0.00148)\n",
            "epoch: 1214, train_loss: 0.036625, val loss: 0.041012,  train_metric: 0.037 test_metric: 0.041 lr: 0.00148)\n",
            "epoch: 1215, train_loss: 0.036002, val loss: 0.041122,  train_metric: 0.036 test_metric: 0.041 lr: 0.00148)\n",
            "epoch: 1216, train_loss: 0.035898, val loss: 0.040696,  train_metric: 0.036 test_metric: 0.041 lr: 0.00148)\n",
            "epoch: 1217, train_loss: 0.036274, val loss: 0.041014,  train_metric: 0.036 test_metric: 0.041 lr: 0.00148)\n",
            "epoch: 1218, train_loss: 0.035749, val loss: 0.040930,  train_metric: 0.036 test_metric: 0.041 lr: 0.00148)\n",
            "epoch: 1219, train_loss: 0.036145, val loss: 0.040679,  train_metric: 0.036 test_metric: 0.041 lr: 0.00148)\n",
            "epoch: 1220, train_loss: 0.036425, val loss: 0.040527,  train_metric: 0.036 test_metric: 0.041 lr: 0.00147)\n",
            "epoch: 1221, train_loss: 0.035892, val loss: 0.040496,  train_metric: 0.036 test_metric: 0.040 lr: 0.00147)\n",
            "epoch: 1222, train_loss: 0.035942, val loss: 0.040705,  train_metric: 0.036 test_metric: 0.041 lr: 0.00147)\n",
            "epoch: 1223, train_loss: 0.035689, val loss: 0.040730,  train_metric: 0.036 test_metric: 0.041 lr: 0.00147)\n",
            "epoch: 1224, train_loss: 0.035779, val loss: 0.040696,  train_metric: 0.036 test_metric: 0.041 lr: 0.00147)\n",
            "epoch: 1225, train_loss: 0.036067, val loss: 0.041016,  train_metric: 0.036 test_metric: 0.041 lr: 0.00147)\n",
            "epoch: 1226, train_loss: 0.035650, val loss: 0.040459,  train_metric: 0.036 test_metric: 0.040 lr: 0.00146)\n",
            "epoch: 1227, train_loss: 0.035841, val loss: 0.040476,  train_metric: 0.036 test_metric: 0.040 lr: 0.00146)\n",
            "epoch: 1228, train_loss: 0.035821, val loss: 0.040831,  train_metric: 0.036 test_metric: 0.041 lr: 0.00146)\n",
            "epoch: 1229, train_loss: 0.035890, val loss: 0.040966,  train_metric: 0.036 test_metric: 0.041 lr: 0.00146)\n",
            "epoch: 1230, train_loss: 0.035746, val loss: 0.041007,  train_metric: 0.036 test_metric: 0.041 lr: 0.00146)\n",
            "epoch: 1231, train_loss: 0.035619, val loss: 0.040475,  train_metric: 0.036 test_metric: 0.040 lr: 0.00146)\n",
            "epoch: 1232, train_loss: 0.035658, val loss: 0.040643,  train_metric: 0.036 test_metric: 0.041 lr: 0.00146)\n",
            "epoch: 1233, train_loss: 0.035703, val loss: 0.040409,  train_metric: 0.036 test_metric: 0.040 lr: 0.00145)\n",
            "epoch: 1234, train_loss: 0.036213, val loss: 0.041559,  train_metric: 0.036 test_metric: 0.042 lr: 0.00145)\n",
            "epoch: 1235, train_loss: 0.036140, val loss: 0.040938,  train_metric: 0.036 test_metric: 0.041 lr: 0.00145)\n",
            "epoch: 1236, train_loss: 0.035944, val loss: 0.040572,  train_metric: 0.036 test_metric: 0.041 lr: 0.00145)\n",
            "epoch: 1237, train_loss: 0.036017, val loss: 0.041075,  train_metric: 0.036 test_metric: 0.041 lr: 0.00145)\n",
            "epoch: 1238, train_loss: 0.036249, val loss: 0.040580,  train_metric: 0.036 test_metric: 0.041 lr: 0.00145)\n",
            "epoch: 1239, train_loss: 0.035770, val loss: 0.041028,  train_metric: 0.036 test_metric: 0.041 lr: 0.00145)\n",
            "epoch: 1240, train_loss: 0.035889, val loss: 0.040572,  train_metric: 0.036 test_metric: 0.041 lr: 0.00144)\n",
            "epoch: 1241, train_loss: 0.035805, val loss: 0.040537,  train_metric: 0.036 test_metric: 0.041 lr: 0.00144)\n",
            "epoch: 1242, train_loss: 0.035591, val loss: 0.040457,  train_metric: 0.036 test_metric: 0.040 lr: 0.00144)\n",
            "epoch: 1243, train_loss: 0.035462, val loss: 0.040441,  train_metric: 0.035 test_metric: 0.040 lr: 0.00144)\n",
            "epoch: 1244, train_loss: 0.035476, val loss: 0.040424,  train_metric: 0.035 test_metric: 0.040 lr: 0.00144)\n",
            "epoch: 1245, train_loss: 0.035841, val loss: 0.040748,  train_metric: 0.036 test_metric: 0.041 lr: 0.00144)\n",
            "epoch: 1246, train_loss: 0.035935, val loss: 0.040806,  train_metric: 0.036 test_metric: 0.041 lr: 0.00144)\n",
            "epoch: 1247, train_loss: 0.036042, val loss: 0.040462,  train_metric: 0.036 test_metric: 0.040 lr: 0.00143)\n",
            "epoch: 1248, train_loss: 0.035668, val loss: 0.040601,  train_metric: 0.036 test_metric: 0.041 lr: 0.00143)\n",
            "epoch: 1249, train_loss: 0.035602, val loss: 0.040852,  train_metric: 0.036 test_metric: 0.041 lr: 0.00143)\n",
            "epoch: 1250, train_loss: 0.036977, val loss: 0.042404,  train_metric: 0.037 test_metric: 0.042 lr: 0.00143)\n",
            "epoch: 1251, train_loss: 0.037101, val loss: 0.041222,  train_metric: 0.037 test_metric: 0.041 lr: 0.00143)\n",
            "epoch: 1252, train_loss: 0.038278, val loss: 0.041996,  train_metric: 0.038 test_metric: 0.042 lr: 0.00143)\n",
            "epoch: 1253, train_loss: 0.037075, val loss: 0.042037,  train_metric: 0.037 test_metric: 0.042 lr: 0.00143)\n",
            "epoch: 1254, train_loss: 0.037035, val loss: 0.041188,  train_metric: 0.037 test_metric: 0.041 lr: 0.00142)\n",
            "epoch: 1255, train_loss: 0.036381, val loss: 0.040733,  train_metric: 0.036 test_metric: 0.041 lr: 0.00142)\n",
            "epoch: 1256, train_loss: 0.035928, val loss: 0.040785,  train_metric: 0.036 test_metric: 0.041 lr: 0.00142)\n",
            "epoch: 1257, train_loss: 0.036037, val loss: 0.040826,  train_metric: 0.036 test_metric: 0.041 lr: 0.00142)\n",
            "epoch: 1258, train_loss: 0.035770, val loss: 0.040974,  train_metric: 0.036 test_metric: 0.041 lr: 0.00142)\n",
            "epoch: 1259, train_loss: 0.035801, val loss: 0.040870,  train_metric: 0.036 test_metric: 0.041 lr: 0.00142)\n",
            "epoch: 1260, train_loss: 0.035846, val loss: 0.040680,  train_metric: 0.036 test_metric: 0.041 lr: 0.00142)\n",
            "epoch: 1261, train_loss: 0.036345, val loss: 0.040800,  train_metric: 0.036 test_metric: 0.041 lr: 0.00141)\n",
            "epoch: 1262, train_loss: 0.035706, val loss: 0.040408,  train_metric: 0.036 test_metric: 0.040 lr: 0.00141)\n",
            "epoch: 1263, train_loss: 0.035634, val loss: 0.041067,  train_metric: 0.036 test_metric: 0.041 lr: 0.00141)\n",
            "epoch: 1264, train_loss: 0.035910, val loss: 0.040438,  train_metric: 0.036 test_metric: 0.040 lr: 0.00141)\n",
            "epoch: 1265, train_loss: 0.035779, val loss: 0.040498,  train_metric: 0.036 test_metric: 0.040 lr: 0.00141)\n",
            "epoch: 1266, train_loss: 0.035489, val loss: 0.040437,  train_metric: 0.035 test_metric: 0.040 lr: 0.00141)\n",
            "epoch: 1267, train_loss: 0.035356, val loss: 0.040382,  train_metric: 0.035 test_metric: 0.040 lr: 0.00141)\n",
            "epoch: 1268, train_loss: 0.035486, val loss: 0.040447,  train_metric: 0.035 test_metric: 0.040 lr: 0.00140)\n",
            "epoch: 1269, train_loss: 0.035557, val loss: 0.040300,  train_metric: 0.036 test_metric: 0.040 lr: 0.00140)\n",
            "epoch: 1270, train_loss: 0.036359, val loss: 0.040859,  train_metric: 0.036 test_metric: 0.041 lr: 0.00140)\n",
            "epoch: 1271, train_loss: 0.036576, val loss: 0.041343,  train_metric: 0.037 test_metric: 0.041 lr: 0.00140)\n",
            "epoch: 1272, train_loss: 0.036255, val loss: 0.040844,  train_metric: 0.036 test_metric: 0.041 lr: 0.00140)\n",
            "epoch: 1273, train_loss: 0.036118, val loss: 0.041636,  train_metric: 0.036 test_metric: 0.042 lr: 0.00140)\n",
            "epoch: 1274, train_loss: 0.036406, val loss: 0.041640,  train_metric: 0.036 test_metric: 0.042 lr: 0.00140)\n",
            "epoch: 1275, train_loss: 0.036490, val loss: 0.040868,  train_metric: 0.036 test_metric: 0.041 lr: 0.00139)\n",
            "epoch: 1276, train_loss: 0.036847, val loss: 0.041507,  train_metric: 0.037 test_metric: 0.042 lr: 0.00139)\n",
            "epoch: 1277, train_loss: 0.036263, val loss: 0.041226,  train_metric: 0.036 test_metric: 0.041 lr: 0.00139)\n",
            "epoch: 1278, train_loss: 0.036399, val loss: 0.041831,  train_metric: 0.036 test_metric: 0.042 lr: 0.00139)\n",
            "epoch: 1279, train_loss: 0.036642, val loss: 0.041852,  train_metric: 0.037 test_metric: 0.042 lr: 0.00139)\n",
            "epoch: 1280, train_loss: 0.036288, val loss: 0.041315,  train_metric: 0.036 test_metric: 0.041 lr: 0.00139)\n",
            "epoch: 1281, train_loss: 0.036153, val loss: 0.041007,  train_metric: 0.036 test_metric: 0.041 lr: 0.00139)\n",
            "epoch: 1282, train_loss: 0.036173, val loss: 0.040597,  train_metric: 0.036 test_metric: 0.041 lr: 0.00139)\n",
            "epoch: 1283, train_loss: 0.036268, val loss: 0.041253,  train_metric: 0.036 test_metric: 0.041 lr: 0.00138)\n",
            "epoch: 1284, train_loss: 0.036523, val loss: 0.040904,  train_metric: 0.037 test_metric: 0.041 lr: 0.00138)\n",
            "epoch: 1285, train_loss: 0.036248, val loss: 0.041687,  train_metric: 0.036 test_metric: 0.042 lr: 0.00138)\n",
            "epoch: 1286, train_loss: 0.036254, val loss: 0.040542,  train_metric: 0.036 test_metric: 0.041 lr: 0.00138)\n",
            "epoch: 1287, train_loss: 0.036113, val loss: 0.041589,  train_metric: 0.036 test_metric: 0.042 lr: 0.00138)\n",
            "epoch: 1288, train_loss: 0.036981, val loss: 0.043664,  train_metric: 0.037 test_metric: 0.044 lr: 0.00138)\n",
            "epoch: 1289, train_loss: 0.036767, val loss: 0.041323,  train_metric: 0.037 test_metric: 0.041 lr: 0.00138)\n",
            "epoch: 1290, train_loss: 0.036157, val loss: 0.040800,  train_metric: 0.036 test_metric: 0.041 lr: 0.00137)\n",
            "epoch: 1291, train_loss: 0.035745, val loss: 0.040912,  train_metric: 0.036 test_metric: 0.041 lr: 0.00137)\n",
            "epoch: 1292, train_loss: 0.035832, val loss: 0.040613,  train_metric: 0.036 test_metric: 0.041 lr: 0.00137)\n",
            "epoch: 1293, train_loss: 0.035882, val loss: 0.040731,  train_metric: 0.036 test_metric: 0.041 lr: 0.00137)\n",
            "epoch: 1294, train_loss: 0.036275, val loss: 0.040833,  train_metric: 0.036 test_metric: 0.041 lr: 0.00137)\n",
            "epoch: 1295, train_loss: 0.036255, val loss: 0.040321,  train_metric: 0.036 test_metric: 0.040 lr: 0.00137)\n",
            "epoch: 1296, train_loss: 0.035611, val loss: 0.040866,  train_metric: 0.036 test_metric: 0.041 lr: 0.00137)\n",
            "epoch: 1297, train_loss: 0.035880, val loss: 0.040804,  train_metric: 0.036 test_metric: 0.041 lr: 0.00136)\n",
            "epoch: 1298, train_loss: 0.035922, val loss: 0.040980,  train_metric: 0.036 test_metric: 0.041 lr: 0.00136)\n",
            "epoch: 1299, train_loss: 0.035911, val loss: 0.040607,  train_metric: 0.036 test_metric: 0.041 lr: 0.00136)\n",
            "epoch: 1300, train_loss: 0.035722, val loss: 0.040673,  train_metric: 0.036 test_metric: 0.041 lr: 0.00136)\n",
            "epoch: 1301, train_loss: 0.036102, val loss: 0.041161,  train_metric: 0.036 test_metric: 0.041 lr: 0.00136)\n",
            "epoch: 1302, train_loss: 0.036668, val loss: 0.040393,  train_metric: 0.037 test_metric: 0.040 lr: 0.00136)\n",
            "epoch: 1303, train_loss: 0.036351, val loss: 0.041822,  train_metric: 0.036 test_metric: 0.042 lr: 0.00136)\n",
            "epoch: 1304, train_loss: 0.036224, val loss: 0.040794,  train_metric: 0.036 test_metric: 0.041 lr: 0.00135)\n",
            "epoch: 1305, train_loss: 0.035743, val loss: 0.040598,  train_metric: 0.036 test_metric: 0.041 lr: 0.00135)\n",
            "epoch: 1306, train_loss: 0.035641, val loss: 0.040460,  train_metric: 0.036 test_metric: 0.040 lr: 0.00135)\n",
            "epoch: 1307, train_loss: 0.035908, val loss: 0.040938,  train_metric: 0.036 test_metric: 0.041 lr: 0.00135)\n",
            "epoch: 1308, train_loss: 0.036340, val loss: 0.040685,  train_metric: 0.036 test_metric: 0.041 lr: 0.00135)\n",
            "epoch: 1309, train_loss: 0.035878, val loss: 0.040515,  train_metric: 0.036 test_metric: 0.041 lr: 0.00135)\n",
            "epoch: 1310, train_loss: 0.035968, val loss: 0.040724,  train_metric: 0.036 test_metric: 0.041 lr: 0.00135)\n",
            "epoch: 1311, train_loss: 0.035879, val loss: 0.041291,  train_metric: 0.036 test_metric: 0.041 lr: 0.00135)\n",
            "epoch: 1312, train_loss: 0.035978, val loss: 0.040865,  train_metric: 0.036 test_metric: 0.041 lr: 0.00134)\n",
            "epoch: 1313, train_loss: 0.035776, val loss: 0.040672,  train_metric: 0.036 test_metric: 0.041 lr: 0.00134)\n",
            "epoch: 1314, train_loss: 0.036510, val loss: 0.040609,  train_metric: 0.037 test_metric: 0.041 lr: 0.00134)\n",
            "epoch: 1315, train_loss: 0.036131, val loss: 0.041670,  train_metric: 0.036 test_metric: 0.042 lr: 0.00134)\n",
            "epoch: 1316, train_loss: 0.036875, val loss: 0.040766,  train_metric: 0.037 test_metric: 0.041 lr: 0.00134)\n",
            "epoch: 1317, train_loss: 0.036346, val loss: 0.041200,  train_metric: 0.036 test_metric: 0.041 lr: 0.00134)\n",
            "epoch: 1318, train_loss: 0.036125, val loss: 0.041689,  train_metric: 0.036 test_metric: 0.042 lr: 0.00134)\n",
            "epoch: 1319, train_loss: 0.036821, val loss: 0.041148,  train_metric: 0.037 test_metric: 0.041 lr: 0.00133)\n",
            "epoch: 1320, train_loss: 0.036616, val loss: 0.042883,  train_metric: 0.037 test_metric: 0.043 lr: 0.00133)\n",
            "epoch: 1321, train_loss: 0.036702, val loss: 0.041368,  train_metric: 0.037 test_metric: 0.041 lr: 0.00133)\n",
            "epoch: 1322, train_loss: 0.036243, val loss: 0.040835,  train_metric: 0.036 test_metric: 0.041 lr: 0.00133)\n",
            "epoch: 1323, train_loss: 0.036474, val loss: 0.041197,  train_metric: 0.036 test_metric: 0.041 lr: 0.00133)\n",
            "epoch: 1324, train_loss: 0.036257, val loss: 0.041703,  train_metric: 0.036 test_metric: 0.042 lr: 0.00133)\n",
            "epoch: 1325, train_loss: 0.037122, val loss: 0.041521,  train_metric: 0.037 test_metric: 0.042 lr: 0.00133)\n",
            "epoch: 1326, train_loss: 0.036487, val loss: 0.041752,  train_metric: 0.036 test_metric: 0.042 lr: 0.00133)\n",
            "epoch: 1327, train_loss: 0.036243, val loss: 0.041289,  train_metric: 0.036 test_metric: 0.041 lr: 0.00132)\n",
            "epoch: 1328, train_loss: 0.036123, val loss: 0.041051,  train_metric: 0.036 test_metric: 0.041 lr: 0.00132)\n",
            "epoch: 1329, train_loss: 0.036332, val loss: 0.041167,  train_metric: 0.036 test_metric: 0.041 lr: 0.00132)\n",
            "epoch: 1330, train_loss: 0.036341, val loss: 0.042574,  train_metric: 0.036 test_metric: 0.043 lr: 0.00132)\n",
            "epoch: 1331, train_loss: 0.036342, val loss: 0.041221,  train_metric: 0.036 test_metric: 0.041 lr: 0.00132)\n",
            "epoch: 1332, train_loss: 0.035986, val loss: 0.040556,  train_metric: 0.036 test_metric: 0.041 lr: 0.00132)\n",
            "epoch: 1333, train_loss: 0.036258, val loss: 0.040928,  train_metric: 0.036 test_metric: 0.041 lr: 0.00132)\n",
            "epoch: 1334, train_loss: 0.035876, val loss: 0.040868,  train_metric: 0.036 test_metric: 0.041 lr: 0.00131)\n",
            "epoch: 1335, train_loss: 0.035937, val loss: 0.041062,  train_metric: 0.036 test_metric: 0.041 lr: 0.00131)\n",
            "epoch: 1336, train_loss: 0.036428, val loss: 0.040714,  train_metric: 0.036 test_metric: 0.041 lr: 0.00131)\n",
            "epoch: 1337, train_loss: 0.035694, val loss: 0.040444,  train_metric: 0.036 test_metric: 0.040 lr: 0.00131)\n",
            "epoch: 1338, train_loss: 0.035700, val loss: 0.040591,  train_metric: 0.036 test_metric: 0.041 lr: 0.00131)\n",
            "epoch: 1339, train_loss: 0.035556, val loss: 0.040340,  train_metric: 0.036 test_metric: 0.040 lr: 0.00131)\n",
            "epoch: 1340, train_loss: 0.035439, val loss: 0.040426,  train_metric: 0.035 test_metric: 0.040 lr: 0.00131)\n",
            "epoch: 1341, train_loss: 0.035386, val loss: 0.040499,  train_metric: 0.035 test_metric: 0.040 lr: 0.00131)\n",
            "epoch: 1342, train_loss: 0.035392, val loss: 0.040505,  train_metric: 0.035 test_metric: 0.041 lr: 0.00130)\n",
            "epoch: 1343, train_loss: 0.035577, val loss: 0.040432,  train_metric: 0.036 test_metric: 0.040 lr: 0.00130)\n",
            "epoch: 1344, train_loss: 0.036064, val loss: 0.041002,  train_metric: 0.036 test_metric: 0.041 lr: 0.00130)\n",
            "epoch: 1345, train_loss: 0.035971, val loss: 0.040418,  train_metric: 0.036 test_metric: 0.040 lr: 0.00130)\n",
            "epoch: 1346, train_loss: 0.036328, val loss: 0.040850,  train_metric: 0.036 test_metric: 0.041 lr: 0.00130)\n",
            "epoch: 1347, train_loss: 0.036258, val loss: 0.041276,  train_metric: 0.036 test_metric: 0.041 lr: 0.00130)\n",
            "epoch: 1348, train_loss: 0.036061, val loss: 0.041458,  train_metric: 0.036 test_metric: 0.041 lr: 0.00130)\n",
            "epoch: 1349, train_loss: 0.036389, val loss: 0.040891,  train_metric: 0.036 test_metric: 0.041 lr: 0.00130)\n",
            "epoch: 1350, train_loss: 0.035705, val loss: 0.040955,  train_metric: 0.036 test_metric: 0.041 lr: 0.00129)\n",
            "epoch: 1351, train_loss: 0.035725, val loss: 0.040354,  train_metric: 0.036 test_metric: 0.040 lr: 0.00129)\n",
            "epoch: 1352, train_loss: 0.035438, val loss: 0.040538,  train_metric: 0.035 test_metric: 0.041 lr: 0.00129)\n",
            "epoch: 1353, train_loss: 0.035459, val loss: 0.040513,  train_metric: 0.035 test_metric: 0.041 lr: 0.00129)\n",
            "epoch: 1354, train_loss: 0.035856, val loss: 0.040759,  train_metric: 0.036 test_metric: 0.041 lr: 0.00129)\n",
            "epoch: 1355, train_loss: 0.035736, val loss: 0.040545,  train_metric: 0.036 test_metric: 0.041 lr: 0.00129)\n",
            "epoch: 1356, train_loss: 0.035439, val loss: 0.040390,  train_metric: 0.035 test_metric: 0.040 lr: 0.00129)\n",
            "epoch: 1357, train_loss: 0.035589, val loss: 0.040539,  train_metric: 0.036 test_metric: 0.041 lr: 0.00128)\n",
            "epoch: 1358, train_loss: 0.035514, val loss: 0.040347,  train_metric: 0.036 test_metric: 0.040 lr: 0.00128)\n",
            "epoch: 1359, train_loss: 0.035344, val loss: 0.040515,  train_metric: 0.035 test_metric: 0.041 lr: 0.00128)\n",
            "epoch: 1360, train_loss: 0.035434, val loss: 0.040517,  train_metric: 0.035 test_metric: 0.041 lr: 0.00128)\n",
            "epoch: 1361, train_loss: 0.035482, val loss: 0.040645,  train_metric: 0.035 test_metric: 0.041 lr: 0.00128)\n",
            "epoch: 1362, train_loss: 0.035991, val loss: 0.041958,  train_metric: 0.036 test_metric: 0.042 lr: 0.00128)\n",
            "epoch: 1363, train_loss: 0.036408, val loss: 0.041379,  train_metric: 0.036 test_metric: 0.041 lr: 0.00128)\n",
            "epoch: 1364, train_loss: 0.036420, val loss: 0.042644,  train_metric: 0.036 test_metric: 0.043 lr: 0.00128)\n",
            "epoch: 1365, train_loss: 0.037391, val loss: 0.044871,  train_metric: 0.037 test_metric: 0.045 lr: 0.00127)\n",
            "epoch: 1366, train_loss: 0.037618, val loss: 0.042213,  train_metric: 0.038 test_metric: 0.042 lr: 0.00127)\n",
            "epoch: 1367, train_loss: 0.036550, val loss: 0.041256,  train_metric: 0.037 test_metric: 0.041 lr: 0.00127)\n",
            "epoch: 1368, train_loss: 0.036536, val loss: 0.040603,  train_metric: 0.037 test_metric: 0.041 lr: 0.00127)\n",
            "epoch: 1369, train_loss: 0.036074, val loss: 0.041040,  train_metric: 0.036 test_metric: 0.041 lr: 0.00127)\n",
            "epoch: 1370, train_loss: 0.036615, val loss: 0.040743,  train_metric: 0.037 test_metric: 0.041 lr: 0.00127)\n",
            "epoch: 1371, train_loss: 0.035972, val loss: 0.040956,  train_metric: 0.036 test_metric: 0.041 lr: 0.00127)\n",
            "epoch: 1372, train_loss: 0.035717, val loss: 0.040466,  train_metric: 0.036 test_metric: 0.040 lr: 0.00127)\n",
            "epoch: 1373, train_loss: 0.035759, val loss: 0.040891,  train_metric: 0.036 test_metric: 0.041 lr: 0.00126)\n",
            "epoch: 1374, train_loss: 0.036026, val loss: 0.040879,  train_metric: 0.036 test_metric: 0.041 lr: 0.00126)\n",
            "epoch: 1375, train_loss: 0.035987, val loss: 0.040711,  train_metric: 0.036 test_metric: 0.041 lr: 0.00126)\n",
            "epoch: 1376, train_loss: 0.035407, val loss: 0.040402,  train_metric: 0.035 test_metric: 0.040 lr: 0.00126)\n",
            "epoch: 1377, train_loss: 0.035743, val loss: 0.040170,  train_metric: 0.036 test_metric: 0.040 lr: 0.00126)\n",
            "epoch: 1378, train_loss: 0.035533, val loss: 0.040613,  train_metric: 0.036 test_metric: 0.041 lr: 0.00126)\n",
            "epoch: 1379, train_loss: 0.035420, val loss: 0.040428,  train_metric: 0.035 test_metric: 0.040 lr: 0.00126)\n",
            "epoch: 1380, train_loss: 0.035469, val loss: 0.040471,  train_metric: 0.035 test_metric: 0.040 lr: 0.00126)\n",
            "epoch: 1381, train_loss: 0.035605, val loss: 0.040280,  train_metric: 0.036 test_metric: 0.040 lr: 0.00125)\n",
            "epoch: 1382, train_loss: 0.035381, val loss: 0.040412,  train_metric: 0.035 test_metric: 0.040 lr: 0.00125)\n",
            "epoch: 1383, train_loss: 0.035540, val loss: 0.040428,  train_metric: 0.036 test_metric: 0.040 lr: 0.00125)\n",
            "epoch: 1384, train_loss: 0.035391, val loss: 0.040624,  train_metric: 0.035 test_metric: 0.041 lr: 0.00125)\n",
            "epoch: 1385, train_loss: 0.035305, val loss: 0.040325,  train_metric: 0.035 test_metric: 0.040 lr: 0.00125)\n",
            "epoch: 1386, train_loss: 0.035420, val loss: 0.040274,  train_metric: 0.035 test_metric: 0.040 lr: 0.00125)\n",
            "epoch: 1387, train_loss: 0.035557, val loss: 0.040389,  train_metric: 0.036 test_metric: 0.040 lr: 0.00125)\n",
            "epoch: 1388, train_loss: 0.035598, val loss: 0.040310,  train_metric: 0.036 test_metric: 0.040 lr: 0.00125)\n",
            "epoch: 1389, train_loss: 0.035384, val loss: 0.040328,  train_metric: 0.035 test_metric: 0.040 lr: 0.00124)\n",
            "epoch: 1390, train_loss: 0.035506, val loss: 0.040179,  train_metric: 0.036 test_metric: 0.040 lr: 0.00124)\n",
            "epoch: 1391, train_loss: 0.035412, val loss: 0.040160,  train_metric: 0.035 test_metric: 0.040 lr: 0.00124)\n",
            "epoch: 1392, train_loss: 0.035682, val loss: 0.040622,  train_metric: 0.036 test_metric: 0.041 lr: 0.00124)\n",
            "epoch: 1393, train_loss: 0.035711, val loss: 0.040221,  train_metric: 0.036 test_metric: 0.040 lr: 0.00124)\n",
            "epoch: 1394, train_loss: 0.035550, val loss: 0.040636,  train_metric: 0.036 test_metric: 0.041 lr: 0.00124)\n",
            "epoch: 1395, train_loss: 0.035511, val loss: 0.040240,  train_metric: 0.036 test_metric: 0.040 lr: 0.00124)\n",
            "epoch: 1396, train_loss: 0.035272, val loss: 0.040264,  train_metric: 0.035 test_metric: 0.040 lr: 0.00124)\n",
            "epoch: 1397, train_loss: 0.035327, val loss: 0.040586,  train_metric: 0.035 test_metric: 0.041 lr: 0.00123)\n",
            "epoch: 1398, train_loss: 0.035389, val loss: 0.040173,  train_metric: 0.035 test_metric: 0.040 lr: 0.00123)\n",
            "epoch: 1399, train_loss: 0.035345, val loss: 0.040330,  train_metric: 0.035 test_metric: 0.040 lr: 0.00123)\n",
            "epoch: 1400, train_loss: 0.035592, val loss: 0.040302,  train_metric: 0.036 test_metric: 0.040 lr: 0.00123)\n",
            "epoch: 1401, train_loss: 0.035498, val loss: 0.040161,  train_metric: 0.035 test_metric: 0.040 lr: 0.00123)\n",
            "epoch: 1402, train_loss: 0.035918, val loss: 0.040493,  train_metric: 0.036 test_metric: 0.040 lr: 0.00123)\n",
            "epoch: 1403, train_loss: 0.035442, val loss: 0.040518,  train_metric: 0.035 test_metric: 0.041 lr: 0.00123)\n",
            "epoch: 1404, train_loss: 0.035419, val loss: 0.040534,  train_metric: 0.035 test_metric: 0.041 lr: 0.00123)\n",
            "epoch: 1405, train_loss: 0.035984, val loss: 0.040480,  train_metric: 0.036 test_metric: 0.040 lr: 0.00122)\n",
            "epoch: 1406, train_loss: 0.035656, val loss: 0.040755,  train_metric: 0.036 test_metric: 0.041 lr: 0.00122)\n",
            "epoch: 1407, train_loss: 0.036017, val loss: 0.040490,  train_metric: 0.036 test_metric: 0.040 lr: 0.00122)\n",
            "epoch: 1408, train_loss: 0.035838, val loss: 0.040231,  train_metric: 0.036 test_metric: 0.040 lr: 0.00122)\n",
            "epoch: 1409, train_loss: 0.035676, val loss: 0.040755,  train_metric: 0.036 test_metric: 0.041 lr: 0.00122)\n",
            "epoch: 1410, train_loss: 0.035530, val loss: 0.040271,  train_metric: 0.036 test_metric: 0.040 lr: 0.00122)\n",
            "epoch: 1411, train_loss: 0.035559, val loss: 0.040341,  train_metric: 0.036 test_metric: 0.040 lr: 0.00122)\n",
            "epoch: 1412, train_loss: 0.035544, val loss: 0.040756,  train_metric: 0.036 test_metric: 0.041 lr: 0.00122)\n",
            "epoch: 1413, train_loss: 0.035354, val loss: 0.040297,  train_metric: 0.035 test_metric: 0.040 lr: 0.00121)\n",
            "epoch: 1414, train_loss: 0.035489, val loss: 0.040478,  train_metric: 0.035 test_metric: 0.040 lr: 0.00121)\n",
            "epoch: 1415, train_loss: 0.035287, val loss: 0.040372,  train_metric: 0.035 test_metric: 0.040 lr: 0.00121)\n",
            "epoch: 1416, train_loss: 0.035363, val loss: 0.040235,  train_metric: 0.035 test_metric: 0.040 lr: 0.00121)\n",
            "epoch: 1417, train_loss: 0.035617, val loss: 0.040241,  train_metric: 0.036 test_metric: 0.040 lr: 0.00121)\n",
            "epoch: 1418, train_loss: 0.035615, val loss: 0.040797,  train_metric: 0.036 test_metric: 0.041 lr: 0.00121)\n",
            "epoch: 1419, train_loss: 0.036241, val loss: 0.040775,  train_metric: 0.036 test_metric: 0.041 lr: 0.00121)\n",
            "epoch: 1420, train_loss: 0.036032, val loss: 0.040396,  train_metric: 0.036 test_metric: 0.040 lr: 0.00121)\n",
            "epoch: 1421, train_loss: 0.035695, val loss: 0.041028,  train_metric: 0.036 test_metric: 0.041 lr: 0.00121)\n",
            "epoch: 1422, train_loss: 0.035471, val loss: 0.040300,  train_metric: 0.035 test_metric: 0.040 lr: 0.00120)\n",
            "epoch: 1423, train_loss: 0.035274, val loss: 0.040241,  train_metric: 0.035 test_metric: 0.040 lr: 0.00120)\n",
            "epoch: 1424, train_loss: 0.035612, val loss: 0.040526,  train_metric: 0.036 test_metric: 0.041 lr: 0.00120)\n",
            "epoch: 1425, train_loss: 0.035578, val loss: 0.040391,  train_metric: 0.036 test_metric: 0.040 lr: 0.00120)\n",
            "epoch: 1426, train_loss: 0.035460, val loss: 0.040398,  train_metric: 0.035 test_metric: 0.040 lr: 0.00120)\n",
            "epoch: 1427, train_loss: 0.035649, val loss: 0.041107,  train_metric: 0.036 test_metric: 0.041 lr: 0.00120)\n",
            "epoch: 1428, train_loss: 0.036011, val loss: 0.040522,  train_metric: 0.036 test_metric: 0.041 lr: 0.00120)\n",
            "epoch: 1429, train_loss: 0.035709, val loss: 0.040491,  train_metric: 0.036 test_metric: 0.040 lr: 0.00120)\n",
            "epoch: 1430, train_loss: 0.035593, val loss: 0.040883,  train_metric: 0.036 test_metric: 0.041 lr: 0.00119)\n",
            "epoch: 1431, train_loss: 0.035456, val loss: 0.040370,  train_metric: 0.035 test_metric: 0.040 lr: 0.00119)\n",
            "epoch: 1432, train_loss: 0.035403, val loss: 0.040403,  train_metric: 0.035 test_metric: 0.040 lr: 0.00119)\n",
            "epoch: 1433, train_loss: 0.035673, val loss: 0.040487,  train_metric: 0.036 test_metric: 0.040 lr: 0.00119)\n",
            "epoch: 1434, train_loss: 0.036013, val loss: 0.041148,  train_metric: 0.036 test_metric: 0.041 lr: 0.00119)\n",
            "epoch: 1435, train_loss: 0.035678, val loss: 0.040452,  train_metric: 0.036 test_metric: 0.040 lr: 0.00119)\n",
            "epoch: 1436, train_loss: 0.035453, val loss: 0.040457,  train_metric: 0.035 test_metric: 0.040 lr: 0.00119)\n",
            "epoch: 1437, train_loss: 0.036147, val loss: 0.040962,  train_metric: 0.036 test_metric: 0.041 lr: 0.00119)\n",
            "epoch: 1438, train_loss: 0.035914, val loss: 0.040636,  train_metric: 0.036 test_metric: 0.041 lr: 0.00118)\n",
            "epoch: 1439, train_loss: 0.035570, val loss: 0.040476,  train_metric: 0.036 test_metric: 0.040 lr: 0.00118)\n",
            "epoch: 1440, train_loss: 0.035640, val loss: 0.040942,  train_metric: 0.036 test_metric: 0.041 lr: 0.00118)\n",
            "epoch: 1441, train_loss: 0.036201, val loss: 0.040631,  train_metric: 0.036 test_metric: 0.041 lr: 0.00118)\n",
            "epoch: 1442, train_loss: 0.036053, val loss: 0.040968,  train_metric: 0.036 test_metric: 0.041 lr: 0.00118)\n",
            "epoch: 1443, train_loss: 0.036032, val loss: 0.041292,  train_metric: 0.036 test_metric: 0.041 lr: 0.00118)\n",
            "epoch: 1444, train_loss: 0.035814, val loss: 0.040659,  train_metric: 0.036 test_metric: 0.041 lr: 0.00118)\n",
            "epoch: 1445, train_loss: 0.035431, val loss: 0.040304,  train_metric: 0.035 test_metric: 0.040 lr: 0.00118)\n",
            "epoch: 1446, train_loss: 0.035296, val loss: 0.040322,  train_metric: 0.035 test_metric: 0.040 lr: 0.00118)\n",
            "epoch: 1447, train_loss: 0.035357, val loss: 0.040323,  train_metric: 0.035 test_metric: 0.040 lr: 0.00117)\n",
            "epoch: 1448, train_loss: 0.035263, val loss: 0.040342,  train_metric: 0.035 test_metric: 0.040 lr: 0.00117)\n",
            "epoch: 1449, train_loss: 0.035516, val loss: 0.040612,  train_metric: 0.036 test_metric: 0.041 lr: 0.00117)\n",
            "epoch: 1450, train_loss: 0.036217, val loss: 0.040860,  train_metric: 0.036 test_metric: 0.041 lr: 0.00117)\n",
            "epoch: 1451, train_loss: 0.035553, val loss: 0.041233,  train_metric: 0.036 test_metric: 0.041 lr: 0.00117)\n",
            "epoch: 1452, train_loss: 0.035791, val loss: 0.040682,  train_metric: 0.036 test_metric: 0.041 lr: 0.00117)\n",
            "epoch: 1453, train_loss: 0.035605, val loss: 0.040573,  train_metric: 0.036 test_metric: 0.041 lr: 0.00117)\n",
            "epoch: 1454, train_loss: 0.035599, val loss: 0.040551,  train_metric: 0.036 test_metric: 0.041 lr: 0.00117)\n",
            "epoch: 1455, train_loss: 0.035483, val loss: 0.041041,  train_metric: 0.035 test_metric: 0.041 lr: 0.00116)\n",
            "epoch: 1456, train_loss: 0.035715, val loss: 0.040821,  train_metric: 0.036 test_metric: 0.041 lr: 0.00116)\n",
            "epoch: 1457, train_loss: 0.035993, val loss: 0.040544,  train_metric: 0.036 test_metric: 0.041 lr: 0.00116)\n",
            "epoch: 1458, train_loss: 0.035526, val loss: 0.040728,  train_metric: 0.036 test_metric: 0.041 lr: 0.00116)\n",
            "epoch: 1459, train_loss: 0.035675, val loss: 0.040500,  train_metric: 0.036 test_metric: 0.041 lr: 0.00116)\n",
            "epoch: 1460, train_loss: 0.035574, val loss: 0.040561,  train_metric: 0.036 test_metric: 0.041 lr: 0.00116)\n",
            "epoch: 1461, train_loss: 0.035314, val loss: 0.040378,  train_metric: 0.035 test_metric: 0.040 lr: 0.00116)\n",
            "epoch: 1462, train_loss: 0.035569, val loss: 0.040397,  train_metric: 0.036 test_metric: 0.040 lr: 0.00116)\n",
            "epoch: 1463, train_loss: 0.035395, val loss: 0.040415,  train_metric: 0.035 test_metric: 0.040 lr: 0.00116)\n",
            "epoch: 1464, train_loss: 0.035377, val loss: 0.040141,  train_metric: 0.035 test_metric: 0.040 lr: 0.00115)\n",
            "epoch: 1465, train_loss: 0.035416, val loss: 0.040711,  train_metric: 0.035 test_metric: 0.041 lr: 0.00115)\n",
            "epoch: 1466, train_loss: 0.035212, val loss: 0.040203,  train_metric: 0.035 test_metric: 0.040 lr: 0.00115)\n",
            "epoch: 1467, train_loss: 0.035249, val loss: 0.040137,  train_metric: 0.035 test_metric: 0.040 lr: 0.00115)\n",
            "epoch: 1468, train_loss: 0.035291, val loss: 0.040311,  train_metric: 0.035 test_metric: 0.040 lr: 0.00115)\n",
            "epoch: 1469, train_loss: 0.035403, val loss: 0.040148,  train_metric: 0.035 test_metric: 0.040 lr: 0.00115)\n",
            "epoch: 1470, train_loss: 0.035634, val loss: 0.040306,  train_metric: 0.036 test_metric: 0.040 lr: 0.00115)\n",
            "epoch: 1471, train_loss: 0.035402, val loss: 0.040364,  train_metric: 0.035 test_metric: 0.040 lr: 0.00115)\n",
            "epoch: 1472, train_loss: 0.035255, val loss: 0.040276,  train_metric: 0.035 test_metric: 0.040 lr: 0.00115)\n",
            "epoch: 1473, train_loss: 0.035255, val loss: 0.040279,  train_metric: 0.035 test_metric: 0.040 lr: 0.00114)\n",
            "epoch: 1474, train_loss: 0.036017, val loss: 0.041356,  train_metric: 0.036 test_metric: 0.041 lr: 0.00114)\n",
            "epoch: 1475, train_loss: 0.036242, val loss: 0.040835,  train_metric: 0.036 test_metric: 0.041 lr: 0.00114)\n",
            "epoch: 1476, train_loss: 0.035895, val loss: 0.040752,  train_metric: 0.036 test_metric: 0.041 lr: 0.00114)\n",
            "epoch: 1477, train_loss: 0.037021, val loss: 0.041409,  train_metric: 0.037 test_metric: 0.041 lr: 0.00114)\n",
            "epoch: 1478, train_loss: 0.036111, val loss: 0.041055,  train_metric: 0.036 test_metric: 0.041 lr: 0.00114)\n",
            "epoch: 1479, train_loss: 0.035923, val loss: 0.041101,  train_metric: 0.036 test_metric: 0.041 lr: 0.00114)\n",
            "epoch: 1480, train_loss: 0.036367, val loss: 0.040504,  train_metric: 0.036 test_metric: 0.041 lr: 0.00114)\n",
            "epoch: 1481, train_loss: 0.036195, val loss: 0.040553,  train_metric: 0.036 test_metric: 0.041 lr: 0.00114)\n",
            "epoch: 1482, train_loss: 0.035529, val loss: 0.040416,  train_metric: 0.036 test_metric: 0.040 lr: 0.00113)\n",
            "epoch: 1483, train_loss: 0.035346, val loss: 0.040485,  train_metric: 0.035 test_metric: 0.040 lr: 0.00113)\n",
            "epoch: 1484, train_loss: 0.035311, val loss: 0.040391,  train_metric: 0.035 test_metric: 0.040 lr: 0.00113)\n",
            "epoch: 1485, train_loss: 0.036253, val loss: 0.040382,  train_metric: 0.036 test_metric: 0.040 lr: 0.00113)\n",
            "epoch: 1486, train_loss: 0.035738, val loss: 0.040626,  train_metric: 0.036 test_metric: 0.041 lr: 0.00113)\n",
            "epoch: 1487, train_loss: 0.035548, val loss: 0.040489,  train_metric: 0.036 test_metric: 0.040 lr: 0.00113)\n",
            "epoch: 1488, train_loss: 0.035615, val loss: 0.040638,  train_metric: 0.036 test_metric: 0.041 lr: 0.00113)\n",
            "epoch: 1489, train_loss: 0.035574, val loss: 0.040393,  train_metric: 0.036 test_metric: 0.040 lr: 0.00113)\n",
            "epoch: 1490, train_loss: 0.035724, val loss: 0.040568,  train_metric: 0.036 test_metric: 0.041 lr: 0.00112)\n",
            "epoch: 1491, train_loss: 0.036015, val loss: 0.040676,  train_metric: 0.036 test_metric: 0.041 lr: 0.00112)\n",
            "epoch: 1492, train_loss: 0.036214, val loss: 0.041577,  train_metric: 0.036 test_metric: 0.042 lr: 0.00112)\n",
            "epoch: 1493, train_loss: 0.036582, val loss: 0.041407,  train_metric: 0.037 test_metric: 0.041 lr: 0.00112)\n",
            "epoch: 1494, train_loss: 0.036831, val loss: 0.041597,  train_metric: 0.037 test_metric: 0.042 lr: 0.00112)\n",
            "epoch: 1495, train_loss: 0.036485, val loss: 0.041616,  train_metric: 0.036 test_metric: 0.042 lr: 0.00112)\n",
            "epoch: 1496, train_loss: 0.036341, val loss: 0.041015,  train_metric: 0.036 test_metric: 0.041 lr: 0.00112)\n",
            "epoch: 1497, train_loss: 0.036436, val loss: 0.041769,  train_metric: 0.036 test_metric: 0.042 lr: 0.00112)\n",
            "epoch: 1498, train_loss: 0.036409, val loss: 0.042855,  train_metric: 0.036 test_metric: 0.043 lr: 0.00112)\n",
            "epoch: 1499, train_loss: 0.036499, val loss: 0.042419,  train_metric: 0.036 test_metric: 0.042 lr: 0.00111)\n",
            "epoch: 1500, train_loss: 0.036306, val loss: 0.041054,  train_metric: 0.036 test_metric: 0.041 lr: 0.00111)\n",
            "epoch: 1501, train_loss: 0.036353, val loss: 0.040658,  train_metric: 0.036 test_metric: 0.041 lr: 0.00111)\n",
            "epoch: 1502, train_loss: 0.036505, val loss: 0.041483,  train_metric: 0.037 test_metric: 0.041 lr: 0.00111)\n",
            "epoch: 1503, train_loss: 0.035923, val loss: 0.041358,  train_metric: 0.036 test_metric: 0.041 lr: 0.00111)\n",
            "epoch: 1504, train_loss: 0.036369, val loss: 0.041028,  train_metric: 0.036 test_metric: 0.041 lr: 0.00111)\n",
            "epoch: 1505, train_loss: 0.036470, val loss: 0.040538,  train_metric: 0.036 test_metric: 0.041 lr: 0.00111)\n",
            "epoch: 1506, train_loss: 0.036094, val loss: 0.041042,  train_metric: 0.036 test_metric: 0.041 lr: 0.00111)\n",
            "epoch: 1507, train_loss: 0.036126, val loss: 0.041080,  train_metric: 0.036 test_metric: 0.041 lr: 0.00111)\n",
            "epoch: 1508, train_loss: 0.036098, val loss: 0.041067,  train_metric: 0.036 test_metric: 0.041 lr: 0.00110)\n",
            "epoch: 1509, train_loss: 0.035841, val loss: 0.040890,  train_metric: 0.036 test_metric: 0.041 lr: 0.00110)\n",
            "epoch: 1510, train_loss: 0.035796, val loss: 0.040532,  train_metric: 0.036 test_metric: 0.041 lr: 0.00110)\n",
            "epoch: 1511, train_loss: 0.036268, val loss: 0.040623,  train_metric: 0.036 test_metric: 0.041 lr: 0.00110)\n",
            "epoch: 1512, train_loss: 0.035842, val loss: 0.040557,  train_metric: 0.036 test_metric: 0.041 lr: 0.00110)\n",
            "epoch: 1513, train_loss: 0.035576, val loss: 0.040429,  train_metric: 0.036 test_metric: 0.040 lr: 0.00110)\n",
            "epoch: 1514, train_loss: 0.035746, val loss: 0.040565,  train_metric: 0.036 test_metric: 0.041 lr: 0.00110)\n",
            "epoch: 1515, train_loss: 0.035315, val loss: 0.040500,  train_metric: 0.035 test_metric: 0.041 lr: 0.00110)\n",
            "epoch: 1516, train_loss: 0.035486, val loss: 0.040163,  train_metric: 0.035 test_metric: 0.040 lr: 0.00110)\n",
            "epoch: 1517, train_loss: 0.035434, val loss: 0.040152,  train_metric: 0.035 test_metric: 0.040 lr: 0.00109)\n",
            "epoch: 1518, train_loss: 0.035425, val loss: 0.040174,  train_metric: 0.035 test_metric: 0.040 lr: 0.00109)\n",
            "epoch: 1519, train_loss: 0.035299, val loss: 0.040256,  train_metric: 0.035 test_metric: 0.040 lr: 0.00109)\n",
            "epoch: 1520, train_loss: 0.035278, val loss: 0.040383,  train_metric: 0.035 test_metric: 0.040 lr: 0.00109)\n",
            "epoch: 1521, train_loss: 0.035437, val loss: 0.040231,  train_metric: 0.035 test_metric: 0.040 lr: 0.00109)\n",
            "epoch: 1522, train_loss: 0.035562, val loss: 0.040241,  train_metric: 0.036 test_metric: 0.040 lr: 0.00109)\n",
            "epoch: 1523, train_loss: 0.035414, val loss: 0.040618,  train_metric: 0.035 test_metric: 0.041 lr: 0.00109)\n",
            "epoch: 1524, train_loss: 0.035518, val loss: 0.040511,  train_metric: 0.036 test_metric: 0.041 lr: 0.00109)\n",
            "epoch: 1525, train_loss: 0.035448, val loss: 0.040352,  train_metric: 0.035 test_metric: 0.040 lr: 0.00109)\n",
            "epoch: 1526, train_loss: 0.035487, val loss: 0.040385,  train_metric: 0.035 test_metric: 0.040 lr: 0.00109)\n",
            "epoch: 1527, train_loss: 0.035420, val loss: 0.040438,  train_metric: 0.035 test_metric: 0.040 lr: 0.00108)\n",
            "epoch: 1528, train_loss: 0.035591, val loss: 0.040522,  train_metric: 0.036 test_metric: 0.041 lr: 0.00108)\n",
            "epoch: 1529, train_loss: 0.035507, val loss: 0.040449,  train_metric: 0.036 test_metric: 0.040 lr: 0.00108)\n",
            "epoch: 1530, train_loss: 0.035658, val loss: 0.040705,  train_metric: 0.036 test_metric: 0.041 lr: 0.00108)\n",
            "epoch: 1531, train_loss: 0.035739, val loss: 0.040479,  train_metric: 0.036 test_metric: 0.040 lr: 0.00108)\n",
            "epoch: 1532, train_loss: 0.035372, val loss: 0.040271,  train_metric: 0.035 test_metric: 0.040 lr: 0.00108)\n",
            "epoch: 1533, train_loss: 0.035259, val loss: 0.040435,  train_metric: 0.035 test_metric: 0.040 lr: 0.00108)\n",
            "epoch: 1534, train_loss: 0.035481, val loss: 0.040447,  train_metric: 0.035 test_metric: 0.040 lr: 0.00108)\n",
            "epoch: 1535, train_loss: 0.035251, val loss: 0.040399,  train_metric: 0.035 test_metric: 0.040 lr: 0.00108)\n",
            "epoch: 1536, train_loss: 0.035425, val loss: 0.040303,  train_metric: 0.035 test_metric: 0.040 lr: 0.00107)\n",
            "epoch: 1537, train_loss: 0.035316, val loss: 0.040413,  train_metric: 0.035 test_metric: 0.040 lr: 0.00107)\n",
            "epoch: 1538, train_loss: 0.035371, val loss: 0.040161,  train_metric: 0.035 test_metric: 0.040 lr: 0.00107)\n",
            "epoch: 1539, train_loss: 0.035236, val loss: 0.040155,  train_metric: 0.035 test_metric: 0.040 lr: 0.00107)\n",
            "epoch: 1540, train_loss: 0.035285, val loss: 0.040133,  train_metric: 0.035 test_metric: 0.040 lr: 0.00107)\n",
            "epoch: 1541, train_loss: 0.035204, val loss: 0.040266,  train_metric: 0.035 test_metric: 0.040 lr: 0.00107)\n",
            "epoch: 1542, train_loss: 0.035308, val loss: 0.040365,  train_metric: 0.035 test_metric: 0.040 lr: 0.00107)\n",
            "epoch: 1543, train_loss: 0.035182, val loss: 0.040309,  train_metric: 0.035 test_metric: 0.040 lr: 0.00107)\n",
            "epoch: 1544, train_loss: 0.035178, val loss: 0.040209,  train_metric: 0.035 test_metric: 0.040 lr: 0.00107)\n",
            "epoch: 1545, train_loss: 0.035220, val loss: 0.040206,  train_metric: 0.035 test_metric: 0.040 lr: 0.00106)\n",
            "epoch: 1546, train_loss: 0.035176, val loss: 0.040200,  train_metric: 0.035 test_metric: 0.040 lr: 0.00106)\n",
            "epoch: 1547, train_loss: 0.035239, val loss: 0.040156,  train_metric: 0.035 test_metric: 0.040 lr: 0.00106)\n",
            "epoch: 1548, train_loss: 0.035402, val loss: 0.040195,  train_metric: 0.035 test_metric: 0.040 lr: 0.00106)\n",
            "epoch: 1549, train_loss: 0.035607, val loss: 0.040473,  train_metric: 0.036 test_metric: 0.040 lr: 0.00106)\n",
            "epoch: 1550, train_loss: 0.035498, val loss: 0.040271,  train_metric: 0.035 test_metric: 0.040 lr: 0.00106)\n",
            "epoch: 1551, train_loss: 0.035259, val loss: 0.040386,  train_metric: 0.035 test_metric: 0.040 lr: 0.00106)\n",
            "epoch: 1552, train_loss: 0.035199, val loss: 0.040209,  train_metric: 0.035 test_metric: 0.040 lr: 0.00106)\n",
            "epoch: 1553, train_loss: 0.035789, val loss: 0.040067,  train_metric: 0.036 test_metric: 0.040 lr: 0.00106)\n",
            "epoch: 1554, train_loss: 0.035462, val loss: 0.040694,  train_metric: 0.035 test_metric: 0.041 lr: 0.00106)\n",
            "epoch: 1555, train_loss: 0.035387, val loss: 0.040282,  train_metric: 0.035 test_metric: 0.040 lr: 0.00105)\n",
            "epoch: 1556, train_loss: 0.035284, val loss: 0.040528,  train_metric: 0.035 test_metric: 0.041 lr: 0.00105)\n",
            "epoch: 1557, train_loss: 0.035545, val loss: 0.040571,  train_metric: 0.036 test_metric: 0.041 lr: 0.00105)\n",
            "epoch: 1558, train_loss: 0.035466, val loss: 0.040257,  train_metric: 0.035 test_metric: 0.040 lr: 0.00105)\n",
            "epoch: 1559, train_loss: 0.035254, val loss: 0.040230,  train_metric: 0.035 test_metric: 0.040 lr: 0.00105)\n",
            "epoch: 1560, train_loss: 0.035350, val loss: 0.040422,  train_metric: 0.035 test_metric: 0.040 lr: 0.00105)\n",
            "epoch: 1561, train_loss: 0.035410, val loss: 0.040367,  train_metric: 0.035 test_metric: 0.040 lr: 0.00105)\n",
            "epoch: 1562, train_loss: 0.035216, val loss: 0.040253,  train_metric: 0.035 test_metric: 0.040 lr: 0.00105)\n",
            "epoch: 1563, train_loss: 0.035167, val loss: 0.040156,  train_metric: 0.035 test_metric: 0.040 lr: 0.00105)\n",
            "epoch: 1564, train_loss: 0.035231, val loss: 0.040107,  train_metric: 0.035 test_metric: 0.040 lr: 0.00104)\n",
            "epoch: 1565, train_loss: 0.035342, val loss: 0.040159,  train_metric: 0.035 test_metric: 0.040 lr: 0.00104)\n",
            "epoch: 1566, train_loss: 0.035149, val loss: 0.040149,  train_metric: 0.035 test_metric: 0.040 lr: 0.00104)\n",
            "epoch: 1567, train_loss: 0.035240, val loss: 0.040285,  train_metric: 0.035 test_metric: 0.040 lr: 0.00104)\n",
            "epoch: 1568, train_loss: 0.035254, val loss: 0.040331,  train_metric: 0.035 test_metric: 0.040 lr: 0.00104)\n",
            "epoch: 1569, train_loss: 0.035482, val loss: 0.040344,  train_metric: 0.035 test_metric: 0.040 lr: 0.00104)\n",
            "epoch: 1570, train_loss: 0.035436, val loss: 0.040339,  train_metric: 0.035 test_metric: 0.040 lr: 0.00104)\n",
            "epoch: 1571, train_loss: 0.035491, val loss: 0.040383,  train_metric: 0.035 test_metric: 0.040 lr: 0.00104)\n",
            "epoch: 1572, train_loss: 0.035207, val loss: 0.040326,  train_metric: 0.035 test_metric: 0.040 lr: 0.00104)\n",
            "epoch: 1573, train_loss: 0.035333, val loss: 0.040408,  train_metric: 0.035 test_metric: 0.040 lr: 0.00104)\n",
            "epoch: 1574, train_loss: 0.035434, val loss: 0.040481,  train_metric: 0.035 test_metric: 0.040 lr: 0.00103)\n",
            "epoch: 1575, train_loss: 0.035282, val loss: 0.040221,  train_metric: 0.035 test_metric: 0.040 lr: 0.00103)\n",
            "epoch: 1576, train_loss: 0.035134, val loss: 0.040252,  train_metric: 0.035 test_metric: 0.040 lr: 0.00103)\n",
            "epoch: 1577, train_loss: 0.035208, val loss: 0.040104,  train_metric: 0.035 test_metric: 0.040 lr: 0.00103)\n",
            "epoch: 1578, train_loss: 0.035300, val loss: 0.040154,  train_metric: 0.035 test_metric: 0.040 lr: 0.00103)\n",
            "epoch: 1579, train_loss: 0.035439, val loss: 0.040329,  train_metric: 0.035 test_metric: 0.040 lr: 0.00103)\n",
            "epoch: 1580, train_loss: 0.036360, val loss: 0.040875,  train_metric: 0.036 test_metric: 0.041 lr: 0.00103)\n",
            "epoch: 1581, train_loss: 0.035738, val loss: 0.040333,  train_metric: 0.036 test_metric: 0.040 lr: 0.00103)\n",
            "epoch: 1582, train_loss: 0.035282, val loss: 0.040479,  train_metric: 0.035 test_metric: 0.040 lr: 0.00103)\n",
            "epoch: 1583, train_loss: 0.035542, val loss: 0.040157,  train_metric: 0.036 test_metric: 0.040 lr: 0.00102)\n",
            "epoch: 1584, train_loss: 0.035275, val loss: 0.040182,  train_metric: 0.035 test_metric: 0.040 lr: 0.00102)\n",
            "epoch: 1585, train_loss: 0.035194, val loss: 0.040267,  train_metric: 0.035 test_metric: 0.040 lr: 0.00102)\n",
            "epoch: 1586, train_loss: 0.035258, val loss: 0.040145,  train_metric: 0.035 test_metric: 0.040 lr: 0.00102)\n",
            "epoch: 1587, train_loss: 0.035651, val loss: 0.040083,  train_metric: 0.036 test_metric: 0.040 lr: 0.00102)\n",
            "epoch: 1588, train_loss: 0.035149, val loss: 0.040188,  train_metric: 0.035 test_metric: 0.040 lr: 0.00102)\n",
            "epoch: 1589, train_loss: 0.035592, val loss: 0.040253,  train_metric: 0.036 test_metric: 0.040 lr: 0.00102)\n",
            "epoch: 1590, train_loss: 0.035242, val loss: 0.040150,  train_metric: 0.035 test_metric: 0.040 lr: 0.00102)\n",
            "epoch: 1591, train_loss: 0.035261, val loss: 0.040306,  train_metric: 0.035 test_metric: 0.040 lr: 0.00102)\n",
            "epoch: 1592, train_loss: 0.035222, val loss: 0.040056,  train_metric: 0.035 test_metric: 0.040 lr: 0.00102)\n",
            "epoch: 1593, train_loss: 0.035632, val loss: 0.040376,  train_metric: 0.036 test_metric: 0.040 lr: 0.00101)\n",
            "epoch: 1594, train_loss: 0.035271, val loss: 0.040228,  train_metric: 0.035 test_metric: 0.040 lr: 0.00101)\n",
            "epoch: 1595, train_loss: 0.035193, val loss: 0.040192,  train_metric: 0.035 test_metric: 0.040 lr: 0.00101)\n",
            "epoch: 1596, train_loss: 0.035272, val loss: 0.040325,  train_metric: 0.035 test_metric: 0.040 lr: 0.00101)\n",
            "epoch: 1597, train_loss: 0.035332, val loss: 0.040164,  train_metric: 0.035 test_metric: 0.040 lr: 0.00101)\n",
            "epoch: 1598, train_loss: 0.035386, val loss: 0.040040,  train_metric: 0.035 test_metric: 0.040 lr: 0.00101)\n",
            "epoch: 1599, train_loss: 0.035604, val loss: 0.040532,  train_metric: 0.036 test_metric: 0.041 lr: 0.00101)\n",
            "epoch: 1600, train_loss: 0.035333, val loss: 0.040383,  train_metric: 0.035 test_metric: 0.040 lr: 0.00101)\n",
            "epoch: 1601, train_loss: 0.035576, val loss: 0.040530,  train_metric: 0.036 test_metric: 0.041 lr: 0.00101)\n",
            "epoch: 1602, train_loss: 0.035529, val loss: 0.040430,  train_metric: 0.036 test_metric: 0.040 lr: 0.00101)\n",
            "epoch: 1603, train_loss: 0.035719, val loss: 0.040479,  train_metric: 0.036 test_metric: 0.040 lr: 0.00100)\n",
            "epoch: 1604, train_loss: 0.035360, val loss: 0.040106,  train_metric: 0.035 test_metric: 0.040 lr: 0.00100)\n",
            "epoch: 1605, train_loss: 0.035264, val loss: 0.040754,  train_metric: 0.035 test_metric: 0.041 lr: 0.00100)\n",
            "epoch: 1606, train_loss: 0.035374, val loss: 0.040399,  train_metric: 0.035 test_metric: 0.040 lr: 0.00100)\n",
            "epoch: 1607, train_loss: 0.035424, val loss: 0.040265,  train_metric: 0.035 test_metric: 0.040 lr: 0.00100)\n",
            "epoch: 1608, train_loss: 0.035389, val loss: 0.040306,  train_metric: 0.035 test_metric: 0.040 lr: 0.00100)\n",
            "epoch: 1609, train_loss: 0.035328, val loss: 0.040345,  train_metric: 0.035 test_metric: 0.040 lr: 0.00100)\n",
            "epoch: 1610, train_loss: 0.035353, val loss: 0.040403,  train_metric: 0.035 test_metric: 0.040 lr: 0.00100)\n",
            "epoch: 1611, train_loss: 0.035378, val loss: 0.040320,  train_metric: 0.035 test_metric: 0.040 lr: 0.00100)\n",
            "epoch: 1612, train_loss: 0.035654, val loss: 0.040522,  train_metric: 0.036 test_metric: 0.041 lr: 0.00100)\n",
            "epoch: 1613, train_loss: 0.035532, val loss: 0.040145,  train_metric: 0.036 test_metric: 0.040 lr: 0.00099)\n",
            "epoch: 1614, train_loss: 0.035418, val loss: 0.040286,  train_metric: 0.035 test_metric: 0.040 lr: 0.00099)\n",
            "epoch: 1615, train_loss: 0.035311, val loss: 0.040072,  train_metric: 0.035 test_metric: 0.040 lr: 0.00099)\n",
            "epoch: 1616, train_loss: 0.035354, val loss: 0.040327,  train_metric: 0.035 test_metric: 0.040 lr: 0.00099)\n",
            "epoch: 1617, train_loss: 0.035235, val loss: 0.040551,  train_metric: 0.035 test_metric: 0.041 lr: 0.00099)\n",
            "epoch: 1618, train_loss: 0.035467, val loss: 0.040555,  train_metric: 0.035 test_metric: 0.041 lr: 0.00099)\n",
            "epoch: 1619, train_loss: 0.035411, val loss: 0.040376,  train_metric: 0.035 test_metric: 0.040 lr: 0.00099)\n",
            "epoch: 1620, train_loss: 0.035683, val loss: 0.040224,  train_metric: 0.036 test_metric: 0.040 lr: 0.00099)\n",
            "epoch: 1621, train_loss: 0.035388, val loss: 0.040199,  train_metric: 0.035 test_metric: 0.040 lr: 0.00099)\n",
            "epoch: 1622, train_loss: 0.035193, val loss: 0.040169,  train_metric: 0.035 test_metric: 0.040 lr: 0.00099)\n",
            "epoch: 1623, train_loss: 0.035507, val loss: 0.040294,  train_metric: 0.036 test_metric: 0.040 lr: 0.00098)\n",
            "epoch: 1624, train_loss: 0.035155, val loss: 0.040181,  train_metric: 0.035 test_metric: 0.040 lr: 0.00098)\n",
            "epoch: 1625, train_loss: 0.035402, val loss: 0.040292,  train_metric: 0.035 test_metric: 0.040 lr: 0.00098)\n",
            "epoch: 1626, train_loss: 0.035350, val loss: 0.040181,  train_metric: 0.035 test_metric: 0.040 lr: 0.00098)\n",
            "epoch: 1627, train_loss: 0.035170, val loss: 0.040178,  train_metric: 0.035 test_metric: 0.040 lr: 0.00098)\n",
            "epoch: 1628, train_loss: 0.035196, val loss: 0.040215,  train_metric: 0.035 test_metric: 0.040 lr: 0.00098)\n",
            "epoch: 1629, train_loss: 0.035588, val loss: 0.040372,  train_metric: 0.036 test_metric: 0.040 lr: 0.00098)\n",
            "epoch: 1630, train_loss: 0.035739, val loss: 0.040432,  train_metric: 0.036 test_metric: 0.040 lr: 0.00098)\n",
            "epoch: 1631, train_loss: 0.035666, val loss: 0.040816,  train_metric: 0.036 test_metric: 0.041 lr: 0.00098)\n",
            "epoch: 1632, train_loss: 0.035403, val loss: 0.040467,  train_metric: 0.035 test_metric: 0.040 lr: 0.00098)\n",
            "epoch: 1633, train_loss: 0.035457, val loss: 0.040300,  train_metric: 0.035 test_metric: 0.040 lr: 0.00097)\n",
            "epoch: 1634, train_loss: 0.035693, val loss: 0.040341,  train_metric: 0.036 test_metric: 0.040 lr: 0.00097)\n",
            "epoch: 1635, train_loss: 0.035460, val loss: 0.040314,  train_metric: 0.035 test_metric: 0.040 lr: 0.00097)\n",
            "epoch: 1636, train_loss: 0.035317, val loss: 0.040215,  train_metric: 0.035 test_metric: 0.040 lr: 0.00097)\n",
            "epoch: 1637, train_loss: 0.035191, val loss: 0.040256,  train_metric: 0.035 test_metric: 0.040 lr: 0.00097)\n",
            "epoch: 1638, train_loss: 0.035307, val loss: 0.040220,  train_metric: 0.035 test_metric: 0.040 lr: 0.00097)\n",
            "epoch: 1639, train_loss: 0.035445, val loss: 0.040090,  train_metric: 0.035 test_metric: 0.040 lr: 0.00097)\n",
            "epoch: 1640, train_loss: 0.035576, val loss: 0.040499,  train_metric: 0.036 test_metric: 0.040 lr: 0.00097)\n",
            "epoch: 1641, train_loss: 0.035216, val loss: 0.040194,  train_metric: 0.035 test_metric: 0.040 lr: 0.00097)\n",
            "epoch: 1642, train_loss: 0.035158, val loss: 0.040289,  train_metric: 0.035 test_metric: 0.040 lr: 0.00097)\n",
            "epoch: 1643, train_loss: 0.035154, val loss: 0.040325,  train_metric: 0.035 test_metric: 0.040 lr: 0.00097)\n",
            "epoch: 1644, train_loss: 0.035175, val loss: 0.040155,  train_metric: 0.035 test_metric: 0.040 lr: 0.00096)\n",
            "epoch: 1645, train_loss: 0.035308, val loss: 0.040223,  train_metric: 0.035 test_metric: 0.040 lr: 0.00096)\n",
            "epoch: 1646, train_loss: 0.035498, val loss: 0.040112,  train_metric: 0.035 test_metric: 0.040 lr: 0.00096)\n",
            "epoch: 1647, train_loss: 0.035318, val loss: 0.040353,  train_metric: 0.035 test_metric: 0.040 lr: 0.00096)\n",
            "epoch: 1648, train_loss: 0.035083, val loss: 0.040176,  train_metric: 0.035 test_metric: 0.040 lr: 0.00096)\n",
            "epoch: 1649, train_loss: 0.035252, val loss: 0.040219,  train_metric: 0.035 test_metric: 0.040 lr: 0.00096)\n",
            "epoch: 1650, train_loss: 0.035204, val loss: 0.040214,  train_metric: 0.035 test_metric: 0.040 lr: 0.00096)\n",
            "epoch: 1651, train_loss: 0.035381, val loss: 0.040080,  train_metric: 0.035 test_metric: 0.040 lr: 0.00096)\n",
            "epoch: 1652, train_loss: 0.035221, val loss: 0.040208,  train_metric: 0.035 test_metric: 0.040 lr: 0.00096)\n",
            "epoch: 1653, train_loss: 0.035448, val loss: 0.040333,  train_metric: 0.035 test_metric: 0.040 lr: 0.00096)\n",
            "epoch: 1654, train_loss: 0.035575, val loss: 0.040124,  train_metric: 0.036 test_metric: 0.040 lr: 0.00095)\n",
            "epoch: 1655, train_loss: 0.035338, val loss: 0.040247,  train_metric: 0.035 test_metric: 0.040 lr: 0.00095)\n",
            "epoch: 1656, train_loss: 0.035157, val loss: 0.040178,  train_metric: 0.035 test_metric: 0.040 lr: 0.00095)\n",
            "epoch: 1657, train_loss: 0.035213, val loss: 0.040282,  train_metric: 0.035 test_metric: 0.040 lr: 0.00095)\n",
            "epoch: 1658, train_loss: 0.035361, val loss: 0.040390,  train_metric: 0.035 test_metric: 0.040 lr: 0.00095)\n",
            "epoch: 1659, train_loss: 0.035324, val loss: 0.040447,  train_metric: 0.035 test_metric: 0.040 lr: 0.00095)\n",
            "epoch: 1660, train_loss: 0.035487, val loss: 0.040425,  train_metric: 0.035 test_metric: 0.040 lr: 0.00095)\n",
            "epoch: 1661, train_loss: 0.035559, val loss: 0.040400,  train_metric: 0.036 test_metric: 0.040 lr: 0.00095)\n",
            "epoch: 1662, train_loss: 0.035945, val loss: 0.040398,  train_metric: 0.036 test_metric: 0.040 lr: 0.00095)\n",
            "epoch: 1663, train_loss: 0.035484, val loss: 0.040209,  train_metric: 0.035 test_metric: 0.040 lr: 0.00095)\n",
            "epoch: 1664, train_loss: 0.035317, val loss: 0.040371,  train_metric: 0.035 test_metric: 0.040 lr: 0.00095)\n",
            "epoch: 1665, train_loss: 0.035339, val loss: 0.040167,  train_metric: 0.035 test_metric: 0.040 lr: 0.00094)\n",
            "epoch: 1666, train_loss: 0.035194, val loss: 0.040248,  train_metric: 0.035 test_metric: 0.040 lr: 0.00094)\n",
            "epoch: 1667, train_loss: 0.035439, val loss: 0.040308,  train_metric: 0.035 test_metric: 0.040 lr: 0.00094)\n",
            "epoch: 1668, train_loss: 0.035810, val loss: 0.040358,  train_metric: 0.036 test_metric: 0.040 lr: 0.00094)\n",
            "epoch: 1669, train_loss: 0.035347, val loss: 0.040529,  train_metric: 0.035 test_metric: 0.041 lr: 0.00094)\n",
            "epoch: 1670, train_loss: 0.035425, val loss: 0.040157,  train_metric: 0.035 test_metric: 0.040 lr: 0.00094)\n",
            "epoch: 1671, train_loss: 0.035294, val loss: 0.040342,  train_metric: 0.035 test_metric: 0.040 lr: 0.00094)\n",
            "epoch: 1672, train_loss: 0.035332, val loss: 0.040252,  train_metric: 0.035 test_metric: 0.040 lr: 0.00094)\n",
            "epoch: 1673, train_loss: 0.035420, val loss: 0.040279,  train_metric: 0.035 test_metric: 0.040 lr: 0.00094)\n",
            "epoch: 1674, train_loss: 0.035180, val loss: 0.040114,  train_metric: 0.035 test_metric: 0.040 lr: 0.00094)\n",
            "epoch: 1675, train_loss: 0.035555, val loss: 0.040398,  train_metric: 0.036 test_metric: 0.040 lr: 0.00093)\n",
            "epoch: 1676, train_loss: 0.035246, val loss: 0.040164,  train_metric: 0.035 test_metric: 0.040 lr: 0.00093)\n",
            "epoch: 1677, train_loss: 0.035181, val loss: 0.040094,  train_metric: 0.035 test_metric: 0.040 lr: 0.00093)\n",
            "epoch: 1678, train_loss: 0.035570, val loss: 0.040400,  train_metric: 0.036 test_metric: 0.040 lr: 0.00093)\n",
            "epoch: 1679, train_loss: 0.035070, val loss: 0.040218,  train_metric: 0.035 test_metric: 0.040 lr: 0.00093)\n",
            "epoch: 1680, train_loss: 0.035618, val loss: 0.040215,  train_metric: 0.036 test_metric: 0.040 lr: 0.00093)\n",
            "epoch: 1681, train_loss: 0.035316, val loss: 0.040493,  train_metric: 0.035 test_metric: 0.040 lr: 0.00093)\n",
            "epoch: 1682, train_loss: 0.035325, val loss: 0.040177,  train_metric: 0.035 test_metric: 0.040 lr: 0.00093)\n",
            "epoch: 1683, train_loss: 0.035241, val loss: 0.040174,  train_metric: 0.035 test_metric: 0.040 lr: 0.00093)\n",
            "epoch: 1684, train_loss: 0.035097, val loss: 0.040196,  train_metric: 0.035 test_metric: 0.040 lr: 0.00093)\n",
            "epoch: 1685, train_loss: 0.035233, val loss: 0.040165,  train_metric: 0.035 test_metric: 0.040 lr: 0.00093)\n",
            "epoch: 1686, train_loss: 0.035175, val loss: 0.040241,  train_metric: 0.035 test_metric: 0.040 lr: 0.00092)\n",
            "epoch: 1687, train_loss: 0.035134, val loss: 0.040204,  train_metric: 0.035 test_metric: 0.040 lr: 0.00092)\n",
            "epoch: 1688, train_loss: 0.035543, val loss: 0.040324,  train_metric: 0.036 test_metric: 0.040 lr: 0.00092)\n",
            "epoch: 1689, train_loss: 0.035384, val loss: 0.040261,  train_metric: 0.035 test_metric: 0.040 lr: 0.00092)\n",
            "epoch: 1690, train_loss: 0.035471, val loss: 0.040375,  train_metric: 0.035 test_metric: 0.040 lr: 0.00092)\n",
            "epoch: 1691, train_loss: 0.035344, val loss: 0.040287,  train_metric: 0.035 test_metric: 0.040 lr: 0.00092)\n",
            "epoch: 1692, train_loss: 0.036073, val loss: 0.040238,  train_metric: 0.036 test_metric: 0.040 lr: 0.00092)\n",
            "epoch: 1693, train_loss: 0.035290, val loss: 0.040375,  train_metric: 0.035 test_metric: 0.040 lr: 0.00092)\n",
            "epoch: 1694, train_loss: 0.035135, val loss: 0.040123,  train_metric: 0.035 test_metric: 0.040 lr: 0.00092)\n",
            "epoch: 1695, train_loss: 0.035179, val loss: 0.040169,  train_metric: 0.035 test_metric: 0.040 lr: 0.00092)\n",
            "epoch: 1696, train_loss: 0.035693, val loss: 0.040638,  train_metric: 0.036 test_metric: 0.041 lr: 0.00092)\n",
            "epoch: 1697, train_loss: 0.035404, val loss: 0.040079,  train_metric: 0.035 test_metric: 0.040 lr: 0.00091)\n",
            "epoch: 1698, train_loss: 0.035260, val loss: 0.040224,  train_metric: 0.035 test_metric: 0.040 lr: 0.00091)\n",
            "epoch: 1699, train_loss: 0.035528, val loss: 0.040373,  train_metric: 0.036 test_metric: 0.040 lr: 0.00091)\n",
            "epoch: 1700, train_loss: 0.035652, val loss: 0.040069,  train_metric: 0.036 test_metric: 0.040 lr: 0.00091)\n",
            "epoch: 1701, train_loss: 0.035271, val loss: 0.040320,  train_metric: 0.035 test_metric: 0.040 lr: 0.00091)\n",
            "epoch: 1702, train_loss: 0.035145, val loss: 0.040151,  train_metric: 0.035 test_metric: 0.040 lr: 0.00091)\n",
            "epoch: 1703, train_loss: 0.035077, val loss: 0.040115,  train_metric: 0.035 test_metric: 0.040 lr: 0.00091)\n",
            "epoch: 1704, train_loss: 0.035532, val loss: 0.040469,  train_metric: 0.036 test_metric: 0.040 lr: 0.00091)\n",
            "epoch: 1705, train_loss: 0.035247, val loss: 0.040123,  train_metric: 0.035 test_metric: 0.040 lr: 0.00091)\n",
            "epoch: 1706, train_loss: 0.035266, val loss: 0.040160,  train_metric: 0.035 test_metric: 0.040 lr: 0.00091)\n",
            "epoch: 1707, train_loss: 0.035207, val loss: 0.040293,  train_metric: 0.035 test_metric: 0.040 lr: 0.00091)\n",
            "epoch: 1708, train_loss: 0.035677, val loss: 0.040262,  train_metric: 0.036 test_metric: 0.040 lr: 0.00090)\n",
            "epoch: 1709, train_loss: 0.035375, val loss: 0.040256,  train_metric: 0.035 test_metric: 0.040 lr: 0.00090)\n",
            "epoch: 1710, train_loss: 0.035321, val loss: 0.040224,  train_metric: 0.035 test_metric: 0.040 lr: 0.00090)\n",
            "epoch: 1711, train_loss: 0.035228, val loss: 0.040144,  train_metric: 0.035 test_metric: 0.040 lr: 0.00090)\n",
            "epoch: 1712, train_loss: 0.035344, val loss: 0.040344,  train_metric: 0.035 test_metric: 0.040 lr: 0.00090)\n",
            "epoch: 1713, train_loss: 0.035430, val loss: 0.040190,  train_metric: 0.035 test_metric: 0.040 lr: 0.00090)\n",
            "epoch: 1714, train_loss: 0.035374, val loss: 0.040285,  train_metric: 0.035 test_metric: 0.040 lr: 0.00090)\n",
            "epoch: 1715, train_loss: 0.035117, val loss: 0.040169,  train_metric: 0.035 test_metric: 0.040 lr: 0.00090)\n",
            "epoch: 1716, train_loss: 0.035535, val loss: 0.040386,  train_metric: 0.036 test_metric: 0.040 lr: 0.00090)\n",
            "epoch: 1717, train_loss: 0.035289, val loss: 0.040353,  train_metric: 0.035 test_metric: 0.040 lr: 0.00090)\n",
            "epoch: 1718, train_loss: 0.035289, val loss: 0.040305,  train_metric: 0.035 test_metric: 0.040 lr: 0.00090)\n",
            "epoch: 1719, train_loss: 0.035304, val loss: 0.040304,  train_metric: 0.035 test_metric: 0.040 lr: 0.00089)\n",
            "epoch: 1720, train_loss: 0.035528, val loss: 0.040211,  train_metric: 0.036 test_metric: 0.040 lr: 0.00089)\n",
            "epoch: 1721, train_loss: 0.035470, val loss: 0.040540,  train_metric: 0.035 test_metric: 0.041 lr: 0.00089)\n",
            "epoch: 1722, train_loss: 0.035581, val loss: 0.040522,  train_metric: 0.036 test_metric: 0.041 lr: 0.00089)\n",
            "epoch: 1723, train_loss: 0.035372, val loss: 0.040104,  train_metric: 0.035 test_metric: 0.040 lr: 0.00089)\n",
            "epoch: 1724, train_loss: 0.035140, val loss: 0.040384,  train_metric: 0.035 test_metric: 0.040 lr: 0.00089)\n",
            "epoch: 1725, train_loss: 0.035396, val loss: 0.040185,  train_metric: 0.035 test_metric: 0.040 lr: 0.00089)\n",
            "epoch: 1726, train_loss: 0.035399, val loss: 0.040162,  train_metric: 0.035 test_metric: 0.040 lr: 0.00089)\n",
            "epoch: 1727, train_loss: 0.035311, val loss: 0.040186,  train_metric: 0.035 test_metric: 0.040 lr: 0.00089)\n",
            "epoch: 1728, train_loss: 0.035326, val loss: 0.040502,  train_metric: 0.035 test_metric: 0.041 lr: 0.00089)\n",
            "epoch: 1729, train_loss: 0.035389, val loss: 0.040247,  train_metric: 0.035 test_metric: 0.040 lr: 0.00089)\n",
            "epoch: 1730, train_loss: 0.035258, val loss: 0.040672,  train_metric: 0.035 test_metric: 0.041 lr: 0.00088)\n",
            "epoch: 1731, train_loss: 0.035360, val loss: 0.040578,  train_metric: 0.035 test_metric: 0.041 lr: 0.00088)\n",
            "epoch: 1732, train_loss: 0.035544, val loss: 0.040588,  train_metric: 0.036 test_metric: 0.041 lr: 0.00088)\n",
            "epoch: 1733, train_loss: 0.035401, val loss: 0.040334,  train_metric: 0.035 test_metric: 0.040 lr: 0.00088)\n",
            "epoch: 1734, train_loss: 0.035321, val loss: 0.040269,  train_metric: 0.035 test_metric: 0.040 lr: 0.00088)\n",
            "epoch: 1735, train_loss: 0.035060, val loss: 0.040068,  train_metric: 0.035 test_metric: 0.040 lr: 0.00088)\n",
            "epoch: 1736, train_loss: 0.035161, val loss: 0.040067,  train_metric: 0.035 test_metric: 0.040 lr: 0.00088)\n",
            "epoch: 1737, train_loss: 0.035653, val loss: 0.040260,  train_metric: 0.036 test_metric: 0.040 lr: 0.00088)\n",
            "epoch: 1738, train_loss: 0.035398, val loss: 0.040058,  train_metric: 0.035 test_metric: 0.040 lr: 0.00088)\n",
            "epoch: 1739, train_loss: 0.035613, val loss: 0.040567,  train_metric: 0.036 test_metric: 0.041 lr: 0.00088)\n",
            "epoch: 1740, train_loss: 0.035500, val loss: 0.040188,  train_metric: 0.035 test_metric: 0.040 lr: 0.00088)\n",
            "epoch: 1741, train_loss: 0.035424, val loss: 0.040575,  train_metric: 0.035 test_metric: 0.041 lr: 0.00088)\n",
            "epoch: 1742, train_loss: 0.035268, val loss: 0.040446,  train_metric: 0.035 test_metric: 0.040 lr: 0.00087)\n",
            "epoch: 1743, train_loss: 0.035182, val loss: 0.040402,  train_metric: 0.035 test_metric: 0.040 lr: 0.00087)\n",
            "epoch: 1744, train_loss: 0.035298, val loss: 0.040098,  train_metric: 0.035 test_metric: 0.040 lr: 0.00087)\n",
            "epoch: 1745, train_loss: 0.035199, val loss: 0.040351,  train_metric: 0.035 test_metric: 0.040 lr: 0.00087)\n",
            "epoch: 1746, train_loss: 0.035387, val loss: 0.040175,  train_metric: 0.035 test_metric: 0.040 lr: 0.00087)\n",
            "epoch: 1747, train_loss: 0.035303, val loss: 0.040106,  train_metric: 0.035 test_metric: 0.040 lr: 0.00087)\n",
            "epoch: 1748, train_loss: 0.035190, val loss: 0.040182,  train_metric: 0.035 test_metric: 0.040 lr: 0.00087)\n",
            "epoch: 1749, train_loss: 0.035318, val loss: 0.040537,  train_metric: 0.035 test_metric: 0.041 lr: 0.00087)\n",
            "epoch: 1750, train_loss: 0.035587, val loss: 0.040663,  train_metric: 0.036 test_metric: 0.041 lr: 0.00087)\n",
            "epoch: 1751, train_loss: 0.035578, val loss: 0.040259,  train_metric: 0.036 test_metric: 0.040 lr: 0.00087)\n",
            "epoch: 1752, train_loss: 0.035530, val loss: 0.040644,  train_metric: 0.036 test_metric: 0.041 lr: 0.00087)\n",
            "epoch: 1753, train_loss: 0.035594, val loss: 0.040240,  train_metric: 0.036 test_metric: 0.040 lr: 0.00086)\n",
            "epoch: 1754, train_loss: 0.035536, val loss: 0.040412,  train_metric: 0.036 test_metric: 0.040 lr: 0.00086)\n",
            "epoch: 1755, train_loss: 0.035833, val loss: 0.040840,  train_metric: 0.036 test_metric: 0.041 lr: 0.00086)\n",
            "epoch: 1756, train_loss: 0.035592, val loss: 0.040190,  train_metric: 0.036 test_metric: 0.040 lr: 0.00086)\n",
            "epoch: 1757, train_loss: 0.035387, val loss: 0.040358,  train_metric: 0.035 test_metric: 0.040 lr: 0.00086)\n",
            "epoch: 1758, train_loss: 0.035247, val loss: 0.040378,  train_metric: 0.035 test_metric: 0.040 lr: 0.00086)\n",
            "epoch: 1759, train_loss: 0.035350, val loss: 0.040263,  train_metric: 0.035 test_metric: 0.040 lr: 0.00086)\n",
            "epoch: 1760, train_loss: 0.035379, val loss: 0.040298,  train_metric: 0.035 test_metric: 0.040 lr: 0.00086)\n",
            "epoch: 1761, train_loss: 0.035499, val loss: 0.040366,  train_metric: 0.035 test_metric: 0.040 lr: 0.00086)\n",
            "epoch: 1762, train_loss: 0.035369, val loss: 0.040093,  train_metric: 0.035 test_metric: 0.040 lr: 0.00086)\n",
            "epoch: 1763, train_loss: 0.035244, val loss: 0.040288,  train_metric: 0.035 test_metric: 0.040 lr: 0.00086)\n",
            "epoch: 1764, train_loss: 0.035185, val loss: 0.040174,  train_metric: 0.035 test_metric: 0.040 lr: 0.00086)\n",
            "epoch: 1765, train_loss: 0.035250, val loss: 0.040310,  train_metric: 0.035 test_metric: 0.040 lr: 0.00085)\n",
            "epoch: 1766, train_loss: 0.035619, val loss: 0.040723,  train_metric: 0.036 test_metric: 0.041 lr: 0.00085)\n",
            "epoch: 1767, train_loss: 0.035771, val loss: 0.040461,  train_metric: 0.036 test_metric: 0.040 lr: 0.00085)\n",
            "epoch: 1768, train_loss: 0.035408, val loss: 0.040473,  train_metric: 0.035 test_metric: 0.040 lr: 0.00085)\n",
            "epoch: 1769, train_loss: 0.035257, val loss: 0.040289,  train_metric: 0.035 test_metric: 0.040 lr: 0.00085)\n",
            "epoch: 1770, train_loss: 0.035119, val loss: 0.040120,  train_metric: 0.035 test_metric: 0.040 lr: 0.00085)\n",
            "epoch: 1771, train_loss: 0.035340, val loss: 0.040100,  train_metric: 0.035 test_metric: 0.040 lr: 0.00085)\n",
            "epoch: 1772, train_loss: 0.035076, val loss: 0.040135,  train_metric: 0.035 test_metric: 0.040 lr: 0.00085)\n",
            "epoch: 1773, train_loss: 0.035477, val loss: 0.040132,  train_metric: 0.035 test_metric: 0.040 lr: 0.00085)\n",
            "epoch: 1774, train_loss: 0.035173, val loss: 0.040331,  train_metric: 0.035 test_metric: 0.040 lr: 0.00085)\n",
            "epoch: 1775, train_loss: 0.035185, val loss: 0.040271,  train_metric: 0.035 test_metric: 0.040 lr: 0.00085)\n",
            "epoch: 1776, train_loss: 0.035412, val loss: 0.040127,  train_metric: 0.035 test_metric: 0.040 lr: 0.00084)\n",
            "epoch: 1777, train_loss: 0.035177, val loss: 0.040315,  train_metric: 0.035 test_metric: 0.040 lr: 0.00084)\n",
            "epoch: 1778, train_loss: 0.035231, val loss: 0.040170,  train_metric: 0.035 test_metric: 0.040 lr: 0.00084)\n",
            "epoch: 1779, train_loss: 0.035388, val loss: 0.040375,  train_metric: 0.035 test_metric: 0.040 lr: 0.00084)\n",
            "epoch: 1780, train_loss: 0.035654, val loss: 0.040390,  train_metric: 0.036 test_metric: 0.040 lr: 0.00084)\n",
            "epoch: 1781, train_loss: 0.035425, val loss: 0.040332,  train_metric: 0.035 test_metric: 0.040 lr: 0.00084)\n",
            "epoch: 1782, train_loss: 0.035211, val loss: 0.040114,  train_metric: 0.035 test_metric: 0.040 lr: 0.00084)\n",
            "epoch: 1783, train_loss: 0.035231, val loss: 0.040243,  train_metric: 0.035 test_metric: 0.040 lr: 0.00084)\n",
            "epoch: 1784, train_loss: 0.035306, val loss: 0.040228,  train_metric: 0.035 test_metric: 0.040 lr: 0.00084)\n",
            "epoch: 1785, train_loss: 0.035598, val loss: 0.040260,  train_metric: 0.036 test_metric: 0.040 lr: 0.00084)\n",
            "epoch: 1786, train_loss: 0.035195, val loss: 0.040142,  train_metric: 0.035 test_metric: 0.040 lr: 0.00084)\n",
            "epoch: 1787, train_loss: 0.035147, val loss: 0.040420,  train_metric: 0.035 test_metric: 0.040 lr: 0.00084)\n",
            "epoch: 1788, train_loss: 0.035371, val loss: 0.040172,  train_metric: 0.035 test_metric: 0.040 lr: 0.00083)\n",
            "epoch: 1789, train_loss: 0.035320, val loss: 0.040017,  train_metric: 0.035 test_metric: 0.040 lr: 0.00083)\n",
            "epoch: 1790, train_loss: 0.035383, val loss: 0.040311,  train_metric: 0.035 test_metric: 0.040 lr: 0.00083)\n",
            "epoch: 1791, train_loss: 0.035440, val loss: 0.040440,  train_metric: 0.035 test_metric: 0.040 lr: 0.00083)\n",
            "epoch: 1792, train_loss: 0.035474, val loss: 0.040433,  train_metric: 0.035 test_metric: 0.040 lr: 0.00083)\n",
            "epoch: 1793, train_loss: 0.035391, val loss: 0.040519,  train_metric: 0.035 test_metric: 0.041 lr: 0.00083)\n",
            "epoch: 1794, train_loss: 0.035532, val loss: 0.040263,  train_metric: 0.036 test_metric: 0.040 lr: 0.00083)\n",
            "epoch: 1795, train_loss: 0.035503, val loss: 0.040226,  train_metric: 0.036 test_metric: 0.040 lr: 0.00083)\n",
            "epoch: 1796, train_loss: 0.035592, val loss: 0.040801,  train_metric: 0.036 test_metric: 0.041 lr: 0.00083)\n",
            "epoch: 1797, train_loss: 0.035877, val loss: 0.040985,  train_metric: 0.036 test_metric: 0.041 lr: 0.00083)\n",
            "epoch: 1798, train_loss: 0.036187, val loss: 0.040705,  train_metric: 0.036 test_metric: 0.041 lr: 0.00083)\n",
            "epoch: 1799, train_loss: 0.035560, val loss: 0.040436,  train_metric: 0.036 test_metric: 0.040 lr: 0.00083)\n",
            "epoch: 1800, train_loss: 0.035625, val loss: 0.040843,  train_metric: 0.036 test_metric: 0.041 lr: 0.00082)\n",
            "epoch: 1801, train_loss: 0.035803, val loss: 0.040871,  train_metric: 0.036 test_metric: 0.041 lr: 0.00082)\n",
            "epoch: 1802, train_loss: 0.035930, val loss: 0.040923,  train_metric: 0.036 test_metric: 0.041 lr: 0.00082)\n",
            "epoch: 1803, train_loss: 0.036014, val loss: 0.040949,  train_metric: 0.036 test_metric: 0.041 lr: 0.00082)\n",
            "epoch: 1804, train_loss: 0.035863, val loss: 0.040346,  train_metric: 0.036 test_metric: 0.040 lr: 0.00082)\n",
            "epoch: 1805, train_loss: 0.035723, val loss: 0.040464,  train_metric: 0.036 test_metric: 0.040 lr: 0.00082)\n",
            "epoch: 1806, train_loss: 0.035755, val loss: 0.040285,  train_metric: 0.036 test_metric: 0.040 lr: 0.00082)\n",
            "epoch: 1807, train_loss: 0.035509, val loss: 0.040427,  train_metric: 0.036 test_metric: 0.040 lr: 0.00082)\n",
            "epoch: 1808, train_loss: 0.035303, val loss: 0.040167,  train_metric: 0.035 test_metric: 0.040 lr: 0.00082)\n",
            "epoch: 1809, train_loss: 0.035517, val loss: 0.040365,  train_metric: 0.036 test_metric: 0.040 lr: 0.00082)\n",
            "epoch: 1810, train_loss: 0.035501, val loss: 0.040198,  train_metric: 0.036 test_metric: 0.040 lr: 0.00082)\n",
            "epoch: 1811, train_loss: 0.035252, val loss: 0.040262,  train_metric: 0.035 test_metric: 0.040 lr: 0.00082)\n",
            "epoch: 1812, train_loss: 0.035235, val loss: 0.040239,  train_metric: 0.035 test_metric: 0.040 lr: 0.00082)\n",
            "epoch: 1813, train_loss: 0.035158, val loss: 0.040181,  train_metric: 0.035 test_metric: 0.040 lr: 0.00081)\n",
            "epoch: 1814, train_loss: 0.035429, val loss: 0.040327,  train_metric: 0.035 test_metric: 0.040 lr: 0.00081)\n",
            "epoch: 1815, train_loss: 0.035225, val loss: 0.040216,  train_metric: 0.035 test_metric: 0.040 lr: 0.00081)\n",
            "epoch: 1816, train_loss: 0.035311, val loss: 0.040202,  train_metric: 0.035 test_metric: 0.040 lr: 0.00081)\n",
            "epoch: 1817, train_loss: 0.035147, val loss: 0.040258,  train_metric: 0.035 test_metric: 0.040 lr: 0.00081)\n",
            "epoch: 1818, train_loss: 0.035475, val loss: 0.040238,  train_metric: 0.035 test_metric: 0.040 lr: 0.00081)\n",
            "epoch: 1819, train_loss: 0.035133, val loss: 0.040167,  train_metric: 0.035 test_metric: 0.040 lr: 0.00081)\n",
            "epoch: 1820, train_loss: 0.035145, val loss: 0.040123,  train_metric: 0.035 test_metric: 0.040 lr: 0.00081)\n",
            "epoch: 1821, train_loss: 0.035039, val loss: 0.040103,  train_metric: 0.035 test_metric: 0.040 lr: 0.00081)\n",
            "epoch: 1822, train_loss: 0.035186, val loss: 0.040275,  train_metric: 0.035 test_metric: 0.040 lr: 0.00081)\n",
            "epoch: 1823, train_loss: 0.035294, val loss: 0.040217,  train_metric: 0.035 test_metric: 0.040 lr: 0.00081)\n",
            "epoch: 1824, train_loss: 0.035303, val loss: 0.040411,  train_metric: 0.035 test_metric: 0.040 lr: 0.00081)\n",
            "epoch: 1825, train_loss: 0.035333, val loss: 0.040057,  train_metric: 0.035 test_metric: 0.040 lr: 0.00080)\n",
            "epoch: 1826, train_loss: 0.035462, val loss: 0.040094,  train_metric: 0.035 test_metric: 0.040 lr: 0.00080)\n",
            "epoch: 1827, train_loss: 0.035124, val loss: 0.040338,  train_metric: 0.035 test_metric: 0.040 lr: 0.00080)\n",
            "epoch: 1828, train_loss: 0.035298, val loss: 0.040066,  train_metric: 0.035 test_metric: 0.040 lr: 0.00080)\n",
            "epoch: 1829, train_loss: 0.035097, val loss: 0.040324,  train_metric: 0.035 test_metric: 0.040 lr: 0.00080)\n",
            "epoch: 1830, train_loss: 0.035312, val loss: 0.040370,  train_metric: 0.035 test_metric: 0.040 lr: 0.00080)\n",
            "epoch: 1831, train_loss: 0.035200, val loss: 0.040185,  train_metric: 0.035 test_metric: 0.040 lr: 0.00080)\n",
            "epoch: 1832, train_loss: 0.035469, val loss: 0.040495,  train_metric: 0.035 test_metric: 0.040 lr: 0.00080)\n",
            "epoch: 1833, train_loss: 0.035264, val loss: 0.040333,  train_metric: 0.035 test_metric: 0.040 lr: 0.00080)\n",
            "epoch: 1834, train_loss: 0.035303, val loss: 0.040394,  train_metric: 0.035 test_metric: 0.040 lr: 0.00080)\n",
            "epoch: 1835, train_loss: 0.035232, val loss: 0.040263,  train_metric: 0.035 test_metric: 0.040 lr: 0.00080)\n",
            "epoch: 1836, train_loss: 0.035402, val loss: 0.040289,  train_metric: 0.035 test_metric: 0.040 lr: 0.00080)\n",
            "epoch: 1837, train_loss: 0.035223, val loss: 0.040245,  train_metric: 0.035 test_metric: 0.040 lr: 0.00079)\n",
            "epoch: 1838, train_loss: 0.035300, val loss: 0.040359,  train_metric: 0.035 test_metric: 0.040 lr: 0.00079)\n",
            "epoch: 1839, train_loss: 0.035133, val loss: 0.040119,  train_metric: 0.035 test_metric: 0.040 lr: 0.00079)\n",
            "epoch: 1840, train_loss: 0.035180, val loss: 0.040111,  train_metric: 0.035 test_metric: 0.040 lr: 0.00079)\n",
            "epoch: 1841, train_loss: 0.035203, val loss: 0.040185,  train_metric: 0.035 test_metric: 0.040 lr: 0.00079)\n",
            "epoch: 1842, train_loss: 0.035183, val loss: 0.040171,  train_metric: 0.035 test_metric: 0.040 lr: 0.00079)\n",
            "epoch: 1843, train_loss: 0.035440, val loss: 0.040345,  train_metric: 0.035 test_metric: 0.040 lr: 0.00079)\n",
            "epoch: 1844, train_loss: 0.035646, val loss: 0.040143,  train_metric: 0.036 test_metric: 0.040 lr: 0.00079)\n",
            "epoch: 1845, train_loss: 0.035224, val loss: 0.040228,  train_metric: 0.035 test_metric: 0.040 lr: 0.00079)\n",
            "epoch: 1846, train_loss: 0.035937, val loss: 0.040544,  train_metric: 0.036 test_metric: 0.041 lr: 0.00079)\n",
            "epoch: 1847, train_loss: 0.035476, val loss: 0.040155,  train_metric: 0.035 test_metric: 0.040 lr: 0.00079)\n",
            "epoch: 1848, train_loss: 0.035121, val loss: 0.040172,  train_metric: 0.035 test_metric: 0.040 lr: 0.00079)\n",
            "epoch: 1849, train_loss: 0.035395, val loss: 0.040207,  train_metric: 0.035 test_metric: 0.040 lr: 0.00079)\n",
            "epoch: 1850, train_loss: 0.035174, val loss: 0.040102,  train_metric: 0.035 test_metric: 0.040 lr: 0.00078)\n",
            "epoch: 1851, train_loss: 0.035446, val loss: 0.040359,  train_metric: 0.035 test_metric: 0.040 lr: 0.00078)\n",
            "epoch: 1852, train_loss: 0.036017, val loss: 0.040170,  train_metric: 0.036 test_metric: 0.040 lr: 0.00078)\n",
            "epoch: 1853, train_loss: 0.035549, val loss: 0.040800,  train_metric: 0.036 test_metric: 0.041 lr: 0.00078)\n",
            "epoch: 1854, train_loss: 0.035307, val loss: 0.040324,  train_metric: 0.035 test_metric: 0.040 lr: 0.00078)\n",
            "epoch: 1855, train_loss: 0.035347, val loss: 0.040103,  train_metric: 0.035 test_metric: 0.040 lr: 0.00078)\n",
            "epoch: 1856, train_loss: 0.035072, val loss: 0.040274,  train_metric: 0.035 test_metric: 0.040 lr: 0.00078)\n",
            "epoch: 1857, train_loss: 0.035123, val loss: 0.040040,  train_metric: 0.035 test_metric: 0.040 lr: 0.00078)\n",
            "epoch: 1858, train_loss: 0.035013, val loss: 0.040035,  train_metric: 0.035 test_metric: 0.040 lr: 0.00078)\n",
            "epoch: 1859, train_loss: 0.035175, val loss: 0.040102,  train_metric: 0.035 test_metric: 0.040 lr: 0.00078)\n",
            "epoch: 1860, train_loss: 0.035026, val loss: 0.040195,  train_metric: 0.035 test_metric: 0.040 lr: 0.00078)\n",
            "epoch: 1861, train_loss: 0.035140, val loss: 0.040123,  train_metric: 0.035 test_metric: 0.040 lr: 0.00078)\n",
            "epoch: 1862, train_loss: 0.035038, val loss: 0.040050,  train_metric: 0.035 test_metric: 0.040 lr: 0.00078)\n",
            "epoch: 1863, train_loss: 0.035147, val loss: 0.040256,  train_metric: 0.035 test_metric: 0.040 lr: 0.00077)\n",
            "epoch: 1864, train_loss: 0.035228, val loss: 0.040248,  train_metric: 0.035 test_metric: 0.040 lr: 0.00077)\n",
            "epoch: 1865, train_loss: 0.035144, val loss: 0.040102,  train_metric: 0.035 test_metric: 0.040 lr: 0.00077)\n",
            "epoch: 1866, train_loss: 0.035636, val loss: 0.040382,  train_metric: 0.036 test_metric: 0.040 lr: 0.00077)\n",
            "epoch: 1867, train_loss: 0.035406, val loss: 0.040110,  train_metric: 0.035 test_metric: 0.040 lr: 0.00077)\n",
            "epoch: 1868, train_loss: 0.035317, val loss: 0.040420,  train_metric: 0.035 test_metric: 0.040 lr: 0.00077)\n",
            "epoch: 1869, train_loss: 0.035279, val loss: 0.040292,  train_metric: 0.035 test_metric: 0.040 lr: 0.00077)\n",
            "epoch: 1870, train_loss: 0.035301, val loss: 0.040201,  train_metric: 0.035 test_metric: 0.040 lr: 0.00077)\n",
            "epoch: 1871, train_loss: 0.035780, val loss: 0.040342,  train_metric: 0.036 test_metric: 0.040 lr: 0.00077)\n",
            "epoch: 1872, train_loss: 0.035116, val loss: 0.040050,  train_metric: 0.035 test_metric: 0.040 lr: 0.00077)\n",
            "epoch: 1873, train_loss: 0.035199, val loss: 0.040162,  train_metric: 0.035 test_metric: 0.040 lr: 0.00077)\n",
            "epoch: 1874, train_loss: 0.035348, val loss: 0.040145,  train_metric: 0.035 test_metric: 0.040 lr: 0.00077)\n",
            "epoch: 1875, train_loss: 0.035258, val loss: 0.040245,  train_metric: 0.035 test_metric: 0.040 lr: 0.00077)\n",
            "epoch: 1876, train_loss: 0.035488, val loss: 0.040132,  train_metric: 0.035 test_metric: 0.040 lr: 0.00076)\n",
            "epoch: 1877, train_loss: 0.035353, val loss: 0.040413,  train_metric: 0.035 test_metric: 0.040 lr: 0.00076)\n",
            "epoch: 1878, train_loss: 0.035528, val loss: 0.040138,  train_metric: 0.036 test_metric: 0.040 lr: 0.00076)\n",
            "epoch: 1879, train_loss: 0.035137, val loss: 0.040148,  train_metric: 0.035 test_metric: 0.040 lr: 0.00076)\n",
            "epoch: 1880, train_loss: 0.035249, val loss: 0.040245,  train_metric: 0.035 test_metric: 0.040 lr: 0.00076)\n",
            "epoch: 1881, train_loss: 0.035155, val loss: 0.040291,  train_metric: 0.035 test_metric: 0.040 lr: 0.00076)\n",
            "epoch: 1882, train_loss: 0.035102, val loss: 0.040227,  train_metric: 0.035 test_metric: 0.040 lr: 0.00076)\n",
            "epoch: 1883, train_loss: 0.035110, val loss: 0.040186,  train_metric: 0.035 test_metric: 0.040 lr: 0.00076)\n",
            "epoch: 1884, train_loss: 0.035025, val loss: 0.040054,  train_metric: 0.035 test_metric: 0.040 lr: 0.00076)\n",
            "epoch: 1885, train_loss: 0.035040, val loss: 0.039970,  train_metric: 0.035 test_metric: 0.040 lr: 0.00076)\n",
            "epoch: 1886, train_loss: 0.035139, val loss: 0.040078,  train_metric: 0.035 test_metric: 0.040 lr: 0.00076)\n",
            "epoch: 1887, train_loss: 0.034985, val loss: 0.040037,  train_metric: 0.035 test_metric: 0.040 lr: 0.00076)\n",
            "epoch: 1888, train_loss: 0.035128, val loss: 0.040089,  train_metric: 0.035 test_metric: 0.040 lr: 0.00076)\n",
            "epoch: 1889, train_loss: 0.035101, val loss: 0.040112,  train_metric: 0.035 test_metric: 0.040 lr: 0.00075)\n",
            "epoch: 1890, train_loss: 0.035016, val loss: 0.040075,  train_metric: 0.035 test_metric: 0.040 lr: 0.00075)\n",
            "epoch: 1891, train_loss: 0.035012, val loss: 0.040074,  train_metric: 0.035 test_metric: 0.040 lr: 0.00075)\n",
            "epoch: 1892, train_loss: 0.035076, val loss: 0.040254,  train_metric: 0.035 test_metric: 0.040 lr: 0.00075)\n",
            "epoch: 1893, train_loss: 0.035177, val loss: 0.040052,  train_metric: 0.035 test_metric: 0.040 lr: 0.00075)\n",
            "epoch: 1894, train_loss: 0.035190, val loss: 0.040148,  train_metric: 0.035 test_metric: 0.040 lr: 0.00075)\n",
            "epoch: 1895, train_loss: 0.035584, val loss: 0.040323,  train_metric: 0.036 test_metric: 0.040 lr: 0.00075)\n",
            "epoch: 1896, train_loss: 0.035263, val loss: 0.040002,  train_metric: 0.035 test_metric: 0.040 lr: 0.00075)\n",
            "epoch: 1897, train_loss: 0.035088, val loss: 0.040105,  train_metric: 0.035 test_metric: 0.040 lr: 0.00075)\n",
            "epoch: 1898, train_loss: 0.035101, val loss: 0.040046,  train_metric: 0.035 test_metric: 0.040 lr: 0.00075)\n",
            "epoch: 1899, train_loss: 0.034992, val loss: 0.040016,  train_metric: 0.035 test_metric: 0.040 lr: 0.00075)\n",
            "epoch: 1900, train_loss: 0.035038, val loss: 0.040186,  train_metric: 0.035 test_metric: 0.040 lr: 0.00075)\n",
            "epoch: 1901, train_loss: 0.035127, val loss: 0.040139,  train_metric: 0.035 test_metric: 0.040 lr: 0.00075)\n",
            "epoch: 1902, train_loss: 0.035189, val loss: 0.040318,  train_metric: 0.035 test_metric: 0.040 lr: 0.00074)\n",
            "epoch: 1903, train_loss: 0.035195, val loss: 0.040078,  train_metric: 0.035 test_metric: 0.040 lr: 0.00074)\n",
            "epoch: 1904, train_loss: 0.035119, val loss: 0.040303,  train_metric: 0.035 test_metric: 0.040 lr: 0.00074)\n",
            "epoch: 1905, train_loss: 0.035042, val loss: 0.040048,  train_metric: 0.035 test_metric: 0.040 lr: 0.00074)\n",
            "epoch: 1906, train_loss: 0.035463, val loss: 0.040130,  train_metric: 0.035 test_metric: 0.040 lr: 0.00074)\n",
            "epoch: 1907, train_loss: 0.035477, val loss: 0.040484,  train_metric: 0.035 test_metric: 0.040 lr: 0.00074)\n",
            "epoch: 1908, train_loss: 0.035420, val loss: 0.040106,  train_metric: 0.035 test_metric: 0.040 lr: 0.00074)\n",
            "epoch: 1909, train_loss: 0.035150, val loss: 0.040475,  train_metric: 0.035 test_metric: 0.040 lr: 0.00074)\n",
            "epoch: 1910, train_loss: 0.035191, val loss: 0.040225,  train_metric: 0.035 test_metric: 0.040 lr: 0.00074)\n",
            "epoch: 1911, train_loss: 0.035073, val loss: 0.040047,  train_metric: 0.035 test_metric: 0.040 lr: 0.00074)\n",
            "epoch: 1912, train_loss: 0.035242, val loss: 0.040153,  train_metric: 0.035 test_metric: 0.040 lr: 0.00074)\n",
            "epoch: 1913, train_loss: 0.035115, val loss: 0.040154,  train_metric: 0.035 test_metric: 0.040 lr: 0.00074)\n",
            "epoch: 1914, train_loss: 0.035062, val loss: 0.040070,  train_metric: 0.035 test_metric: 0.040 lr: 0.00074)\n",
            "epoch: 1915, train_loss: 0.035110, val loss: 0.040020,  train_metric: 0.035 test_metric: 0.040 lr: 0.00074)\n",
            "epoch: 1916, train_loss: 0.035111, val loss: 0.040150,  train_metric: 0.035 test_metric: 0.040 lr: 0.00073)\n",
            "epoch: 1917, train_loss: 0.035130, val loss: 0.040043,  train_metric: 0.035 test_metric: 0.040 lr: 0.00073)\n",
            "epoch: 1918, train_loss: 0.035218, val loss: 0.040237,  train_metric: 0.035 test_metric: 0.040 lr: 0.00073)\n",
            "epoch: 1919, train_loss: 0.035336, val loss: 0.040142,  train_metric: 0.035 test_metric: 0.040 lr: 0.00073)\n",
            "epoch: 1920, train_loss: 0.035202, val loss: 0.040359,  train_metric: 0.035 test_metric: 0.040 lr: 0.00073)\n",
            "epoch: 1921, train_loss: 0.035087, val loss: 0.040177,  train_metric: 0.035 test_metric: 0.040 lr: 0.00073)\n",
            "epoch: 1922, train_loss: 0.035315, val loss: 0.039984,  train_metric: 0.035 test_metric: 0.040 lr: 0.00073)\n",
            "epoch: 1923, train_loss: 0.035200, val loss: 0.040304,  train_metric: 0.035 test_metric: 0.040 lr: 0.00073)\n",
            "epoch: 1924, train_loss: 0.035086, val loss: 0.040031,  train_metric: 0.035 test_metric: 0.040 lr: 0.00073)\n",
            "epoch: 1925, train_loss: 0.035068, val loss: 0.040073,  train_metric: 0.035 test_metric: 0.040 lr: 0.00073)\n",
            "epoch: 1926, train_loss: 0.035199, val loss: 0.040217,  train_metric: 0.035 test_metric: 0.040 lr: 0.00073)\n",
            "epoch: 1927, train_loss: 0.035072, val loss: 0.040064,  train_metric: 0.035 test_metric: 0.040 lr: 0.00073)\n",
            "epoch: 1928, train_loss: 0.035186, val loss: 0.040160,  train_metric: 0.035 test_metric: 0.040 lr: 0.00073)\n",
            "epoch: 1929, train_loss: 0.035171, val loss: 0.040078,  train_metric: 0.035 test_metric: 0.040 lr: 0.00073)\n",
            "epoch: 1930, train_loss: 0.034999, val loss: 0.040063,  train_metric: 0.035 test_metric: 0.040 lr: 0.00072)\n",
            "epoch: 1931, train_loss: 0.035053, val loss: 0.040022,  train_metric: 0.035 test_metric: 0.040 lr: 0.00072)\n",
            "epoch: 1932, train_loss: 0.035049, val loss: 0.040025,  train_metric: 0.035 test_metric: 0.040 lr: 0.00072)\n",
            "epoch: 1933, train_loss: 0.035074, val loss: 0.040028,  train_metric: 0.035 test_metric: 0.040 lr: 0.00072)\n",
            "epoch: 1934, train_loss: 0.035082, val loss: 0.040181,  train_metric: 0.035 test_metric: 0.040 lr: 0.00072)\n",
            "epoch: 1935, train_loss: 0.035362, val loss: 0.040375,  train_metric: 0.035 test_metric: 0.040 lr: 0.00072)\n",
            "epoch: 1936, train_loss: 0.035256, val loss: 0.040518,  train_metric: 0.035 test_metric: 0.041 lr: 0.00072)\n",
            "epoch: 1937, train_loss: 0.035370, val loss: 0.040535,  train_metric: 0.035 test_metric: 0.041 lr: 0.00072)\n",
            "epoch: 1938, train_loss: 0.035572, val loss: 0.040225,  train_metric: 0.036 test_metric: 0.040 lr: 0.00072)\n",
            "epoch: 1939, train_loss: 0.035759, val loss: 0.041148,  train_metric: 0.036 test_metric: 0.041 lr: 0.00072)\n",
            "epoch: 1940, train_loss: 0.035637, val loss: 0.040587,  train_metric: 0.036 test_metric: 0.041 lr: 0.00072)\n",
            "epoch: 1941, train_loss: 0.035911, val loss: 0.040930,  train_metric: 0.036 test_metric: 0.041 lr: 0.00072)\n",
            "epoch: 1942, train_loss: 0.035796, val loss: 0.040274,  train_metric: 0.036 test_metric: 0.040 lr: 0.00072)\n",
            "epoch: 1943, train_loss: 0.035459, val loss: 0.040321,  train_metric: 0.035 test_metric: 0.040 lr: 0.00071)\n",
            "epoch: 1944, train_loss: 0.035304, val loss: 0.040301,  train_metric: 0.035 test_metric: 0.040 lr: 0.00071)\n",
            "epoch: 1945, train_loss: 0.036053, val loss: 0.040250,  train_metric: 0.036 test_metric: 0.040 lr: 0.00071)\n",
            "epoch: 1946, train_loss: 0.035453, val loss: 0.040394,  train_metric: 0.035 test_metric: 0.040 lr: 0.00071)\n",
            "epoch: 1947, train_loss: 0.035279, val loss: 0.040297,  train_metric: 0.035 test_metric: 0.040 lr: 0.00071)\n",
            "epoch: 1948, train_loss: 0.035240, val loss: 0.040244,  train_metric: 0.035 test_metric: 0.040 lr: 0.00071)\n",
            "epoch: 1949, train_loss: 0.035219, val loss: 0.040270,  train_metric: 0.035 test_metric: 0.040 lr: 0.00071)\n",
            "epoch: 1950, train_loss: 0.035140, val loss: 0.040263,  train_metric: 0.035 test_metric: 0.040 lr: 0.00071)\n",
            "epoch: 1951, train_loss: 0.035192, val loss: 0.040056,  train_metric: 0.035 test_metric: 0.040 lr: 0.00071)\n",
            "epoch: 1952, train_loss: 0.035231, val loss: 0.040118,  train_metric: 0.035 test_metric: 0.040 lr: 0.00071)\n",
            "epoch: 1953, train_loss: 0.035326, val loss: 0.040044,  train_metric: 0.035 test_metric: 0.040 lr: 0.00071)\n",
            "epoch: 1954, train_loss: 0.035100, val loss: 0.040243,  train_metric: 0.035 test_metric: 0.040 lr: 0.00071)\n",
            "epoch: 1955, train_loss: 0.035177, val loss: 0.040246,  train_metric: 0.035 test_metric: 0.040 lr: 0.00071)\n",
            "epoch: 1956, train_loss: 0.035254, val loss: 0.040195,  train_metric: 0.035 test_metric: 0.040 lr: 0.00071)\n",
            "epoch: 1957, train_loss: 0.035209, val loss: 0.040124,  train_metric: 0.035 test_metric: 0.040 lr: 0.00071)\n",
            "epoch: 1958, train_loss: 0.035136, val loss: 0.040146,  train_metric: 0.035 test_metric: 0.040 lr: 0.00070)\n",
            "epoch: 1959, train_loss: 0.035138, val loss: 0.039998,  train_metric: 0.035 test_metric: 0.040 lr: 0.00070)\n",
            "epoch: 1960, train_loss: 0.035149, val loss: 0.040263,  train_metric: 0.035 test_metric: 0.040 lr: 0.00070)\n",
            "epoch: 1961, train_loss: 0.035096, val loss: 0.040129,  train_metric: 0.035 test_metric: 0.040 lr: 0.00070)\n",
            "epoch: 1962, train_loss: 0.035173, val loss: 0.040119,  train_metric: 0.035 test_metric: 0.040 lr: 0.00070)\n",
            "epoch: 1963, train_loss: 0.035347, val loss: 0.040350,  train_metric: 0.035 test_metric: 0.040 lr: 0.00070)\n",
            "epoch: 1964, train_loss: 0.035056, val loss: 0.040075,  train_metric: 0.035 test_metric: 0.040 lr: 0.00070)\n",
            "epoch: 1965, train_loss: 0.035041, val loss: 0.040157,  train_metric: 0.035 test_metric: 0.040 lr: 0.00070)\n",
            "epoch: 1966, train_loss: 0.035320, val loss: 0.040034,  train_metric: 0.035 test_metric: 0.040 lr: 0.00070)\n",
            "epoch: 1967, train_loss: 0.035236, val loss: 0.039973,  train_metric: 0.035 test_metric: 0.040 lr: 0.00070)\n",
            "epoch: 1968, train_loss: 0.035047, val loss: 0.040115,  train_metric: 0.035 test_metric: 0.040 lr: 0.00070)\n",
            "epoch: 1969, train_loss: 0.034948, val loss: 0.040056,  train_metric: 0.035 test_metric: 0.040 lr: 0.00070)\n",
            "epoch: 1970, train_loss: 0.035035, val loss: 0.040146,  train_metric: 0.035 test_metric: 0.040 lr: 0.00070)\n",
            "epoch: 1971, train_loss: 0.035079, val loss: 0.040082,  train_metric: 0.035 test_metric: 0.040 lr: 0.00070)\n",
            "epoch: 1972, train_loss: 0.035273, val loss: 0.039992,  train_metric: 0.035 test_metric: 0.040 lr: 0.00069)\n",
            "epoch: 1973, train_loss: 0.035445, val loss: 0.040418,  train_metric: 0.035 test_metric: 0.040 lr: 0.00069)\n",
            "epoch: 1974, train_loss: 0.035129, val loss: 0.040055,  train_metric: 0.035 test_metric: 0.040 lr: 0.00069)\n",
            "epoch: 1975, train_loss: 0.035029, val loss: 0.039955,  train_metric: 0.035 test_metric: 0.040 lr: 0.00069)\n",
            "epoch: 1976, train_loss: 0.034946, val loss: 0.040110,  train_metric: 0.035 test_metric: 0.040 lr: 0.00069)\n",
            "epoch: 1977, train_loss: 0.035247, val loss: 0.040313,  train_metric: 0.035 test_metric: 0.040 lr: 0.00069)\n",
            "epoch: 1978, train_loss: 0.035314, val loss: 0.039985,  train_metric: 0.035 test_metric: 0.040 lr: 0.00069)\n",
            "epoch: 1979, train_loss: 0.035173, val loss: 0.040253,  train_metric: 0.035 test_metric: 0.040 lr: 0.00069)\n",
            "epoch: 1980, train_loss: 0.035032, val loss: 0.040063,  train_metric: 0.035 test_metric: 0.040 lr: 0.00069)\n",
            "epoch: 1981, train_loss: 0.034993, val loss: 0.039948,  train_metric: 0.035 test_metric: 0.040 lr: 0.00069)\n",
            "epoch: 1982, train_loss: 0.035116, val loss: 0.040116,  train_metric: 0.035 test_metric: 0.040 lr: 0.00069)\n",
            "epoch: 1983, train_loss: 0.035007, val loss: 0.040097,  train_metric: 0.035 test_metric: 0.040 lr: 0.00069)\n",
            "epoch: 1984, train_loss: 0.035095, val loss: 0.040146,  train_metric: 0.035 test_metric: 0.040 lr: 0.00069)\n",
            "epoch: 1985, train_loss: 0.035264, val loss: 0.040090,  train_metric: 0.035 test_metric: 0.040 lr: 0.00069)\n",
            "epoch: 1986, train_loss: 0.035072, val loss: 0.040032,  train_metric: 0.035 test_metric: 0.040 lr: 0.00068)\n",
            "epoch: 1987, train_loss: 0.035093, val loss: 0.040247,  train_metric: 0.035 test_metric: 0.040 lr: 0.00068)\n",
            "epoch: 1988, train_loss: 0.035175, val loss: 0.040193,  train_metric: 0.035 test_metric: 0.040 lr: 0.00068)\n",
            "epoch: 1989, train_loss: 0.035072, val loss: 0.040130,  train_metric: 0.035 test_metric: 0.040 lr: 0.00068)\n",
            "epoch: 1990, train_loss: 0.035184, val loss: 0.040206,  train_metric: 0.035 test_metric: 0.040 lr: 0.00068)\n",
            "epoch: 1991, train_loss: 0.035031, val loss: 0.040116,  train_metric: 0.035 test_metric: 0.040 lr: 0.00068)\n",
            "epoch: 1992, train_loss: 0.035008, val loss: 0.040036,  train_metric: 0.035 test_metric: 0.040 lr: 0.00068)\n",
            "epoch: 1993, train_loss: 0.035010, val loss: 0.040065,  train_metric: 0.035 test_metric: 0.040 lr: 0.00068)\n",
            "epoch: 1994, train_loss: 0.034963, val loss: 0.040055,  train_metric: 0.035 test_metric: 0.040 lr: 0.00068)\n",
            "epoch: 1995, train_loss: 0.035234, val loss: 0.040023,  train_metric: 0.035 test_metric: 0.040 lr: 0.00068)\n",
            "epoch: 1996, train_loss: 0.034988, val loss: 0.040089,  train_metric: 0.035 test_metric: 0.040 lr: 0.00068)\n",
            "epoch: 1997, train_loss: 0.034952, val loss: 0.040063,  train_metric: 0.035 test_metric: 0.040 lr: 0.00068)\n",
            "epoch: 1998, train_loss: 0.034967, val loss: 0.040097,  train_metric: 0.035 test_metric: 0.040 lr: 0.00068)\n",
            "epoch: 1999, train_loss: 0.034952, val loss: 0.040104,  train_metric: 0.035 test_metric: 0.040 lr: 0.00068)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQQ_3WAafsvW"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "nF8f30Pdfb53",
        "outputId": "31b24211-2e32-46d2-ee19-cc62f28826a7"
      },
      "source": [
        "# Run on cpu\r\n",
        "device = torch.device(\"cpu\")\r\n",
        "\r\n",
        "# Load Regression\r\n",
        "R_model = RegressionNet(num_inputs=1)\r\n",
        "R_weights = torch.load(\"weights_regression.pt\")\r\n",
        "R_model.load_state_dict(R_weights)\r\n",
        "R_model = R_model.to(device)\r\n",
        "\r\n",
        "# Predict Regression\r\n",
        "R_Xt = torch.from_numpy(R_X).type(torch.float32).unsqueeze(1)\r\n",
        "R_Xt = (R_Xt - R_mean) / R_std\r\n",
        "R_Y_hat = R_model.forward(R_Xt).detach().numpy().reshape(-1)\r\n",
        "\r\n",
        "# Visualize Regression\r\n",
        "fig = go.Figure()\r\n",
        "fig.add_traces( go.Scatter(x=R_X, y=R_Y, name=\"Real\",hovertemplate='x: %{x} <br>y: %{y}') )\r\n",
        "fig.add_traces( go.Scatter(x=R_X, y=R_Y_hat, name=\"Predicted\",hovertemplate='x: %{x} <br>y: %{y}') )\r\n",
        "fig.update_layout(title=\"Regression Results\",hovermode=\"x\")\r\n",
        "fig.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"fc611f77-78b8-4923-90c5-91b6a1a4741d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"fc611f77-78b8-4923-90c5-91b6a1a4741d\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'fc611f77-78b8-4923-90c5-91b6a1a4741d',\n",
              "                        [{\"hovertemplate\": \"x: %{x} <br>y: %{y}\", \"name\": \"Real\", \"type\": \"scatter\", \"x\": [-2.0, -1.98998998998999, -1.97997997997998, -1.96996996996997, -1.95995995995996, -1.94994994994995, -1.93993993993994, -1.92992992992993, -1.91991991991992, -1.90990990990991, -1.8998998998999, -1.88988988988989, -1.87987987987988, -1.86986986986987, -1.85985985985986, -1.84984984984985, -1.83983983983984, -1.82982982982983, -1.8198198198198199, -1.8098098098098099, -1.7997997997997999, -1.7897897897897899, -1.7797797797797799, -1.7697697697697699, -1.7597597597597598, -1.7497497497497498, -1.7397397397397398, -1.7297297297297298, -1.7197197197197198, -1.7097097097097098, -1.6996996996996998, -1.6896896896896898, -1.6796796796796798, -1.6696696696696698, -1.6596596596596598, -1.6496496496496498, -1.6396396396396398, -1.6296296296296298, -1.6196196196196198, -1.6096096096096097, -1.5995995995995997, -1.5895895895895895, -1.5795795795795795, -1.5695695695695695, -1.5595595595595595, -1.5495495495495495, -1.5395395395395395, -1.5295295295295295, -1.5195195195195195, -1.5095095095095095, -1.4994994994994995, -1.4894894894894894, -1.4794794794794794, -1.4694694694694694, -1.4594594594594594, -1.4494494494494494, -1.4394394394394394, -1.4294294294294294, -1.4194194194194194, -1.4094094094094094, -1.3993993993993994, -1.3893893893893894, -1.3793793793793794, -1.3693693693693694, -1.3593593593593594, -1.3493493493493494, -1.3393393393393394, -1.3293293293293293, -1.3193193193193193, -1.3093093093093093, -1.2992992992992993, -1.2892892892892893, -1.2792792792792793, -1.2692692692692693, -1.2592592592592593, -1.2492492492492493, -1.2392392392392393, -1.2292292292292293, -1.2192192192192193, -1.2092092092092093, -1.1991991991991993, -1.189189189189189, -1.179179179179179, -1.169169169169169, -1.159159159159159, -1.149149149149149, -1.139139139139139, -1.129129129129129, -1.119119119119119, -1.109109109109109, -1.099099099099099, -1.089089089089089, -1.079079079079079, -1.069069069069069, -1.059059059059059, -1.049049049049049, -1.039039039039039, -1.029029029029029, -1.019019019019019, -1.009009009009009, -0.9989989989989989, -0.9889889889889889, -0.9789789789789789, -0.9689689689689689, -0.9589589589589589, -0.9489489489489489, -0.9389389389389389, -0.9289289289289289, -0.9189189189189189, -0.9089089089089089, -0.8988988988988988, -0.8888888888888888, -0.8788788788788788, -0.8688688688688688, -0.8588588588588588, -0.8488488488488488, -0.8388388388388388, -0.8288288288288288, -0.8188188188188188, -0.8088088088088088, -0.7987987987987988, -0.7887887887887888, -0.7787787787787788, -0.7687687687687688, -0.7587587587587588, -0.7487487487487487, -0.7387387387387387, -0.7287287287287287, -0.7187187187187187, -0.7087087087087087, -0.6986986986986987, -0.6886886886886887, -0.6786786786786787, -0.6686686686686687, -0.6586586586586587, -0.6486486486486487, -0.6386386386386387, -0.6286286286286287, -0.6186186186186187, -0.6086086086086087, -0.5985985985985987, -0.5885885885885886, -0.5785785785785786, -0.5685685685685686, -0.5585585585585586, -0.5485485485485486, -0.5385385385385386, -0.5285285285285286, -0.5185185185185186, -0.5085085085085086, -0.4984984984984986, -0.4884884884884886, -0.4784784784784786, -0.46846846846846857, -0.45845845845845856, -0.44844844844844856, -0.43843843843843855, -0.42842842842842854, -0.41841841841841854, -0.40840840840840853, -0.3983983983983985, -0.3883883883883883, -0.3783783783783783, -0.3683683683683683, -0.3583583583583583, -0.34834834834834827, -0.33833833833833826, -0.32832832832832826, -0.31831831831831825, -0.30830830830830824, -0.29829829829829824, -0.28828828828828823, -0.2782782782782782, -0.2682682682682682, -0.2582582582582582, -0.2482482482482482, -0.2382382382382382, -0.2282282282282282, -0.21821821821821819, -0.20820820820820818, -0.19819819819819817, -0.18818818818818817, -0.17817817817817816, -0.16816816816816815, -0.15815815815815815, -0.14814814814814814, -0.13813813813813813, -0.12812812812812813, -0.11811811811811812, -0.10810810810810811, -0.09809809809809811, -0.0880880880880881, -0.0780780780780781, -0.06806806806806809, -0.05805805805805808, -0.048048048048048075, -0.03803803803803807, -0.028028028028028062, -0.018018018018018056, -0.00800800800800805, 0.002002002002002179, 0.012012012012012185, 0.022022022022022192, 0.0320320320320322, 0.042042042042042205, 0.05205205205205221, 0.06206206206206222, 0.07207207207207222, 0.08208208208208223, 0.09209209209209224, 0.10210210210210224, 0.11211211211211225, 0.12212212212212226, 0.13213213213213226, 0.14214214214214227, 0.15215215215215228, 0.16216216216216228, 0.1721721721721723, 0.1821821821821823, 0.1921921921921923, 0.2022022022022023, 0.21221221221221231, 0.22222222222222232, 0.23223223223223233, 0.24224224224224233, 0.25225225225225234, 0.26226226226226235, 0.27227227227227235, 0.28228228228228236, 0.29229229229229237, 0.3023023023023024, 0.3123123123123124, 0.3223223223223224, 0.3323323323323324, 0.3423423423423424, 0.3523523523523524, 0.3623623623623624, 0.3723723723723724, 0.3823823823823824, 0.39239239239239243, 0.40240240240240244, 0.41241241241241244, 0.42242242242242245, 0.43243243243243246, 0.44244244244244246, 0.45245245245245247, 0.4624624624624625, 0.4724724724724725, 0.4824824824824825, 0.4924924924924925, 0.5025025025025025, 0.5125125125125125, 0.5225225225225225, 0.5325325325325325, 0.5425425425425425, 0.5525525525525525, 0.5625625625625625, 0.5725725725725725, 0.5825825825825826, 0.5925925925925926, 0.6026026026026026, 0.6126126126126126, 0.6226226226226226, 0.6326326326326326, 0.6426426426426426, 0.6526526526526526, 0.6626626626626626, 0.6726726726726726, 0.6826826826826826, 0.6926926926926926, 0.7027027027027026, 0.7127127127127126, 0.7227227227227226, 0.7327327327327327, 0.7427427427427427, 0.7527527527527527, 0.7627627627627627, 0.7727727727727727, 0.7827827827827827, 0.7927927927927927, 0.8028028028028027, 0.8128128128128127, 0.8228228228228227, 0.8328328328328327, 0.8428428428428427, 0.8528528528528527, 0.8628628628628627, 0.8728728728728727, 0.8828828828828827, 0.8928928928928928, 0.9029029029029028, 0.9129129129129128, 0.9229229229229228, 0.9329329329329328, 0.9429429429429428, 0.9529529529529528, 0.9629629629629628, 0.9729729729729728, 0.9829829829829828, 0.9929929929929928, 1.0030030030030028, 1.0130130130130128, 1.0230230230230228, 1.0330330330330328, 1.0430430430430429, 1.0530530530530529, 1.0630630630630629, 1.0730730730730729, 1.0830830830830829, 1.0930930930930929, 1.1031031031031029, 1.113113113113113, 1.123123123123123, 1.133133133133133, 1.143143143143143, 1.153153153153153, 1.163163163163163, 1.173173173173173, 1.183183183183183, 1.193193193193193, 1.203203203203203, 1.2132132132132134, 1.2232232232232234, 1.2332332332332334, 1.2432432432432434, 1.2532532532532534, 1.2632632632632634, 1.2732732732732734, 1.2832832832832834, 1.2932932932932935, 1.3033033033033035, 1.3133133133133135, 1.3233233233233235, 1.3333333333333335, 1.3433433433433435, 1.3533533533533535, 1.3633633633633635, 1.3733733733733735, 1.3833833833833835, 1.3933933933933935, 1.4034034034034035, 1.4134134134134135, 1.4234234234234235, 1.4334334334334335, 1.4434434434434436, 1.4534534534534536, 1.4634634634634636, 1.4734734734734736, 1.4834834834834836, 1.4934934934934936, 1.5035035035035036, 1.5135135135135136, 1.5235235235235236, 1.5335335335335336, 1.5435435435435436, 1.5535535535535536, 1.5635635635635636, 1.5735735735735736, 1.5835835835835836, 1.5935935935935936, 1.6036036036036037, 1.6136136136136137, 1.6236236236236237, 1.6336336336336337, 1.6436436436436437, 1.6536536536536537, 1.6636636636636637, 1.6736736736736737, 1.6836836836836837, 1.6936936936936937, 1.7037037037037037, 1.7137137137137137, 1.7237237237237237, 1.7337337337337337, 1.7437437437437437, 1.7537537537537538, 1.7637637637637638, 1.7737737737737738, 1.7837837837837838, 1.7937937937937938, 1.8038038038038038, 1.8138138138138138, 1.8238238238238238, 1.8338338338338338, 1.8438438438438438, 1.8538538538538538, 1.8638638638638638, 1.8738738738738738, 1.8838838838838838, 1.8938938938938938, 1.9039039039039038, 1.9139139139139139, 1.9239239239239239, 1.9339339339339339, 1.9439439439439439, 1.9539539539539539, 1.9639639639639639, 1.973973973973974, 1.983983983983984, 1.993993993993994, 2.0040040040040044, 2.0140140140140144, 2.0240240240240244, 2.0340340340340344, 2.0440440440440444, 2.0540540540540544, 2.0640640640640644, 2.0740740740740744, 2.0840840840840844, 2.0940940940940944, 2.1041041041041044, 2.1141141141141144, 2.1241241241241244, 2.1341341341341344, 2.1441441441441444, 2.1541541541541545, 2.1641641641641645, 2.1741741741741745, 2.1841841841841845, 2.1941941941941945, 2.2042042042042045, 2.2142142142142145, 2.2242242242242245, 2.2342342342342345, 2.2442442442442445, 2.2542542542542545, 2.2642642642642645, 2.2742742742742745, 2.2842842842842845, 2.2942942942942945, 2.3043043043043046, 2.3143143143143146, 2.3243243243243246, 2.3343343343343346, 2.3443443443443446, 2.3543543543543546, 2.3643643643643646, 2.3743743743743746, 2.3843843843843846, 2.3943943943943946, 2.4044044044044046, 2.4144144144144146, 2.4244244244244246, 2.4344344344344346, 2.4444444444444446, 2.4544544544544546, 2.4644644644644647, 2.4744744744744747, 2.4844844844844847, 2.4944944944944947, 2.5045045045045047, 2.5145145145145147, 2.5245245245245247, 2.5345345345345347, 2.5445445445445447, 2.5545545545545547, 2.5645645645645647, 2.5745745745745747, 2.5845845845845847, 2.5945945945945947, 2.6046046046046047, 2.6146146146146148, 2.6246246246246248, 2.6346346346346348, 2.6446446446446448, 2.6546546546546548, 2.664664664664665, 2.674674674674675, 2.684684684684685, 2.694694694694695, 2.704704704704705, 2.714714714714715, 2.724724724724725, 2.734734734734735, 2.744744744744745, 2.754754754754755, 2.764764764764765, 2.774774774774775, 2.784784784784785, 2.794794794794795, 2.804804804804805, 2.814814814814815, 2.824824824824825, 2.834834834834835, 2.844844844844845, 2.854854854854855, 2.864864864864865, 2.874874874874875, 2.884884884884885, 2.894894894894895, 2.904904904904905, 2.914914914914915, 2.924924924924925, 2.934934934934935, 2.944944944944945, 2.954954954954955, 2.964964964964965, 2.974974974974975, 2.984984984984985, 2.994994994994995, 3.005005005005005, 3.015015015015015, 3.025025025025025, 3.035035035035035, 3.045045045045045, 3.055055055055055, 3.065065065065065, 3.075075075075075, 3.085085085085085, 3.095095095095095, 3.105105105105105, 3.115115115115115, 3.125125125125125, 3.135135135135135, 3.145145145145145, 3.155155155155155, 3.165165165165165, 3.175175175175175, 3.185185185185185, 3.195195195195195, 3.205205205205205, 3.215215215215215, 3.225225225225225, 3.235235235235235, 3.245245245245245, 3.255255255255255, 3.265265265265265, 3.275275275275275, 3.285285285285285, 3.295295295295295, 3.305305305305305, 3.315315315315315, 3.325325325325325, 3.335335335335335, 3.3453453453453452, 3.3553553553553552, 3.3653653653653652, 3.3753753753753752, 3.3853853853853852, 3.3953953953953953, 3.4054054054054053, 3.4154154154154153, 3.4254254254254253, 3.4354354354354353, 3.4454454454454453, 3.4554554554554553, 3.4654654654654653, 3.4754754754754753, 3.4854854854854853, 3.4954954954954953, 3.5055055055055053, 3.5155155155155153, 3.5255255255255253, 3.5355355355355353, 3.5455455455455454, 3.5555555555555554, 3.5655655655655654, 3.5755755755755754, 3.5855855855855854, 3.5955955955955954, 3.6056056056056054, 3.6156156156156154, 3.6256256256256254, 3.6356356356356354, 3.6456456456456454, 3.6556556556556554, 3.6656656656656654, 3.6756756756756754, 3.6856856856856854, 3.6956956956956954, 3.7057057057057055, 3.7157157157157155, 3.7257257257257255, 3.7357357357357355, 3.7457457457457455, 3.7557557557557555, 3.7657657657657655, 3.7757757757757755, 3.7857857857857855, 3.7957957957957955, 3.8058058058058055, 3.8158158158158155, 3.8258258258258255, 3.8358358358358355, 3.8458458458458455, 3.8558558558558556, 3.8658658658658656, 3.8758758758758756, 3.8858858858858856, 3.8958958958958956, 3.9059059059059056, 3.9159159159159156, 3.9259259259259256, 3.9359359359359356, 3.9459459459459456, 3.9559559559559556, 3.9659659659659656, 3.9759759759759756, 3.9859859859859856, 3.9959959959959956, 4.006006006006006, 4.016016016016016, 4.026026026026026, 4.036036036036036, 4.046046046046046, 4.056056056056056, 4.066066066066066, 4.076076076076076, 4.086086086086086, 4.096096096096096, 4.106106106106106, 4.116116116116116, 4.126126126126126, 4.136136136136136, 4.146146146146146, 4.156156156156156, 4.166166166166166, 4.176176176176176, 4.186186186186186, 4.196196196196196, 4.206206206206206, 4.216216216216216, 4.226226226226226, 4.236236236236236, 4.246246246246246, 4.256256256256256, 4.266266266266266, 4.276276276276276, 4.286286286286286, 4.296296296296296, 4.306306306306306, 4.316316316316316, 4.326326326326326, 4.336336336336336, 4.346346346346346, 4.356356356356356, 4.366366366366366, 4.376376376376376, 4.386386386386386, 4.396396396396396, 4.406406406406406, 4.416416416416417, 4.426426426426427, 4.436436436436437, 4.446446446446447, 4.456456456456457, 4.466466466466467, 4.476476476476477, 4.486486486486487, 4.496496496496497, 4.506506506506507, 4.516516516516517, 4.526526526526527, 4.536536536536537, 4.546546546546547, 4.556556556556557, 4.566566566566567, 4.576576576576577, 4.586586586586587, 4.596596596596597, 4.606606606606607, 4.616616616616617, 4.626626626626627, 4.636636636636637, 4.646646646646647, 4.656656656656657, 4.666666666666667, 4.676676676676677, 4.686686686686687, 4.696696696696697, 4.706706706706707, 4.716716716716717, 4.726726726726727, 4.736736736736737, 4.746746746746747, 4.756756756756757, 4.766766766766767, 4.776776776776777, 4.786786786786787, 4.796796796796797, 4.806806806806807, 4.816816816816817, 4.826826826826827, 4.836836836836837, 4.846846846846847, 4.856856856856857, 4.866866866866867, 4.876876876876877, 4.886886886886887, 4.896896896896897, 4.906906906906907, 4.916916916916917, 4.926926926926927, 4.936936936936937, 4.946946946946947, 4.956956956956957, 4.966966966966967, 4.976976976976977, 4.986986986986987, 4.996996996996997, 5.007007007007007, 5.017017017017017, 5.027027027027027, 5.037037037037037, 5.047047047047047, 5.057057057057057, 5.067067067067067, 5.077077077077077, 5.087087087087087, 5.097097097097097, 5.107107107107107, 5.117117117117117, 5.127127127127127, 5.137137137137137, 5.147147147147147, 5.157157157157157, 5.167167167167167, 5.177177177177177, 5.187187187187187, 5.197197197197197, 5.207207207207207, 5.217217217217217, 5.227227227227227, 5.237237237237237, 5.247247247247247, 5.257257257257257, 5.267267267267267, 5.277277277277277, 5.287287287287287, 5.297297297297297, 5.307307307307307, 5.317317317317317, 5.327327327327327, 5.337337337337337, 5.347347347347347, 5.357357357357357, 5.367367367367367, 5.377377377377377, 5.387387387387387, 5.397397397397397, 5.407407407407407, 5.4174174174174174, 5.4274274274274275, 5.4374374374374375, 5.4474474474474475, 5.4574574574574575, 5.4674674674674675, 5.4774774774774775, 5.4874874874874875, 5.4974974974974975, 5.5075075075075075, 5.5175175175175175, 5.5275275275275275, 5.5375375375375375, 5.5475475475475475, 5.5575575575575575, 5.5675675675675675, 5.5775775775775776, 5.587587587587588, 5.597597597597598, 5.607607607607608, 5.617617617617618, 5.627627627627628, 5.637637637637638, 5.647647647647648, 5.657657657657658, 5.667667667667668, 5.677677677677678, 5.687687687687688, 5.697697697697698, 5.707707707707708, 5.717717717717718, 5.727727727727728, 5.737737737737738, 5.747747747747748, 5.757757757757758, 5.767767767767768, 5.777777777777778, 5.787787787787788, 5.797797797797798, 5.807807807807808, 5.817817817817818, 5.827827827827828, 5.837837837837838, 5.847847847847848, 5.857857857857858, 5.867867867867868, 5.877877877877878, 5.887887887887888, 5.897897897897898, 5.907907907907908, 5.917917917917918, 5.927927927927928, 5.937937937937938, 5.947947947947948, 5.957957957957958, 5.967967967967968, 5.977977977977978, 5.987987987987988, 5.997997997997998, 6.008008008008009, 6.018018018018019, 6.028028028028029, 6.038038038038039, 6.048048048048049, 6.058058058058059, 6.068068068068069, 6.078078078078079, 6.088088088088089, 6.098098098098099, 6.108108108108109, 6.118118118118119, 6.128128128128129, 6.138138138138139, 6.148148148148149, 6.158158158158159, 6.168168168168169, 6.178178178178179, 6.188188188188189, 6.198198198198199, 6.208208208208209, 6.218218218218219, 6.228228228228229, 6.238238238238239, 6.248248248248249, 6.258258258258259, 6.268268268268269, 6.278278278278279, 6.288288288288289, 6.298298298298299, 6.308308308308309, 6.318318318318319, 6.328328328328329, 6.338338338338339, 6.348348348348349, 6.358358358358359, 6.368368368368369, 6.378378378378379, 6.388388388388389, 6.398398398398399, 6.408408408408409, 6.418418418418419, 6.428428428428429, 6.438438438438439, 6.448448448448449, 6.458458458458459, 6.468468468468469, 6.478478478478479, 6.488488488488489, 6.498498498498499, 6.508508508508509, 6.518518518518519, 6.528528528528529, 6.538538538538539, 6.548548548548549, 6.558558558558559, 6.568568568568569, 6.578578578578579, 6.588588588588589, 6.598598598598599, 6.608608608608609, 6.618618618618619, 6.628628628628629, 6.638638638638639, 6.648648648648649, 6.658658658658659, 6.668668668668669, 6.678678678678679, 6.688688688688689, 6.698698698698699, 6.708708708708709, 6.718718718718719, 6.728728728728729, 6.738738738738739, 6.748748748748749, 6.758758758758759, 6.768768768768769, 6.778778778778779, 6.788788788788789, 6.798798798798799, 6.808808808808809, 6.818818818818819, 6.828828828828829, 6.838838838838839, 6.848848848848849, 6.858858858858859, 6.868868868868869, 6.878878878878879, 6.888888888888889, 6.898898898898899, 6.908908908908909, 6.918918918918919, 6.928928928928929, 6.938938938938939, 6.948948948948949, 6.958958958958959, 6.968968968968969, 6.978978978978979, 6.988988988988989, 6.998998998998999, 7.009009009009009, 7.019019019019019, 7.029029029029029, 7.039039039039039, 7.049049049049049, 7.059059059059059, 7.069069069069069, 7.079079079079079, 7.089089089089089, 7.099099099099099, 7.109109109109109, 7.119119119119119, 7.129129129129129, 7.1391391391391394, 7.1491491491491495, 7.1591591591591595, 7.1691691691691695, 7.1791791791791795, 7.1891891891891895, 7.1991991991991995, 7.2092092092092095, 7.2192192192192195, 7.2292292292292295, 7.2392392392392395, 7.2492492492492495, 7.2592592592592595, 7.2692692692692695, 7.2792792792792795, 7.2892892892892895, 7.2992992992992995, 7.3093093093093096, 7.31931931931932, 7.32932932932933, 7.33933933933934, 7.34934934934935, 7.35935935935936, 7.36936936936937, 7.37937937937938, 7.38938938938939, 7.3993993993994, 7.40940940940941, 7.41941941941942, 7.42942942942943, 7.43943943943944, 7.44944944944945, 7.45945945945946, 7.46946946946947, 7.47947947947948, 7.48948948948949, 7.4994994994995, 7.50950950950951, 7.51951951951952, 7.52952952952953, 7.53953953953954, 7.54954954954955, 7.55955955955956, 7.56956956956957, 7.57957957957958, 7.58958958958959, 7.5995995995996, 7.60960960960961, 7.61961961961962, 7.62962962962963, 7.63963963963964, 7.64964964964965, 7.65965965965966, 7.66966966966967, 7.67967967967968, 7.68968968968969, 7.6996996996997, 7.70970970970971, 7.71971971971972, 7.72972972972973, 7.73973973973974, 7.74974974974975, 7.75975975975976, 7.76976976976977, 7.77977977977978, 7.78978978978979, 7.7997997997998, 7.80980980980981, 7.81981981981982, 7.82982982982983, 7.83983983983984, 7.84984984984985, 7.85985985985986, 7.86986986986987, 7.87987987987988, 7.88988988988989, 7.8998998998999, 7.90990990990991, 7.91991991991992, 7.92992992992993, 7.93993993993994, 7.94994994994995, 7.95995995995996, 7.96996996996997, 7.97997997997998, 7.98998998998999, 8.0], \"y\": [4.348766175087199, 4.277192966093969, 4.205102954463674, 4.132487146126188, 4.059336629423321, 3.9856425908123354, 3.911396330578623, 3.8365892785425917, 3.7612130097457555, 3.6852592601009544, 3.6087199419916254, 3.5315871598050066, 3.4538532253841696, 3.375510673383772, 3.296552276514456, 3.2169710606608466, 3.1367603198581673, 3.055913631112543, 2.974424869050148, 2.892288220380445, 2.8094981981588774, 2.7260496558344807, 2.641937801068032, 2.5571582093064924, 2.4717068370996693, 2.385580035145178, 2.2987745610480106, 2.211287591781181, 2.123116735834155, 2.0342600450359956, 1.9447160260403917, 1.8544836514599983, 1.763562370637775, 1.6719521200432987, 1.579653333282303, 1.4866669507080115, 1.392994428623144, 1.2986377480617897, 1.2035994231406981, 1.1078825089698654, 1.0114906091126674, 0.9144278825861474, 0.8166990503924698, 0.7183094015728864, 0.6192647987760354, 0.5195716833327357, 0.41923707982988656, 0.31826860017649905, 0.21667444715532413, 0.11446341745397193, 0.011644904169876624, -0.09177110121609311, -0.19577400740211237, -0.3003526223682337, -0.40549515302337114, -0.5111892053997501, -0.6174217853976534, -0.7241793000828051, -0.8314475595382479, -0.9392117792720592, -1.0474565831817677, -1.1561660070758175, -1.2653235027519267, -1.3749119426316847, -1.484913624950209, -1.595310279499198, -1.706083073921177, -1.8172126205522456, -1.9286789838101188, -2.0404616881237443, -2.15253972640026, -2.2648915690245763, -2.377495173386338, -2.4903279939285246, -2.6033669927114618, -2.716588650485514, -2.8299689782652266, -2.9434835293972257, -3.0571074121136768, -3.1708153025626418, -3.284581458306205, -3.3983797322767635, -3.5121835871814295, -3.6259661103440575, -3.7397000289739224, -3.8533577258496927, -3.9669112554068846, -4.080332360216582, -4.193592487842789, -4.3066628080653935, -4.419514230455329, -4.532117422288148, -4.644442826781849, -4.756460681644461, -4.868141037916503, -4.9794537790931885, -5.0903686405108175, -5.200855228981597, -5.310883042660767, -5.420421491129657, -5.529439915678046, -5.637907609768911, -5.745793839668446, -5.8530678652239985, -5.959698960772354, -6.065656436160612, -6.170909657861748, -6.275428070166713, -6.379181216434895, -6.482138760384516, -6.584270507404513, -6.685546425869305, -6.7859366684377775, -6.88541159331777, -6.983941785477266, -7.081498077783479, -7.178051572051032, -7.2735736599803715, -7.368036043967623, -7.461410757767144, -7.553670186988011, -7.644787089405842, -7.734734615071362, -7.823486326197288, -7.9110162168051685, -7.997298732114024, -8.082308787652714, -8.166021788078188, -8.24841364568196, -8.329460798567293, -8.409140228479904, -8.487429478275137, -8.564306669004896, -8.639750516607812, -8.71374034818652, -8.78625611785608, -8.857278422148063, -8.926788514954989, -8.994768322000299, -9.061200454819298, -9.126068224236976, -9.189355653328905, -9.251047489851947, -9.311129218131782, -9.369587070394845, -9.426408037532582, -9.4815798792865, -9.535091133842874, -9.586931126826506, -9.637089979683441, -9.68555861744299, -9.732328775849988, -9.777393007858722, -9.820744689480492, -9.862378024977327, -9.902288051394937, -9.940470642428538, -9.976922511615763, -10.011641214851448, -10.044625152219691, -10.075873569139151, -10.105386556818152, -10.133165052016768, -10.159210836113736, -10.183526533476522, -10.206115609133608, -10.226982365748656, -10.2461319398968, -10.263570297643966, -10.27930422943074, -10.293341344262984, -10.305690063211918, -10.31635961222714, -10.325360014266613, -10.332702080748282, -10.338397402328647, -10.342458339014197, -10.34489800961226, -10.34573028052846, -10.344969753918493, -10.342631755202705, -10.338732319952378, -10.333288180157382, -10.326316749885335, -10.317836110343055, -10.307864994351664, -10.296422770247228, -10.283529425219443, -10.26920554810139, -10.253472311623915, -10.236351454148753, -10.217865260895032, -10.198036544674254, -10.17688862614946, -10.15444531363461, -10.130730882450868, -10.105770053856764, -10.079587973569812, -10.052210189897455, -10.02366263149575, -9.993971584774501, -9.963163670968031, -9.93126582289107, -9.898305261399686, -9.86430947157741, -9.829306178667146, -9.793323323769677, -9.756389039329914, -9.718531624432263, -9.679779519926786, -9.640161283408018, -9.599705564068548, -9.558441077449636, -9.516396580111353, -9.473600844244842, -9.430082632249466, -9.385870671297702, -9.340993627910766, -9.29548008256797, -9.24935850437294, -9.202657225799813, -9.155404417542508, -9.107628063490283, -9.059355935852572, -9.010615570456242, -8.961434242238193, -8.911838940956192, -8.86185634714073, -8.811512808310493, -8.760834315473982, -8.709846479939516, -8.658574510455768, -8.607043190704692, -8.555276857168487, -8.503299377391981, -8.451134128661536, -8.398803977121258, -8.346331257347012, -8.293737752398362, -8.241044674368224, -8.188272645449604, -8.13544167953846, -8.082571164391203, -8.029679844355053, -7.9767858036888795, -7.923906450491755, -7.871058501255976, -7.818257966060714, -7.7655201344220455, -7.712859561814483, -7.660290056878594, -7.6078246693287745, -7.555475678574574, -7.503254583068439, -7.4511720903920695, -7.399238108093019, -7.347461735282465, -7.295851255004445, -7.244414127386212, -7.193156983578656, -7.142085620495041, -7.091204996355662, -7.040519227045238, -6.990031583289241, -6.939744488654514, -6.889659518378907, -6.8397773990338875, -6.790098009023257, -6.740620379920504, -6.6913426986464, -6.642262310487838, -6.593375722957994, -6.544678610497276, -6.496165820013618, -6.447831377259997, -6.39966849404622, -6.351669576281285, -6.303826232841837, -6.2561292852614505, -6.208568778234713, -6.161133990929321, -6.113813449098615, -6.0665949379862205, -6.019465516013725, -5.972411529241536, -5.925418626592335, -5.878471775825825, -5.831555280252663, -5.7846527961748455, -5.737747351039019, -5.6908213622884976, -5.643856656899113, -5.596834491583287, -5.549735573646058, -5.502540082476148, -5.455227691654493, -5.40777759166201, -5.360168513167784, -5.312378750878208, -5.264386187927086, -5.2161683207860285, -5.167702284674023, -5.1189648794444444, -5.069932595927254, -5.020581642703672, -4.970887973290062, -4.920827313707345, -4.8703751904117745, -4.819506958562505, -4.768197830600948, -4.716422905116556, -4.6641571959732735, -4.61137566167058, -4.558053234912709, -4.50416485235933, -4.449685484530726, -4.394590165840208, -4.3388540247263165, -4.28245231385714, -4.225360440378891, -4.16755399618074, -4.109008788147777, -4.0497008683738525, -3.9896065643059693, -3.9287025087918632, -3.866965670002368, -3.804373381200133, -3.740903370326329, -3.6765337893770025, -3.611243243540793, -3.545010820069881, -3.4778161168561006, -3.40963927068435, -3.3404609851355844, -3.270262558111895, -3.199025908956392, -3.1267336051408874, -3.053368888494635, -2.978915700947703, -2.9033587097628804, -2.8266833322303846, -2.7488757598000113, -2.669922981625776, -2.5898128074985363, -2.5085338901425227, -2.4260757468522085, -2.342428780446422, -2.2575842995171604, -2.171534537951091, -2.0842726737023045, -1.9957928467954744, -1.906090176539199, -1.8151607779299195, -1.7230017772274842, -1.6296113266840813, -1.5349886184089783, -1.4391338973522037, -1.3420484733910423, -1.24373473250397, -1.144196147017411, -1.0434372849114877, -0.9414638181717343, -0.838282530174549, -0.7339013220950003, -0.6283292183264301, -0.5215763709021658, -0.41365406291050655, -0.3045747108950356, -0.1943518662331991, -0.08300021548698666, 0.029464420279539327, 0.14302508722019924, 0.25766370146755135, 0.3733610529371285, 0.4900968101765919, 0.6078495262604091, 0.7265966457297051, 0.8463145125759866, 0.9669783792664914, 1.0885624168079528, 1.2110397258446128, 1.3343823487853694, 1.4585612829539754, 1.583546494755262, 1.7093069348494034, 1.835810554325292, 1.9630243218631453, 2.090914241875538, 2.219445373615101, 2.34858185123623, 2.4782869047971854, 2.6085228821881135, 2.7392512719695548, 2.8704327271051713, 3.0020270895715004, 3.133993415826708, 3.2662900031194466, 3.3988744166180713, 3.53170351733967, 3.66473349085752, 3.7979198767648183, 3.931217598871703, 4.064580996111949, 4.197963854134772, 4.331319437556655, 4.464600522847288, 4.597759431823088, 4.730748065721046, 4.86351793982508, 4.996020218616403, 5.128205751418833, 5.260025108509433, 5.3914286176642685, 5.522366401108609, 5.65278841284036, 5.782644476295081, 5.911884322320461, 6.040457627427765, 6.168314052287326, 6.295403280434818, 6.421675057154756, 6.547079228507284, 6.671565780464135, 6.795084878119327, 6.9175869049400145, 7.039022502022695, 7.159342607319804, 7.278498494801697, 7.396441813518818, 7.513124626528885, 7.628499449653879, 7.742519290031576, 7.8551376844265075, 7.966308737265197, 8.075987158360707, 8.184128300291611, 8.290688195400712, 8.395623592379014, 8.498891992400681, 8.600451684774983, 8.700261782081544, 8.798282254755511, 8.894473965089635, 8.988798700620645, 9.081219206867713, 9.171699219391266, 9.260203495140903, 9.34669784306162, 9.431149153928208, 9.513525429378124, 9.593795810113859, 9.671930603246393, 9.747901308751963, 9.821680645015165, 9.893242573431987, 9.962562322047196, 10.029616408201301, 10.094382660162976, 10.156840237723788, 10.216969651732825, 10.274752782549744, 10.33017289739556, 10.383214666581562, 10.433864178597473, 10.48210895404111, 10.527937958372647, 10.57134161347766, 10.612311808024073, 10.650841906599188, 10.686926757614064, 10.720562699963477, 10.751747568430844, 10.78048069782859, 10.806762925865455, 10.830596594733443, 10.851985551408205, 10.870935146657773, 10.887452232755718, 10.901545159895951, 10.91322377130758, 10.922499397069318, 10.929384846624243, 10.933894399996719, 10.936043797714618, 10.935850229441066, 10.933332321321153, 10.928510122050232, 10.921405087671591, 10.912040065112459, 10.900439274468523, 10.886628290048241, 10.870634020189433, 10.852484685861787, 10.832209798070078, 10.809840134073971, 10.785407712441529, 10.758945766954538, 10.73048871938497, 10.700072151162914, 10.66773277395747, 10.63350839919309, 10.597437906524956, 10.559561211297957, 10.519919231014917, 10.478553850840655, 10.435507888169434, 10.390825056284386, 10.344549927138306, 10.29672789328623, 10.247405129001036, 10.196628550604173, 10.144445776044455, 10.090905083758708, 10.036055370848718, 9.979946110609887, 9.922627309447439, 9.864149463217032, 9.804563513026999, 9.743920800540291, 9.682273022814663, 9.61967218672025, 9.556170562974263, 9.491820639832925, 9.42667507648131, 9.360786656162208, 9.294208239085407, 9.22699271515926, 9.159192956586699, 9.090861770368068, 9.022051850753524, 8.95281573168783, 8.883205739290625, 8.81327394441538, 8.743072115330262, 8.672651670564306, 8.602063631962231, 8.531358577991192, 8.460586597342793, 8.389797242873492, 8.31903948592642, 8.248361671077463, 8.177811471348246, 8.107435843928323, 8.037280986448712, 7.9673922938484045, 7.897814315875275, 7.828590715262258, 7.759764226619268, 7.691376616080868, 7.623468641749072, 7.5560800149701866, 7.489249362483946, 7.423014189482531, 7.357410843616417, 7.292474479983277, 7.228239027135354, 7.1647371541400116, 7.102000238727275, 7.0400583365573715, 6.978940151640332, 6.918673007938847, 6.859282822184577, 6.800794077937142, 6.743229800913996, 6.68661153561834, 6.630959323291161, 6.576291681212377, 6.522625583374912, 6.469976442554433, 6.418358093796188, 6.367782779339304, 6.31826113499759, 6.269802178014638, 6.222413296409805, 6.17610023983028, 6.130867111923216, 6.086716364240486, 6.043648791687345, 6.0016635295249, 5.960758051934876, 5.920928172153827, 5.882168044182504, 5.844470166074709, 5.807825384808474, 5.772222902741094, 5.737650285647962, 5.704093472343827, 5.671536785883562, 5.6399629463381755, 5.609353085140203, 5.579686760991327, 5.550941977323491, 5.523095201303375, 5.496121384368668, 5.469993984283102, 5.4446849886957525, 5.420164940188773, 5.396402962796191, 5.37336678997507, 5.3510227940089115, 5.329336016821781, 5.30827020218028, 5.287787829259118, 5.26785014754472, 5.2484172130499385, 5.229447925811702, 5.210900068642074, 5.192730347101995, 5.17489443066572, 5.157346995042734, 5.140041765622765, 5.122931562008336, 5.10596834359817, 5.089103256183654, 5.072286679519485, 5.055468275828595, 5.038597039200407, 5.021621345840529, 5.0044890051290425, 4.987147311443584, 4.969543096702615, 4.9516227835833995, 4.933332439368401, 4.914617830373074, 4.895424476907289, 4.875697708721963, 4.855382720891809, 4.834424630084545, 4.812768531166325, 4.790359554092671, 4.767142921033679, 4.743064003681909, 4.718068380690924, 4.692101895192162, 4.66511071233753, 4.637041376814851, 4.607840870283129, 4.577456668674436, 4.545836799309146, 4.5129298977711745, 4.478685264489892, 4.4430529209754335, 4.405983665654244, 4.36742912925174, 4.327341829669365, 4.285675226303332, 4.24238377375282, 4.197422974865617, 4.150749433069659, 4.102320903939326, 4.05209634594584, 4.000035970341659, 3.946101290129342, 3.8902551680659654, 3.832461863654893, 3.772687079077378, 3.71089800401729, 3.6470633593330235, 3.5811534395315645, 3.513140154000544, 3.4429970669550864, 3.370699436057243, 3.29622424966683, 3.219550262683585, 3.1406580309416343, 3.059529944118472, 2.9761502571217964, 2.890505119918813, 2.8025826057738885, 2.7123727378617035, 2.6198675142244667, 2.5250609310430416, 2.4279490041933367, 2.3285297890606613, 2.2268033985863003, 2.1227720195220074, 2.016439926869658, 1.9078134964848836, 1.7969012158250526, 1.6837136928235923, 1.568263662874276, 1.450565993910725, 1.3306376895680794, 1.2084978904154515, 1.084167873249493, 0.957671048441135, 0.8290329553292759, 0.6982812556569626, 0.5654457250473572, 0.43055824251854546, 0.29365277803803824, 0.15476537811958613, 0.013934149466708678, -0.12880075933083823, -0.27339717803967556, -0.4198109369547032, -0.5679958909887011, -0.7179039456662692, -0.8694850850069475, -1.0226874012805889, -1.177457126616284, -1.3337386664443802, -1.4914746347493921, -1.6506058911098629, -1.8110715794995116, -1.972809168822312, -2.1357544951524496, -2.2998418056484415, -2.4650038041090614, -2.631171698137088, -2.798275247875295, -2.9662428162775343, -3.135001420876215, -3.3044767870059584, -3.4745934024417426, -3.6452745734083765, -3.81644248191673, -3.9880182443807684, -4.159921971468075, -4.332072829135237, -4.504389100798229, -4.676788250586616, -4.849186987629303, -5.021501331318337, -5.193646677496183, -5.36553786551087, -5.5370892460822985, -5.708214749922152, -5.878827957048844, -6.048842166738094, -6.218170468048917, -6.386725810864032, -6.554421077382977, -6.721169154005559, -6.886883003542663, -7.051475737690883, -7.214860689706944, -7.376951487217433, -7.53766212509899, -7.696907038363763, -7.854601174984654, -8.010660068594701, -8.164999910994766, -8.317537624403581, -8.468190933384241, -8.616878436381157, -8.763519676801652, -8.908035213576497, -9.050346691133855, -9.190376908721426, -9.328049889011861, -9.463290945926932, -9.596026751616355, -9.726185402527692, -9.85369648450435, -9.978491136849224, -10.100502115292329, -10.219663853801432, -10.33591252517552, -10.449186100361784, -10.55942440643775, -10.666569183201101, -10.770564138310826, -10.871355000924336, -10.968889573776442, -11.06311778364712, -11.1539917301664, -11.241465732905873, -11.325496376707742, -11.40604255520373, -11.483065512477527, -11.556528882826072, -11.626398728576397, -11.692643575916412, -11.755234448699612, -11.81414490018536, -11.869351042678115, -11.920831575030743, -11.968567807978783, -12.012543687274487, -12.052745814591157, -12.089163466170362, -12.1217886091864, -12.15061591580448, -12.175642774910964, -12.196869301496102, -12.21429834367169, -12.227935487308233, -12.23778905827816, -12.243870122293862, -12.246192482331399, -12.244772673632877, -12.239629956282675, -12.230786305354844, -12.218266398631224, -12.202097601892001, -12.182309951782615, -12.15893613626318, -12.132011472648724, -12.101573883250854, -12.067663868633568, -12.030324478498246, -11.98960128021492, -11.945542325019304, -11.898198111897067, -11.847621549179163, -11.793867913874058, -11.736994808764988, -11.677062117302384, -11.614131956323805, -11.548268626635767, -11.479538561493962, -11.408010273020334, -11.333754296597625, -11.25684313328384, -11.177351190291192, -11.095354719575866, -11.01093175458699, -10.92416204522489, -10.835126991060706, -10.74390957287104, -10.65059428254319, -10.555267051408146, -10.45801517706018, -10.358927248723473, -10.258093071227792, -10.155603587656678, -10.05155080073312, -9.946027693009059, -9.839128145926402, -9.730946857818525, -9.621579260922486, -9.511121437473317, -9.399670034952871, -9.287322180566802, -9.174175395024173, -9.06032750569515, -8.945876559223114, -8.830920733668261, -8.715558250260548, -8.599887284840468, -8.48400587906672, -8.36801185147039, -8.252002708435683, -8.136075555187645, -8.020327006867607, -7.904853099777321, -7.789749202872942, -7.675109929590075, -7.561029050081119, -7.447599403946139, -7.3349128135382715, -7.223059997924567, -7.11213048758283, -7.002212539914661, -6.893393055654519, -6.785757496254121, -6.679389802320847, -6.574372313188264, -6.470785687696118, -6.368708826256327, -6.268218794280683, -6.1693907470450045, -6.07229785606347, -5.977011237045797, -5.88359987950876, -5.792130578112314, -5.702667865789349, -5.615273948736656, -5.530008643333332, -5.446929315051358, -5.366090819421446, -5.287545445115754, -5.21134285920728, -5.13753005466408, -5.066151300134576, -4.997248092078453, -4.930859109295625, -4.867020169903874, -4.805764190813655, -4.747121149746523, -4.691118049841552, -4.637778886891797, -4.587124619250819, -4.539173140446831, -4.493939254539883, -4.451434654254989, -4.411667901921836, -4.374644413249209, -4.340366443959809, -4.308833079308677, -4.2800402265058475, -4.2539806100613795, -4.2306437700682, -4.210016063435735, -4.192080668084552, -4.176817590109626, -4.164203673917198, -4.154212615337467, -4.146814977712665, -4.141978210957384, -4.139666673585298, -4.1398416576936405, -4.1424614168942036, -4.147481197176742, -4.154853270688072, -4.164526972407357, -4.1764487396953935, -4.190562154693, -4.206807989540907, -4.225124254390895, -4.245446248175236, -4.267706612098888, -4.2918353858162295, -4.31776006625158, -4.345405669020107, -4.374694792403274, -4.405547683830409, -4.4378823088154915, -4.471614422295899, -4.506657642317348, -4.542923526006987, -4.580321647774252, -4.618759679676819, -4.658143473886788, -4.69837714719004, -4.739363167449615, -4.781002441961843, -4.823194407632048, -4.865837122894565, -4.908827361300027, -4.952060706691028, -4.995431649885444, -5.038833686785085, -5.082159417825674, -5.125300648682573, -5.168148492145254, -5.210593471072026, -5.25252562233524, -5.2938346016658935, -5.334409789305427, -5.374140396371308, -5.412915571842073, -5.45062451006647, -5.487156558700541, -5.522401326975663, -5.5562487941999015, -5.588589418394416, -5.619314244966119, -5.648315015317388, -5.675484275293268, -5.700715483366334, -5.723903118459248, -5.744942787304947, -5.763731331244422, -5.780166932362148, -5.794149218859424, -5.805579369566189, -5.814360217492217, -5.820396352319115, -5.823594221735092, -5.823862231515073, -5.821110844249599, -5.815252676626628, -5.806202595171439, -5.793877810350731, -5.778197968948188, -5.759085244619953, -5.736464426539731, -5.710263006044626, -5.680411261194252, -5.64684233915722, -5.6094923363407165, -5.5683003761805985, -5.5232086845112285, -5.474162662436128, -5.42111095662249, -5.364005526944633, -5.302801711403535, -5.237458288251825, -5.16793753525583, -5.094205286028574, -5.016230983370076, -4.933987729553711, -4.847452333499936, -4.756605354781256, -4.661431144405003, -4.561917882323138, -4.458057611621113, -4.349846269340639, -4.237283713894041, -4.1203737490308585, -3.999124144320276, -3.8735466521160222, -3.743657020973415, -3.6094750054913023, -3.471024372554849, -3.3283329039582323, -3.18143239538952, -3.03035865176329]}, {\"hovertemplate\": \"x: %{x} <br>y: %{y}\", \"name\": \"Predicted\", \"type\": \"scatter\", \"x\": [-2.0, -1.98998998998999, -1.97997997997998, -1.96996996996997, -1.95995995995996, -1.94994994994995, -1.93993993993994, -1.92992992992993, -1.91991991991992, -1.90990990990991, -1.8998998998999, -1.88988988988989, -1.87987987987988, -1.86986986986987, -1.85985985985986, -1.84984984984985, -1.83983983983984, -1.82982982982983, -1.8198198198198199, -1.8098098098098099, -1.7997997997997999, -1.7897897897897899, -1.7797797797797799, -1.7697697697697699, -1.7597597597597598, -1.7497497497497498, -1.7397397397397398, -1.7297297297297298, -1.7197197197197198, -1.7097097097097098, -1.6996996996996998, -1.6896896896896898, -1.6796796796796798, -1.6696696696696698, -1.6596596596596598, -1.6496496496496498, -1.6396396396396398, -1.6296296296296298, -1.6196196196196198, -1.6096096096096097, -1.5995995995995997, -1.5895895895895895, -1.5795795795795795, -1.5695695695695695, -1.5595595595595595, -1.5495495495495495, -1.5395395395395395, -1.5295295295295295, -1.5195195195195195, -1.5095095095095095, -1.4994994994994995, -1.4894894894894894, -1.4794794794794794, -1.4694694694694694, -1.4594594594594594, -1.4494494494494494, -1.4394394394394394, -1.4294294294294294, -1.4194194194194194, -1.4094094094094094, -1.3993993993993994, -1.3893893893893894, -1.3793793793793794, -1.3693693693693694, -1.3593593593593594, -1.3493493493493494, -1.3393393393393394, -1.3293293293293293, -1.3193193193193193, -1.3093093093093093, -1.2992992992992993, -1.2892892892892893, -1.2792792792792793, -1.2692692692692693, -1.2592592592592593, -1.2492492492492493, -1.2392392392392393, -1.2292292292292293, -1.2192192192192193, -1.2092092092092093, -1.1991991991991993, -1.189189189189189, -1.179179179179179, -1.169169169169169, -1.159159159159159, -1.149149149149149, -1.139139139139139, -1.129129129129129, -1.119119119119119, -1.109109109109109, -1.099099099099099, -1.089089089089089, -1.079079079079079, -1.069069069069069, -1.059059059059059, -1.049049049049049, -1.039039039039039, -1.029029029029029, -1.019019019019019, -1.009009009009009, -0.9989989989989989, -0.9889889889889889, -0.9789789789789789, -0.9689689689689689, -0.9589589589589589, -0.9489489489489489, -0.9389389389389389, -0.9289289289289289, -0.9189189189189189, -0.9089089089089089, -0.8988988988988988, -0.8888888888888888, -0.8788788788788788, -0.8688688688688688, -0.8588588588588588, -0.8488488488488488, -0.8388388388388388, -0.8288288288288288, -0.8188188188188188, -0.8088088088088088, -0.7987987987987988, -0.7887887887887888, -0.7787787787787788, -0.7687687687687688, -0.7587587587587588, -0.7487487487487487, -0.7387387387387387, -0.7287287287287287, -0.7187187187187187, -0.7087087087087087, -0.6986986986986987, -0.6886886886886887, -0.6786786786786787, -0.6686686686686687, -0.6586586586586587, -0.6486486486486487, -0.6386386386386387, -0.6286286286286287, -0.6186186186186187, -0.6086086086086087, -0.5985985985985987, -0.5885885885885886, -0.5785785785785786, -0.5685685685685686, -0.5585585585585586, -0.5485485485485486, -0.5385385385385386, -0.5285285285285286, -0.5185185185185186, -0.5085085085085086, -0.4984984984984986, -0.4884884884884886, -0.4784784784784786, -0.46846846846846857, -0.45845845845845856, -0.44844844844844856, -0.43843843843843855, -0.42842842842842854, -0.41841841841841854, -0.40840840840840853, -0.3983983983983985, -0.3883883883883883, -0.3783783783783783, -0.3683683683683683, -0.3583583583583583, -0.34834834834834827, -0.33833833833833826, -0.32832832832832826, -0.31831831831831825, -0.30830830830830824, -0.29829829829829824, -0.28828828828828823, -0.2782782782782782, -0.2682682682682682, -0.2582582582582582, -0.2482482482482482, -0.2382382382382382, -0.2282282282282282, -0.21821821821821819, -0.20820820820820818, -0.19819819819819817, -0.18818818818818817, -0.17817817817817816, -0.16816816816816815, -0.15815815815815815, -0.14814814814814814, -0.13813813813813813, -0.12812812812812813, -0.11811811811811812, -0.10810810810810811, -0.09809809809809811, -0.0880880880880881, -0.0780780780780781, -0.06806806806806809, -0.05805805805805808, -0.048048048048048075, -0.03803803803803807, -0.028028028028028062, -0.018018018018018056, -0.00800800800800805, 0.002002002002002179, 0.012012012012012185, 0.022022022022022192, 0.0320320320320322, 0.042042042042042205, 0.05205205205205221, 0.06206206206206222, 0.07207207207207222, 0.08208208208208223, 0.09209209209209224, 0.10210210210210224, 0.11211211211211225, 0.12212212212212226, 0.13213213213213226, 0.14214214214214227, 0.15215215215215228, 0.16216216216216228, 0.1721721721721723, 0.1821821821821823, 0.1921921921921923, 0.2022022022022023, 0.21221221221221231, 0.22222222222222232, 0.23223223223223233, 0.24224224224224233, 0.25225225225225234, 0.26226226226226235, 0.27227227227227235, 0.28228228228228236, 0.29229229229229237, 0.3023023023023024, 0.3123123123123124, 0.3223223223223224, 0.3323323323323324, 0.3423423423423424, 0.3523523523523524, 0.3623623623623624, 0.3723723723723724, 0.3823823823823824, 0.39239239239239243, 0.40240240240240244, 0.41241241241241244, 0.42242242242242245, 0.43243243243243246, 0.44244244244244246, 0.45245245245245247, 0.4624624624624625, 0.4724724724724725, 0.4824824824824825, 0.4924924924924925, 0.5025025025025025, 0.5125125125125125, 0.5225225225225225, 0.5325325325325325, 0.5425425425425425, 0.5525525525525525, 0.5625625625625625, 0.5725725725725725, 0.5825825825825826, 0.5925925925925926, 0.6026026026026026, 0.6126126126126126, 0.6226226226226226, 0.6326326326326326, 0.6426426426426426, 0.6526526526526526, 0.6626626626626626, 0.6726726726726726, 0.6826826826826826, 0.6926926926926926, 0.7027027027027026, 0.7127127127127126, 0.7227227227227226, 0.7327327327327327, 0.7427427427427427, 0.7527527527527527, 0.7627627627627627, 0.7727727727727727, 0.7827827827827827, 0.7927927927927927, 0.8028028028028027, 0.8128128128128127, 0.8228228228228227, 0.8328328328328327, 0.8428428428428427, 0.8528528528528527, 0.8628628628628627, 0.8728728728728727, 0.8828828828828827, 0.8928928928928928, 0.9029029029029028, 0.9129129129129128, 0.9229229229229228, 0.9329329329329328, 0.9429429429429428, 0.9529529529529528, 0.9629629629629628, 0.9729729729729728, 0.9829829829829828, 0.9929929929929928, 1.0030030030030028, 1.0130130130130128, 1.0230230230230228, 1.0330330330330328, 1.0430430430430429, 1.0530530530530529, 1.0630630630630629, 1.0730730730730729, 1.0830830830830829, 1.0930930930930929, 1.1031031031031029, 1.113113113113113, 1.123123123123123, 1.133133133133133, 1.143143143143143, 1.153153153153153, 1.163163163163163, 1.173173173173173, 1.183183183183183, 1.193193193193193, 1.203203203203203, 1.2132132132132134, 1.2232232232232234, 1.2332332332332334, 1.2432432432432434, 1.2532532532532534, 1.2632632632632634, 1.2732732732732734, 1.2832832832832834, 1.2932932932932935, 1.3033033033033035, 1.3133133133133135, 1.3233233233233235, 1.3333333333333335, 1.3433433433433435, 1.3533533533533535, 1.3633633633633635, 1.3733733733733735, 1.3833833833833835, 1.3933933933933935, 1.4034034034034035, 1.4134134134134135, 1.4234234234234235, 1.4334334334334335, 1.4434434434434436, 1.4534534534534536, 1.4634634634634636, 1.4734734734734736, 1.4834834834834836, 1.4934934934934936, 1.5035035035035036, 1.5135135135135136, 1.5235235235235236, 1.5335335335335336, 1.5435435435435436, 1.5535535535535536, 1.5635635635635636, 1.5735735735735736, 1.5835835835835836, 1.5935935935935936, 1.6036036036036037, 1.6136136136136137, 1.6236236236236237, 1.6336336336336337, 1.6436436436436437, 1.6536536536536537, 1.6636636636636637, 1.6736736736736737, 1.6836836836836837, 1.6936936936936937, 1.7037037037037037, 1.7137137137137137, 1.7237237237237237, 1.7337337337337337, 1.7437437437437437, 1.7537537537537538, 1.7637637637637638, 1.7737737737737738, 1.7837837837837838, 1.7937937937937938, 1.8038038038038038, 1.8138138138138138, 1.8238238238238238, 1.8338338338338338, 1.8438438438438438, 1.8538538538538538, 1.8638638638638638, 1.8738738738738738, 1.8838838838838838, 1.8938938938938938, 1.9039039039039038, 1.9139139139139139, 1.9239239239239239, 1.9339339339339339, 1.9439439439439439, 1.9539539539539539, 1.9639639639639639, 1.973973973973974, 1.983983983983984, 1.993993993993994, 2.0040040040040044, 2.0140140140140144, 2.0240240240240244, 2.0340340340340344, 2.0440440440440444, 2.0540540540540544, 2.0640640640640644, 2.0740740740740744, 2.0840840840840844, 2.0940940940940944, 2.1041041041041044, 2.1141141141141144, 2.1241241241241244, 2.1341341341341344, 2.1441441441441444, 2.1541541541541545, 2.1641641641641645, 2.1741741741741745, 2.1841841841841845, 2.1941941941941945, 2.2042042042042045, 2.2142142142142145, 2.2242242242242245, 2.2342342342342345, 2.2442442442442445, 2.2542542542542545, 2.2642642642642645, 2.2742742742742745, 2.2842842842842845, 2.2942942942942945, 2.3043043043043046, 2.3143143143143146, 2.3243243243243246, 2.3343343343343346, 2.3443443443443446, 2.3543543543543546, 2.3643643643643646, 2.3743743743743746, 2.3843843843843846, 2.3943943943943946, 2.4044044044044046, 2.4144144144144146, 2.4244244244244246, 2.4344344344344346, 2.4444444444444446, 2.4544544544544546, 2.4644644644644647, 2.4744744744744747, 2.4844844844844847, 2.4944944944944947, 2.5045045045045047, 2.5145145145145147, 2.5245245245245247, 2.5345345345345347, 2.5445445445445447, 2.5545545545545547, 2.5645645645645647, 2.5745745745745747, 2.5845845845845847, 2.5945945945945947, 2.6046046046046047, 2.6146146146146148, 2.6246246246246248, 2.6346346346346348, 2.6446446446446448, 2.6546546546546548, 2.664664664664665, 2.674674674674675, 2.684684684684685, 2.694694694694695, 2.704704704704705, 2.714714714714715, 2.724724724724725, 2.734734734734735, 2.744744744744745, 2.754754754754755, 2.764764764764765, 2.774774774774775, 2.784784784784785, 2.794794794794795, 2.804804804804805, 2.814814814814815, 2.824824824824825, 2.834834834834835, 2.844844844844845, 2.854854854854855, 2.864864864864865, 2.874874874874875, 2.884884884884885, 2.894894894894895, 2.904904904904905, 2.914914914914915, 2.924924924924925, 2.934934934934935, 2.944944944944945, 2.954954954954955, 2.964964964964965, 2.974974974974975, 2.984984984984985, 2.994994994994995, 3.005005005005005, 3.015015015015015, 3.025025025025025, 3.035035035035035, 3.045045045045045, 3.055055055055055, 3.065065065065065, 3.075075075075075, 3.085085085085085, 3.095095095095095, 3.105105105105105, 3.115115115115115, 3.125125125125125, 3.135135135135135, 3.145145145145145, 3.155155155155155, 3.165165165165165, 3.175175175175175, 3.185185185185185, 3.195195195195195, 3.205205205205205, 3.215215215215215, 3.225225225225225, 3.235235235235235, 3.245245245245245, 3.255255255255255, 3.265265265265265, 3.275275275275275, 3.285285285285285, 3.295295295295295, 3.305305305305305, 3.315315315315315, 3.325325325325325, 3.335335335335335, 3.3453453453453452, 3.3553553553553552, 3.3653653653653652, 3.3753753753753752, 3.3853853853853852, 3.3953953953953953, 3.4054054054054053, 3.4154154154154153, 3.4254254254254253, 3.4354354354354353, 3.4454454454454453, 3.4554554554554553, 3.4654654654654653, 3.4754754754754753, 3.4854854854854853, 3.4954954954954953, 3.5055055055055053, 3.5155155155155153, 3.5255255255255253, 3.5355355355355353, 3.5455455455455454, 3.5555555555555554, 3.5655655655655654, 3.5755755755755754, 3.5855855855855854, 3.5955955955955954, 3.6056056056056054, 3.6156156156156154, 3.6256256256256254, 3.6356356356356354, 3.6456456456456454, 3.6556556556556554, 3.6656656656656654, 3.6756756756756754, 3.6856856856856854, 3.6956956956956954, 3.7057057057057055, 3.7157157157157155, 3.7257257257257255, 3.7357357357357355, 3.7457457457457455, 3.7557557557557555, 3.7657657657657655, 3.7757757757757755, 3.7857857857857855, 3.7957957957957955, 3.8058058058058055, 3.8158158158158155, 3.8258258258258255, 3.8358358358358355, 3.8458458458458455, 3.8558558558558556, 3.8658658658658656, 3.8758758758758756, 3.8858858858858856, 3.8958958958958956, 3.9059059059059056, 3.9159159159159156, 3.9259259259259256, 3.9359359359359356, 3.9459459459459456, 3.9559559559559556, 3.9659659659659656, 3.9759759759759756, 3.9859859859859856, 3.9959959959959956, 4.006006006006006, 4.016016016016016, 4.026026026026026, 4.036036036036036, 4.046046046046046, 4.056056056056056, 4.066066066066066, 4.076076076076076, 4.086086086086086, 4.096096096096096, 4.106106106106106, 4.116116116116116, 4.126126126126126, 4.136136136136136, 4.146146146146146, 4.156156156156156, 4.166166166166166, 4.176176176176176, 4.186186186186186, 4.196196196196196, 4.206206206206206, 4.216216216216216, 4.226226226226226, 4.236236236236236, 4.246246246246246, 4.256256256256256, 4.266266266266266, 4.276276276276276, 4.286286286286286, 4.296296296296296, 4.306306306306306, 4.316316316316316, 4.326326326326326, 4.336336336336336, 4.346346346346346, 4.356356356356356, 4.366366366366366, 4.376376376376376, 4.386386386386386, 4.396396396396396, 4.406406406406406, 4.416416416416417, 4.426426426426427, 4.436436436436437, 4.446446446446447, 4.456456456456457, 4.466466466466467, 4.476476476476477, 4.486486486486487, 4.496496496496497, 4.506506506506507, 4.516516516516517, 4.526526526526527, 4.536536536536537, 4.546546546546547, 4.556556556556557, 4.566566566566567, 4.576576576576577, 4.586586586586587, 4.596596596596597, 4.606606606606607, 4.616616616616617, 4.626626626626627, 4.636636636636637, 4.646646646646647, 4.656656656656657, 4.666666666666667, 4.676676676676677, 4.686686686686687, 4.696696696696697, 4.706706706706707, 4.716716716716717, 4.726726726726727, 4.736736736736737, 4.746746746746747, 4.756756756756757, 4.766766766766767, 4.776776776776777, 4.786786786786787, 4.796796796796797, 4.806806806806807, 4.816816816816817, 4.826826826826827, 4.836836836836837, 4.846846846846847, 4.856856856856857, 4.866866866866867, 4.876876876876877, 4.886886886886887, 4.896896896896897, 4.906906906906907, 4.916916916916917, 4.926926926926927, 4.936936936936937, 4.946946946946947, 4.956956956956957, 4.966966966966967, 4.976976976976977, 4.986986986986987, 4.996996996996997, 5.007007007007007, 5.017017017017017, 5.027027027027027, 5.037037037037037, 5.047047047047047, 5.057057057057057, 5.067067067067067, 5.077077077077077, 5.087087087087087, 5.097097097097097, 5.107107107107107, 5.117117117117117, 5.127127127127127, 5.137137137137137, 5.147147147147147, 5.157157157157157, 5.167167167167167, 5.177177177177177, 5.187187187187187, 5.197197197197197, 5.207207207207207, 5.217217217217217, 5.227227227227227, 5.237237237237237, 5.247247247247247, 5.257257257257257, 5.267267267267267, 5.277277277277277, 5.287287287287287, 5.297297297297297, 5.307307307307307, 5.317317317317317, 5.327327327327327, 5.337337337337337, 5.347347347347347, 5.357357357357357, 5.367367367367367, 5.377377377377377, 5.387387387387387, 5.397397397397397, 5.407407407407407, 5.4174174174174174, 5.4274274274274275, 5.4374374374374375, 5.4474474474474475, 5.4574574574574575, 5.4674674674674675, 5.4774774774774775, 5.4874874874874875, 5.4974974974974975, 5.5075075075075075, 5.5175175175175175, 5.5275275275275275, 5.5375375375375375, 5.5475475475475475, 5.5575575575575575, 5.5675675675675675, 5.5775775775775776, 5.587587587587588, 5.597597597597598, 5.607607607607608, 5.617617617617618, 5.627627627627628, 5.637637637637638, 5.647647647647648, 5.657657657657658, 5.667667667667668, 5.677677677677678, 5.687687687687688, 5.697697697697698, 5.707707707707708, 5.717717717717718, 5.727727727727728, 5.737737737737738, 5.747747747747748, 5.757757757757758, 5.767767767767768, 5.777777777777778, 5.787787787787788, 5.797797797797798, 5.807807807807808, 5.817817817817818, 5.827827827827828, 5.837837837837838, 5.847847847847848, 5.857857857857858, 5.867867867867868, 5.877877877877878, 5.887887887887888, 5.897897897897898, 5.907907907907908, 5.917917917917918, 5.927927927927928, 5.937937937937938, 5.947947947947948, 5.957957957957958, 5.967967967967968, 5.977977977977978, 5.987987987987988, 5.997997997997998, 6.008008008008009, 6.018018018018019, 6.028028028028029, 6.038038038038039, 6.048048048048049, 6.058058058058059, 6.068068068068069, 6.078078078078079, 6.088088088088089, 6.098098098098099, 6.108108108108109, 6.118118118118119, 6.128128128128129, 6.138138138138139, 6.148148148148149, 6.158158158158159, 6.168168168168169, 6.178178178178179, 6.188188188188189, 6.198198198198199, 6.208208208208209, 6.218218218218219, 6.228228228228229, 6.238238238238239, 6.248248248248249, 6.258258258258259, 6.268268268268269, 6.278278278278279, 6.288288288288289, 6.298298298298299, 6.308308308308309, 6.318318318318319, 6.328328328328329, 6.338338338338339, 6.348348348348349, 6.358358358358359, 6.368368368368369, 6.378378378378379, 6.388388388388389, 6.398398398398399, 6.408408408408409, 6.418418418418419, 6.428428428428429, 6.438438438438439, 6.448448448448449, 6.458458458458459, 6.468468468468469, 6.478478478478479, 6.488488488488489, 6.498498498498499, 6.508508508508509, 6.518518518518519, 6.528528528528529, 6.538538538538539, 6.548548548548549, 6.558558558558559, 6.568568568568569, 6.578578578578579, 6.588588588588589, 6.598598598598599, 6.608608608608609, 6.618618618618619, 6.628628628628629, 6.638638638638639, 6.648648648648649, 6.658658658658659, 6.668668668668669, 6.678678678678679, 6.688688688688689, 6.698698698698699, 6.708708708708709, 6.718718718718719, 6.728728728728729, 6.738738738738739, 6.748748748748749, 6.758758758758759, 6.768768768768769, 6.778778778778779, 6.788788788788789, 6.798798798798799, 6.808808808808809, 6.818818818818819, 6.828828828828829, 6.838838838838839, 6.848848848848849, 6.858858858858859, 6.868868868868869, 6.878878878878879, 6.888888888888889, 6.898898898898899, 6.908908908908909, 6.918918918918919, 6.928928928928929, 6.938938938938939, 6.948948948948949, 6.958958958958959, 6.968968968968969, 6.978978978978979, 6.988988988988989, 6.998998998998999, 7.009009009009009, 7.019019019019019, 7.029029029029029, 7.039039039039039, 7.049049049049049, 7.059059059059059, 7.069069069069069, 7.079079079079079, 7.089089089089089, 7.099099099099099, 7.109109109109109, 7.119119119119119, 7.129129129129129, 7.1391391391391394, 7.1491491491491495, 7.1591591591591595, 7.1691691691691695, 7.1791791791791795, 7.1891891891891895, 7.1991991991991995, 7.2092092092092095, 7.2192192192192195, 7.2292292292292295, 7.2392392392392395, 7.2492492492492495, 7.2592592592592595, 7.2692692692692695, 7.2792792792792795, 7.2892892892892895, 7.2992992992992995, 7.3093093093093096, 7.31931931931932, 7.32932932932933, 7.33933933933934, 7.34934934934935, 7.35935935935936, 7.36936936936937, 7.37937937937938, 7.38938938938939, 7.3993993993994, 7.40940940940941, 7.41941941941942, 7.42942942942943, 7.43943943943944, 7.44944944944945, 7.45945945945946, 7.46946946946947, 7.47947947947948, 7.48948948948949, 7.4994994994995, 7.50950950950951, 7.51951951951952, 7.52952952952953, 7.53953953953954, 7.54954954954955, 7.55955955955956, 7.56956956956957, 7.57957957957958, 7.58958958958959, 7.5995995995996, 7.60960960960961, 7.61961961961962, 7.62962962962963, 7.63963963963964, 7.64964964964965, 7.65965965965966, 7.66966966966967, 7.67967967967968, 7.68968968968969, 7.6996996996997, 7.70970970970971, 7.71971971971972, 7.72972972972973, 7.73973973973974, 7.74974974974975, 7.75975975975976, 7.76976976976977, 7.77977977977978, 7.78978978978979, 7.7997997997998, 7.80980980980981, 7.81981981981982, 7.82982982982983, 7.83983983983984, 7.84984984984985, 7.85985985985986, 7.86986986986987, 7.87987987987988, 7.88988988988989, 7.8998998998999, 7.90990990990991, 7.91991991991992, 7.92992992992993, 7.93993993993994, 7.94994994994995, 7.95995995995996, 7.96996996996997, 7.97997997997998, 7.98998998998999, 8.0], \"y\": [4.3259758949279785, 4.260013580322266, 4.192658424377441, 4.123969078063965, 4.054020881652832, 3.982867479324341, 3.9105796813964844, 3.8372178077697754, 3.7628514766693115, 3.687532901763916, 3.611318826675415, 3.5342600345611572, 3.456408977508545, 3.3777992725372314, 3.298475980758667, 3.2184605598449707, 3.137789487838745, 3.056472063064575, 2.9745371341705322, 2.8919825553894043, 2.808828592300415, 2.725067377090454, 2.6407108306884766, 2.5557470321655273, 2.470181941986084, 2.383999824523926, 2.2972044944763184, 2.209777355194092, 2.121723175048828, 2.0330212116241455, 1.943672776222229, 1.8536678552627563, 1.7630019187927246, 1.6716586351394653, 1.5796436071395874, 1.4869478940963745, 1.393570065498352, 1.299498438835144, 1.2047410011291504, 1.1092922687530518, 1.013156771659851, 0.916324257850647, 0.8188059329986572, 0.7206066250801086, 0.6217224597930908, 0.5221624374389648, 0.42192885279655457, 0.3210415840148926, 0.21949535608291626, 0.1173059344291687, 0.01447838544845581, -0.08896148204803467, -0.19301974773406982, -0.2976658344268799, -0.4029008150100708, -0.5086953639984131, -0.6150472164154053, -0.7219245433807373, -0.8293290138244629, -0.9372262954711914, -1.0456156730651855, -1.154462456703186, -1.2637665271759033, -1.3734952211380005, -1.4836456775665283, -1.594184398651123, -1.705108880996704, -1.8163857460021973, -1.928011417388916, -2.0399575233459473, -2.152212142944336, -2.264744997024536, -2.377551317214966, -2.490600824356079, -2.6038718223571777, -2.717339515686035, -2.830993175506592, -2.94480037689209, -3.0587315559387207, -3.172762870788574, -3.2868735790252686, -3.401028871536255, -3.515197992324829, -3.629350423812866, -3.7434613704681396, -3.857490301132202, -3.9714062213897705, -4.085171699523926, -4.198763847351074, -4.312135696411133, -4.425257682800293, -4.538090705871582, -4.650606155395508, -4.762765884399414, -4.874539375305176, -4.98588752746582, -5.096784591674805, -5.207200050354004, -5.317096710205078, -5.426455497741699, -5.535243034362793, -5.643438339233398, -5.751006126403809, -5.857937812805176, -5.96419620513916, -6.069772720336914, -6.174640655517578, -6.278779983520508, -6.382171630859375, -6.484797477722168, -6.586641311645508, -6.687677383422852, -6.78788948059082, -6.88725471496582, -6.985748291015625, -7.083349227905273, -7.180026054382324, -7.275753974914551, -7.3704986572265625, -7.464228630065918, -7.55690860748291, -7.648499488830566, -7.738964080810547, -7.828259468078613, -7.91634464263916, -8.00317668914795, -8.088715553283691, -8.1729154586792, -8.255739212036133, -8.337146759033203, -8.417098045349121, -8.495567321777344, -8.57252025604248, -8.647927284240723, -8.721769332885742, -8.794028282165527, -8.8646879196167, -8.933736801147461, -9.001172065734863, -9.066986083984375, -9.131179809570312, -9.193756103515625, -9.254717826843262, -9.314069747924805, -9.371818542480469, -9.427970886230469, -9.482527732849121, -9.535499572753906, -9.586883544921875, -9.636687278747559, -9.684905052185059, -9.731534957885742, -9.77657413482666, -9.820012092590332, -9.861842155456543, -9.902050971984863, -9.940625190734863, -9.977550506591797, -10.012812614440918, -10.046394348144531, -10.078278541564941, -10.108453750610352, -10.136899948120117, -10.163603782653809, -10.188554763793945, -10.211743354797363, -10.233158111572266, -10.252793312072754, -10.270648002624512, -10.28671932220459, -10.301010131835938, -10.313526153564453, -10.324272155761719, -10.333261489868164, -10.340506553649902, -10.346020698547363, -10.349822998046875, -10.351933479309082, -10.352372169494629, -10.35116195678711, -10.348328590393066, -10.34389591217041, -10.337890625, -10.330340385437012, -10.321273803710938, -10.310718536376953, -10.29870319366455, -10.28525447845459, -10.270401000976562, -10.254173278808594, -10.23659610748291, -10.21769905090332, -10.197510719299316, -10.176054000854492, -10.153358459472656, -10.129448890686035, -10.104351043701172, -10.07809066772461, -10.050690650939941, -10.022177696228027, -9.992574691772461, -9.96190357208252, -9.930190086364746, -9.897456169128418, -9.863725662231445, -9.829022407531738, -9.793366432189941, -9.756783485412598, -9.7192964553833, -9.680926322937012, -9.641697883605957, -9.601633071899414, -9.560758590698242, -9.519095420837402, -9.476668357849121, -9.433504104614258, -9.389622688293457, -9.345054626464844, -9.299821853637695, -9.253951072692871, -9.207468032836914, -9.160400390625, -9.112772941589355, -9.064611434936523, -9.015946388244629, -8.966803550720215, -8.917210578918457, -8.867193222045898, -8.816780090332031, -8.765998840332031, -8.71487808227539, -8.663442611694336, -8.611721992492676, -8.559739112854004, -8.507525444030762, -8.455102920532227, -8.402501106262207, -8.34974479675293, -8.296854972839355, -8.243861198425293, -8.19078254699707, -8.137646675109863, -8.084471702575684, -8.031281471252441, -7.978095054626465, -7.924934387207031, -7.871817588806152, -7.818759918212891, -7.765783309936523, -7.712899208068848, -7.660129547119141, -7.607481002807617, -7.554970741271973, -7.502610206604004, -7.450409889221191, -7.398380279541016, -7.346534729003906, -7.29487419128418, -7.243412971496582, -7.1921491622924805, -7.141097068786621, -7.090251922607422, -7.039626121520996, -6.989212989807129, -6.939020156860352, -6.889043807983398, -6.839286804199219, -6.7897443771362305, -6.740416526794434, -6.6912994384765625, -6.642387390136719, -6.593676567077637, -6.545161247253418, -6.496833801269531, -6.448686599731445, -6.400712013244629, -6.352900505065918, -6.305240631103516, -6.257723808288574, -6.210336685180664, -6.163068771362305, -6.11590576171875, -6.068835258483887, -6.021841049194336, -5.974909782409668, -5.9280242919921875, -5.881170272827148, -5.834328651428223, -5.787482261657715, -5.740614891052246, -5.693706512451172, -5.646738052368164, -5.5996904373168945, -5.552543640136719, -5.505277633666992, -5.457871437072754, -5.41030216217041, -5.362552642822266, -5.314596176147461, -5.266415596008301, -5.217984199523926, -5.169283866882324, -5.120288848876953, -5.070978164672852, -5.021328926086426, -4.971317291259766, -4.920923233032227, -4.870121955871582, -4.818891525268555, -4.767209053039551, -4.71505069732666, -4.662397384643555, -4.609225273132324, -4.555511474609375, -4.501235008239746, -4.446374893188477, -4.390907287597656, -4.334812164306641, -4.278069496154785, -4.220656394958496, -4.162550926208496, -4.103732109069824, -4.044179916381836, -3.983873128890991, -3.922790765762329, -3.860910654067993, -3.7982118129730225, -3.734672784805298, -3.6702721118927, -3.604985475540161, -3.5387918949127197, -3.471669912338257, -3.403595209121704, -3.3345444202423096, -3.264495372772217, -3.1934220790863037, -3.121302843093872, -3.0481131076812744, -2.973830223083496, -2.8984293937683105, -2.8218894004821777, -2.7441887855529785, -2.6653056144714355, -2.5852222442626953, -2.503920078277588, -2.4213855266571045, -2.3376011848449707, -2.252558708190918, -2.1662487983703613, -2.078667640686035, -1.9898097515106201, -1.899679183959961, -1.8082795143127441, -1.7156171798706055, -1.6217021942138672, -1.5265462398529053, -1.430164098739624, -1.3325716257095337, -1.2337861061096191, -1.133826732635498, -1.0327112674713135, -0.9304565191268921, -0.827080249786377, -0.7225972414016724, -0.6170204877853394, -0.5103628635406494, -0.40263211727142334, -0.2938377261161804, -0.18398278951644897, -0.07307314872741699, 0.03889197111129761, 0.15190690755844116, 0.265970915555954, 0.38108205795288086, 0.49723517894744873, 0.6144240498542786, 0.7326413989067078, 0.8518727421760559, 0.9721038341522217, 1.0933148860931396, 1.2154808044433594, 1.3385742902755737, 1.4625645875930786, 1.5874135494232178, 1.7130849361419678, 1.8395355939865112, 1.9667223691940308, 2.0945992469787598, 2.223118782043457, 2.35223650932312, 2.4819066524505615, 2.612077236175537, 2.7427072525024414, 2.873750925064087, 3.005167007446289, 3.136915445327759, 3.2689554691314697, 3.4012534618377686, 3.5337729454040527, 3.6664812564849854, 3.799344301223755, 3.932331085205078, 4.065408229827881, 4.198541641235352, 4.331697463989258, 4.464838027954102, 4.597922325134277, 4.730904579162598, 4.863737106323242, 4.996366500854492, 5.128734588623047, 5.26077938079834, 5.392433166503906, 5.523628234863281, 5.654293060302734, 5.784355163574219, 5.913744926452637, 6.042396545410156, 6.1702423095703125, 6.297228813171387, 6.423306465148926, 6.548427581787109, 6.672560691833496, 6.795679092407227, 6.917764663696289, 7.0388031005859375, 7.158782005310059, 7.277691841125488, 7.395519256591797, 7.512245178222656, 7.627842903137207, 7.74227237701416, 7.8554792404174805, 7.967398643493652, 8.077947616577148, 8.187031745910645, 8.294550895690918, 8.400388717651367, 8.504435539245605, 8.606578826904297, 8.706720352172852, 8.804766654968262, 8.900647163391113, 8.994309425354004, 9.085719108581543, 9.174869537353516, 9.261770248413086, 9.346450805664062, 9.42895221710205, 9.50932788848877, 9.587635040283203, 9.663930892944336, 9.738266944885254, 9.810684204101562, 9.881209373474121, 9.949854850769043, 10.016615867614746, 10.081463813781738, 10.144357681274414, 10.205240249633789, 10.264039993286133, 10.320676803588867, 10.375064849853516, 10.427118301391602, 10.476752281188965, 10.52388858795166, 10.568456649780273, 10.61040210723877, 10.649678230285645, 10.68625545501709, 10.720117568969727, 10.751263618469238, 10.779706001281738, 10.805464744567871, 10.828577041625977, 10.849078178405762, 10.86701774597168, 10.882447242736816, 10.895417213439941, 10.905982971191406, 10.914196014404297, 10.92010498046875, 10.923757553100586, 10.925196647644043, 10.924457550048828, 10.9215726852417, 10.916569709777832, 10.909469604492188, 10.900288581848145, 10.889039993286133, 10.875728607177734, 10.860363006591797, 10.842941284179688, 10.823467254638672, 10.801942825317383, 10.778365135192871, 10.752739906311035, 10.72507095336914, 10.695367813110352, 10.663646697998047, 10.629925727844238, 10.594230651855469, 10.556596755981445, 10.517061233520508, 10.475666999816895, 10.432472229003906, 10.387532234191895, 10.340909004211426, 10.292671203613281, 10.242886543273926, 10.191631317138672, 10.138973236083984, 10.084985733032227, 10.029736518859863, 9.973294258117676, 9.915719032287598, 9.857072830200195, 9.797409057617188, 9.736776351928711, 9.675222396850586, 9.612789154052734, 9.549516677856445, 9.48544692993164, 9.420616149902344, 9.355061531066895, 9.288824081420898, 9.221943855285645, 9.154463768005371, 9.086434364318848, 9.017902374267578, 8.948920249938965, 8.879544258117676, 8.809833526611328, 8.73984432220459, 8.669637680053711, 8.599273681640625, 8.52881145477295, 8.458306312561035, 8.38781452178955, 8.317388534545898, 8.247071266174316, 8.176912307739258, 8.106949806213379, 8.037221908569336, 7.967761039733887, 7.898597717285156, 7.829760551452637, 7.76127815246582, 7.693173408508301, 7.625473976135254, 7.558204650878906, 7.491393089294434, 7.425067901611328, 7.359255790710449, 7.293993949890137, 7.229312896728516, 7.165249824523926, 7.10184383392334, 7.039133071899414, 6.9771623611450195, 6.915972709655762, 6.855607032775879, 6.796110153198242, 6.737521171569824, 6.679884910583496, 6.623238563537598, 6.567622184753418, 6.513069152832031, 6.459611892700195, 6.407279014587402, 6.356096267700195, 6.306082725524902, 6.257256507873535, 6.209630012512207, 6.163212776184082, 6.118009567260742, 6.074021339416504, 6.031242370605469, 5.9896697998046875, 5.949291229248047, 5.910094261169434, 5.872061729431152, 5.835175514221191, 5.799412727355957, 5.7647504806518555, 5.731162071228027, 5.698619842529297, 5.667093276977539, 5.636552810668945, 5.606964111328125, 5.578295707702637, 5.55051326751709, 5.523581504821777, 5.497463226318359, 5.4721221923828125, 5.447524070739746, 5.423628807067871, 5.400402069091797, 5.377801895141602, 5.355792999267578, 5.33433723449707, 5.313396453857422, 5.292932510375977, 5.272906303405762, 5.2532806396484375, 5.234016418457031, 5.215073585510254, 5.196415901184082, 5.178004264831543, 5.159799575805664, 5.141762733459473, 5.123856544494629, 5.106039047241211, 5.088273048400879, 5.070517539978027, 5.052733421325684, 5.034878730773926, 5.016915321350098, 4.998800277709961, 4.980493545532227, 4.961949348449707, 4.943129539489746, 4.923985481262207, 4.904477119445801, 4.884556770324707, 4.864180564880371, 4.843299865722656, 4.821866989135742, 4.799833297729492, 4.7771501541137695, 4.753766059875488, 4.729628562927246, 4.704686164855957, 4.678882598876953, 4.652164459228516, 4.624475479125977, 4.595756530761719, 4.565951347351074, 4.535000801086426, 4.502845764160156, 4.469422817230225, 4.4346723556518555, 4.39853572845459, 4.360947608947754, 4.3218488693237305, 4.281177997589111, 4.2388739585876465, 4.194876670837402, 4.1491289138793945, 4.101571083068848, 4.052151203155518, 4.00081205368042, 3.947507858276367, 3.892186403274536, 3.8348066806793213, 3.775324583053589, 3.7137064933776855, 3.649914503097534, 3.5839226245880127, 3.5157036781311035, 3.445239543914795, 3.3725109100341797, 3.2975103855133057, 3.220224142074585, 3.140655517578125, 3.058797597885132, 2.974658966064453, 2.888240098953247, 2.7995574474334717, 2.7086093425750732, 2.6154205799102783, 2.5199904441833496, 2.4223461151123047, 2.3224871158599854, 2.2204394340515137, 2.1162004470825195, 2.0097970962524414, 1.9012236595153809, 1.7905036211013794, 1.6776280403137207, 1.5626139640808105, 1.4454679489135742, 1.3261873722076416, 1.2047841548919678, 1.0812498331069946, 0.9556045532226562, 0.8278371691703796, 0.6979728937149048, 0.5660030245780945, 0.43195641040802, 0.2958304286003113, 0.1576651930809021, 0.017461836338043213, -0.12472999095916748, -0.26889747381210327, -0.41498255729675293, -0.5629606246948242, -0.7127676010131836, -0.8643603324890137, -1.017668604850769, -1.1726371049880981, -1.3291877508163452, -1.4872572422027588, -1.6467576026916504, -1.8076212406158447, -1.969756841659546, -2.1330933570861816, -2.297536849975586, -2.4630184173583984, -2.629446029663086, -2.796753168106079, -2.964855670928955, -3.1336913108825684, -3.3031790256500244, -3.4732658863067627, -3.64387583732605, -3.814959764480591, -3.9864470958709717, -4.15828800201416, -4.330409049987793, -4.502758979797363, -4.675262451171875, -4.847840309143066, -5.020424842834473, -5.192917823791504, -5.365236282348633, -5.53726863861084, -5.708918571472168, -5.880061149597168, -6.050595283508301, -6.220394134521484, -6.38935661315918, -6.557365417480469, -6.724332809448242, -6.8901567459106445, -7.054773330688477, -7.218103408813477, -7.3801164627075195, -7.540744781494141, -7.699978828430176, -7.857765197753906, -8.014092445373535, -8.168901443481445, -8.322163581848145, -8.473787307739258, -8.623700141906738, -8.771766662597656, -8.917858123779297, -9.061782836914062, -9.203364372253418, -9.34237003326416, -9.47860050201416, -9.611823081970215, -9.741857528686523, -9.868524551391602, -9.991718292236328, -10.111366271972656, -10.227478981018066, -10.34011173248291, -10.4493989944458, -10.555519104003906, -10.658676147460938, -10.759112358093262, -10.857039451599121, -10.952658653259277, -11.046091079711914, -11.137399673461914, -11.22653579711914, -11.31336498260498, -11.3976469039917, -11.479076385498047, -11.557276725769043, -11.631855964660645, -11.702421188354492, -11.768621444702148, -11.830162048339844, -11.886835098266602, -11.938508033752441, -11.985148429870605, -12.026790618896484, -12.06354808807373, -12.095577239990234, -12.123079299926758, -12.146270751953125, -12.165380477905273, -12.180632591247559, -12.192241668701172, -12.200403213500977, -12.205291748046875, -12.207056045532227, -12.205821990966797, -12.201688766479492, -12.194726943969727, -12.184990882873535, -12.172506332397461, -12.157281875610352, -12.139307022094727, -12.118555068969727, -12.094982147216797, -12.068537712097168, -12.039155960083008, -12.006768226623535, -11.971296310424805, -11.932661056518555, -11.890783309936523, -11.8455810546875, -11.79698371887207, -11.74492073059082, -11.689336776733398, -11.630182266235352, -11.567427635192871, -11.501057624816895, -11.431073188781738, -11.357501029968262, -11.28038501739502, -11.199793815612793, -11.115821838378906, -11.028584480285645, -10.938222885131836, -10.844898223876953, -10.748791694641113, -10.650104522705078, -10.549056053161621, -10.44586181640625, -10.340770721435547, -10.234007835388184, -10.125823020935059, -10.016438484191895, -9.906089782714844, -9.794976234436035, -9.683304786682129, -9.57123851776123, -9.458946228027344, -9.34654426574707, -9.234156608581543, -9.121846199035645, -9.009692192077637, -8.897714614868164, -8.785944938659668, -8.674365997314453, -8.562969207763672, -8.451733589172363, -8.340605735778809, -8.229562759399414, -8.118544578552246, -8.00753116607666, -7.89647102355957, -7.785362243652344, -7.6741790771484375, -7.562948226928711, -7.451677322387695, -7.340429306030273, -7.2292585372924805, -7.118254661560059, -7.007524490356445, -6.897194862365723, -6.787412643432617, -6.678335189819336, -6.570137023925781, -6.463001251220703, -6.357117652893066, -6.252683639526367, -6.149890899658203, -6.0489301681518555, -5.949989318847656, -5.853240966796875, -5.75885009765625, -5.666966438293457, -5.577722549438477, -5.491233825683594, -5.407598495483398, -5.3268938064575195, -5.249177932739258, -5.174492835998535, -5.102861404418945, -5.034287452697754, -4.968755722045898, -4.906247138977051, -4.846714973449707, -4.7901153564453125, -4.736377716064453, -4.6854352951049805, -4.637216567993164, -4.5916290283203125, -4.548595428466797, -4.5080156326293945, -4.4698076248168945, -4.433872222900391, -4.400128364562988, -4.36848258972168, -4.338854789733887, -4.311164855957031, -4.285341262817383, -4.261316299438477, -4.23903751373291, -4.218454360961914, -4.199533462524414, -4.1822509765625, -4.166603088378906, -4.1525983810424805, -4.140267372131348, -4.129660606384277, -4.120857238769531, -4.113961219787598, -4.109108924865723, -4.106467247009277, -4.106241226196289, -4.108664512634277, -4.114012718200684, -4.12258243560791, -4.134696006774902, -4.150681495666504, -4.170858383178711, -4.1955060958862305, -4.224841117858887, -4.258971214294434, -4.297869682312012, -4.341335296630859, -4.3889665603637695, -4.44016170501709, -4.494122505187988, -4.549890518188477, -4.606417655944824, -4.662611961364746, -4.717441558837891, -4.76997184753418, -4.819448471069336, -4.865298271179199, -4.907166481018066, -4.944878578186035, -4.978435516357422, -5.007965087890625, -5.033700942993164, -5.055932998657227, -5.074994087219238, -5.091222763061523, -5.104957580566406, -5.116518020629883, -5.126198768615723, -5.134267807006836, -5.140963554382324, -5.146496772766113, -5.151047706604004, -5.154773712158203, -5.157807350158691, -5.1602678298950195, -5.1622467041015625, -5.1638288497924805, -5.165081024169922, -5.1660614013671875, -5.166818618774414, -5.167391777038574, -5.167815208435059, -5.168116569519043, -5.168318748474121, -5.168439865112305, -5.168497085571289, -5.168503761291504, -5.168468475341797, -5.168402671813965, -5.168312072753906, -5.168203353881836, -5.168081283569336, -5.1679487228393555, -5.167810440063477, -5.167668342590332, -5.167523384094238, -5.167379379272461, -5.167235374450684, -5.167095184326172, -5.166956901550293, -5.16682243347168, -5.166691780090332, -5.166565895080566, -5.166444778442383, -5.166328430175781, -5.166215896606445, -5.166108131408691, -5.1660051345825195, -5.16590690612793, -5.1658124923706055, -5.165722846984863, -5.165637969970703, -5.165555953979492, -5.165478706359863, -5.165406227111816, -5.165334701538086, -5.165268898010254, -5.165205955505371, -5.1651458740234375, -5.165088653564453, -5.165034294128418, -5.164982795715332, -5.164934158325195, -5.164887428283691, -5.164843559265137, -5.164802551269531, -5.164762496948242, -5.164724349975586, -5.164689064025879]}],\n",
              "                        {\"hovermode\": \"x\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Regression Results\"}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('fc611f77-78b8-4923-90c5-91b6a1a4741d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
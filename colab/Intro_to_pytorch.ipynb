{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro-to-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDNg1CKkeE63mMHEQFYIeQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visiont3lab/deep-learning-course/blob/main/colab/Intro_to_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38zhmhg57csZ"
      },
      "source": [
        "# Getting Started With Pytorch for Deep Learning\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrxlHaHG53rG"
      },
      "source": [
        "* [PyTorch Computer Vision Cookbook](https://github.com/PacktPublishing/PyTorch-Computer-Vision-Cookbook)\r\n",
        "* [Deep Learning Sample Code](https://github.com/PacktPublishing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BidnhEVh7tn6"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNjKSYqb7_PD"
      },
      "source": [
        "* Local (Own Pc)\r\n",
        "\r\n",
        "    Install [python 3.8](https://www.python.org/downloads/)\r\n",
        "\r\n",
        "    ```\r\n",
        "    pip3 install virtualenv\r\n",
        "    virtualenv env\r\n",
        "    source env/bin/activate # Linux - Mac\r\n",
        "    source env/Scripts/activate # Windows\r\n",
        "    pip install torch torchvision\r\n",
        "    ```\r\n",
        "\r\n",
        "\r\n",
        "* Google Colab\r\n",
        "\r\n",
        "    ```\r\n",
        "    pip3 install torch torchvision\r\n",
        "    ```\r\n",
        "These requirements should be arealdy satisied\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tH0Ck6L5kTQ",
        "outputId": "ab33673e-d014-44ca-d533-09a04cdb47fd"
      },
      "source": [
        "# Verify installation\r\n",
        "!pip list | grep torch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch                         1.7.1+cu101   \n",
            "torchsummary                  1.5.1         \n",
            "torchtext                     0.3.1         \n",
            "torchvision                   0.8.2+cu101   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2rHdZ4a9Gl9",
        "outputId": "7686e2a1-86c2-4266-906d-a4822b37ba7c"
      },
      "source": [
        "# Change runtime colab type to enable GPU\r\n",
        "# Click on Runtime --> Select Change Runtime Type --> Select Hardware Accelarion GPU\r\n",
        "\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "print(\"Torch Version: \",torch.__version__)\r\n",
        "print(\"Torch Vision Version:\", torchvision.__version__)\r\n",
        "print(\"Is Cuda available: \", torch.cuda.is_available())\r\n",
        "print(\"Number of  Cuda device: \", torch.cuda.device_count())\r\n",
        "print(\"Get Cuda Current device: \", torch.cuda.current_device())\r\n",
        "print(\"Get Name of Cuda  device: \", torch.cuda.get_device_name(0))"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch Version:  1.7.1+cu101\n",
            "Torch Vision Version: 0.8.2+cu101\n",
            "Is Cuda available:  True\n",
            "Number of  Cuda device:  1\n",
            "Get Cuda Current device:  0\n",
            "Get Name of Cuda  device:  Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV3_nBpi_YJ5"
      },
      "source": [
        "## Pytorch: Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERI7NZCk_dIu"
      },
      "source": [
        "> Tensor: n-dimensional array\r\n",
        "\r\n",
        "* [Pytoch documentation](https://pytorch.org/docs/stable/index.html)\r\n",
        "* [List of Pytorch Type](https://pytorch.org/docs/stable/tensors.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Daxr3q3798gv",
        "outputId": "eaead4dc-d3ba-468a-eec9-a03840a5cbde"
      },
      "source": [
        "import torch\r\n",
        "x = torch.ones(4,4)\r\n",
        "print(x)\r\n",
        "print(\"Shape: \", x.shape)\r\n",
        "print(\"Type : \", x.dtype) # Torch default data type is torch.float32"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "Shape:  torch.Size([4, 4])\n",
            "Type :  torch.float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSM5NQafADMj",
        "outputId": "59e98a9b-6784-4de4-ed35-9401b4fec3da"
      },
      "source": [
        "# Specify tensor type\r\n",
        "x = 2*torch.ones(1,3,3, dtype=torch.int8)\r\n",
        "print(x)\r\n",
        "print(\"Shape: \", x.shape)\r\n",
        "print(\"Type : \", x.dtype)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[2, 2, 2],\n",
            "         [2, 2, 2],\n",
            "         [2, 2, 2]]], dtype=torch.int8)\n",
            "Shape:  torch.Size([1, 3, 3])\n",
            "Type :  torch.int8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyLARNB4AjiJ",
        "outputId": "c2e9980d-8c77-40b3-ae8d-e67e905429a4"
      },
      "source": [
        "# Change tensor type\r\n",
        "x = torch.rand(3, dtype=torch.float32) # random uniform 0-1\r\n",
        "x = 5*x # Multiply by 5\r\n",
        "print(\"Tensor: %s , Type: %s \" % (x, x.dtype))\r\n",
        "x = x.type(torch.uint8) # change data type to unit8\r\n",
        "print(\"Tensor: %s , Type: %s \" % (x, x.dtype))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor: tensor([2.7060, 2.9286, 1.7063]) , Type: torch.float32 \n",
            "Tensor: tensor([2, 2, 1], dtype=torch.uint8) , Type: torch.uint8 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb-gH_3EBFAC",
        "outputId": "d1b595b2-3db4-4605-b087-a5506254e0fe"
      },
      "source": [
        "# Tensor to numpy array\r\n",
        "x = torch.sin( torch.rand(4) + 2*torch.rand(4) )\r\n",
        "xnp = x.numpy()\r\n",
        "print(\"Numpy Array: %s , Type: %s \" % (xnp, xnp.dtype))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numpy Array: [0.9927129  0.83238405 0.88557565 0.44139844] , Type: float32 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QNpDjIwCPlK",
        "outputId": "e7ee5492-1000-4376-de0f-ce955e2aa661"
      },
      "source": [
        "# Numpy array to tensor\r\n",
        "import numpy as np\r\n",
        "xnp = np.sin( np.random.rand(4) + 2*np.random.rand(4) ) # Float64 by default numpy\r\n",
        "x = torch.from_numpy(xnp)\r\n",
        "x = x.type(torch.float32)\r\n",
        "print(\"Tensor: %s , Type: %s \" % (x, x.dtype))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor: tensor([0.7397, 0.8691, 0.9946, 0.7989]) , Type: torch.float32 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEN8ncC-CsCK",
        "outputId": "8729c6ad-6b21-42a2-818c-f9648010b42c"
      },
      "source": [
        "# Moving tensor bettwen cpu and cuda device\r\n",
        "# If u do not specify the device the tensorf will be hosted by default on cpu\r\n",
        "x = torch.tensor([[1,3,4.4,5.6]])\r\n",
        "print(\"Tensor: %s , Type: %s ,Shape: %s, Device: %s\" % (x.tolist(), x.dtype,x.shape,x.device))\r\n",
        "if torch.cuda.is_available():\r\n",
        "    device = torch.device(\"cuda:0\")\r\n",
        "    x = x.to(device)\r\n",
        "    print(\"Tensor: %s , Type: %s ,Shape: %s, Device: %s\" % (x.tolist(), x.dtype,x.shape,x.device))\r\n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor: [[1.0, 3.0, 4.400000095367432, 5.599999904632568]] , Type: torch.float32 ,Shape: torch.Size([1, 4]), Device: cpu\n",
            "Tensor: [[1.0, 3.0, 4.400000095367432, 5.599999904632568]] , Type: torch.float32 ,Shape: torch.Size([1, 4]), Device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ3DJaT3Fxhb"
      },
      "source": [
        "## Pytorch: Dataset tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX5F3V--GHJB"
      },
      "source": [
        "* [Torchvision Datasets](https://pytorch.org/vision/stable/datasets.html)\r\n",
        "* [Pytorch transforms augmentation](https://pytorch.org/docs/stable/torchvision/transforms.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM4RWPCoDaWz"
      },
      "source": [
        "from torchvision import datasets\r\n",
        "\r\n",
        "# Get Mnist Train Datasets inside folder dataset\r\n",
        "train_data = datasets.MNIST(\"./dataset\", train=True, download=True)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giqyTFjWF74e",
        "outputId": "de1ed323-a597-43a5-b461-59f7b8f0fbba"
      },
      "source": [
        "# Extract Train data\r\n",
        "x_train, y_train = train_data.data, train_data.targets\r\n",
        "print(\"Training Dataset\")\r\n",
        "print(\"Shape: %s , Type: %s \" % (x_train.shape, x_train.dtype))\r\n",
        "print(\"Shape: %s , Type: %s \" % (y_train.shape, y_train.dtype))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Dataset\n",
            "Shape: torch.Size([60000, 28, 28]) , Type: torch.uint8 \n",
            "Shape: torch.Size([60000]) , Type: torch.int64 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtrfZKF7G00y",
        "outputId": "627a506e-83b1-4375-9254-03a9820335f8"
      },
      "source": [
        "# Get Mnist Validation Datasets inside folder dataset\r\n",
        "val_data = datasets.MNIST(\"./dataset\", train=False, download=True)\r\n",
        "print(val_data)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./dataset\n",
            "    Split: Test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_bdy5qfHI2p",
        "outputId": "814ca4ce-c267-44cb-f88f-a88840842363"
      },
      "source": [
        "# Extract Validation data\r\n",
        "x_val, y_val = val_data.data, val_data.targets\r\n",
        "print(\"Validation Dataset\")\r\n",
        "print(\"Shape: %s , Type: %s \" % (x_val.shape, x_val.dtype))\r\n",
        "print(\"Shape: %s , Type: %s \" % (y_val.shape, y_val.dtype))"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Dataset\n",
            "Shape: torch.Size([10000, 28, 28]) , Type: torch.uint8 \n",
            "Shape: torch.Size([10000]) , Type: torch.int64 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbiTZiumHija",
        "outputId": "7825c826-ff1e-4614-80d8-000bce2c5c18"
      },
      "source": [
        "# Modify tensor dimension\r\n",
        "print(\"Shape x_train: \", x_train.shape)\r\n",
        "print(\"Shape x_val: \", x_val.shape)\r\n",
        "if (len(x_train.shape)==3):\r\n",
        "    x_train=x_train.unsqueeze(1) # this nember specify where to  add new tensor\r\n",
        "    x_val = x_val.unsqueeze(1)\r\n",
        "print(\"Shape x_train: \", x_train.shape)\r\n",
        "print(\"Shape x_val: \", x_val.shape)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape x_train:  torch.Size([60000, 28, 28])\n",
            "Shape x_val:  torch.Size([10000, 28, 28])\n",
            "Shape x_train:  torch.Size([60000, 1, 28, 28])\n",
            "Shape x_val:  torch.Size([10000, 1, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "BI9nAczpJMHC",
        "outputId": "bcc2c40f-48b9-4d83-af3c-dbe1ea66ec32"
      },
      "source": [
        "# Display Images\r\n",
        "from torchvision import utils \r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "# sets the backend of matplotlib to the 'inline' backend\r\n",
        "%matplotlib inline \r\n",
        "\r\n",
        "def show_matplotlib(img):\r\n",
        "    # img is a tensor!\r\n",
        "    # Convert tensor to numpy array\r\n",
        "    img_np = img.numpy()\r\n",
        "    # Reshape image\r\n",
        "    img_np = np.transpose(img_np, (1,2,0))\r\n",
        "    # Display using matplotlib\r\n",
        "    plt.imshow(img_np, interpolation=\"nearest\") #,aspect='auto')\r\n",
        "    plt.axis('off')\r\n",
        "\r\n",
        "def show_cv2(img):\r\n",
        "    # img is a tensor!\r\n",
        "    # Convert tensor to numpy array\r\n",
        "    img_np = img.numpy()\r\n",
        "    # Reshape image\r\n",
        "    img_np = np.transpose(img_np, (1,2,0))\r\n",
        "    # Display using opencv\r\n",
        "    scale_percent = 300 # percent of original size\r\n",
        "    width = int(img_np.shape[1] * scale_percent / 100)\r\n",
        "    height = int(img_np.shape[0] * scale_percent / 100)\r\n",
        "    dim = (width, height)  \r\n",
        "    img_np = cv2.resize(img_np, dim, interpolation = cv2.INTER_AREA)\r\n",
        "    print(\"Shape x_grid Resize: \", img_np.shape)\r\n",
        "    cv2_imshow(img_np)\r\n",
        "\r\n",
        "# Let's create a grid image that contains 40 images of the train dataset\r\n",
        "x_grid = utils.make_grid(x_train[:20], nrow=8, padding=5)\r\n",
        "print(\"Shape x_grid: \", x_grid.shape)\r\n",
        "#show_matplotlib(x_grid)\r\n",
        "show_cv2(x_grid)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape x_grid:  torch.Size([3, 104, 269])\n",
            "Shape x_grid Resize:  (312, 807, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=807x312 at 0x7F8251897B10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "ikQhesIAIbZa",
        "outputId": "836de896-4cc3-4be3-cf98-01766fe99693"
      },
      "source": [
        "# Data transformation \r\n",
        "# Pytorch transform: https://pytorch.org/docs/stable/torchvision/transforms.html\r\n",
        "# Let's modify images to create some augmented ones\r\n",
        "from torchvision import datasets\r\n",
        "from torchvision import transforms\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "import torch\r\n",
        "from torchvision import utils \r\n",
        "import matplotlib.pyplot as plt \r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "# Get Mnist Train Datasets inside folder dataset\r\n",
        "train_data = datasets.MNIST(\"./dataset\", train=True, download=True)\r\n",
        "x_train, y_train = train_data.data, train_data.targets\r\n",
        "if (len(x_train.shape)==3):\r\n",
        "    x_train=x_train.unsqueeze(1) # this nember specify where to  add new tensor\r\n",
        "\r\n",
        "# Transformation applied to an Image\r\n",
        "data_transform = transforms.Compose([\r\n",
        "                                    transforms.ToPILImage(),  # Tensor of Numpy array to Pillow\r\n",
        "                                    #transforms.RandomHorizontalFlip(p=1), # Pillow transformation\r\n",
        "                                    transforms.RandomVerticalFlip(p=1), # Pillow transformation\r\n",
        "                                    transforms.ToTensor(),          # Pillow to tensor\r\n",
        "                            ])\r\n",
        "\r\n",
        "img = x_train[0][0] # Pillow images (size)\r\n",
        "print(\"Shape: \", img.shape) # Black and white image\r\n",
        "img_tr = data_transform(img).numpy()  # range 0-1\r\n",
        "print(\"Transformed Shape: \", img_tr.shape)\r\n",
        "\r\n",
        "# Display images\r\n",
        "#cv2_imshow(img)\r\n",
        "#cv2_imshow(img_tr[0]*255)\r\n",
        "plt.subplot(1,2,1)\r\n",
        "plt.imshow(img, cmap=\"gray\")\r\n",
        "plt.title(\"Original\")\r\n",
        "plt.subplot(1,2,2)\r\n",
        "plt.imshow(img_tr[0], cmap=\"gray\")\r\n",
        "plt.title(\"Transformed\")\r\n",
        "plt.axis(\"off\")\r\n",
        "plt.show()"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape:  torch.Size([28, 28])\n",
            "Transformed Shape:  (1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADHCAYAAAAAoQhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVbElEQVR4nO3dfbRVdZ3H8fcnlAoVES0klEgzHHQMlwiNw0JZKikDS1CWSybNRkeaGUkts4wytQbHCXEcBqcFPiXW+FBZImtW+IjUaCxQAQ2zXI4P0FVSQUBQA77zx9k0F/bv3nvuebp3Hz6vtVj3nO/5nb1/B/b98ju/p62IwMzMiucDXV0BMzOrjBO4mVlBOYGbmRWUE7iZWUE5gZuZFZQTuJlZQTmBdxFJ0yTdXOuyZRwrJH2yFscyK4ekfpIWS9ooaWZX16c1SYOy34k9uroulShkpbsjSV8ALgUOBTYAPwO+ERHrU+Uj4ppyj92ZsmatSdrU6mkv4D1gW/b8ixHxowZUYwrwBtA7vPCkptwCrwFJlwL/ClwG7At8Bvg48KCknony/o/TGiIi9t7xB3gFGN8q9ufkXedr8uPAqkqSt39X2ucEXiVJvYGrgS9FxC8i4k8R8RJwJjAIOFvSVZJ+IumHkjYAX8hiP2x1nM9LelnSm5KukPSSpJOy1/5cttVXvnMlvSLpDUnfbHWc4ZKekLReUouk2an/RGz3JukESaslfV3Sa8BtkvaTtEDSHyWtyx4f1Oo9iyR9V9L/ZN0hD0g6IHvtQ9n1/WZ27S3Nuk5+AJwLfE3SJkknSfqgpBsk/SH7c4OkD7ZTr6sk/Tg7/kZJz0j6lKRvSFor6VVJY1rVc19Jt2TX/xpJ/yypR/ZaD0nXZb83LwJ/07i/9dpzAq/eccCHgHtbByNiE/DfwMlZ6DTgJ0AfYKevrZKGAP8JfA7oT6kVP6CD844EBgMnAt+W9BdZfBvwZeAA4K+y1/+pgs9lze9AoC+lFvIUSvngtuz5QGALMHuX9/wt8HfAR4GewFez+LmUrtuDgf2BfwC2RMQXKF3v38ta/Q8B36T0LXUo8GlgOPCtduoFMB64A9gPeBpYmNV3APAdYE6r9/8A2Ap8EjgaGAP8ffbaBcC4LD4MmFTOX1R35QRevQOANyJia+K1lux1gCci4ucRsT0ituxSbhJwf0T8KiLeB74NdPR18+qI2BIRK4AVlH4RiIgnI+LXEbE1+yYwBzi+so9mTW47cGVEvJddS29GxE8jYnNEbASmk792bouI32XX8D2UkjDAnygl7k9GxLbsOtzQxnk/B3wnItZGxB8pfYM9p616ZbFfRsTC7Pfsx8BHgGsj4k/AXcAgSX0k9QPGApdExDsRsRb4N+Cs7DhnAjdExKsR8RbwL53/a+s+3L9UvTeAAyTtkUji/bPXAV5t5xgfa/16RGyW9GYH532t1ePNwN4Akj4FXE+pddGL0r/xkx19CNst/TEi3t3xRFIvSsnuFEotXYB9JPWIiB0Dn8nrjlLr+GDgLkl9gB8C38wS7K4+Brzc6vnLWSxZr8zrrR5vodRo2tbqOVldPgbsCbRI2lH+A/z/79dOv2u71KNw3AKv3hOURvZPbx2UtDdwKvBwFmqvRd0CtO5r/DCl1kwlvg/8FjgsInoD0wC1/xbbTe16TV5KqVtuRHbtjMriHV4/2djP1RExhFK34jjg820U/wOl7pEdBmaxturVGa9S+n08ICL6ZH96R8QR2estlP6jaX3uwnICr1JEvE3pK+B/SDpF0p6SBlH6ermaUsukIz8Bxks6LhtwvIrKk+4+lKYxbpJ0OPCPFR7Hdj/7UGrNrpfUF7iy3DdKGi3pL7PBwg2UulS2t1H8TuBbkj6SDYJ+m1KLvWoR0QI8AMyU1FvSByQdKmlHV9A9wEWSDpK0H3B5Lc7bVZzAayAivkeppXsdpYt3CaWWwIkR8V4Z7/8N8CVKfXktwCZgLaWWRGd9ldJA00bgJuDuCo5hu6cbgA9T6vb7NfCLTrz3QEoNkQ3Ac8BjtN14+WdgGbASeAZ4KovVyucpDbCuAtZl9eqfvXYTpQHQFdl5700doCjkefXdT9b9sp5SN8j/dnV9zKx7cgu8m5A0XlIvSXtRask/A7zUtbUys+7MCbz7OI3SQM4fgMOAs7zs2Mza4y4UM7OCcgvczKygqkrg2bS55yW9IKnQ03HMzIqm4i6UbL7n7yjt9bEaWApMjohV7bzH/TVWVxHRJYuWfG1bvaWu7Wpa4MOBFyLixWz/jrsoDcSZmVkDVJPAB7DzngKrSeygJ2mKpGWSllVxLjMz20XdN7OKiLnAXPDXTDOzWqqmBb6GnTeFOSiLmZlZA1STwJcCh0n6RLYB01nA/NpUy8zMOlJxF0pEbJU0ldLGMD2AW7NNmczMrAEauhLTfeBWb55GaM2q1tMIzcysCzmBm5kVlBO4mVlBOYGbmRWUE7iZWUE5gZuZFZQTuJlZQdV9LxQzq9y1116bi1122WVVH3fVqvyuzwsWLEiW3bp1ay42c+bMZNn169dXVzHrFLfAzcwKygnczKygnMDNzArKCdzMrKCcwM3MCsq7EVpTabbdCEeMGJGLtTULZfjw4bnYgAG5uxzWxObNm5PxWbNm5WLXXHNNLvbOO+/UvE7NzrsRmpk1ESdwM7OCcgI3MysoJ3Azs4KqahBT0kvARmAbsDUihnVQ3oOYQI8ePXKxfffdt6pjTp06NRnv1atXLjZ48OBk2QsvvDAXu+6665JlJ0+enIu9++67ybKp5eBXX311smy1mm0QszP69u2bi82ZMydZdujQobnYIYccUvM6ATz++OO5WFtL8RcuXJiLbdmypeZ1KqLUtV2LvVBGR8QbNTiOmZl1grtQzMwKqtoEHsADkp6UNKUWFTIzs/JU24UyMiLWSPoo8KCk30bE4tYFssTu5G5mVmNVtcAjYk32cy3wMyC3FCwi5kbEsI4GOM3MrHMqnoUiaS/gAxGxMXv8IPCdiPhFO+/p8pH6zhg4cGAu1rNnz2TZ4447LhcbOXJksmyfPn1ysTPOOKOTtavc6tWrk/GlS5fmYhMnTkyWTS2FXrFiRbLsFVdckYstWrSonRpWbneehdIZ/fr1y8WGDBmSLDt79uxc7PDDD695nQCWLFmSi82YMSNZ9r777svFtm/fXvM6dRe1noXSD/iZpB3H+a/2kreZmdVWxQk8Il4EPl3DupiZWSd4GqGZWUE5gZuZFZT3Aye9rBjgkUceycWqXfLeaKlBnfPOOy9ZdtOmTWUft6WlJRdbt25dsuzzzz9f9nGr5UHM2jvwwANzsdRWCpDe0mHQoEG1rhIAy5Yty8WmT5+eLDt//vy61KGRvB+4mVkTcQI3MysoJ3Azs4JyAjczKygncDOzgvIsFNIb4UN6WW+9Nr0v9/wA69evz8VGjx6dLPv+++/nYkWbSdMZnoXStVI3C0nNTDn99NOT70/NeOmMbdu2JeMPPfRQLjZ27NiqztVonoViZtZEnMDNzArKCdzMrKCcwM3MCsqDmO2YMGFCLjZu3Lhk2aeffjoXmzVrVtnnWr58eS42atSoZNnUXtxHHHFEsuzFF1+ci02Z0rw3SPIgZjEcddRRyfikSZNysWOPPTZZdsyYMWWfb+XKlbnYMccckyzbXfcU9yCmmVkTcQI3MysoJ3Azs4JyAjczK6gOE7ikWyWtlfRsq1hfSQ9K+n32c7/6VtPMzHbV4SwUSaOATcC8iDgyi30PeCsirpV0ObBfRHy9w5M1wUh97969k/GNGzfmYnPmzEmWPf/883Oxs88+Oxe78847O1k78yyU3cd7772Xi+2xR/o2v1u3bs3FPvvZzybLLlq0qKp61UtFs1AiYjHw1i7h04Dbs8e3A/n5dmZmVleV9oH3i4gd99R6DehXo/qYmVmZ0t83OiEior2vj5KmAM27csTMrItU2gJ/XVJ/gOzn2rYKRsTciBgWEcMqPJeZmSVU2gKfD5wLXJv9vK9mNermNmzYUHbZt99+u+yyF1xwQS529913J8t216W+ZtXq06dPLjZ+/Phk2R49epR93MWLF+di3XWwsjPKmUZ4J/AEMFjSaknnU0rcJ0v6PXBS9tzMzBqowxZ4RExu46UTa1wXMzPrBK/ENDMrKCdwM7OCcgI3Myso39Chjvbaa69k/P7778/Fjj/++Fzs1FNPTb7/gQceqK5iTcxL6YvhyCOPTMavv/76XOzEE8sfbmtr+4rp06fnYmvWrCn7uN2Bb+hgZtZEnMDNzArKCdzMrKCcwM3MCsqDmF3g0EMPzcWeeuqpXGz9+vXJ9z/66KO52LJly5Jlb7zxxlyskf/mjeZBzO5n4sSJudhtt92WLLvPPvuUfdxp06blYvPmzUuWbWlpScaLxIOYZmZNxAnczKygnMDNzArKCdzMrKA8iNlNeKCnNjyI2bU8QF8/HsQ0M2siTuBmZgXlBG5mVlBO4GZmBVXOPTFvlbRW0rOtYldJWiNpefZnbH2raWZmu+pwFoqkUcAmYF5EHJnFrgI2RcR1nTqZR+o7xXsmd55noTSG97pvvIpmoUTEYuCtutTIzMwqVk0f+FRJK7Mulv1qViMzMytLpQn8+8ChwFCgBZjZVkFJUyQtk5SejW9mZhWpKIFHxOsRsS0itgM3AcPbKTs3IoZFxLBKK2lmZnllLaWXNAhY0GoQs39EtGSPvwyMiIizyjjObjXQUy99+vTJxcaPH58sm1qOL6XH+R555JFc7OSTT+5k7bqWBzEbY8aMGcn4V77ylVzssccey8VOOumk5Pu3b99eXcWaWOra3qOjN0m6EzgBOEDSauBK4ARJQ4EAXgK+WNOamplZhzpM4BExORG+pQ51MTOzTvBKTDOzgnICNzMrKCdwM7OC6rAP3Lqf1Gb4d9xxR7LszTffnIvtsUf6n33UqFG52AknnJAsu2jRorYraIXUu3fvZHzjxo252L777lv2cW+66aZczLNNasMtcDOzgnICNzMrKCdwM7OCcgI3Myso35W+GzvqqKOS8UmTJuVixx57bLLsmDFjyj7fypUrc7FjjjkmWba7DkJ5KX15JkyYkIuNGzcuWfbpp5/OxWbNmlX2uZYvX56LpQbMAd55551c7IgjjkiWvfjii3OxKVOmlF2vovFd6c3MmogTuJlZQTmBm5kVlBO4mVlBOYGbmRWUZ6F0gcGDB+diU6dOzcVOP/305PsPPPDAqs6/bdu2ZPyhhx7KxcaOHVvVuRrNs1B21rdv32R8yZIludghhxxS7+q0e35IbxMxevToZNn3338/F+vMEv+i8SwUM7Mm4gRuZlZQTuBmZgXlBG5mVlDl3NT4YGAe0I/STYznRsS/S+oL3A0MonRj4zMjYl39qtq9pQYWJ09O3U40PWA5aNCgWlcJgGXLluVi06dPT5adP39+XepgXWfgwIHJ+P7779/gmuxsxIgRVR8jta/9Oeeckyy7adOmso/b0tKSi61bl05tzz//fNnHrYdyWuBbgUsjYgjwGeBCSUOAy4GHI+Iw4OHsuZmZNUiHCTwiWiLiqezxRuA5YABwGnB7Vux2IL87jpmZ1U2nbqkmaRBwNLAE6BcRO75rvEapiyX1nilA824RZmbWRcoexJS0N/BT4JKI2ND6tSitBkouZIiIuRExLCKGVVVTMzPbSVkJXNKelJL3jyLi3iz8uqT+2ev9gbX1qaKZmaV0uJRekij1cb8VEZe0is8A3oyIayVdDvSNiK91cKxuudy4Lf365XuFhgwZkiw7e/bsXOzwww+veZ0gvQx5xowZybL33XdfLtZdb8ZQC15KX57U7JSePXsmyx533HG52MiRI5Nl+/Tpk4udccYZnaxd5VavXp2ML126NBebOHFismzqphIrVqxIlr3iiitysUWLFrVTw8qlru1y+sD/GjgHeEbSjltrTAOuBe6RdD7wMnBmrSpqZmYd6zCBR8SvgLZaNSfWtjpmZlYur8Q0MysoJ3Azs4Lq1DzwZpDaH3nOnDnJskOHDs3F6rVn8uOPP56LzZw5M1l24cKFudiWLVtqXidrXq+88krZZV944YVcbN68ecmyPXr0yMWq3aM7tfUEQK9evXKx1F77ABdddFEultpPHNJbYAwblp4Fffzxx+di9RrETHEL3MysoJzAzcwKygnczKygnMDNzArKCdzMrKCa4q70qc3hL7vssmTZ4cOH52IDBgyoeZ0ANm/enIzPmjUrF7vmmmtysdSSXmufl9Jbs/Jd6c3MmogTuJlZQTmBm5kVlBO4mVlBNcVS+tS+vm3t9dsZq1atysUWLFiQLLt169ZcrK2l8OvXr6+uYmZmuAVuZlZYTuBmZgXlBG5mVlBO4GZmBdVhApd0sKRHJa2S9BtJF2fxqyStkbQ8+zO2/tU1M7MdyrkrfX+gf0Q8JWkf4ElgAqWbGG+KiOvKPpmXG1udeSm9NauK7kofES1AS/Z4o6TngPpsHmJmZmXrVB+4pEHA0cCSLDRV0kpJt0rar433TJG0TNKyqmpqZmY7KXs3Qkl7A48B0yPiXkn9gDeAAL5LqZvlvA6O4a+ZVlfuQrFmlbq2y0rgkvYEFgALI+L6xOuDgAURcWQHx/FFbnXlBG7NqqLtZCUJuAV4rnXyzgY3d5gIPFuLSpqZWXnKmYUyEvgl8AywPQtPAyYDQyl1obwEfDEb8GzvWG6lWF25BW7NquIulFrxRW715gRuzcp35DEzayJO4GZmBeUEbmZWUE7gZmYF5QRuZlZQTuBmZgXlBG5mVlBO4GZmBdXou9K/AbycPT4ge95s/Lm6zse76sRdtYDIdm8NXYm504mlZRExrEtOXkf+XGbWKO5CMTMrKCdwM7OC6soEPrcLz11P/lxm1hBd1gduZmbVcReKmVlBNTyBSzpF0vOSXpB0eaPPX0vZzZzXSnq2VayvpAcl/T77mbzZc3cm6WBJj0paJek3ki7O4oX/bGbNpKEJXFIP4EbgVGAIMFnSkEbWocZ+AJyyS+xy4OGIOAx4OHteNFuBSyNiCPAZ4MLs36kZPptZ02h0C3w48EJEvBgR7wN3Aac1uA41ExGLgbd2CZ8G3J49vh2Y0NBK1UBEtETEU9njjcBzwACa4LOZNZNGJ/ABwKutnq/OYs2kX6t7g74G9OvKylRL0iDgaGAJTfbZzIrOg5h1FKUpPoWd5iNpb+CnwCURsaH1a0X/bGbNoNEJfA1wcKvnB2WxZvK6pP4A2c+1XVyfikjak1Ly/lFE3JuFm+KzmTWLRifwpcBhkj4hqSdwFjC/wXWot/nAudnjc4H7urAuFZEk4BbguYi4vtVLhf9sZs2k4Qt5JI0FbgB6ALdGxPSGVqCGJN0JnEBpp77XgSuBnwP3AAMp7bx4ZkTsOtDZrUkaCfwSeAbYnoWnUeoHL/RnM2smXolpZlZQHsQ0MysoJ3Azs4JyAjczKygncDOzgnICNzMrKCdwM7OCcgI3MysoJ3Azs4L6P5c/+n/SeySoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IWlkAIHV7gf"
      },
      "source": [
        "# Wrapping Tensors into dataset\r\n",
        "from torchvision import datasets\r\n",
        "from torchvision import transforms\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "import torch\r\n",
        "from torchvision import utils \r\n",
        "import matplotlib.pyplot as plt \r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "from torch.utils.data import TensorDataset,Dataset\r\n",
        "\r\n",
        "class CustomTensorDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self, x,y, transform=None):\r\n",
        "        self.x = x\r\n",
        "        self.y = y\r\n",
        "        self.transform = transform\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        x = self.x[index]\r\n",
        "        if self.transform:\r\n",
        "            x = 255*self.transform(x)\r\n",
        "        y = self.y[index]\r\n",
        "        return x, y\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.x.shape[0]\r\n",
        "\r\n",
        "\r\n",
        "# Transformation\r\n",
        "# Transformation applied to an Image\r\n",
        "data_transform = transforms.Compose([\r\n",
        "                                    transforms.ToPILImage(),  # Tensor of Numpy array to Pillow\r\n",
        "                                    transforms.RandomApply( \r\n",
        "                                        torch.nn.ModuleList([\r\n",
        "                                            transforms.RandomAffine((-10,10), translate=(0.1,0.2), scale=(0.5,1.3)),\r\n",
        "                                            transforms.ColorJitter(brightness=(0.3,2), contrast=(0.9,1.1)),\r\n",
        "                                        ]),\r\n",
        "                                        p=0.5),\r\n",
        "                                    #transforms.RandomHorizontalFlip(p=1), # Pillow transformation\r\n",
        "                                    #transforms.RandomVerticalFlip(p=1), # Pillow transformation\r\n",
        "                                    transforms.ToTensor(),          # Pillow to tensor (scale 0-1)\r\n",
        "                            ])\r\n",
        "# Loading data\r\n",
        "train_data = datasets.MNIST(\"./dataset\", train=True, download=True) #,transform=data_transform) # This transform will only be appluied on data loader\r\n",
        "val_data = datasets.MNIST(\"./dataset\", train=False, download=True) #,transform=data_transform)\r\n",
        "x_train, y_train = train_data.data, train_data.targets\r\n",
        "x_val, y_val = val_data.data, val_data.targets\r\n",
        "\r\n",
        "# Add extra dimension (Preprocessing)\r\n",
        "if (len(x_train.shape)==3):\r\n",
        "    x_train=x_train.unsqueeze(1) \r\n",
        "if (len(x_val.shape)==3):\r\n",
        "    x_val=x_val.unsqueeze(1) \r\n",
        "\r\n",
        "# Create a Tensor dataset\r\n",
        "#train_ds = TensorDataset(x_train, y_train)\r\n",
        "#val_ds = TensorDataset(x_val, y_val)\r\n",
        "train_ds = CustomTensorDataset(x_train, y_train,data_transform)\r\n",
        "val_ds = CustomTensorDataset(x_val, y_val,data_transform)\r\n",
        "\r\n",
        "# Visualization Tensor Dataset\r\n",
        "i = 0\r\n",
        "for x,y in train_ds:\r\n",
        "    print(\"Y = \", y.item())\r\n",
        "    #print(\"X shape: \", x[0].shape)\r\n",
        "    cv2_imshow(x[0].numpy())\r\n",
        "    i=i+1\r\n",
        "    if i>10:\r\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zJ2DXKwYO6B"
      },
      "source": [
        "# Creating data Loader\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "train_dl = DataLoader(train_ds, batch_size=8)\r\n",
        "val_dl = DataLoader(val_ds, batch_size=8)\r\n",
        "\r\n",
        "i = 0\r\n",
        "for xb,yb in train_dl:\r\n",
        "    #print(xb[0].dtype)\r\n",
        "    #print(xb[0].min())\r\n",
        "    #print(xb[0].max())\r\n",
        "    #print(yb.shape)\r\n",
        "    print(yb[2].item())\r\n",
        "    cv2_imshow(xb[2][0].numpy())\r\n",
        "    i=i+1\r\n",
        "    if (i>10):\r\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhP54Orvmuuy"
      },
      "source": [
        "# Pytorch: Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlX1U9rbTkPt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro-to-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMrrxXQsIlECa3JKff8snPn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visiont3lab/deep-learning-course/blob/main/colab/Intro_to_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38zhmhg57csZ"
      },
      "source": [
        "# Getting Started With Python for Deep Learning\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrxlHaHG53rG"
      },
      "source": [
        "* [PyTorch Computer Vision Cookbook](https://github.com/PacktPublishing/PyTorch-Computer-Vision-Cookbook)\r\n",
        "* [Nerual Network from sctract](https://www.kickstarter.com/projects/sentdex/neural-networks-from-scratch-in-python?lang=it)\r\n",
        "* [Programming Pytorch for Deep Learning](https://www.amazon.it/Programming-Pytorch-Deep-Learning-Applications/dp/1492045357)\r\n",
        "* [Deep Learning Sample Code](https://github.com/PacktPublishing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37QzybuNIvsn"
      },
      "source": [
        "## Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drqLSjwyIbV7"
      },
      "source": [
        "## Python Basic Requirements\r\n",
        "\r\n",
        "* List\r\n",
        "* array\r\n",
        "* dictionary\r\n",
        "* function\r\n",
        "* class\r\n",
        "* python packages and pip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BidnhEVh7tn6"
      },
      "source": [
        "## Pytorch: Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNjKSYqb7_PD"
      },
      "source": [
        "* Local (Own Pc)\r\n",
        "\r\n",
        "    Install [python 3.8](https://www.python.org/downloads/)\r\n",
        "\r\n",
        "    ```\r\n",
        "    pip3 install virtualenv\r\n",
        "    virtualenv env\r\n",
        "    source env/bin/activate # Linux - Mac\r\n",
        "    source env/Scripts/activate # Windows\r\n",
        "    pip install torch torchvision\r\n",
        "    ```\r\n",
        "\r\n",
        "\r\n",
        "* Google Colab\r\n",
        "\r\n",
        "    ```\r\n",
        "    pip3 install torch torchvision\r\n",
        "    ```\r\n",
        "These requirements should be arealdy satisied\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tH0Ck6L5kTQ",
        "outputId": "d3525904-7b6c-4dab-cbd7-96a281909355"
      },
      "source": [
        "# Verify installation\r\n",
        "!pip list | grep torch"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch                         1.7.1+cu101   \n",
            "torchsummary                  1.5.1         \n",
            "torchtext                     0.3.1         \n",
            "torchvision                   0.8.2+cu101   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2rHdZ4a9Gl9",
        "outputId": "2441b6bf-864a-454c-ea50-569ee72eba89"
      },
      "source": [
        "# Change runtime colab type to enable GPU\r\n",
        "# Click on Runtime --> Select Change Runtime Type --> Select Hardware Accelarion GPU\r\n",
        "\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "print(\"Torch Version: \",torch.__version__)\r\n",
        "print(\"Torch Vision Version:\", torchvision.__version__)\r\n",
        "print(\"Is Cuda available: \", torch.cuda.is_available())\r\n",
        "print(\"Number of  Cuda device: \", torch.cuda.device_count())\r\n",
        "print(\"Get Cuda Current device: \", torch.cuda.current_device())\r\n",
        "print(\"Get Name of Cuda  device: \", torch.cuda.get_device_name(0))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch Version:  1.7.1+cu101\n",
            "Torch Vision Version: 0.8.2+cu101\n",
            "Is Cuda available:  True\n",
            "Number of  Cuda device:  1\n",
            "Get Cuda Current device:  0\n",
            "Get Name of Cuda  device:  Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvzvvjNRKF5b",
        "outputId": "4e2e32bb-9cb2-413b-c899-d89e71149cb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! pip3 install torch torchvision"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.7.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.8.2+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV3_nBpi_YJ5"
      },
      "source": [
        "## Pytorch: Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERI7NZCk_dIu"
      },
      "source": [
        "> Tensor: n-dimensional array\r\n",
        "\r\n",
        "* [Pytoch documentation](https://pytorch.org/docs/stable/index.html)\r\n",
        "* [List of Pytorch Type](https://pytorch.org/docs/stable/tensors.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Daxr3q3798gv",
        "outputId": "f0c1823f-2f00-44ad-e655-495742adb12c"
      },
      "source": [
        "import torch\r\n",
        "x = torch.ones(4,4)\r\n",
        "print(x)\r\n",
        "print(\"Shape: \", x.shape)\r\n",
        "print(\"Type : \", x.dtype) # Torch default data type is torch.float32"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "Shape:  torch.Size([4, 4])\n",
            "Type :  torch.float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSM5NQafADMj",
        "outputId": "56adcee4-bc87-4145-9f40-06adf4010b51"
      },
      "source": [
        "# Specify tensor type\r\n",
        "x = 2*torch.ones(1,3,3, dtype=torch.int8)\r\n",
        "print(x)\r\n",
        "print(\"Shape: \", x.shape)\r\n",
        "print(\"Type : \", x.dtype)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[2, 2, 2],\n",
            "         [2, 2, 2],\n",
            "         [2, 2, 2]]], dtype=torch.int8)\n",
            "Shape:  torch.Size([1, 3, 3])\n",
            "Type :  torch.int8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyLARNB4AjiJ",
        "outputId": "14b7211e-9c9a-4dbb-b895-d6f9cf2be221"
      },
      "source": [
        "# Change tensor type\r\n",
        "x = torch.rand(3, dtype=torch.float32) # random uniform 0-1\r\n",
        "x = 5*x # Multiply by 5\r\n",
        "print(\"Tensor: %s , Type: %s \" % (x, x.dtype))\r\n",
        "x = x.type(torch.uint8) # change data type to unit8\r\n",
        "print(\"Tensor: %s , Type: %s \" % (x, x.dtype))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor: tensor([0.6724, 2.9342, 2.4417]) , Type: torch.float32 \n",
            "Tensor: tensor([0, 2, 2], dtype=torch.uint8) , Type: torch.uint8 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb-gH_3EBFAC",
        "outputId": "cc1a754d-5803-48af-ffa0-27b0283468d1"
      },
      "source": [
        "# Tensor to numpy array\r\n",
        "x = torch.sin( torch.rand(4) + 2*torch.rand(4) )\r\n",
        "xnp = x.numpy()\r\n",
        "print(\"Numpy Array: %s , Type: %s \" % (xnp, xnp.dtype))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numpy Array: [0.28262943 0.4665296  0.68194443 0.9914474 ] , Type: float32 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QNpDjIwCPlK",
        "outputId": "836f0c64-6ff7-452e-b0bc-73be86974edd"
      },
      "source": [
        "# Numpy array to tensor\r\n",
        "import numpy as np\r\n",
        "xnp = np.sin( np.random.rand(4) + 2*np.random.rand(4) ) # Float64 by default numpy\r\n",
        "x = torch.from_numpy(xnp)\r\n",
        "x = x.type(torch.float32)\r\n",
        "print(\"Tensor: %s , Type: %s \" % (x, x.dtype))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor: tensor([0.9923, 0.7732, 0.8701, 0.9995]) , Type: torch.float32 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEN8ncC-CsCK",
        "outputId": "7a8326f4-1fdb-4f95-f275-fe430b554df8"
      },
      "source": [
        "# Moving tensor bettwen cpu and cuda device\r\n",
        "# If u do not specify the device the tensorf will be hosted by default on cpu\r\n",
        "x = torch.tensor([[1,3,4.4,5.6]])\r\n",
        "print(\"Tensor: %s , Type: %s ,Shape: %s, Device: %s\" % (x.tolist(), x.dtype,x.shape,x.device))\r\n",
        "if torch.cuda.is_available():\r\n",
        "    device = torch.device(\"cuda:0\")\r\n",
        "    x = x.to(device)\r\n",
        "    print(\"Tensor: %s , Type: %s ,Shape: %s, Device: %s\" % (x.tolist(), x.dtype,x.shape,x.device))\r\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor: [[1.0, 3.0, 4.400000095367432, 5.599999904632568]] , Type: torch.float32 ,Shape: torch.Size([1, 4]), Device: cpu\n",
            "Tensor: [[1.0, 3.0, 4.400000095367432, 5.599999904632568]] , Type: torch.float32 ,Shape: torch.Size([1, 4]), Device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ3DJaT3Fxhb"
      },
      "source": [
        "## Pytorch: Dataset tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX5F3V--GHJB"
      },
      "source": [
        "* [Torchvision Datasets](https://pytorch.org/vision/stable/datasets.html)\r\n",
        "* [Pytorch transforms augmentation](https://pytorch.org/docs/stable/torchvision/transforms.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydZl9oaOOEbG"
      },
      "source": [
        "from keras.datasets import mnist\r\n",
        "(x_train,y_train),(x_val, y_val) = mnist.load_data()\r\n",
        "x_train = torch.from_numpy(x_train)\r\n",
        "y_train = torch.from_numpy(y_train)\r\n",
        "x_val = torch.from_numpy(x_val)\r\n",
        "y_val = torch.from_numpy(y_val)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM4RWPCoDaWz"
      },
      "source": [
        "from torchvision import datasets\r\n",
        "\r\n",
        "# Get Mnist Train Datasets inside folder dataset\r\n",
        "train_data = datasets.MNIST(\"./dataset\", train=True, download=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giqyTFjWF74e"
      },
      "source": [
        "# Extract Train data\r\n",
        "x_train, y_train = train_data.data, train_data.targets\r\n",
        "print(\"Training Dataset\")\r\n",
        "print(\"Shape: %s , Type: %s \" % (x_train.shape, x_train.dtype))\r\n",
        "print(\"Shape: %s , Type: %s \" % (y_train.shape, y_train.dtype))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtrfZKF7G00y"
      },
      "source": [
        "# Get Mnist Validation Datasets inside folder dataset\r\n",
        "val_data = datasets.MNIST(\"./dataset\", train=False, download=True)\r\n",
        "print(val_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_bdy5qfHI2p"
      },
      "source": [
        "# Extract Validation data\r\n",
        "x_val, y_val = val_data.data, val_data.targets\r\n",
        "print(\"Validation Dataset\")\r\n",
        "print(\"Shape: %s , Type: %s \" % (x_val.shape, x_val.dtype))\r\n",
        "print(\"Shape: %s , Type: %s \" % (y_val.shape, y_val.dtype))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbiTZiumHija",
        "outputId": "2344b2a6-799d-4228-a67c-5eecd9c57012"
      },
      "source": [
        "# Modify tensor dimension\r\n",
        "print(\"Shape x_train: \", x_train.shape)\r\n",
        "print(\"Shape x_val: \", x_val.shape)\r\n",
        "if (len(x_train.shape)==3):\r\n",
        "    x_train=x_train.unsqueeze(1) # this nember specify where to  add new tensor\r\n",
        "    x_val = x_val.unsqueeze(1)\r\n",
        "print(\"Shape x_train: \", x_train.shape)\r\n",
        "print(\"Shape x_val: \", x_val.shape)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape x_train:  torch.Size([60000, 28, 28])\n",
            "Shape x_val:  torch.Size([10000, 28, 28])\n",
            "Shape x_train:  torch.Size([60000, 1, 28, 28])\n",
            "Shape x_val:  torch.Size([10000, 1, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "BI9nAczpJMHC",
        "outputId": "bcd48567-0126-4922-9140-325ddd138217"
      },
      "source": [
        "# Display Images\r\n",
        "from torchvision import utils \r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "# sets the backend of matplotlib to the 'inline' backend\r\n",
        "%matplotlib inline \r\n",
        "\r\n",
        "def show_matplotlib(img):\r\n",
        "    # img is a tensor!\r\n",
        "    # Convert tensor to numpy array\r\n",
        "    img_np = img.numpy()\r\n",
        "    # Reshape image\r\n",
        "    img_np = np.transpose(img_np, (1,2,0))\r\n",
        "    # Display using matplotlib\r\n",
        "    plt.imshow(img_np, interpolation=\"nearest\") #,aspect='auto')\r\n",
        "    plt.axis('off')\r\n",
        "\r\n",
        "def show_cv2(img):\r\n",
        "    # img is a tensor!\r\n",
        "    # Convert tensor to numpy array\r\n",
        "    img_np = img.numpy()\r\n",
        "    # Reshape image\r\n",
        "    img_np = np.transpose(img_np, (1,2,0))\r\n",
        "    # Display using opencv\r\n",
        "    scale_percent = 300 # percent of original size\r\n",
        "    width = int(img_np.shape[1] * scale_percent / 100)\r\n",
        "    height = int(img_np.shape[0] * scale_percent / 100)\r\n",
        "    dim = (width, height)  \r\n",
        "    img_np = cv2.resize(img_np, dim, interpolation = cv2.INTER_AREA)\r\n",
        "    print(\"Shape x_grid Resize: \", img_np.shape)\r\n",
        "    cv2_imshow(img_np)\r\n",
        "\r\n",
        "# Let's create a grid image that contains 40 images of the train dataset\r\n",
        "x_grid = utils.make_grid(x_train[:20], nrow=8, padding=5)\r\n",
        "print(\"Shape x_grid: \", x_grid.shape)\r\n",
        "#show_matplotlib(x_grid)\r\n",
        "show_cv2(x_grid)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape x_grid:  torch.Size([3, 104, 269])\n",
            "Shape x_grid Resize:  (312, 807, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=807x312 at 0x7F333AC56750>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "ikQhesIAIbZa",
        "outputId": "836de896-4cc3-4be3-cf98-01766fe99693"
      },
      "source": [
        "# Data transformation \r\n",
        "# Pytorch transform: https://pytorch.org/docs/stable/torchvision/transforms.html\r\n",
        "# Let's modify images to create some augmented ones\r\n",
        "from torchvision import datasets\r\n",
        "from torchvision import transforms\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "import torch\r\n",
        "from torchvision import utils \r\n",
        "import matplotlib.pyplot as plt \r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "# Get Mnist Train Datasets inside folder dataset\r\n",
        "train_data = datasets.MNIST(\"./dataset\", train=True, download=True)\r\n",
        "x_train, y_train = train_data.data, train_data.targets\r\n",
        "if (len(x_train.shape)==3):\r\n",
        "    x_train=x_train.unsqueeze(1) # this nember specify where to  add new tensor\r\n",
        "\r\n",
        "# Transformation applied to an Image\r\n",
        "data_transform = transforms.Compose([\r\n",
        "                                    transforms.ToPILImage(),  # Tensor of Numpy array to Pillow\r\n",
        "                                    #transforms.RandomHorizontalFlip(p=1), # Pillow transformation\r\n",
        "                                    transforms.RandomVerticalFlip(p=1), # Pillow transformation\r\n",
        "                                    transforms.ToTensor(),          # Pillow to tensor\r\n",
        "                            ])\r\n",
        "\r\n",
        "img = x_train[0][0] # Pillow images (size)\r\n",
        "print(\"Shape: \", img.shape) # Black and white image\r\n",
        "img_tr = data_transform(img).numpy()  # range 0-1\r\n",
        "print(\"Transformed Shape: \", img_tr.shape)\r\n",
        "\r\n",
        "# Display images\r\n",
        "#cv2_imshow(img)\r\n",
        "#cv2_imshow(img_tr[0]*255)\r\n",
        "plt.subplot(1,2,1)\r\n",
        "plt.imshow(img, cmap=\"gray\")\r\n",
        "plt.title(\"Original\")\r\n",
        "plt.subplot(1,2,2)\r\n",
        "plt.imshow(img_tr[0], cmap=\"gray\")\r\n",
        "plt.title(\"Transformed\")\r\n",
        "plt.axis(\"off\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape:  torch.Size([28, 28])\n",
            "Transformed Shape:  (1, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADHCAYAAAAAoQhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVbElEQVR4nO3dfbRVdZ3H8fcnlAoVES0klEgzHHQMlwiNw0JZKikDS1CWSybNRkeaGUkts4wytQbHCXEcBqcFPiXW+FBZImtW+IjUaCxQAQ2zXI4P0FVSQUBQA77zx9k0F/bv3nvuebp3Hz6vtVj3nO/5nb1/B/b98ju/p62IwMzMiucDXV0BMzOrjBO4mVlBOYGbmRWUE7iZWUE5gZuZFZQTuJlZQTmBdxFJ0yTdXOuyZRwrJH2yFscyK4ekfpIWS9ooaWZX16c1SYOy34k9uroulShkpbsjSV8ALgUOBTYAPwO+ERHrU+Uj4ppyj92ZsmatSdrU6mkv4D1gW/b8ixHxowZUYwrwBtA7vPCkptwCrwFJlwL/ClwG7At8Bvg48KCknony/o/TGiIi9t7xB3gFGN8q9ufkXedr8uPAqkqSt39X2ucEXiVJvYGrgS9FxC8i4k8R8RJwJjAIOFvSVZJ+IumHkjYAX8hiP2x1nM9LelnSm5KukPSSpJOy1/5cttVXvnMlvSLpDUnfbHWc4ZKekLReUouk2an/RGz3JukESaslfV3Sa8BtkvaTtEDSHyWtyx4f1Oo9iyR9V9L/ZN0hD0g6IHvtQ9n1/WZ27S3Nuk5+AJwLfE3SJkknSfqgpBsk/SH7c4OkD7ZTr6sk/Tg7/kZJz0j6lKRvSFor6VVJY1rVc19Jt2TX/xpJ/yypR/ZaD0nXZb83LwJ/07i/9dpzAq/eccCHgHtbByNiE/DfwMlZ6DTgJ0AfYKevrZKGAP8JfA7oT6kVP6CD844EBgMnAt+W9BdZfBvwZeAA4K+y1/+pgs9lze9AoC+lFvIUSvngtuz5QGALMHuX9/wt8HfAR4GewFez+LmUrtuDgf2BfwC2RMQXKF3v38ta/Q8B36T0LXUo8GlgOPCtduoFMB64A9gPeBpYmNV3APAdYE6r9/8A2Ap8EjgaGAP8ffbaBcC4LD4MmFTOX1R35QRevQOANyJia+K1lux1gCci4ucRsT0ituxSbhJwf0T8KiLeB74NdPR18+qI2BIRK4AVlH4RiIgnI+LXEbE1+yYwBzi+so9mTW47cGVEvJddS29GxE8jYnNEbASmk792bouI32XX8D2UkjDAnygl7k9GxLbsOtzQxnk/B3wnItZGxB8pfYM9p616ZbFfRsTC7Pfsx8BHgGsj4k/AXcAgSX0k9QPGApdExDsRsRb4N+Cs7DhnAjdExKsR8RbwL53/a+s+3L9UvTeAAyTtkUji/bPXAV5t5xgfa/16RGyW9GYH532t1ePNwN4Akj4FXE+pddGL0r/xkx19CNst/TEi3t3xRFIvSsnuFEotXYB9JPWIiB0Dn8nrjlLr+GDgLkl9gB8C38wS7K4+Brzc6vnLWSxZr8zrrR5vodRo2tbqOVldPgbsCbRI2lH+A/z/79dOv2u71KNw3AKv3hOURvZPbx2UtDdwKvBwFmqvRd0CtO5r/DCl1kwlvg/8FjgsInoD0wC1/xbbTe16TV5KqVtuRHbtjMriHV4/2djP1RExhFK34jjg820U/wOl7pEdBmaxturVGa9S+n08ICL6ZH96R8QR2estlP6jaX3uwnICr1JEvE3pK+B/SDpF0p6SBlH6ermaUsukIz8Bxks6LhtwvIrKk+4+lKYxbpJ0OPCPFR7Hdj/7UGrNrpfUF7iy3DdKGi3pL7PBwg2UulS2t1H8TuBbkj6SDYJ+m1KLvWoR0QI8AMyU1FvSByQdKmlHV9A9wEWSDpK0H3B5Lc7bVZzAayAivkeppXsdpYt3CaWWwIkR8V4Z7/8N8CVKfXktwCZgLaWWRGd9ldJA00bgJuDuCo5hu6cbgA9T6vb7NfCLTrz3QEoNkQ3Ac8BjtN14+WdgGbASeAZ4KovVyucpDbCuAtZl9eqfvXYTpQHQFdl5700doCjkefXdT9b9sp5SN8j/dnV9zKx7cgu8m5A0XlIvSXtRask/A7zUtbUys+7MCbz7OI3SQM4fgMOAs7zs2Mza4y4UM7OCcgvczKygqkrg2bS55yW9IKnQ03HMzIqm4i6UbL7n7yjt9bEaWApMjohV7bzH/TVWVxHRJYuWfG1bvaWu7Wpa4MOBFyLixWz/jrsoDcSZmVkDVJPAB7DzngKrSeygJ2mKpGWSllVxLjMz20XdN7OKiLnAXPDXTDOzWqqmBb6GnTeFOSiLmZlZA1STwJcCh0n6RLYB01nA/NpUy8zMOlJxF0pEbJU0ldLGMD2AW7NNmczMrAEauhLTfeBWb55GaM2q1tMIzcysCzmBm5kVlBO4mVlBOYGbmRWUE7iZWUE5gZuZFZQTuJlZQdV9LxQzq9y1116bi1122WVVH3fVqvyuzwsWLEiW3bp1ay42c+bMZNn169dXVzHrFLfAzcwKygnczKygnMDNzArKCdzMrKCcwM3MCsq7EVpTabbdCEeMGJGLtTULZfjw4bnYgAG5uxzWxObNm5PxWbNm5WLXXHNNLvbOO+/UvE7NzrsRmpk1ESdwM7OCcgI3MysoJ3Azs4KqahBT0kvARmAbsDUihnVQ3oOYQI8ePXKxfffdt6pjTp06NRnv1atXLjZ48OBk2QsvvDAXu+6665JlJ0+enIu9++67ybKp5eBXX311smy1mm0QszP69u2bi82ZMydZdujQobnYIYccUvM6ATz++OO5WFtL8RcuXJiLbdmypeZ1KqLUtV2LvVBGR8QbNTiOmZl1grtQzMwKqtoEHsADkp6UNKUWFTIzs/JU24UyMiLWSPoo8KCk30bE4tYFssTu5G5mVmNVtcAjYk32cy3wMyC3FCwi5kbEsI4GOM3MrHMqnoUiaS/gAxGxMXv8IPCdiPhFO+/p8pH6zhg4cGAu1rNnz2TZ4447LhcbOXJksmyfPn1ysTPOOKOTtavc6tWrk/GlS5fmYhMnTkyWTS2FXrFiRbLsFVdckYstWrSonRpWbneehdIZ/fr1y8WGDBmSLDt79uxc7PDDD695nQCWLFmSi82YMSNZ9r777svFtm/fXvM6dRe1noXSD/iZpB3H+a/2kreZmdVWxQk8Il4EPl3DupiZWSd4GqGZWUE5gZuZFZT3Aye9rBjgkUceycWqXfLeaKlBnfPOOy9ZdtOmTWUft6WlJRdbt25dsuzzzz9f9nGr5UHM2jvwwANzsdRWCpDe0mHQoEG1rhIAy5Yty8WmT5+eLDt//vy61KGRvB+4mVkTcQI3MysoJ3Azs4JyAjczKygncDOzgvIsFNIb4UN6WW+9Nr0v9/wA69evz8VGjx6dLPv+++/nYkWbSdMZnoXStVI3C0nNTDn99NOT70/NeOmMbdu2JeMPPfRQLjZ27NiqztVonoViZtZEnMDNzArKCdzMrKCcwM3MCsqDmO2YMGFCLjZu3Lhk2aeffjoXmzVrVtnnWr58eS42atSoZNnUXtxHHHFEsuzFF1+ci02Z0rw3SPIgZjEcddRRyfikSZNysWOPPTZZdsyYMWWfb+XKlbnYMccckyzbXfcU9yCmmVkTcQI3MysoJ3Azs4JyAjczK6gOE7ikWyWtlfRsq1hfSQ9K+n32c7/6VtPMzHbV4SwUSaOATcC8iDgyi30PeCsirpV0ObBfRHy9w5M1wUh97969k/GNGzfmYnPmzEmWPf/883Oxs88+Oxe78847O1k78yyU3cd7772Xi+2xR/o2v1u3bs3FPvvZzybLLlq0qKp61UtFs1AiYjHw1i7h04Dbs8e3A/n5dmZmVleV9oH3i4gd99R6DehXo/qYmVmZ0t83OiEior2vj5KmAM27csTMrItU2gJ/XVJ/gOzn2rYKRsTciBgWEcMqPJeZmSVU2gKfD5wLXJv9vK9mNermNmzYUHbZt99+u+yyF1xwQS529913J8t216W+ZtXq06dPLjZ+/Phk2R49epR93MWLF+di3XWwsjPKmUZ4J/AEMFjSaknnU0rcJ0v6PXBS9tzMzBqowxZ4RExu46UTa1wXMzPrBK/ENDMrKCdwM7OCcgI3Myso39Chjvbaa69k/P7778/Fjj/++Fzs1FNPTb7/gQceqK5iTcxL6YvhyCOPTMavv/76XOzEE8sfbmtr+4rp06fnYmvWrCn7uN2Bb+hgZtZEnMDNzArKCdzMrKCcwM3MCsqDmF3g0EMPzcWeeuqpXGz9+vXJ9z/66KO52LJly5Jlb7zxxlyskf/mjeZBzO5n4sSJudhtt92WLLvPPvuUfdxp06blYvPmzUuWbWlpScaLxIOYZmZNxAnczKygnMDNzArKCdzMrKA8iNlNeKCnNjyI2bU8QF8/HsQ0M2siTuBmZgXlBG5mVlBO4GZmBVXOPTFvlbRW0rOtYldJWiNpefZnbH2raWZmu+pwFoqkUcAmYF5EHJnFrgI2RcR1nTqZR+o7xXsmd55noTSG97pvvIpmoUTEYuCtutTIzMwqVk0f+FRJK7Mulv1qViMzMytLpQn8+8ChwFCgBZjZVkFJUyQtk5SejW9mZhWpKIFHxOsRsS0itgM3AcPbKTs3IoZFxLBKK2lmZnllLaWXNAhY0GoQs39EtGSPvwyMiIizyjjObjXQUy99+vTJxcaPH58sm1qOL6XH+R555JFc7OSTT+5k7bqWBzEbY8aMGcn4V77ylVzssccey8VOOumk5Pu3b99eXcWaWOra3qOjN0m6EzgBOEDSauBK4ARJQ4EAXgK+WNOamplZhzpM4BExORG+pQ51MTOzTvBKTDOzgnICNzMrKCdwM7OC6rAP3Lqf1Gb4d9xxR7LszTffnIvtsUf6n33UqFG52AknnJAsu2jRorYraIXUu3fvZHzjxo252L777lv2cW+66aZczLNNasMtcDOzgnICNzMrKCdwM7OCcgI3Myso35W+GzvqqKOS8UmTJuVixx57bLLsmDFjyj7fypUrc7FjjjkmWba7DkJ5KX15JkyYkIuNGzcuWfbpp5/OxWbNmlX2uZYvX56LpQbMAd55551c7IgjjkiWvfjii3OxKVOmlF2vovFd6c3MmogTuJlZQTmBm5kVlBO4mVlBOYGbmRWUZ6F0gcGDB+diU6dOzcVOP/305PsPPPDAqs6/bdu2ZPyhhx7KxcaOHVvVuRrNs1B21rdv32R8yZIludghhxxS7+q0e35IbxMxevToZNn3338/F+vMEv+i8SwUM7Mm4gRuZlZQTuBmZgXlBG5mVlDl3NT4YGAe0I/STYznRsS/S+oL3A0MonRj4zMjYl39qtq9pQYWJ09O3U40PWA5aNCgWlcJgGXLluVi06dPT5adP39+XepgXWfgwIHJ+P7779/gmuxsxIgRVR8jta/9Oeeckyy7adOmso/b0tKSi61bl05tzz//fNnHrYdyWuBbgUsjYgjwGeBCSUOAy4GHI+Iw4OHsuZmZNUiHCTwiWiLiqezxRuA5YABwGnB7Vux2IL87jpmZ1U2nbqkmaRBwNLAE6BcRO75rvEapiyX1nilA824RZmbWRcoexJS0N/BT4JKI2ND6tSitBkouZIiIuRExLCKGVVVTMzPbSVkJXNKelJL3jyLi3iz8uqT+2ev9gbX1qaKZmaV0uJRekij1cb8VEZe0is8A3oyIayVdDvSNiK91cKxuudy4Lf365XuFhgwZkiw7e/bsXOzwww+veZ0gvQx5xowZybL33XdfLtZdb8ZQC15KX57U7JSePXsmyx533HG52MiRI5Nl+/Tpk4udccYZnaxd5VavXp2ML126NBebOHFismzqphIrVqxIlr3iiitysUWLFrVTw8qlru1y+sD/GjgHeEbSjltrTAOuBe6RdD7wMnBmrSpqZmYd6zCBR8SvgLZaNSfWtjpmZlYur8Q0MysoJ3Azs4Lq1DzwZpDaH3nOnDnJskOHDs3F6rVn8uOPP56LzZw5M1l24cKFudiWLVtqXidrXq+88krZZV944YVcbN68ecmyPXr0yMWq3aM7tfUEQK9evXKx1F77ABdddFEultpPHNJbYAwblp4Fffzxx+di9RrETHEL3MysoJzAzcwKygnczKygnMDNzArKCdzMrKCa4q70qc3hL7vssmTZ4cOH52IDBgyoeZ0ANm/enIzPmjUrF7vmmmtysdSSXmufl9Jbs/Jd6c3MmogTuJlZQTmBm5kVlBO4mVlBNcVS+tS+vm3t9dsZq1atysUWLFiQLLt169ZcrK2l8OvXr6+uYmZmuAVuZlZYTuBmZgXlBG5mVlBO4GZmBdVhApd0sKRHJa2S9BtJF2fxqyStkbQ8+zO2/tU1M7MdyrkrfX+gf0Q8JWkf4ElgAqWbGG+KiOvKPpmXG1udeSm9NauK7kofES1AS/Z4o6TngPpsHmJmZmXrVB+4pEHA0cCSLDRV0kpJt0rar433TJG0TNKyqmpqZmY7KXs3Qkl7A48B0yPiXkn9gDeAAL5LqZvlvA6O4a+ZVlfuQrFmlbq2y0rgkvYEFgALI+L6xOuDgAURcWQHx/FFbnXlBG7NqqLtZCUJuAV4rnXyzgY3d5gIPFuLSpqZWXnKmYUyEvgl8AywPQtPAyYDQyl1obwEfDEb8GzvWG6lWF25BW7NquIulFrxRW715gRuzcp35DEzayJO4GZmBeUEbmZWUE7gZmYF5QRuZlZQTuBmZgXlBG5mVlBO4GZmBdXou9K/AbycPT4ge95s/Lm6zse76sRdtYDIdm8NXYm504mlZRExrEtOXkf+XGbWKO5CMTMrKCdwM7OC6soEPrcLz11P/lxm1hBd1gduZmbVcReKmVlBNTyBSzpF0vOSXpB0eaPPX0vZzZzXSnq2VayvpAcl/T77mbzZc3cm6WBJj0paJek3ki7O4oX/bGbNpKEJXFIP4EbgVGAIMFnSkEbWocZ+AJyyS+xy4OGIOAx4OHteNFuBSyNiCPAZ4MLs36kZPptZ02h0C3w48EJEvBgR7wN3Aac1uA41ExGLgbd2CZ8G3J49vh2Y0NBK1UBEtETEU9njjcBzwACa4LOZNZNGJ/ABwKutnq/OYs2kX6t7g74G9OvKylRL0iDgaGAJTfbZzIrOg5h1FKUpPoWd5iNpb+CnwCURsaH1a0X/bGbNoNEJfA1wcKvnB2WxZvK6pP4A2c+1XVyfikjak1Ly/lFE3JuFm+KzmTWLRifwpcBhkj4hqSdwFjC/wXWot/nAudnjc4H7urAuFZEk4BbguYi4vtVLhf9sZs2k4Qt5JI0FbgB6ALdGxPSGVqCGJN0JnEBpp77XgSuBnwP3AAMp7bx4ZkTsOtDZrUkaCfwSeAbYnoWnUeoHL/RnM2smXolpZlZQHsQ0MysoJ3Azs4JyAjczKygncDOzgnICNzMrKCdwM7OCcgI3MysoJ3Azs4L6P5c/+n/SeySoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IWlkAIHV7gf"
      },
      "source": [
        "# Wrapping Tensors into dataset\r\n",
        "from torchvision import datasets\r\n",
        "from torchvision import transforms\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "import torch\r\n",
        "from torchvision import utils \r\n",
        "import matplotlib.pyplot as plt \r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "from torch.utils.data import TensorDataset,Dataset\r\n",
        "\r\n",
        "class CustomTensorDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self, x,y, transform=None):\r\n",
        "        self.x = x\r\n",
        "        self.y = y\r\n",
        "        self.transform = transform\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        x = self.x[index]\r\n",
        "        if self.transform:\r\n",
        "            x = 255*self.transform(x)\r\n",
        "        y = self.y[index]\r\n",
        "        return x, y\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.x.shape[0]\r\n",
        "\r\n",
        "\r\n",
        "# Transformation\r\n",
        "# Transformation applied to an Image\r\n",
        "data_transform = transforms.Compose([\r\n",
        "                                    transforms.ToPILImage(),  # Tensor of Numpy array to Pillow\r\n",
        "                                    transforms.RandomApply( \r\n",
        "                                        torch.nn.ModuleList([\r\n",
        "                                            transforms.RandomAffine((-10,10), translate=(0.1,0.2), scale=(0.5,1.3)),\r\n",
        "                                            transforms.ColorJitter(brightness=(0.3,2), contrast=(0.9,1.1)),\r\n",
        "                                        ]),\r\n",
        "                                        p=0.5),\r\n",
        "                                    #transforms.RandomHorizontalFlip(p=1), # Pillow transformation\r\n",
        "                                    #transforms.RandomVerticalFlip(p=1), # Pillow transformation\r\n",
        "                                    transforms.ToTensor(),          # Pillow to tensor (scale 0-1)\r\n",
        "                            ])\r\n",
        "# Loading data\r\n",
        "train_data = datasets.MNIST(\"./dataset\", train=True, download=True) #,transform=data_transform) # This transform will only be appluied on data loader\r\n",
        "val_data = datasets.MNIST(\"./dataset\", train=False, download=True) #,transform=data_transform)\r\n",
        "x_train, y_train = train_data.data, train_data.targets\r\n",
        "x_val, y_val = val_data.data, val_data.targets\r\n",
        "\r\n",
        "# Add extra dimension (Preprocessing)\r\n",
        "if (len(x_train.shape)==3):\r\n",
        "    x_train=x_train.unsqueeze(1) \r\n",
        "if (len(x_val.shape)==3):\r\n",
        "    x_val=x_val.unsqueeze(1) \r\n",
        "\r\n",
        "# Create a Tensor dataset\r\n",
        "#train_ds = TensorDataset(x_train, y_train)\r\n",
        "#val_ds = TensorDataset(x_val, y_val)\r\n",
        "train_ds = CustomTensorDataset(x_train, y_train,data_transform)\r\n",
        "val_ds = CustomTensorDataset(x_val, y_val,data_transform)\r\n",
        "\r\n",
        "# Visualization Tensor Dataset\r\n",
        "i = 0\r\n",
        "for x,y in train_ds:\r\n",
        "    print(\"Y = \", y.item())\r\n",
        "    #print(\"X shape: \", x[0].shape)\r\n",
        "    cv2_imshow(x[0].numpy())\r\n",
        "    i=i+1\r\n",
        "    if i>10:\r\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zJ2DXKwYO6B"
      },
      "source": [
        "# Creating data Loader\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "train_dl = DataLoader(train_ds, batch_size=8)\r\n",
        "val_dl = DataLoader(val_ds, batch_size=8)\r\n",
        "\r\n",
        "i = 0\r\n",
        "for xb,yb in train_dl:\r\n",
        "    #print(xb[0].dtype)\r\n",
        "    #print(xb[0].min())\r\n",
        "    #print(xb[0].max())\r\n",
        "    #print(yb.shape)\r\n",
        "    print(yb[2].item())\r\n",
        "    cv2_imshow(xb[2][0].numpy())\r\n",
        "    i=i+1\r\n",
        "    if (i>10):\r\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhP54Orvmuuy"
      },
      "source": [
        "# Pytorch: Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlX1U9rbTkPt",
        "outputId": "34f5b527-3d69-4c17-e1e0-3a57dfcd28fe"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "\r\n",
        "# Tensor 64x1000\r\n",
        "input_tensor = torch.randn(64,1000)\r\n",
        "\r\n",
        "# Fully connected layer (weight dimension 1000x100)\r\n",
        "linear_layer = nn.Linear(1000,100)\r\n",
        "\r\n",
        "# Output 64x100\r\n",
        "output_tensor = linear_layer(input_tensor)\r\n",
        "\r\n",
        "print(\"input_tensor: \", input_tensor.shape) \r\n",
        "print(\"output_tensor: \", output_tensor.shape) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_tensor:  torch.Size([64, 1000])\n",
            "output_tensor:  torch.Size([64, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHK3H2Z9J-yT",
        "outputId": "dfc3e1c2-9471-4833-ca9b-04afd675ad82"
      },
      "source": [
        "# Let's create our first neural network using nn Sequential Module\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "\r\n",
        "model = nn.Sequential(\r\n",
        "    nn.Linear(4,5),\r\n",
        "    nn.ReLU(),\r\n",
        "    nn.Linear(5,1)\r\n",
        ")\r\n",
        "print(model)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=4, out_features=5, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=5, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0KwV0bykvXL"
      },
      "source": [
        "# Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWAMZva0NMPL",
        "outputId": "7f23f282-0c68-4c4a-f847-993f7b179dd1"
      },
      "source": [
        "from torch import nn\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch import optim\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torchsummary import summary\r\n",
        "#!pip install torchsummary\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import TensorDataset,Dataset\r\n",
        "from torchvision import datasets\r\n",
        "from torchvision import transforms\r\n",
        "\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Net,self).__init__()\r\n",
        "        self.conv1 = nn.Conv2d(1,20,5,1)\r\n",
        "        self.conv2 = nn.Conv2d(20,50,5,1)\r\n",
        "        self.fc1 = nn.Linear(4*4*50,500)\r\n",
        "        self.fc2 = nn.Linear(500,10)\r\n",
        "    def forward(self,x):\r\n",
        "        x = F.relu(self.conv1(x))\r\n",
        "        x = F.max_pool2d(x,2,2) \r\n",
        "        x = F.relu(self.conv2(x))\r\n",
        "        x = F.max_pool2d(x,2,2) \r\n",
        "        x = x.reshape(-1,4*4*50)\r\n",
        "        x = F.relu(self.fc1(x))\r\n",
        "        x = self.fc2(x)\r\n",
        "        x = F.log_softmax(x,dim=1)\r\n",
        "        return x\r\n",
        "\r\n",
        "class SimpleNet(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(SimpleNet,self).__init__()\r\n",
        "        self.fc1 = nn.Linear(28*28,500)\r\n",
        "        self.fc2 = nn.Linear(500,100)\r\n",
        "        self.fc3 = nn.Linear(100,10)\r\n",
        "    def forward(self,x):\r\n",
        "        x = x.reshape(-1, 28*28)\r\n",
        "        x = F.relu(self.fc1(x))\r\n",
        "        x = F.relu(self.fc2(x))\r\n",
        "        x = self.fc3(x)\r\n",
        "        x = F.log_softmax(x,dim=1)\r\n",
        "        return x\r\n",
        "\r\n",
        "class CustomTensorDataset(Dataset):\r\n",
        "\r\n",
        "    def __init__(self, x,y, transform=None):\r\n",
        "        self.x = x\r\n",
        "        self.y = y\r\n",
        "        self.transform = transform\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        x = self.x[index]\r\n",
        "        if self.transform:\r\n",
        "            #x = 255*self.transform(x)\r\n",
        "            x = self.transform(x)\r\n",
        "        y = self.y[index]\r\n",
        "        return x, y\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.x.shape[0]\r\n",
        "\r\n",
        "def augmentation_func():\r\n",
        "    data_transform = transforms.Compose([\r\n",
        "                                    transforms.ToPILImage(),  # Tensor of Numpy array to Pillow\r\n",
        "                                    transforms.RandomApply( \r\n",
        "                                        torch.nn.ModuleList([\r\n",
        "                                            transforms.RandomAffine((-10,10), translate=(0.1,0.2), scale=(0.5,1.3)),\r\n",
        "                                            transforms.ColorJitter(brightness=(0.3,2), contrast=(0.9,1.1)),\r\n",
        "                                        ]),\r\n",
        "                                        p=0.5),\r\n",
        "                                    #transforms.RandomHorizontalFlip(p=1), # Pillow transformation\r\n",
        "                                    #transforms.RandomVerticalFlip(p=1), # Pillow transformation\r\n",
        "                                    transforms.ToTensor(),          # Pillow to tensor (scale 0-1)\r\n",
        "                            ])\r\n",
        "    return data_transform\r\n",
        "\r\n",
        "def metrics_batch(target, output):\r\n",
        "    # obtain output class\r\n",
        "    pred = output.argmax(dim=1, keepdim=False)\r\n",
        "    # compare output class with target class\r\n",
        "    #corrects = pred.eq(target.view_as(pred)).sum().item()\r\n",
        "    corrects = torch.sum(pred == target)\r\n",
        "    #print(\"%s/%s\" %(corrects.item(),len(pred)))\r\n",
        "    return corrects\r\n",
        "\r\n",
        "def loss_batch(loss_func, xb,yb,yb_h, opt=None):\r\n",
        "    # obtain loss\r\n",
        "    loss = loss_func(yb_h, yb)\r\n",
        "    # obtain permormance metric \r\n",
        "    metric_b = metrics_batch(yb,yb_h)\r\n",
        "    if opt is not None:\r\n",
        "        loss.backward()\r\n",
        "        opt.step()\r\n",
        "        opt.zero_grad()\r\n",
        "    return loss.item(), metric_b\r\n",
        "\r\n",
        "def loss_epoch(model, loss_func, dataset_dl, opt,device):\r\n",
        "    loss = 0.0\r\n",
        "    metric = 0.0\r\n",
        "    len_data = len(dataset_dl.dataset)\r\n",
        "\r\n",
        "    # Get batch data\r\n",
        "    for xb,yb in dataset_dl:    \r\n",
        "        # Send to cuda the data (batch size)\r\n",
        "        xb = xb.type(torch.float32).to(device)\r\n",
        "        yb = yb.to(device)\r\n",
        "\r\n",
        "        # obtain model output \r\n",
        "        yb_h = model(xb)\r\n",
        "\r\n",
        "        # Loss and Metric Calculation\r\n",
        "        loss_b, metric_b = loss_batch(loss_func, xb,yb,yb_h,opt)\r\n",
        "        loss += loss_b\r\n",
        "        if metric_b is not None:\r\n",
        "            metric+=metric_b \r\n",
        "    \r\n",
        "    loss /=len_data\r\n",
        "    metric /=len_data\r\n",
        "    return loss, metric\r\n",
        "\r\n",
        "def train_val(epochs, model, loss_func, opt, train_dl,val_dl,device):\r\n",
        "    for epoch in range(epochs):\r\n",
        "        model.train()\r\n",
        "        train_loss,train_metric = loss_epoch(model, loss_func, train_dl, opt,device)\r\n",
        "        model.eval()\r\n",
        "        with torch.no_grad():\r\n",
        "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl,opt=None,device=device)\r\n",
        "        accuracy = 100*val_metric\r\n",
        "        print(\"epoch: %d, train_loss: %.6f, val loss: %.6f, accuracy: %.2f\" % (epoch,train_loss, val_loss,accuracy))\r\n",
        "\r\n",
        "# Setup GPU Device\r\n",
        "device = torch.device(\"cpu\")\r\n",
        "if torch.cuda.is_available():\r\n",
        "    device = torch.device(\"cuda:0\")\r\n",
        "\r\n",
        "# Load and Preprocess data \r\n",
        "train_data = datasets.MNIST(\"./dataset\", train=True, download=True) \r\n",
        "val_data = datasets.MNIST(\"./dataset\", train=False, download=True)\r\n",
        "x_train, y_train = train_data.data, train_data.targets\r\n",
        "x_val, y_val = val_data.data, val_data.targets\r\n",
        "if (len(x_train.shape)==3):\r\n",
        "    x_train=x_train.unsqueeze(1) \r\n",
        "if (len(x_val.shape)==3):\r\n",
        "    x_val=x_val.unsqueeze(1) \r\n",
        "\r\n",
        "# Transformation\r\n",
        "transform_aug = augmentation_func()\r\n",
        "train_ds = CustomTensorDataset(x_train, y_train,None)\r\n",
        "val_ds = CustomTensorDataset(x_val, y_val,None)\r\n",
        "#train_ds = TensorDataset(x_train, y_train)\r\n",
        "#val_ds = TensorDataset(x_val, y_val)\r\n",
        "\r\n",
        "# Create Data loader\r\n",
        "train_dl = DataLoader(train_ds, batch_size=128)\r\n",
        "val_dl = DataLoader(val_ds, batch_size=128)\r\n",
        "\r\n",
        "# Define Model, Loss , Optimizer\r\n",
        "#model = SimpleNet()\r\n",
        "model = Net()\r\n",
        "model.to(device)\r\n",
        "#print(model)\r\n",
        "# By default model is hosted on CPU\r\n",
        "print(\"Model Parameter Device: \", next(model.parameters()).device)\r\n",
        "summary(model, input_size=tuple(x_train.shape[1:])) # (1,28,28)\r\n",
        "loss_func = nn.NLLLoss(reduction=\"sum\") # negative log-likelihood function.\r\n",
        "opt = optim.Adam(model.parameters(), lr=1e-4)\r\n",
        "\r\n",
        "# Train\r\n",
        "num_epochs = 5\r\n",
        "train_val(num_epochs,model, loss_func,opt, train_dl, val_dl,device)\r\n",
        "\r\n",
        "# Save Models (It save last weights)\r\n",
        "path2weigths=\"./weights.pt\"\r\n",
        "torch.save(model.state_dict(),path2weigths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Parameter Device:  cuda:0\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 20, 24, 24]             520\n",
            "            Conv2d-2             [-1, 50, 8, 8]          25,050\n",
            "            Linear-3                  [-1, 500]         400,500\n",
            "            Linear-4                   [-1, 10]           5,010\n",
            "================================================================\n",
            "Total params: 431,080\n",
            "Trainable params: 431,080\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.12\n",
            "Params size (MB): 1.64\n",
            "Estimated Total Size (MB): 1.76\n",
            "----------------------------------------------------------------\n",
            "epoch: 0, train_loss: 0.412631, val loss: 0.152888, accuracy: 95.06\n",
            "epoch: 1, train_loss: 0.083941, val loss: 0.081272, accuracy: 97.36\n",
            "epoch: 2, train_loss: 0.052074, val loss: 0.055181, accuracy: 98.16\n",
            "epoch: 3, train_loss: 0.034942, val loss: 0.050851, accuracy: 98.31\n",
            "epoch: 4, train_loss: 0.024788, val loss: 0.049877, accuracy: 98.32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgUJW8V-kzV2"
      },
      "source": [
        "## Deploy Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a931ZM0vihMw",
        "outputId": "00597bfa-9fc2-4a82-c403-16aab31f3ec2"
      },
      "source": [
        "import matplotlib.pyplot as plt \r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "import torch\r\n",
        "\r\n",
        "# Setup GPU Device\r\n",
        "device = torch.device(\"cpu\")\r\n",
        "if torch.cuda.is_available():\r\n",
        "    device = torch.device(\"cuda:0\")\r\n",
        "\r\n",
        "# Define model structure\r\n",
        "md = Net()\r\n",
        "# Load weights\r\n",
        "weights = torch.load(path2weigths)\r\n",
        "# Set weigths\r\n",
        "md.load_state_dict(weights)\r\n",
        "md = md.to(device)\r\n",
        "# Check weights\r\n",
        "#print(next(md.parameters()))\r\n",
        "\r\n",
        "# Get data\r\n",
        "val_data = datasets.MNIST(\"./dataset\", train=False, download=True)\r\n",
        "x_val, y_val = val_data.data, val_data.targets\r\n",
        "\r\n",
        "# Prediction\r\n",
        "for i in range(0,5):\r\n",
        "    #i = 20\r\n",
        "    x = x_val[i]\r\n",
        "    x = x.unsqueeze(0)\r\n",
        "    x = x.unsqueeze(0)\r\n",
        "    x = x.type(torch.float32)\r\n",
        "    x = x.to(device)\r\n",
        "    output = md(x)\r\n",
        "    pred = output.argmax(dim=1, keepdim=False).item()\r\n",
        "    print(\"Prediction: \", pred)\r\n",
        "    xplot = x.to(\"cpu\").numpy()[0][0]\r\n",
        "    plt.imshow(xplot, cmap=\"gray\")\r\n",
        "    plt.axis(\"off\")\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction:  7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Prediction:  2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Prediction:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAEtklEQVR4nO3dMY+MaxiA4Z3jaOgkREclJASJQkKiUYlGq/IDJP6H1k9QSTZRiNBSaihFpVBRqIiGTy3Z792c3dkz95jrKufJt3mbO0+yb76ZxTRNW0DPP6s+ALAzcUKUOCFKnBAlToj6dzRcLBb+lQsHbJqmxU6f25wQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSo4U8AsnnOnDkzO3v//v3w2QcPHgznjx492tOZNpXNCVHihChxQpQ4IUqcECVOiBInRLnn5A+XL1+enf369Wv47KdPn5Z9nI1mc0KUOCFKnBAlTogSJ0SJE6LECVHuOfnDpUuXZmffvn0bPvv06dNlH2ej2ZwQJU6IEidEiROixAlR4oQoVykb5vz588P5/fv3Z2ePHz9e9nEYsDkhSpwQJU6IEidEiROixAlR4oQo95wb5uzZs8P50aNHZ2dPnjxZ9nEYsDkhSpwQJU6IEidEiROixAlR4oSoxTRN88PFYn7IWnrz5s1wfvz48dnZbu+C7vbVmexsmqbFTp/bnBAlTogSJ0SJE6LECVHihChxQpT3Of8yp0+fHs6vXLkynH/48GF25h7z/2VzQpQ4IUqcECVOiBInRIkTosQJUe45/zI3btzY1/NfvnxZ0knYL5sTosQJUeKEKHFClDghSpwQ5SrlL3PhwoV9Pf/w4cMlnYT9sjkhSpwQJU6IEidEiROixAlR4oQoPwG4Zq5evTqcP3/+fDj/+PHjcH7t2rXZ2Y8fP4bPsjd+AhDWjDghSpwQJU6IEidEiROixAlR3udcMzdv3hzOjx07Npy/fPlyOHeX2WFzQpQ4IUqcECVOiBInRIkTosQJUe4518zFixeH89H7uVtbW1vb29vLPA4HyOaEKHFClDghSpwQJU6IEidEiROifG9tzMmTJ4fzd+/eDedfv34dzs+dO/efz8TB8r21sGbECVHihChxQpQ4IUqcEOWVsZh79+4N5ydOnBjOX7x4scTTsEo2J0SJE6LECVHihChxQpQ4IUqcEOWeM+bUqVP7en63V8ZYHzYnRIkTosQJUeKEKHFClDghSpwQ5Z4z5vbt2/t6/tmzZ0s6Catmc0KUOCFKnBAlTogSJ0SJE6LECVHuOVfg+vXrs7PdfgKQzWFzQpQ4IUqcECVOiBInRIkTolylrMCdO3dmZ4cOHRo++/bt2+H89evXezoTPTYnRIkTosQJUeKEKHFClDghSpwQ5Z7zABw5cmQ4v3Xr1p7/9vb29nD+8+fPPf9tWmxOiBInRIkTosQJUeKEKHFClDghajFN0/xwsZgfMuvw4cPD+atXr2Znnz9/Hj579+7d4fz79+/DOT3TNC12+tzmhChxQpQ4IUqcECVOiBInRIkTotxzwoq554Q1I06IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSo4U8AAqtjc0KUOCFKnBAlTogSJ0SJE6J+A+DQksva5TOKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Prediction:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAGR0lEQVR4nO3dv2sUaQDG8Uy8ztZtbGIRiNhZqF0EK1MIaURBRLDwB2LsDYi2ClaGBDv/AJsgKWwEEdJoYZOtBE0johaCBCHoXCfI7bzDZbPZZ93Pp/RhN2OOrwP3MrtVXdcTQJ7JYV8A0Js4IZQ4IZQ4IZQ4IdQ/pbGqKv8rFwasruuq15+7c0IocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUIocUKo4lcAsvf2799f3B88eFDcr169WtzfvHlT3M+ePdu4ffjwofhadpc7J4QSJ4QSJ4QSJ4QSJ4QSJ4QSJ4Sq6rpuHquqeWQgpqeni3u32+3r/Scny/8eLywsNG5LS0t9/Wx6q+u66vXn7pwQSpwQSpwQSpwQSpwQSpwQSpwQyvOcQ9DpdBq3J0+e7OGVkMydE0KJE0KJE0KJE0KJE0KJE0I5ShmA0mNXExMTE/Pz843b8ePHd/ty/pfZ2dnGre1xs7dv3xb3ly9f7uiaxpU7J4QSJ4QSJ4QSJ4QSJ4QSJ4QSJ4Ty0ZgD8PPnz+L+69evPbqS/2o7q+zn2tq+IvDcuXPFve3rCf9WPhoTRow4IZQ4IZQ4IZQ4IZQ4IZQ4IZRzzh1YW1sr7nNzc8V9mOecX79+Le7fv39v3Kampnb7cv6wb9++gb5/KuecMGLECaHECaHECaHECaHECaHECaF8bm0PJ0+eLO4zMzPFve0cc5DnnCsrK8X9+fPnxf3bt2+N26lTp4qvXVxcLO5trl+/3rgtLy/39d6jyJ0TQokTQokTQokTQokTQokTQokTQo3l85yHDh0q7uvr68X9wIEDxb2fz4Zt++zXp0+fFvd79+4V962treJe0vY8Z9vvrdPpFPcfP340bnfu3Cm+9tGjR8V9e3u7uA+T5zlhxIgTQokTQokTQokTQokTQo3lUcr09HRx73a7fb1/21HKixcvGrfz588XX/vly5cdXdNeuHnzZnF/+PBhcS/93toeszt8+HBxf/fuXXEfJkcpMGLECaHECaHECaHECaHECaHECaF8NOYAvH79urhfvny5cUs+x2yzurpa3C9cuFDcjx07tpuXM/LcOSGUOCGUOCGUOCGUOCGUOCGUOCGUc84e2p7HbHPixIldupLRUlU9H0v8re332s/v/e7du8X94sWLO37vYXHnhFDihFDihFDihFDihFDihFDihFBjec557dq14t72Gan0dubMmeJ+9OjR4l76vbf9N2k75xxF7pwQSpwQSpwQSpwQSpwQSpwQSpwQaizPOdvO48ZZp9Np3I4cOVJ87e3bt3f7cn77/Plzcd/e3h7Yzx4Wd04IJU4IJU4IJU4IJU4IJU4INZZHKTRbXFxs3G7cuDHQn/3+/fvG7dKlS8XXbm5u7vLVDJ87J4QSJ4QSJ4QSJ4QSJ4QSJ4QSJ4Ryzjlm1tbWivvMzMweXcl/bWxsNG6vXr3awyvJ4M4JocQJocQJocQJocQJocQJocQJocbynLOqquI+Odnfv1lzc3M7fu3jx4+L+8GDB3f83hMT7X+3YX79oY8s/ZM7J4QSJ4QSJ4QSJ4QSJ4QSJ4QSJ4Qay3PO5eXl4n7//v2+3v/Zs2fFvZ+zxEGfQw7y/VdWVgb23n8jd04IJU4IJU4IJU4IJU4IJU4IVdV13TxWVfM4wqampor7+vp6ce90OsU9+bGstmv79OlT49btdouvvXLlSnH/+PFjcd/a2iruf6u6rns+w+jOCaHECaHECaHECaHECaHECaHECaHG8pyzzezsbHGfn58v7rdu3SruyeecCwsLjdvS0tJuXw4Tzjlh5IgTQokTQokTQokTQokTQokTQjnnHIDTp08X99Jzj21fg7e6ulrc275CsO3rDzc2Nhq3zc3N4mvZGeecMGLECaHECaHECaHECaHECaHECaGcc8KQOeeEESNOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCFX8CkBgeNw5IZQ4IZQ4IZQ4IZQ4IZQ4IdS/5YAxSNsONjwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Prediction:  4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTF1qF4s6Qy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
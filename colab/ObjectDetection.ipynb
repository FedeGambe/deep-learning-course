{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ObjectDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gLO1fDIi8f92",
        "aBK_BnVD9KVb",
        "rw8T85fKg6mU",
        "NvTGrrH6Zyi0"
      ],
      "authorship_tag": "ABX9TyPQDZsrtA3+t9BnI4s0sQS4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visiont3lab/deep-learning-course/blob/main/colab/ObjectDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Glgo-Ecf-Q9K"
      },
      "source": [
        "# Single Object detection project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zG75kk5kEck"
      },
      "source": [
        "## Step 1) Use CVAT data annotation tool to  Collect Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aItIsGh-UQy"
      },
      "source": [
        "* [Annotation CVAT  Github](https://github.com/openvinotoolkit/cvat)\n",
        "\n",
        "1. Accedere al sito [Annotation CVAT  Online](https://cvat.org/)\n",
        "2. Creare un account\n",
        "3. Eseguire login\n",
        "4. Creare un nuovo progetto (face-custo-keypoints)\n",
        "5. Selezionare Constructor e configurare le labels ( eye-point-left, eye-point-right, face-rect, nose)\n",
        "6. Creare e Aprire il progetto\n",
        "7. Creare le label\n",
        "8. Esportare le label come **CVAT for images 1.1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDSkNV3xkJ7a"
      },
      "source": [
        "# wget a file from google drive -- https://silicondales.com/tutorials/g-suite/how-to-wget-files-from-google-drive/\n",
        "# Dataset link: https://drive.google.com/file/d/12ob_5b-A0afVNjhPkmlNPgKIaM5tnA3m/view\n",
        "# Use the below to wget in large files (over 100MB) from Google Drive. Added by Robin Scott of SiliconDales.com. See full instructions at https://silicondales.com/tutorials/g-suite/how-to-wget-files-from-google-drive/\n",
        "# wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=FILE_ID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=FILE_ID\" -O FILE_NAME && rm -rf /tmp/cookies.txt\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=12ob_5b-A0afVNjhPkmlNPgKIaM5tnA3m' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=12ob_5b-A0afVNjhPkmlNPgKIaM5tnA3m\" -O dataset.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip dataset.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MDzAz9jh587"
      },
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd \n",
        "\n",
        "# CVAT for images 1.1\n",
        "# Labels --> filename xc,yc,w,h --> coordinate 0-1 --> coco dataset\n",
        "\n",
        "mydict = {\"filename\": [],  \"xc\": [] ,\"yc\": [],\"w\": [],\"h\": []}\n",
        "xml_file = \"dataset/annotations.xml\"\n",
        "tree = ET.parse(xml_file)\n",
        "root = tree.getroot()\n",
        "for image in root:\n",
        "    if image.tag==\"image\":\n",
        "        #print(image.tag, image.attrib)\n",
        "        iW = float(image.attrib[\"width\"])\n",
        "        iH = float(image.attrib[\"height\"])\n",
        "        for box in image:\n",
        "            #print(box.attrib)\n",
        "            bW = float(box.attrib[\"xbr\"]) - float(box.attrib[\"xtl\"])\n",
        "            bH = float(box.attrib[\"ybr\"]) - float(box.attrib[\"ytl\"])\n",
        "            xc =  float(box.attrib[\"xtl\"]) + bW/ 2.0  # ---> xc : W = xcN : 1\n",
        "            yc =  float(box.attrib[\"ytl\"]) + bH/ 2.0  # ---> yc : W = ycN : 1\n",
        "            xcN = xc / iW\n",
        "            ycN = yc / iH\n",
        "            bWN = bW / iW\n",
        "            bHN = bH / iH\n",
        "            mydict[\"filename\"].append(image.attrib[\"name\"])\n",
        "            mydict[\"xc\"].append(xcN)\n",
        "            mydict[\"yc\"].append(ycN)\n",
        "            mydict[\"w\"].append(bWN)\n",
        "            mydict[\"h\"].append(bHN)\n",
        "            \n",
        "df = pd.DataFrame(mydict)\n",
        "df.to_csv(\"dataset/annotations.csv\", index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko7niUasKd9g"
      },
      "source": [
        "## Step 2) Include library Library and Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsR3Cv8fKgYA"
      },
      "source": [
        "import os\n",
        "from PIL import Image,ImageFont,ImageDraw\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT8AghpkFPmQ"
      },
      "source": [
        "## Step 3) Understading data augmentation: Data Augmentation Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYbcVfCZFR56",
        "outputId": "7342a820-35f3-4c0c-da96-423b02448636"
      },
      "source": [
        "import torchvision.transforms.functional as TF\n",
        "import random \n",
        "\n",
        "def resize_img_label(image,label=(0.,0.),target_size=(256,256)):\n",
        "    w_orig,h_orig = image.size   \n",
        "    w_target,h_target = target_size\n",
        "    cx, cy= label\n",
        "    \n",
        "    # resize image and label\n",
        "    image_new = TF.resize(image,target_size)\n",
        "    label_new= cx/w_orig*w_target, cy/h_orig*h_target\n",
        "    \n",
        "    return image_new,label_new\n",
        "\n",
        "def random_hflip(image,label):\n",
        "    w,h=image.size\n",
        "    x,y=label        \n",
        "\n",
        "    image = TF.hflip(image)\n",
        "    label = w-x, y\n",
        "    return image,label\n",
        "\n",
        "def random_vflip(image,label):\n",
        "    w,h=image.size\n",
        "    x,y=label\n",
        "    image = TF.vflip(image)\n",
        "    label = x, h-y\n",
        "    return image, label\n",
        "\n",
        "def random_shift(image,label,max_translate=(0.2,0.2)):\n",
        "    w,h=image.size\n",
        "    max_t_w, max_t_h=max_translate\n",
        "    cx, cy=label\n",
        "\n",
        "    # translate coeficinet, random [-1,1]\n",
        "    trans_coef=np.random.rand()*2-1\n",
        "    w_t = int(trans_coef*max_t_w*w)\n",
        "    h_t = int(trans_coef*max_t_h*h)\n",
        "\n",
        "    image=TF.affine(image,translate=(w_t, h_t),shear=0,angle=0,scale=1)\n",
        "    label = cx+w_t, cy+h_t\n",
        "        \n",
        "    return image,label\n",
        "\n",
        "def transformer(image, label, params):\n",
        "    image,label=resize_img_label(image,label,params[\"target_size\"])\n",
        "\n",
        "    if random.random() < params[\"p_hflip\"]:\n",
        "        image,label=random_hflip(image,label)\n",
        "        \n",
        "    if random.random() < params[\"p_vflip\"]:            \n",
        "        image,label=random_vflip(image,label)\n",
        "        \n",
        "    if random.random() < params[\"p_shift\"]:                            \n",
        "        image,label=random_shift(image,label, params[\"max_translate\"])\n",
        "\n",
        "    if random.random() < params[\"p_brightness\"]:\n",
        "        brightness_factor=1+(np.random.rand()*2-1)*params[\"brightness_factor\"]\n",
        "        image=TF.adjust_brightness(image,brightness_factor)\n",
        "\n",
        "    if random.random() < params[\"p_contrast\"]:\n",
        "        contrast_factor=1+(np.random.rand()*2-1)*params[\"contrast_factor\"]\n",
        "        image=TF.adjust_contrast(image,contrast_factor)\n",
        "\n",
        "    if random.random() < params[\"p_gamma\"]:\n",
        "        gamma=1+(np.random.rand()*2-1)*params[\"gamma\"]\n",
        "        image=TF.adjust_gamma(image,gamma)\n",
        "\n",
        "    if params[\"scale_label\"]:\n",
        "        label=scale_label(label,params[\"target_size\"])\n",
        "        \n",
        "    image=TF.to_tensor(image)\n",
        "    return image, label\n",
        "\n",
        "def scale_label(a,b):\n",
        "    div = [ai/bi for ai,bi in zip(a,b)]\n",
        "    return div\n",
        "\n",
        "'''\n",
        "params_aug={\n",
        "    \"target_size\" : (416, 416),\n",
        "    \"p_hflip\" : 0.6,\n",
        "    \"p_vflip\" : 0.0,\n",
        "    \"p_shift\" : 0.6,\n",
        "    \"max_translate\": (0.3, 0.2),\n",
        "    \"p_brightness\": 0.8,\n",
        "    \"brightness_factor\": 0.5,\n",
        "    \"p_contrast\": 0.6,\n",
        "    \"contrast_factor\": 0.6,\n",
        "    \"p_gamma\": 0.5,\n",
        "    \"gamma\": 0.4,\n",
        "    \"scale_label\": True,\n",
        "}\n",
        "\n",
        "imgs = []\n",
        "for x,y in train_dl:\n",
        "    for im,l in zip(x,y):\n",
        "        # normalized coordinates\n",
        "        xcN,ycN,wN,hN = l.tolist()\n",
        "        W,H = params_aug[\"target_size\"]\n",
        "        xc,yc,_,_ = rescale_bbox((xcN,ycN,wN,hN),W,H)\n",
        "        img = TF.to_pil_image(im)\n",
        "        imt,l_temp=transformer(img,(xc,yc),params_aug)\n",
        "        xct, yct = l_temp\n",
        "        lt = torch.tensor([xct,yct,wN,hN],dtype=torch.float32)\n",
        "        imgs.append( TF.to_tensor( get_img_bbox(imt,lt,classNames,colors,labelScaled=True) ) ) \n",
        "        #break\n",
        "grid =torchvision.utils.make_grid(torch.stack(imgs,dim=0), nrow=5, padding=5)\n",
        "display(transforms.ToPILImage()(grid))  \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nparams_aug={\\n    \"target_size\" : (416, 416),\\n    \"p_hflip\" : 0.6,\\n    \"p_vflip\" : 0.0,\\n    \"p_shift\" : 0.6,\\n    \"max_translate\": (0.3, 0.2),\\n    \"p_brightness\": 0.8,\\n    \"brightness_factor\": 0.5,\\n    \"p_contrast\": 0.6,\\n    \"contrast_factor\": 0.6,\\n    \"p_gamma\": 0.5,\\n    \"gamma\": 0.4,\\n    \"scale_label\": True,\\n}\\n\\nimgs = []\\nfor x,y in train_dl:\\n    for im,l in zip(x,y):\\n        # normalized coordinates\\n        xcN,ycN,wN,hN = l.tolist()\\n        W,H = params_aug[\"target_size\"]\\n        xc,yc,_,_ = rescale_bbox((xcN,ycN,wN,hN),W,H)\\n        img = TF.to_pil_image(im)\\n        imt,l_temp=transformer(img,(xc,yc),params_aug)\\n        xct, yct = l_temp\\n        lt = torch.tensor([xct,yct,wN,hN],dtype=torch.float32)\\n        imgs.append( TF.to_tensor( get_img_bbox(imt,lt,classNames,colors,labelScaled=True) ) ) \\n        #break\\ngrid =torchvision.utils.make_grid(torch.stack(imgs,dim=0), nrow=5, padding=5)\\ndisplay(transforms.ToPILImage()(grid))  \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9KYT3uArYYX"
      },
      "source": [
        "## Step 4) Dataset Preparation ( Dataset, DataLoader, Data Augmentation ) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Hw68n6v3b2"
      },
      "source": [
        "!wget https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWvu-3A9rVGQ"
      },
      "source": [
        "def rescale_bbox(bb,W,H):\n",
        "    # rescale coordiante 0-1 to W-H (original size)\n",
        "    x,y,w,h = bb\n",
        "    return [x*W,y*H,w*W,h*H]\n",
        "\n",
        "def get_img_bbox(imageTensor,labelTensor,classNames,colors,labelScaled=True):\n",
        "    fnt = ImageFont.truetype('thsarabunnew-webfont.ttf',12)\n",
        "    imagePil = transforms.ToPILImage()(imageTensor)\n",
        "    bbox = labelTensor.tolist()\n",
        "    W,H = imagePil.size\n",
        "    draw = ImageDraw.Draw(imagePil)\n",
        "    name = classNames[0]\n",
        "    if labelScaled:\n",
        "        bbox = rescale_bbox(bbox,W,H)\n",
        "    xc,yc,w,h = bbox\n",
        "    draw.rectangle( [(xc-w/2,yc-h/2), (xc+w/2,yc+h/2)], outline=tuple(colors[0]),width=1)\n",
        "    draw.text( (xc-w/2,yc-h/2), name, font=fnt, fill=(255,0,0,0) )\n",
        "    return imagePil\n",
        "\n",
        "def compare_img_bbox(imageTensor,labelTensor,labelEstimatedTensor):\n",
        "    fnt = ImageFont.truetype('thsarabunnew-webfont.ttf',12)\n",
        "    imagePil = transforms.ToPILImage()(imageTensor)\n",
        "    for l,c in zip([labelTensor,labelEstimatedTensor],[(255,0,0),(0,255,0)]):\n",
        "        bbox = l.tolist()\n",
        "        W,H = imagePil.size\n",
        "        draw = ImageDraw.Draw(imagePil)\n",
        "        bbox = rescale_bbox(bbox,W,H)\n",
        "        xc,yc,w,h = bbox\n",
        "        draw.rectangle( [(xc-w/2,yc-h/2), (xc+w/2,yc+h/2)], outline=tuple(c),width=1)\n",
        "    return imagePil\n",
        "\n",
        "def get_transforms(target_size):\n",
        "    train_tf = transforms.Compose([\n",
        "            transforms.Resize(target_size),\n",
        "            #transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "    test_tf = transforms.Compose([\n",
        "            transforms.Resize(target_size),\n",
        "            transforms.ToTensor(),\n",
        "    ])\n",
        "    return train_tf, test_tf\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, folder_images_path, folder_labels_path,transform=None,params=None):\n",
        "        self.df_labels = pd.read_csv(folder_labels_path) # dataframe containing labels\n",
        "        self.path2images = [ os.path.join(folder_images_path,name) for name in os.listdir(folder_images_path)]\n",
        "        self.transform = transform  \n",
        "        self.params = params\n",
        "    def __getitem__(self, index):\n",
        "        path2img = self.path2images[index]\n",
        "        # get filename \n",
        "        filename = os.path.basename(path2img)\n",
        "        # get label associated to filename\n",
        "        label = self.df_labels[self.df_labels[\"filename\"]==filename].values.tolist()\n",
        "        xc,yc,w,h = label[0][1:] # custom scenario (Normalized labels)\n",
        "        x = Image.open(path2img).convert(\"RGB\")\n",
        "        y = torch.tensor([xc,yc,w,h], dtype=torch.float32)\n",
        "        if self.transform:\n",
        "            x =self.transform(x)\n",
        "        if self.params:\n",
        "            iW,iH = x.size # image size\n",
        "            xcPix, ycPix,_,_ = rescale_bbox((xc,yc,w,h),iW,iH)\n",
        "            x, y_temp=transformer(x,(xcPix,ycPix),self.params)\n",
        "            xct, yct = y_temp\n",
        "            y = torch.tensor([xct,yct,w,h],dtype=torch.float32)  \n",
        "        return x,y\n",
        "    def __len__(self):\n",
        "        return len(self.path2images)\n",
        "\n",
        "# Common params\n",
        "classNames = [\"manuel\"]\n",
        "colors = [(255,0,0)]\n",
        "folder_train_images_path = os.path.join(\"dataset\",\"train\")\n",
        "folder_test_images_path = os.path.join(\"dataset\",\"test\")\n",
        "folder_labels_path = os.path.join(\"dataset\",\"annotations.csv\")\n",
        "\n",
        "# Siple Dataset\n",
        "#target_size = (416,416)\n",
        "#train_tf, test_tf = get_transforms(target_size)\n",
        "#train_ds = CustomDataset(folder_train_images_path,folder_labels_path,train_tf)\n",
        "#test_ds = CustomDataset(folder_test_images_path,folder_labels_path,test_tf)\n",
        "\n",
        "# Data Augmentation\n",
        "params_aug={\n",
        "    \"target_size\" : (416, 416),\n",
        "    \"p_hflip\" : 0.6,\n",
        "    \"p_vflip\" : 0.0,\n",
        "    \"p_shift\" : 0.6,\n",
        "    \"max_translate\": (0.3, 0.2),\n",
        "    \"p_brightness\": 0.8,\n",
        "    \"brightness_factor\": 0.5,\n",
        "    \"p_contrast\": 0.6,\n",
        "    \"contrast_factor\": 0.6,\n",
        "    \"p_gamma\": 0.5,\n",
        "    \"gamma\": 0.4,\n",
        "    \"scale_label\": True,\n",
        "}\n",
        "train_ds = CustomDataset(folder_train_images_path,folder_labels_path,None,params_aug)\n",
        "test_ds = CustomDataset(folder_test_images_path,folder_labels_path,None,params_aug)\n",
        "\n",
        "# Data loader\n",
        "train_dl = torch.utils.data.DataLoader(train_ds,batch_size=32,shuffle=True)\n",
        "test_dl = torch.utils.data.DataLoader(test_ds,batch_size=32,shuffle=True)\n",
        "\n",
        "imgs = []\n",
        "for x,y in train_dl:\n",
        "    for im,l in zip(x,y):\n",
        "        imgs.append( transforms.ToTensor()( get_img_bbox(im,l,classNames,colors,labelScaled=True) ) )\n",
        "grid =torchvision.utils.make_grid(torch.stack(imgs,dim=0), nrow=5, padding=5)\n",
        "display(transforms.ToPILImage()(grid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rie_HIb31Sb"
      },
      "source": [
        "## Step 5) Design Object Detection Model: Deep Residual Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K48zzlBjrVI0",
        "outputId": "7436c55d-9a48-4a66-b0d3-fbe00ca1e50f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# To train both xc,yc,w,h,p we need to pass images with and without person\n",
        "# https://discuss.pytorch.org/t/how-can-i-use-different-losses-to-update-different-branches-respectively-and-sum-grad-together-to-update-master-main-branch/21700/12\n",
        "'''\n",
        "mse = nn.MSELoss(reduction=\"sum\")\n",
        "bce = nn.BCELoss(reduction=\"sum\")\n",
        "loss_xc = mse(torch.tensor(3.4,requires_grad=True),torch.tensor(9.5,requires_grad=True))\n",
        "loss_yc = mse(torch.tensor(3.2,requires_grad=True),torch.tensor(5.6,requires_grad=True))\n",
        "loss_w = mse(torch.tensor(3.1,requires_grad=True),torch.tensor(2.3,requires_grad=True))\n",
        "loss_h = mse(torch.tensor(3.4,requires_grad=True),torch.tensor(1.2,requires_grad=True))\n",
        "loss_p = bce(torch.tensor(0.3,requires_grad=True),torch.tensor(1.0)) #[0-1]\n",
        "print(loss_xc,loss_yc,loss_w,loss_h, loss_p)\n",
        "loss = loss_xc + loss_yc + loss_w + loss_h + loss_p\n",
        "loss.backward()\n",
        "print(loss.item())\n",
        "'''\n",
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self,C_in, size, init_f, num_outputs ):\n",
        "        super(Net,self).__init__()\n",
        "        # size --> H_in , W_in\n",
        "        # C_in, H_in, W_in, init_f, num_outputs = params #(3, 416, 416, 16, 2)\n",
        "        H_in, W_in = size\n",
        "        self.conv1 = nn.Conv2d(C_in, init_f, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(C_in+init_f,2*init_f, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(C_in+3*init_f,4*init_f, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(C_in+7*init_f,8*init_f, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(C_in+15*init_f,16*init_f, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(16*init_f, num_outputs)\n",
        "\n",
        "    def forward(self,x):\n",
        "        # Block 1\n",
        "        identity = F.avg_pool2d(x,kernel_size=4,stride=4)\n",
        "        #print(\"1. Identity \",identity.shape)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        #print(\"1. Conv+Relu \", x.shape)\n",
        "        x = F.max_pool2d(x,kernel_size=2,stride=2)\n",
        "        #print(\"1. MaxPool \",x.shape)\n",
        "        x = torch.cat((x,identity),dim=1)\n",
        "        #print(\"1. Stack \", x.shape)\n",
        "\n",
        "        # Block 2\n",
        "        identity = F.avg_pool2d(x,kernel_size=2,stride=2)\n",
        "        #print(\"2. Identity \",identity.shape)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        #print(\"2. Conv+Relu \", x.shape)\n",
        "        x = F.max_pool2d(x,kernel_size=2,stride=2)\n",
        "        #print(\"2. MaxPool \",x.shape)\n",
        "        x = torch.cat((x,identity),dim=1)\n",
        "        #print(\"2. Stack \", x.shape)\n",
        "\n",
        "        # Block 3\n",
        "        identity = F.avg_pool2d(x,kernel_size=2,stride=2)\n",
        "        #print(\"3. Identity \",identity.shape)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        #print(\"3. Conv+Relu \", x.shape)\n",
        "        x = F.max_pool2d(x,kernel_size=2,stride=2)\n",
        "        #print(\"3. MaxPool \",x.shape)\n",
        "        x = torch.cat((x,identity),dim=1)\n",
        "        #print(\"3. Stack \", x.shape)\n",
        "\n",
        "        # Block 4\n",
        "        identity = F.avg_pool2d(x,kernel_size=2,stride=2)\n",
        "        #print(\"4. Identity \",identity.shape)\n",
        "        x = F.relu(self.conv4(x))\n",
        "        #print(\"4. Conv+Relu \", x.shape)\n",
        "        x = F.max_pool2d(x,kernel_size=2,stride=2)\n",
        "        #print(\"4. MaxPool \",x.shape)\n",
        "        x = torch.cat((x,identity),dim=1)\n",
        "        #print(\"4. Stack \", x.shape)\n",
        "\n",
        "        # Block 5\n",
        "        x = F.relu(self.conv5(x))\n",
        "        #print(\"5. Conv+Relu \", x.shape)\n",
        "        x = F.adaptive_avg_pool2d(x,output_size=1)\n",
        "        #print(\"5. AvgPool \",x.shape)\n",
        "        x = x.reshape(x.size(0),-1)\n",
        "        #print(\"5. Reshape \", x.shape)\n",
        "        x = self.fc1(x)\n",
        "        #print(\"5 FC \",x.shape)\n",
        "        return x\n",
        "\n",
        "W,H = params_aug[\"target_size\"]\n",
        "C = 3\n",
        "net = Net(C_in=C, size=(W,H), init_f=16, num_outputs=4) # outputs xc,yc,w,h\n",
        "dummy_img = torch.rand(1,3,W,H)\n",
        "with torch.no_grad():\n",
        "    dummy_out = net.forward(dummy_img)\n",
        "    print(dummy_out.shape)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvIVfKBH5YfP"
      },
      "source": [
        "## Step 6) Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq7LwXfsrVMI"
      },
      "source": [
        "from torch import optim\n",
        "from torchsummary import summary\n",
        "import copy\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# validation metric classification\n",
        "def metrics_func_classification(target, output):\n",
        "    # Compute number of correct prediction\n",
        "    pred = output.argmax(dim=-1,keepdim=True)\n",
        "    corrects =pred.eq(target.reshape(pred.shape)).sum().item()\n",
        "    return -corrects # minus for coeherence with best result is the most negative one\n",
        "\n",
        "# validation: metric regression\n",
        "def metrics_func_regression(target, output):\n",
        "    # Comptue mean squaer error (Migliora quanto piu' ci avviciniamo a zero)\n",
        "    mse = torch.sum((output - target) ** 2)\n",
        "    return mse\n",
        "\n",
        "# training: loss calculation and backward step\n",
        "def loss_batch(loss_func,metric_func, xb,yb,yb_h, opt=None):\n",
        "    # obtain loss\n",
        "    loss = loss_func(yb_h, yb)\n",
        "    # obtain performance metric \n",
        "    with torch.no_grad():\n",
        "        metric_b = metric_func(yb,yb_h)\n",
        "    if opt is not None:\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    return loss.item(), metric_b\n",
        "\n",
        "# one epoch training\n",
        "def loss_epoch(model, loss_func,metric_func, dataset_dl, sanity_check,opt, device):\n",
        "    loss = 0.0\n",
        "    metric = 0.0\n",
        "    len_data = float(len(dataset_dl.dataset))\n",
        "    # get batch data\n",
        "    for xb,yb in dataset_dl:    \n",
        "        # send to cuda the data (batch size)\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        # obtain model output \n",
        "        yb_h = model.forward(xb)\n",
        "        # loss and metric Calculation\n",
        "        loss_b, metric_b = loss_batch(loss_func,metric_func, xb,yb,yb_h,opt)\n",
        "        # update loss\n",
        "        loss += loss_b\n",
        "        # update metric\n",
        "        if metric_b is not None:\n",
        "            metric+=metric_b \n",
        "        if sanity_check is True:\n",
        "            break\n",
        "    # average loss\n",
        "    loss /=len_data\n",
        "    # average metric\n",
        "    metric /=len_data\n",
        "    return loss, metric\n",
        "\n",
        "# get learning rate from optimizer\n",
        "def get_lr(opt):\n",
        "    # opt.param_groups[0]['lr']\n",
        "    for param_group in opt.param_groups:\n",
        "        return param_group[\"lr\"]\n",
        "\n",
        "# trainig - test loop\n",
        "def train_test(params):\n",
        "    # --> extract params\n",
        "    model = params[\"model\"]\n",
        "    loss_func=params[\"loss_func\"]\n",
        "    metric_func=params[\"metric_func\"]\n",
        "    num_epochs=params[\"num_epochs\"]\n",
        "    opt=params[\"optimizer\"]\n",
        "    lr_scheduler=params[\"lr_scheduler\"]\n",
        "    train_dl=params[\"train_dl\"]\n",
        "    test_dl=params[\"test_dl\"]\n",
        "    device=params[\"device\"]\n",
        "    continue_training=params[\"continue_training\"]\n",
        "    sanity_check=params[\"sanity_check\"]\n",
        "    path2weigths=params[\"path2weigths\"]\n",
        "    # --> send model to device and print device\n",
        "    model = model.to(device)\n",
        "    print(\"--> training device %s\" % (device))\n",
        "    # --> if continue_training=True load path2weigths\n",
        "    if continue_training==True and os.path.isfile(path2weigths):\n",
        "        print(\"--> continue training  from last best weights\")\n",
        "        weights = torch.load(path2weigths)\n",
        "        model.load_state_dict(weights)\n",
        "    # --> history of loss values in each epoch\n",
        "    loss_history={\"train\": [],\"test\":[]}\n",
        "    # --> history of metric values in each epoch\n",
        "    metric_history={\"train\": [],\"test\":[]}\n",
        "    # --> a deep copy of weights for the best performing model\n",
        "    best_model_weights = copy.deepcopy(model.state_dict())\n",
        "    # --> initialiaze best loss to large value\n",
        "    best_loss=float(\"inf\")\n",
        "    # --> main loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # --> get learning rate\n",
        "        lr = get_lr(opt)\n",
        "        print(\"----\\nEpoch %s/%s, lr=%.6f\" % (epoch+1,num_epochs,lr))\n",
        "        # --> train model on training dataset\n",
        "        # we tell to the model to enter in train state. it is important because\n",
        "        # there are somelayers like dropout, batchnorm that behaves \n",
        "        # differently between train and test\n",
        "        model.train()\n",
        "        train_loss,train_metric = loss_epoch(model, loss_func, metric_func,train_dl,sanity_check, opt,device)\n",
        "        # --> collect loss and metric for training dataset\n",
        "        loss_history[\"train\"].append(train_loss)\n",
        "        metric_history[\"train\"].append(train_metric)\n",
        "        # --> tell the model to be in test (validation) mode\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_loss, test_metric = loss_epoch(model, loss_func, metric_func, test_dl,sanity_check,opt=None,device=device)\n",
        "        # --> collect loss and metric for test dataset\n",
        "        loss_history[\"test\"].append(test_loss)\n",
        "        metric_history[\"test\"].append(test_metric)\n",
        "        # --> store best model\n",
        "        if test_loss < best_loss:\n",
        "            print(\"--> model improved! --> saved to %s\" %(path2weigths))\n",
        "            best_loss = test_loss\n",
        "            best_model_weights = copy.deepcopy(model.state_dict())\n",
        "            # --> store weights into local file\n",
        "            torch.save(model.state_dict(),path2weigths)\n",
        "        # --> learning rate scheduler\n",
        "        lr_scheduler.step()\n",
        "        print(\"--> train_loss: %.6f, test_loss: %.6f, train_metric: %.3f, test_metric: %.3f\" % (train_loss,test_loss,train_metric,test_metric))\n",
        "    # --> load best weights\n",
        "    model.load_state_dict(best_model_weights)\n",
        "    return model, loss_history,metric_history\n",
        "\n",
        "# Setup GPU Device\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "opt = optim.Adam(net.parameters(),lr=0.001)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.999)  #  lr = lr * gamma ** last_epoch\n",
        "params = {\n",
        "    \"model\":                 net,\n",
        "    \"loss_func\":             nn.SmoothL1Loss(reduction=\"sum\"), \n",
        "    \"metric_func\":           metrics_func_regression,\n",
        "    \"num_epochs\":            500,\n",
        "    \"optimizer\":             opt,\n",
        "    \"lr_scheduler\":          lr_scheduler,\n",
        "    \"train_dl\":              train_dl,\n",
        "    \"test_dl\":               test_dl,\n",
        "    \"device\":                device,  \n",
        "    \"continue_training\" :    True,  # continue training from last save weights\n",
        "    \"sanity_check\":          False, # if true we only do one batch per epoch\n",
        "    \"path2weigths\":          \"./best_model.pt\"  \n",
        "} \n",
        "model, loss_history,metric_history = train_test(params)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdwmnC9yKAf4"
      },
      "source": [
        "# Comparison train and test set\n",
        "device = torch.device(\"cpu\")\n",
        "net = net.to(device)\n",
        "net.eval()\n",
        "imgs = []\n",
        "for x,y in train_dl: # test_dl\n",
        "    for im,l in zip(x,y):\n",
        "        with torch.no_grad():\n",
        "            l_est = net.forward(im.unsqueeze(0))[0]\n",
        "            res = compare_img_bbox(im,l, l_est)\n",
        "            imgs.append( transforms.ToTensor()( res ) )\n",
        "grid =torchvision.utils.make_grid(torch.stack(imgs,dim=0), nrow=5, padding=5)\n",
        "display(transforms.ToPILImage()(grid))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRKa7SX850ig"
      },
      "source": [
        "## Step 7) Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLO1fDIi8f92"
      },
      "source": [
        "### Record video for testing using google colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d9xCtj45z5d"
      },
      "source": [
        "# https://androidkt.com/how-to-capture-and-play-video-in-google-colab/\n",
        "from IPython.display import display, Javascript,HTML\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "def show_video(video_path, video_width = 1280):\n",
        "  \n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        "\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n",
        "\n",
        "def record_video(filename):\n",
        "  js=Javascript(\"\"\"\n",
        "    async function recordVideo() {\n",
        "      const options = { mimeType: \"video/webm; codecs=vp9\" };\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      const stopCapture = document.createElement(\"button\");\n",
        "      \n",
        "      capture.textContent = \"Start Recording\";\n",
        "      capture.style.background = \"orange\";\n",
        "      capture.style.color = \"white\";\n",
        "\n",
        "      stopCapture.textContent = \"Stop Recording\";\n",
        "      stopCapture.style.background = \"red\";\n",
        "      stopCapture.style.color = \"white\";\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      const recordingVid = document.createElement(\"video\");\n",
        "      video.style.display = 'block';\n",
        "\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({audio:false, video: true});\n",
        "    \n",
        "      let recorder = new MediaRecorder(stream, options);\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      video.muted = true;\n",
        "\n",
        "      await video.play();\n",
        "\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      await new Promise((resolve) => {\n",
        "        capture.onclick = resolve;\n",
        "      });\n",
        "      recorder.start();\n",
        "      capture.replaceWith(stopCapture);\n",
        "\n",
        "      await new Promise((resolve) => stopCapture.onclick = resolve);\n",
        "      recorder.stop();\n",
        "      let recData = await new Promise((resolve) => recorder.ondataavailable = resolve);\n",
        "      let arrBuff = await recData.data.arrayBuffer();\n",
        "      \n",
        "      // stop the stream and remove the video element\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "\n",
        "      let binaryString = \"\";\n",
        "      let bytes = new Uint8Array(arrBuff);\n",
        "      bytes.forEach((byte) => {\n",
        "        binaryString += String.fromCharCode(byte);\n",
        "      })\n",
        "    return btoa(binaryString);\n",
        "    }\n",
        "  \"\"\")\n",
        "  try:\n",
        "    display(js)\n",
        "    data=eval_js('recordVideo({})')\n",
        "    binary=b64decode(data)\n",
        "    with open(filename,\"wb\") as video_file:\n",
        "      video_file.write(binary)\n",
        "    print(f\"Finished recording video at:{filename}\")\n",
        "  except Exception as err:\n",
        "    print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "jgjUx9m-5z8C",
        "outputId": "fbb8b261-c0a7-4234-c836-c1d1c8757e14"
      },
      "source": [
        "video_path = \"test.mp4\"\n",
        "record_video(video_path)\n",
        "# show_video(video_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function recordVideo() {\n",
              "      const options = { mimeType: \"video/webm; codecs=vp9\" };\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      const stopCapture = document.createElement(\"button\");\n",
              "      \n",
              "      capture.textContent = \"Start Recording\";\n",
              "      capture.style.background = \"orange\";\n",
              "      capture.style.color = \"white\";\n",
              "\n",
              "      stopCapture.textContent = \"Stop Recording\";\n",
              "      stopCapture.style.background = \"red\";\n",
              "      stopCapture.style.color = \"white\";\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      const recordingVid = document.createElement(\"video\");\n",
              "      video.style.display = 'block';\n",
              "\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({audio:false, video: true});\n",
              "    \n",
              "      let recorder = new MediaRecorder(stream, options);\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "\n",
              "      video.srcObject = stream;\n",
              "      video.muted = true;\n",
              "\n",
              "      await video.play();\n",
              "\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      await new Promise((resolve) => {\n",
              "        capture.onclick = resolve;\n",
              "      });\n",
              "      recorder.start();\n",
              "      capture.replaceWith(stopCapture);\n",
              "\n",
              "      await new Promise((resolve) => stopCapture.onclick = resolve);\n",
              "      recorder.stop();\n",
              "      let recData = await new Promise((resolve) => recorder.ondataavailable = resolve);\n",
              "      let arrBuff = await recData.data.arrayBuffer();\n",
              "      \n",
              "      // stop the stream and remove the video element\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "\n",
              "      let binaryString = \"\";\n",
              "      let bytes = new Uint8Array(arrBuff);\n",
              "      bytes.forEach((byte) => {\n",
              "        binaryString += String.fromCharCode(byte);\n",
              "      })\n",
              "    return btoa(binaryString);\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finished recording video at:test.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBK_BnVD9KVb"
      },
      "source": [
        "### Test the trained model on the recorded video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV416Sb29GL6"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load Model\n",
        "device = torch.device(\"cpu\")\n",
        "W,H = (416,416)\n",
        "net = Net(C_in=3, size=(W,H), init_f=16, num_outputs=4) # outputs xc,yc,w,h\n",
        "path2weights = \"best_model.pt\"\n",
        "weights = torch.load(path2weights)\n",
        "net.load_state_dict(weights)\n",
        "net.eval()\n",
        "\n",
        "cap = cv2.VideoCapture(\"test.mp4\")\n",
        "\n",
        "while (cap.isOpened()):\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        # Frame \n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        im_pil = transforms.ToPILImage()(frame).resize((W,H))\n",
        "        im_tensor = transforms.ToTensor()( im_pil ).unsqueeze(0) # scale 0-1\n",
        "        with torch.no_grad():\n",
        "            y_hat = net.forward(im_tensor)\n",
        "            #print(y_hat[0])\n",
        "            print(y_hat[0])\n",
        "            res = get_img_bbox(im_tensor[0],y_hat[0],[\"manuel\"],[(0,255,0)])\n",
        "            display(res)\n",
        "    else:\n",
        "        break\n",
        "cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw8T85fKg6mU"
      },
      "source": [
        " ### Test Trained Model on train test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nno4N2U-FRzH"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load Model\n",
        "device = torch.device(\"cpu\")\n",
        "W,H = (416,416)\n",
        "net = Net(C_in=3, size=(W,H), init_f=16, num_outputs=4) # outputs xc,yc,w,h\n",
        "path2weights = \"best_model.pt\"\n",
        "weights = torch.load(path2weights)\n",
        "net.load_state_dict(weights)\n",
        "net.eval()\n",
        "\n",
        "#path = \"dataset/train\"\n",
        "path = \"dataset/test\"\n",
        "names = os.listdir(path)\n",
        "for name in names:\n",
        "    filename = os.path.join(path,name)\n",
        "    frame = cv2.imread(filename,1)\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    im_pil = transforms.ToPILImage()(frame).resize((W,H))\n",
        "    im_tensor = transforms.ToTensor()( im_pil ).unsqueeze(0) # scale 0-1\n",
        "    with torch.no_grad():\n",
        "        y_hat = net.forward(im_tensor)\n",
        "        res = get_img_bbox(im_tensor[0],y_hat[0],[\"manuel\"],[(0,255,0)])\n",
        "        display(res)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvTGrrH6Zyi0"
      },
      "source": [
        "## Extra: Save image inside a folder from video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDTjjzvDZ208"
      },
      "source": [
        "'''\n",
        "import cv2\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "cap = cv2.VideoCapture('manuel.mp4')\n",
        "while (cap.isOpened()):\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        cv2.imshow('frame',frame)\n",
        "        t = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S-%f\")\n",
        "        print(t)\n",
        "        cv2.imwrite(\"images/\"+t+\".png\", frame)\n",
        "        if cv2.waitKey(33) & 0xFF == ord('q'):\n",
        "            break\n",
        "'''\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "cap = cv2.VideoCapture('manuel.mp4')\n",
        "i = 0\n",
        "while (cap.isOpened()):\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        #cv2.imshow('frame',frame)\n",
        "        t = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S-%f\")\n",
        "        print(t)\n",
        "        cv2.imwrite(\"images/\"+t+\".png\", frame)\n",
        "        if i>50:\n",
        "            break\n",
        "        i += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPHtPsoh4B4x1kxNnwyIFsu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visiont3lab/deep-learning-course/blob/main/colab/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3ki_9emN-Nh"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrh2RbiWNw2A"
      },
      "source": [
        "from torch import nn\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch import optim\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torchsummary import summary\r\n",
        "#!pip install torchsummary\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import TensorDataset,Dataset\r\n",
        "from torchvision import datasets\r\n",
        "from torchvision import transforms\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import numpy as np\r\n",
        "import plotly.graph_objects as go\r\n",
        "# Loss function pytorch: https://neptune.ai/blog/pytorch-loss-functions\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "from datetime import datetime"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxJ3655JN8S4"
      },
      "source": [
        "## Neural Network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l18NVUALSl3M"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOpy-O6GYJXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc11612-a06d-4d9f-d81b-294b9318a66f"
      },
      "source": [
        "import pandas as pd\r\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/visiont3lab/project-work-ifoa/main/data/dpc-covid19-ita-regioni-zone.csv\")\r\n",
        "df[\"data\"] = [ datetime.strptime(d, \"%Y-%m-%d %H:%M:%S\") for d in df[\"data\"]]\r\n",
        "#df_f = df[df[\"data\"]>datetime(2020,11,6)].copy()\r\n",
        "df_f = df[df[\"zona\"]!=\"unknown\"]\r\n",
        "inputs = [\"data\",\"denominazione_regione\",\"zona\",\"ricoverati_con_sintomi\",\"terapia_intensiva\",\r\n",
        "        \"totale_ospedalizzati\",\"totale_positivi\",\"isolamento_domiciliare\",\r\n",
        "        \"deceduti\",\"dimessi_guariti\",\"nuovi_positivi\",\"totale_casi\",\"tamponi\"]\r\n",
        "df_f = df_f[inputs]\r\n",
        "df_f = df_f.drop(columns=[\"data\",\"denominazione_regione\"])\r\n",
        "#display(df_f)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#C_Y = np.array([dict_names[d] for d in df_Y],dtype=np.float) #.reshape(-1,1)\r\n",
        "#print(f\"X shape: {C_X.shape} , Y shape: {C_Y.shape}\")\r\n",
        "\r\n",
        "dict_names = {\"bianca\":0,\"gialla\": 1, \"arancione\": 2, \"rossa\": 3}\r\n",
        "C_Y = df_f.pop(\"zona\").tolist()\r\n",
        "C_Y = np.array ( [ dict_names[e] for e in C_Y] , dtype=np.float32).reshape(-1,1)\r\n",
        "\r\n",
        "C_X = df_f.values\r\n",
        "\r\n",
        "print( C_X.shape )\r\n",
        "print( C_Y.shape )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2688, 10)\n",
            "(2688, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4yFxS_Id-LN"
      },
      "source": [
        "## Neural Network\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTsVWGUOd_4O",
        "outputId": "0bb00725-1e29-4cd4-b46b-1c052240777e"
      },
      "source": [
        "class ClassificationNet(nn.Module):\r\n",
        "    def __init__(self,num_inputs, num_classes):\r\n",
        "        super(ClassificationNet,self).__init__()\r\n",
        "        self.num_classes = num_classes\r\n",
        "        self.fc1 = nn.Linear(num_inputs,200)\r\n",
        "        self.fc2 = nn.Linear(200,100)\r\n",
        "        self.fc3 = nn.Linear(100,50)\r\n",
        "        self.fc4 = nn.Linear(50,self.num_classes)\r\n",
        "    def forward(self,x):\r\n",
        "        # torch.sigmoid, torch.tanh, torch.relu\r\n",
        "        x = torch.tanh(self.fc1(x)) \r\n",
        "        x = torch.tanh(self.fc2(x))\r\n",
        "        x = torch.tanh(self.fc3(x))\r\n",
        "        x = torch.log_softmax(self.fc4(x),dim=-1) # sarebbe dim=1  print(self.fc3(x)) print(self.fc3(x).sum(dim=-1))\r\n",
        "        return x\r\n",
        "\r\n",
        "CN = ClassificationNet(num_inputs=10, num_classes=4)\r\n",
        "CN"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ClassificationNet(\n",
              "  (fc1): Linear(in_features=10, out_features=200, bias=True)\n",
              "  (fc2): Linear(in_features=200, out_features=100, bias=True)\n",
              "  (fc3): Linear(in_features=100, out_features=50, bias=True)\n",
              "  (fc4): Linear(in_features=50, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHwKMpshd37u"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0LVCoZ_cLjS",
        "outputId": "dbfbd233-b603-47de-c5c5-8abe36cbc7d1"
      },
      "source": [
        "# Training and Test Set\r\n",
        "C_X_train, C_X_test, C_Y_train, C_Y_test = train_test_split(C_X,C_Y,test_size=0.3,shuffle=True,random_state=2)\r\n",
        "print(f\"X Train shape: {C_X_train.shape} , X Test shape: {C_X_test.shape}\")\r\n",
        "\r\n",
        "# Normalization\r\n",
        "C_mean = np.mean(C_X)\r\n",
        "C_std = np.std(C_X)\r\n",
        "C_min = np.min(C_X)\r\n",
        "C_max = np.max(C_X)\r\n",
        "\r\n",
        "# Tensor Dataset Che converte i dati da numpy a Pytorch\r\n",
        "class CustomTensorDataset(Dataset):\r\n",
        "    def __init__(self, x,y,mean,std):\r\n",
        "        x = (x - mean)/std           # Standard Scaler \r\n",
        "        #x = (x - min) / (max - min) # Min Max Scaler\r\n",
        "        self.x = torch.from_numpy(x).type(torch.float32)\r\n",
        "        self.y = torch.from_numpy(y).type(torch.LongTensor).reshape(-1)\r\n",
        "    def __getitem__(self, index):\r\n",
        "        x = self.x[index]\r\n",
        "        y = self.y[index]\r\n",
        "        return x, y\r\n",
        "    def __len__(self):\r\n",
        "        return self.x.shape[0]\r\n",
        "\r\n",
        "# Dataset generator creation\r\n",
        "C_train_ds = CustomTensorDataset(C_X_train,C_Y_train,C_mean,C_std)\r\n",
        "C_test_ds = CustomTensorDataset(C_X_test,C_Y_test,C_mean,C_std)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X Train shape: (1881, 10) , X Test shape: (807, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPmSf8Xzfx8a"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyA8HmPQpWuS"
      },
      "source": [
        "* numero di epoche\r\n",
        "* learning rate\r\n",
        "* batch size\r\n",
        "* aggiornare la struttura della rete"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arHLuhtQgj9l"
      },
      "source": [
        "# Validation: Metric Regression\r\n",
        "def metrics_func_regression(target, output):\r\n",
        "  # Comptue mean squaer error (Migliora quanto piu' ci avviciniamo a zero)\r\n",
        "  mse = torch.sum((output - target) ** 2)\r\n",
        "  return mse\r\n",
        "\r\n",
        "# Validation: Metric cassification\r\n",
        "def metrics_func_classification(target, output):\r\n",
        "  # Compute number of correct prediction\r\n",
        "  pred = output.argmax(dim=-1,keepdim=True)\r\n",
        "  corrects =pred.eq(target.reshape(pred.shape)).sum().item()\r\n",
        "  return -corrects # minus for coeherence with best result is the most negative one\r\n",
        "\r\n",
        "# Training: Loss calculation and backward step\r\n",
        "def loss_batch(loss_func,metric_func, xb,yb,yb_h, opt=None):\r\n",
        "  # obtain loss\r\n",
        "  loss = loss_func(yb_h, yb)\r\n",
        "  # obtain permormance metric \r\n",
        "  metric_b = metric_func(yb,yb_h)\r\n",
        "  if opt is not None:\r\n",
        "    loss.backward()\r\n",
        "    opt.step()\r\n",
        "    opt.zero_grad()\r\n",
        "  return loss.item(), metric_b\r\n",
        "\r\n",
        "# Trainig: Function 1 epoch\r\n",
        "def loss_epoch(model, loss_func,metric_func, dataset_dl, opt, device):\r\n",
        "  loss = 0.0\r\n",
        "  metric = 0.0\r\n",
        "  len_data = len(dataset_dl.dataset)\r\n",
        "  # Get batch data\r\n",
        "  for xb,yb in dataset_dl:    \r\n",
        "    # Send to cuda the data (batch size)\r\n",
        "    xb = xb.to(device)\r\n",
        "    yb = yb.to(device)\r\n",
        "    # obtain model output \r\n",
        "    yb_h = model.forward(xb)\r\n",
        "    # Loss and Metric Calculation\r\n",
        "    loss_b, metric_b = loss_batch(loss_func,metric_func, xb,yb,yb_h,opt)\r\n",
        "    loss += loss_b\r\n",
        "    if metric_b is not None:\r\n",
        "      metric+=metric_b \r\n",
        "  loss /=len_data\r\n",
        "  metric /=len_data\r\n",
        "  return loss, metric\r\n",
        "\r\n",
        "# Training: Iterate on epochs\r\n",
        "def train_val(epochs, model, loss_func, metric_func, opt, train_dl,test_dl,device, path2weigths=\"./weights.pt\"):\r\n",
        "  lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.999) #  lr = lr * gamma ** last_epoch\r\n",
        "  best_val_metric = 1000000\r\n",
        "  for epoch in range(epochs):\r\n",
        "    model.train()\r\n",
        "    train_loss,train_metric = loss_epoch(model, loss_func, metric_func,train_dl, opt,device)\r\n",
        "    lr_scheduler.step()\r\n",
        "    model.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "      val_loss, val_metric = loss_epoch(model, loss_func, metric_func, test_dl,opt=None,device=device)\r\n",
        "      print(\"epoch: %d, train_loss: %.6f, val loss: %.6f,  train_metric: %.3f test_metric: %.3f lr: %.5f)\" % (epoch,train_loss, val_loss,train_metric,val_metric,opt.param_groups[0]['lr']))\r\n",
        "      if (val_metric <= best_val_metric):        \r\n",
        "        # Save Models (It save last weights)\r\n",
        "        torch.save(model.state_dict(),path2weigths)\r\n",
        "        best_val_metric = val_metric\r\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx_Ha_93hDfo",
        "outputId": "abd4b097-3b09-46a7-c0f3-43851ce08184"
      },
      "source": [
        "# Setup GPU Device\r\n",
        "device = torch.device(\"cpu\")\r\n",
        "if torch.cuda.is_available():\r\n",
        "    device = torch.device(\"cuda:0\")\r\n",
        "\r\n",
        "# Regression\r\n",
        "C_model = ClassificationNet(num_inputs=10, num_classes=4).to(device)\r\n",
        "C_loss_func = nn.NLLLoss(reduction=\"sum\")  #nn.BCELoss  \r\n",
        "C_opt = optim.Adam(C_model.parameters(),lr=0.001)\r\n",
        "C_train_dl = DataLoader(C_train_ds,batch_size=124,shuffle=True)\r\n",
        "C_test_dl = DataLoader(C_test_ds,batch_size=124,shuffle=True)\r\n",
        "\r\n",
        "# Regression\r\n",
        "train_val(3000,C_model,C_loss_func,metrics_func_classification,C_opt, C_train_dl,C_test_dl,device,path2weigths=\"./weights_classification.pt\")\r\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, train_loss: 1.304989, val loss: 1.260151,  train_metric: -0.345 test_metric: -0.347 lr: 0.00100)\n",
            "epoch: 1, train_loss: 1.271598, val loss: 1.261257,  train_metric: -0.330 test_metric: -0.347 lr: 0.00100)\n",
            "epoch: 2, train_loss: 1.269783, val loss: 1.252971,  train_metric: -0.367 test_metric: -0.373 lr: 0.00100)\n",
            "epoch: 3, train_loss: 1.264140, val loss: 1.252161,  train_metric: -0.384 test_metric: -0.347 lr: 0.00100)\n",
            "epoch: 4, train_loss: 1.263188, val loss: 1.247354,  train_metric: -0.383 test_metric: -0.421 lr: 0.00100)\n",
            "epoch: 5, train_loss: 1.261747, val loss: 1.246840,  train_metric: -0.369 test_metric: -0.400 lr: 0.00099)\n",
            "epoch: 6, train_loss: 1.257173, val loss: 1.241778,  train_metric: -0.367 test_metric: -0.414 lr: 0.00099)\n",
            "epoch: 7, train_loss: 1.256002, val loss: 1.239756,  train_metric: -0.385 test_metric: -0.393 lr: 0.00099)\n",
            "epoch: 8, train_loss: 1.253820, val loss: 1.242387,  train_metric: -0.366 test_metric: -0.343 lr: 0.00099)\n",
            "epoch: 9, train_loss: 1.256188, val loss: 1.237368,  train_metric: -0.369 test_metric: -0.347 lr: 0.00099)\n",
            "epoch: 10, train_loss: 1.248013, val loss: 1.227365,  train_metric: -0.386 test_metric: -0.421 lr: 0.00099)\n",
            "epoch: 11, train_loss: 1.249373, val loss: 1.229058,  train_metric: -0.411 test_metric: -0.375 lr: 0.00099)\n",
            "epoch: 12, train_loss: 1.246041, val loss: 1.230723,  train_metric: -0.372 test_metric: -0.415 lr: 0.00099)\n",
            "epoch: 13, train_loss: 1.244140, val loss: 1.217453,  train_metric: -0.395 test_metric: -0.413 lr: 0.00099)\n",
            "epoch: 14, train_loss: 1.239575, val loss: 1.213287,  train_metric: -0.407 test_metric: -0.409 lr: 0.00099)\n",
            "epoch: 15, train_loss: 1.233721, val loss: 1.212476,  train_metric: -0.400 test_metric: -0.424 lr: 0.00098)\n",
            "epoch: 16, train_loss: 1.235995, val loss: 1.214246,  train_metric: -0.408 test_metric: -0.419 lr: 0.00098)\n",
            "epoch: 17, train_loss: 1.234071, val loss: 1.208987,  train_metric: -0.396 test_metric: -0.377 lr: 0.00098)\n",
            "epoch: 18, train_loss: 1.232119, val loss: 1.199711,  train_metric: -0.408 test_metric: -0.450 lr: 0.00098)\n",
            "epoch: 19, train_loss: 1.228015, val loss: 1.196836,  train_metric: -0.423 test_metric: -0.440 lr: 0.00098)\n",
            "epoch: 20, train_loss: 1.224412, val loss: 1.201364,  train_metric: -0.416 test_metric: -0.445 lr: 0.00098)\n",
            "epoch: 21, train_loss: 1.229202, val loss: 1.195148,  train_metric: -0.408 test_metric: -0.431 lr: 0.00098)\n",
            "epoch: 22, train_loss: 1.227014, val loss: 1.195850,  train_metric: -0.418 test_metric: -0.429 lr: 0.00098)\n",
            "epoch: 23, train_loss: 1.224920, val loss: 1.191554,  train_metric: -0.422 test_metric: -0.425 lr: 0.00098)\n",
            "epoch: 24, train_loss: 1.223770, val loss: 1.192996,  train_metric: -0.405 test_metric: -0.463 lr: 0.00098)\n",
            "epoch: 25, train_loss: 1.217843, val loss: 1.193299,  train_metric: -0.429 test_metric: -0.411 lr: 0.00097)\n",
            "epoch: 26, train_loss: 1.222594, val loss: 1.182787,  train_metric: -0.405 test_metric: -0.463 lr: 0.00097)\n",
            "epoch: 27, train_loss: 1.214340, val loss: 1.181273,  train_metric: -0.423 test_metric: -0.449 lr: 0.00097)\n",
            "epoch: 28, train_loss: 1.212723, val loss: 1.179493,  train_metric: -0.416 test_metric: -0.449 lr: 0.00097)\n",
            "epoch: 29, train_loss: 1.207721, val loss: 1.176471,  train_metric: -0.428 test_metric: -0.473 lr: 0.00097)\n",
            "epoch: 30, train_loss: 1.205056, val loss: 1.174556,  train_metric: -0.430 test_metric: -0.450 lr: 0.00097)\n",
            "epoch: 31, train_loss: 1.208337, val loss: 1.177905,  train_metric: -0.425 test_metric: -0.473 lr: 0.00097)\n",
            "epoch: 32, train_loss: 1.202362, val loss: 1.166971,  train_metric: -0.430 test_metric: -0.451 lr: 0.00097)\n",
            "epoch: 33, train_loss: 1.196546, val loss: 1.172034,  train_metric: -0.430 test_metric: -0.454 lr: 0.00097)\n",
            "epoch: 34, train_loss: 1.197658, val loss: 1.158305,  train_metric: -0.438 test_metric: -0.440 lr: 0.00097)\n",
            "epoch: 35, train_loss: 1.191296, val loss: 1.167287,  train_metric: -0.422 test_metric: -0.460 lr: 0.00096)\n",
            "epoch: 36, train_loss: 1.192898, val loss: 1.164962,  train_metric: -0.430 test_metric: -0.462 lr: 0.00096)\n",
            "epoch: 37, train_loss: 1.184252, val loss: 1.156848,  train_metric: -0.423 test_metric: -0.478 lr: 0.00096)\n",
            "epoch: 38, train_loss: 1.183852, val loss: 1.153195,  train_metric: -0.428 test_metric: -0.452 lr: 0.00096)\n",
            "epoch: 39, train_loss: 1.178053, val loss: 1.141973,  train_metric: -0.437 test_metric: -0.468 lr: 0.00096)\n",
            "epoch: 40, train_loss: 1.173564, val loss: 1.152620,  train_metric: -0.426 test_metric: -0.419 lr: 0.00096)\n",
            "epoch: 41, train_loss: 1.178618, val loss: 1.147180,  train_metric: -0.432 test_metric: -0.473 lr: 0.00096)\n",
            "epoch: 42, train_loss: 1.173729, val loss: 1.147007,  train_metric: -0.439 test_metric: -0.463 lr: 0.00096)\n",
            "epoch: 43, train_loss: 1.171356, val loss: 1.147174,  train_metric: -0.448 test_metric: -0.461 lr: 0.00096)\n",
            "epoch: 44, train_loss: 1.169994, val loss: 1.135540,  train_metric: -0.442 test_metric: -0.467 lr: 0.00096)\n",
            "epoch: 45, train_loss: 1.161625, val loss: 1.132136,  train_metric: -0.444 test_metric: -0.477 lr: 0.00096)\n",
            "epoch: 46, train_loss: 1.156189, val loss: 1.132321,  train_metric: -0.446 test_metric: -0.442 lr: 0.00095)\n",
            "epoch: 47, train_loss: 1.158891, val loss: 1.134202,  train_metric: -0.457 test_metric: -0.487 lr: 0.00095)\n",
            "epoch: 48, train_loss: 1.162490, val loss: 1.127289,  train_metric: -0.448 test_metric: -0.481 lr: 0.00095)\n",
            "epoch: 49, train_loss: 1.152937, val loss: 1.127944,  train_metric: -0.460 test_metric: -0.454 lr: 0.00095)\n",
            "epoch: 50, train_loss: 1.149099, val loss: 1.122960,  train_metric: -0.460 test_metric: -0.489 lr: 0.00095)\n",
            "epoch: 51, train_loss: 1.147617, val loss: 1.120820,  train_metric: -0.468 test_metric: -0.501 lr: 0.00095)\n",
            "epoch: 52, train_loss: 1.146426, val loss: 1.127162,  train_metric: -0.472 test_metric: -0.486 lr: 0.00095)\n",
            "epoch: 53, train_loss: 1.147114, val loss: 1.135789,  train_metric: -0.459 test_metric: -0.435 lr: 0.00095)\n",
            "epoch: 54, train_loss: 1.144635, val loss: 1.129443,  train_metric: -0.465 test_metric: -0.499 lr: 0.00095)\n",
            "epoch: 55, train_loss: 1.141175, val loss: 1.122710,  train_metric: -0.481 test_metric: -0.476 lr: 0.00095)\n",
            "epoch: 56, train_loss: 1.136367, val loss: 1.129717,  train_metric: -0.469 test_metric: -0.446 lr: 0.00094)\n",
            "epoch: 57, train_loss: 1.136196, val loss: 1.118096,  train_metric: -0.478 test_metric: -0.507 lr: 0.00094)\n",
            "epoch: 58, train_loss: 1.130580, val loss: 1.117908,  train_metric: -0.478 test_metric: -0.498 lr: 0.00094)\n",
            "epoch: 59, train_loss: 1.131224, val loss: 1.114484,  train_metric: -0.490 test_metric: -0.496 lr: 0.00094)\n",
            "epoch: 60, train_loss: 1.126213, val loss: 1.115026,  train_metric: -0.480 test_metric: -0.491 lr: 0.00094)\n",
            "epoch: 61, train_loss: 1.128947, val loss: 1.117928,  train_metric: -0.483 test_metric: -0.471 lr: 0.00094)\n",
            "epoch: 62, train_loss: 1.130558, val loss: 1.107692,  train_metric: -0.475 test_metric: -0.512 lr: 0.00094)\n",
            "epoch: 63, train_loss: 1.124744, val loss: 1.115947,  train_metric: -0.477 test_metric: -0.515 lr: 0.00094)\n",
            "epoch: 64, train_loss: 1.119256, val loss: 1.103786,  train_metric: -0.498 test_metric: -0.507 lr: 0.00094)\n",
            "epoch: 65, train_loss: 1.116784, val loss: 1.105082,  train_metric: -0.500 test_metric: -0.497 lr: 0.00094)\n",
            "epoch: 66, train_loss: 1.114238, val loss: 1.106473,  train_metric: -0.487 test_metric: -0.511 lr: 0.00094)\n",
            "epoch: 67, train_loss: 1.116320, val loss: 1.111375,  train_metric: -0.494 test_metric: -0.515 lr: 0.00093)\n",
            "epoch: 68, train_loss: 1.114171, val loss: 1.100419,  train_metric: -0.488 test_metric: -0.506 lr: 0.00093)\n",
            "epoch: 69, train_loss: 1.116401, val loss: 1.106743,  train_metric: -0.491 test_metric: -0.503 lr: 0.00093)\n",
            "epoch: 70, train_loss: 1.116232, val loss: 1.101735,  train_metric: -0.493 test_metric: -0.498 lr: 0.00093)\n",
            "epoch: 71, train_loss: 1.108013, val loss: 1.094524,  train_metric: -0.492 test_metric: -0.523 lr: 0.00093)\n",
            "epoch: 72, train_loss: 1.104867, val loss: 1.094317,  train_metric: -0.506 test_metric: -0.506 lr: 0.00093)\n",
            "epoch: 73, train_loss: 1.103481, val loss: 1.095143,  train_metric: -0.501 test_metric: -0.509 lr: 0.00093)\n",
            "epoch: 74, train_loss: 1.105866, val loss: 1.097332,  train_metric: -0.492 test_metric: -0.507 lr: 0.00093)\n",
            "epoch: 75, train_loss: 1.101080, val loss: 1.095784,  train_metric: -0.484 test_metric: -0.511 lr: 0.00093)\n",
            "epoch: 76, train_loss: 1.093034, val loss: 1.087569,  train_metric: -0.499 test_metric: -0.530 lr: 0.00093)\n",
            "epoch: 77, train_loss: 1.093834, val loss: 1.090072,  train_metric: -0.513 test_metric: -0.517 lr: 0.00092)\n",
            "epoch: 78, train_loss: 1.093957, val loss: 1.092300,  train_metric: -0.503 test_metric: -0.508 lr: 0.00092)\n",
            "epoch: 79, train_loss: 1.092919, val loss: 1.099605,  train_metric: -0.506 test_metric: -0.492 lr: 0.00092)\n",
            "epoch: 80, train_loss: 1.087870, val loss: 1.090338,  train_metric: -0.505 test_metric: -0.501 lr: 0.00092)\n",
            "epoch: 81, train_loss: 1.082857, val loss: 1.075313,  train_metric: -0.514 test_metric: -0.527 lr: 0.00092)\n",
            "epoch: 82, train_loss: 1.077351, val loss: 1.080842,  train_metric: -0.511 test_metric: -0.545 lr: 0.00092)\n",
            "epoch: 83, train_loss: 1.084274, val loss: 1.084846,  train_metric: -0.517 test_metric: -0.512 lr: 0.00092)\n",
            "epoch: 84, train_loss: 1.078510, val loss: 1.068347,  train_metric: -0.522 test_metric: -0.535 lr: 0.00092)\n",
            "epoch: 85, train_loss: 1.065696, val loss: 1.078923,  train_metric: -0.518 test_metric: -0.530 lr: 0.00092)\n",
            "epoch: 86, train_loss: 1.071963, val loss: 1.081198,  train_metric: -0.520 test_metric: -0.517 lr: 0.00092)\n",
            "epoch: 87, train_loss: 1.067644, val loss: 1.058843,  train_metric: -0.519 test_metric: -0.554 lr: 0.00092)\n",
            "epoch: 88, train_loss: 1.061982, val loss: 1.059929,  train_metric: -0.531 test_metric: -0.556 lr: 0.00091)\n",
            "epoch: 89, train_loss: 1.061075, val loss: 1.069784,  train_metric: -0.523 test_metric: -0.543 lr: 0.00091)\n",
            "epoch: 90, train_loss: 1.061671, val loss: 1.066469,  train_metric: -0.522 test_metric: -0.539 lr: 0.00091)\n",
            "epoch: 91, train_loss: 1.049740, val loss: 1.057758,  train_metric: -0.539 test_metric: -0.535 lr: 0.00091)\n",
            "epoch: 92, train_loss: 1.053247, val loss: 1.057691,  train_metric: -0.508 test_metric: -0.548 lr: 0.00091)\n",
            "epoch: 93, train_loss: 1.049345, val loss: 1.062363,  train_metric: -0.529 test_metric: -0.525 lr: 0.00091)\n",
            "epoch: 94, train_loss: 1.054701, val loss: 1.073874,  train_metric: -0.533 test_metric: -0.528 lr: 0.00091)\n",
            "epoch: 95, train_loss: 1.049318, val loss: 1.072612,  train_metric: -0.518 test_metric: -0.532 lr: 0.00091)\n",
            "epoch: 96, train_loss: 1.045218, val loss: 1.052707,  train_metric: -0.539 test_metric: -0.563 lr: 0.00091)\n",
            "epoch: 97, train_loss: 1.032059, val loss: 1.064753,  train_metric: -0.540 test_metric: -0.548 lr: 0.00091)\n",
            "epoch: 98, train_loss: 1.058091, val loss: 1.088696,  train_metric: -0.532 test_metric: -0.485 lr: 0.00091)\n",
            "epoch: 99, train_loss: 1.047367, val loss: 1.048274,  train_metric: -0.528 test_metric: -0.545 lr: 0.00090)\n",
            "epoch: 100, train_loss: 1.032306, val loss: 1.047920,  train_metric: -0.543 test_metric: -0.565 lr: 0.00090)\n",
            "epoch: 101, train_loss: 1.031950, val loss: 1.042406,  train_metric: -0.539 test_metric: -0.550 lr: 0.00090)\n",
            "epoch: 102, train_loss: 1.024560, val loss: 1.034883,  train_metric: -0.543 test_metric: -0.559 lr: 0.00090)\n",
            "epoch: 103, train_loss: 1.013368, val loss: 1.028447,  train_metric: -0.548 test_metric: -0.572 lr: 0.00090)\n",
            "epoch: 104, train_loss: 1.018454, val loss: 1.026812,  train_metric: -0.549 test_metric: -0.548 lr: 0.00090)\n",
            "epoch: 105, train_loss: 1.015850, val loss: 1.057043,  train_metric: -0.542 test_metric: -0.554 lr: 0.00090)\n",
            "epoch: 106, train_loss: 1.023324, val loss: 1.031175,  train_metric: -0.550 test_metric: -0.566 lr: 0.00090)\n",
            "epoch: 107, train_loss: 1.016620, val loss: 1.070176,  train_metric: -0.551 test_metric: -0.540 lr: 0.00090)\n",
            "epoch: 108, train_loss: 1.021888, val loss: 1.051218,  train_metric: -0.545 test_metric: -0.550 lr: 0.00090)\n",
            "epoch: 109, train_loss: 1.030030, val loss: 1.031056,  train_metric: -0.544 test_metric: -0.549 lr: 0.00090)\n",
            "epoch: 110, train_loss: 1.009918, val loss: 1.034954,  train_metric: -0.542 test_metric: -0.556 lr: 0.00089)\n",
            "epoch: 111, train_loss: 1.000493, val loss: 1.019614,  train_metric: -0.549 test_metric: -0.571 lr: 0.00089)\n",
            "epoch: 112, train_loss: 0.996285, val loss: 1.036425,  train_metric: -0.542 test_metric: -0.571 lr: 0.00089)\n",
            "epoch: 113, train_loss: 1.004715, val loss: 1.009049,  train_metric: -0.550 test_metric: -0.563 lr: 0.00089)\n",
            "epoch: 114, train_loss: 0.994754, val loss: 1.021463,  train_metric: -0.546 test_metric: -0.549 lr: 0.00089)\n",
            "epoch: 115, train_loss: 0.987861, val loss: 1.042513,  train_metric: -0.553 test_metric: -0.522 lr: 0.00089)\n",
            "epoch: 116, train_loss: 0.998004, val loss: 1.029298,  train_metric: -0.550 test_metric: -0.561 lr: 0.00089)\n",
            "epoch: 117, train_loss: 0.987924, val loss: 1.003397,  train_metric: -0.558 test_metric: -0.553 lr: 0.00089)\n",
            "epoch: 118, train_loss: 0.983997, val loss: 1.003542,  train_metric: -0.550 test_metric: -0.568 lr: 0.00089)\n",
            "epoch: 119, train_loss: 0.983416, val loss: 1.007437,  train_metric: -0.552 test_metric: -0.568 lr: 0.00089)\n",
            "epoch: 120, train_loss: 0.978498, val loss: 1.003352,  train_metric: -0.565 test_metric: -0.566 lr: 0.00089)\n",
            "epoch: 121, train_loss: 0.977388, val loss: 1.003560,  train_metric: -0.553 test_metric: -0.566 lr: 0.00089)\n",
            "epoch: 122, train_loss: 0.978235, val loss: 0.997211,  train_metric: -0.558 test_metric: -0.566 lr: 0.00088)\n",
            "epoch: 123, train_loss: 0.977641, val loss: 1.015025,  train_metric: -0.555 test_metric: -0.556 lr: 0.00088)\n",
            "epoch: 124, train_loss: 0.977712, val loss: 1.012401,  train_metric: -0.552 test_metric: -0.556 lr: 0.00088)\n",
            "epoch: 125, train_loss: 0.973525, val loss: 0.988226,  train_metric: -0.557 test_metric: -0.564 lr: 0.00088)\n",
            "epoch: 126, train_loss: 0.961165, val loss: 0.993438,  train_metric: -0.560 test_metric: -0.559 lr: 0.00088)\n",
            "epoch: 127, train_loss: 0.965755, val loss: 1.015047,  train_metric: -0.565 test_metric: -0.556 lr: 0.00088)\n",
            "epoch: 128, train_loss: 0.972991, val loss: 0.998623,  train_metric: -0.551 test_metric: -0.559 lr: 0.00088)\n",
            "epoch: 129, train_loss: 0.960889, val loss: 0.993969,  train_metric: -0.566 test_metric: -0.568 lr: 0.00088)\n",
            "epoch: 130, train_loss: 0.966016, val loss: 0.991484,  train_metric: -0.560 test_metric: -0.564 lr: 0.00088)\n",
            "epoch: 131, train_loss: 0.953491, val loss: 0.993092,  train_metric: -0.574 test_metric: -0.577 lr: 0.00088)\n",
            "epoch: 132, train_loss: 0.956894, val loss: 0.994569,  train_metric: -0.575 test_metric: -0.574 lr: 0.00088)\n",
            "epoch: 133, train_loss: 0.948127, val loss: 0.979737,  train_metric: -0.572 test_metric: -0.576 lr: 0.00087)\n",
            "epoch: 134, train_loss: 0.958678, val loss: 1.014810,  train_metric: -0.569 test_metric: -0.565 lr: 0.00087)\n",
            "epoch: 135, train_loss: 0.945449, val loss: 0.979705,  train_metric: -0.570 test_metric: -0.580 lr: 0.00087)\n",
            "epoch: 136, train_loss: 0.943578, val loss: 0.988472,  train_metric: -0.575 test_metric: -0.572 lr: 0.00087)\n",
            "epoch: 137, train_loss: 0.940911, val loss: 0.987526,  train_metric: -0.575 test_metric: -0.579 lr: 0.00087)\n",
            "epoch: 138, train_loss: 0.941157, val loss: 1.010699,  train_metric: -0.578 test_metric: -0.570 lr: 0.00087)\n",
            "epoch: 139, train_loss: 0.943943, val loss: 0.977374,  train_metric: -0.572 test_metric: -0.590 lr: 0.00087)\n",
            "epoch: 140, train_loss: 0.936466, val loss: 0.979467,  train_metric: -0.576 test_metric: -0.571 lr: 0.00087)\n",
            "epoch: 141, train_loss: 0.929534, val loss: 0.974771,  train_metric: -0.576 test_metric: -0.582 lr: 0.00087)\n",
            "epoch: 142, train_loss: 0.939902, val loss: 0.969717,  train_metric: -0.574 test_metric: -0.589 lr: 0.00087)\n",
            "epoch: 143, train_loss: 0.946213, val loss: 0.991387,  train_metric: -0.567 test_metric: -0.559 lr: 0.00087)\n",
            "epoch: 144, train_loss: 0.935665, val loss: 1.017397,  train_metric: -0.580 test_metric: -0.563 lr: 0.00086)\n",
            "epoch: 145, train_loss: 0.933360, val loss: 0.983806,  train_metric: -0.582 test_metric: -0.579 lr: 0.00086)\n",
            "epoch: 146, train_loss: 0.945974, val loss: 0.968371,  train_metric: -0.565 test_metric: -0.589 lr: 0.00086)\n",
            "epoch: 147, train_loss: 0.931643, val loss: 0.969391,  train_metric: -0.588 test_metric: -0.581 lr: 0.00086)\n",
            "epoch: 148, train_loss: 0.927804, val loss: 0.975029,  train_metric: -0.582 test_metric: -0.589 lr: 0.00086)\n",
            "epoch: 149, train_loss: 0.931256, val loss: 0.959757,  train_metric: -0.590 test_metric: -0.577 lr: 0.00086)\n",
            "epoch: 150, train_loss: 0.928259, val loss: 0.962029,  train_metric: -0.574 test_metric: -0.600 lr: 0.00086)\n",
            "epoch: 151, train_loss: 0.926850, val loss: 0.991557,  train_metric: -0.594 test_metric: -0.581 lr: 0.00086)\n",
            "epoch: 152, train_loss: 0.918854, val loss: 0.976279,  train_metric: -0.593 test_metric: -0.594 lr: 0.00086)\n",
            "epoch: 153, train_loss: 0.913019, val loss: 0.956299,  train_metric: -0.587 test_metric: -0.595 lr: 0.00086)\n",
            "epoch: 154, train_loss: 0.916746, val loss: 0.956939,  train_metric: -0.585 test_metric: -0.585 lr: 0.00086)\n",
            "epoch: 155, train_loss: 0.905295, val loss: 0.974747,  train_metric: -0.595 test_metric: -0.582 lr: 0.00086)\n",
            "epoch: 156, train_loss: 0.904049, val loss: 0.952241,  train_metric: -0.599 test_metric: -0.595 lr: 0.00085)\n",
            "epoch: 157, train_loss: 0.900114, val loss: 0.955843,  train_metric: -0.600 test_metric: -0.591 lr: 0.00085)\n",
            "epoch: 158, train_loss: 0.914362, val loss: 0.972578,  train_metric: -0.594 test_metric: -0.591 lr: 0.00085)\n",
            "epoch: 159, train_loss: 0.905661, val loss: 0.963914,  train_metric: -0.598 test_metric: -0.586 lr: 0.00085)\n",
            "epoch: 160, train_loss: 0.901697, val loss: 0.957809,  train_metric: -0.602 test_metric: -0.586 lr: 0.00085)\n",
            "epoch: 161, train_loss: 0.899059, val loss: 0.950082,  train_metric: -0.599 test_metric: -0.582 lr: 0.00085)\n",
            "epoch: 162, train_loss: 0.891563, val loss: 0.944493,  train_metric: -0.607 test_metric: -0.601 lr: 0.00085)\n",
            "epoch: 163, train_loss: 0.894435, val loss: 0.948254,  train_metric: -0.598 test_metric: -0.585 lr: 0.00085)\n",
            "epoch: 164, train_loss: 0.892643, val loss: 0.947727,  train_metric: -0.611 test_metric: -0.587 lr: 0.00085)\n",
            "epoch: 165, train_loss: 0.895982, val loss: 0.957769,  train_metric: -0.607 test_metric: -0.586 lr: 0.00085)\n",
            "epoch: 166, train_loss: 0.891705, val loss: 0.946587,  train_metric: -0.601 test_metric: -0.602 lr: 0.00085)\n",
            "epoch: 167, train_loss: 0.880792, val loss: 0.943193,  train_metric: -0.614 test_metric: -0.606 lr: 0.00085)\n",
            "epoch: 168, train_loss: 0.891724, val loss: 0.946748,  train_metric: -0.604 test_metric: -0.586 lr: 0.00084)\n",
            "epoch: 169, train_loss: 0.884839, val loss: 0.952049,  train_metric: -0.604 test_metric: -0.601 lr: 0.00084)\n",
            "epoch: 170, train_loss: 0.898354, val loss: 0.950398,  train_metric: -0.606 test_metric: -0.596 lr: 0.00084)\n",
            "epoch: 171, train_loss: 0.876733, val loss: 0.935257,  train_metric: -0.614 test_metric: -0.595 lr: 0.00084)\n",
            "epoch: 172, train_loss: 0.875849, val loss: 0.952110,  train_metric: -0.609 test_metric: -0.599 lr: 0.00084)\n",
            "epoch: 173, train_loss: 0.887955, val loss: 0.991712,  train_metric: -0.618 test_metric: -0.587 lr: 0.00084)\n",
            "epoch: 174, train_loss: 0.892842, val loss: 0.948699,  train_metric: -0.595 test_metric: -0.597 lr: 0.00084)\n",
            "epoch: 175, train_loss: 0.876700, val loss: 0.947741,  train_metric: -0.618 test_metric: -0.601 lr: 0.00084)\n",
            "epoch: 176, train_loss: 0.874248, val loss: 0.927852,  train_metric: -0.616 test_metric: -0.606 lr: 0.00084)\n",
            "epoch: 177, train_loss: 0.878618, val loss: 0.948492,  train_metric: -0.603 test_metric: -0.607 lr: 0.00084)\n",
            "epoch: 178, train_loss: 0.867617, val loss: 0.979641,  train_metric: -0.619 test_metric: -0.590 lr: 0.00084)\n",
            "epoch: 179, train_loss: 0.876006, val loss: 0.922542,  train_metric: -0.626 test_metric: -0.600 lr: 0.00084)\n",
            "epoch: 180, train_loss: 0.878754, val loss: 0.974632,  train_metric: -0.621 test_metric: -0.574 lr: 0.00083)\n",
            "epoch: 181, train_loss: 0.879372, val loss: 0.965158,  train_metric: -0.606 test_metric: -0.595 lr: 0.00083)\n",
            "epoch: 182, train_loss: 0.885456, val loss: 0.965408,  train_metric: -0.611 test_metric: -0.584 lr: 0.00083)\n",
            "epoch: 183, train_loss: 0.868233, val loss: 0.931247,  train_metric: -0.612 test_metric: -0.605 lr: 0.00083)\n",
            "epoch: 184, train_loss: 0.858041, val loss: 0.940845,  train_metric: -0.633 test_metric: -0.613 lr: 0.00083)\n",
            "epoch: 185, train_loss: 0.859024, val loss: 0.942092,  train_metric: -0.633 test_metric: -0.608 lr: 0.00083)\n",
            "epoch: 186, train_loss: 0.860011, val loss: 0.948451,  train_metric: -0.631 test_metric: -0.599 lr: 0.00083)\n",
            "epoch: 187, train_loss: 0.857481, val loss: 0.956572,  train_metric: -0.629 test_metric: -0.602 lr: 0.00083)\n",
            "epoch: 188, train_loss: 0.853611, val loss: 0.943938,  train_metric: -0.635 test_metric: -0.620 lr: 0.00083)\n",
            "epoch: 189, train_loss: 0.855828, val loss: 0.980749,  train_metric: -0.634 test_metric: -0.566 lr: 0.00083)\n",
            "epoch: 190, train_loss: 0.862404, val loss: 0.919637,  train_metric: -0.621 test_metric: -0.617 lr: 0.00083)\n",
            "epoch: 191, train_loss: 0.847354, val loss: 0.918442,  train_metric: -0.633 test_metric: -0.610 lr: 0.00083)\n",
            "epoch: 192, train_loss: 0.846337, val loss: 0.928216,  train_metric: -0.640 test_metric: -0.623 lr: 0.00082)\n",
            "epoch: 193, train_loss: 0.851997, val loss: 0.914533,  train_metric: -0.631 test_metric: -0.616 lr: 0.00082)\n",
            "epoch: 194, train_loss: 0.845290, val loss: 0.918254,  train_metric: -0.626 test_metric: -0.610 lr: 0.00082)\n",
            "epoch: 195, train_loss: 0.853646, val loss: 0.919690,  train_metric: -0.643 test_metric: -0.616 lr: 0.00082)\n",
            "epoch: 196, train_loss: 0.849332, val loss: 0.908519,  train_metric: -0.623 test_metric: -0.610 lr: 0.00082)\n",
            "epoch: 197, train_loss: 0.834535, val loss: 0.915837,  train_metric: -0.627 test_metric: -0.618 lr: 0.00082)\n",
            "epoch: 198, train_loss: 0.851619, val loss: 0.975829,  train_metric: -0.625 test_metric: -0.587 lr: 0.00082)\n",
            "epoch: 199, train_loss: 0.853825, val loss: 0.962467,  train_metric: -0.628 test_metric: -0.591 lr: 0.00082)\n",
            "epoch: 200, train_loss: 0.841823, val loss: 0.907138,  train_metric: -0.632 test_metric: -0.615 lr: 0.00082)\n",
            "epoch: 201, train_loss: 0.835850, val loss: 0.922494,  train_metric: -0.643 test_metric: -0.627 lr: 0.00082)\n",
            "epoch: 202, train_loss: 0.826351, val loss: 0.910505,  train_metric: -0.645 test_metric: -0.622 lr: 0.00082)\n",
            "epoch: 203, train_loss: 0.825689, val loss: 0.904441,  train_metric: -0.642 test_metric: -0.623 lr: 0.00082)\n",
            "epoch: 204, train_loss: 0.831359, val loss: 0.904552,  train_metric: -0.645 test_metric: -0.634 lr: 0.00081)\n",
            "epoch: 205, train_loss: 0.821504, val loss: 0.902016,  train_metric: -0.644 test_metric: -0.626 lr: 0.00081)\n",
            "epoch: 206, train_loss: 0.819960, val loss: 0.893640,  train_metric: -0.647 test_metric: -0.616 lr: 0.00081)\n",
            "epoch: 207, train_loss: 0.823140, val loss: 0.921395,  train_metric: -0.645 test_metric: -0.621 lr: 0.00081)\n",
            "epoch: 208, train_loss: 0.822486, val loss: 0.902501,  train_metric: -0.647 test_metric: -0.615 lr: 0.00081)\n",
            "epoch: 209, train_loss: 0.820166, val loss: 0.937091,  train_metric: -0.650 test_metric: -0.616 lr: 0.00081)\n",
            "epoch: 210, train_loss: 0.820697, val loss: 0.902227,  train_metric: -0.644 test_metric: -0.625 lr: 0.00081)\n",
            "epoch: 211, train_loss: 0.814424, val loss: 0.897088,  train_metric: -0.645 test_metric: -0.623 lr: 0.00081)\n",
            "epoch: 212, train_loss: 0.826198, val loss: 0.913474,  train_metric: -0.635 test_metric: -0.607 lr: 0.00081)\n",
            "epoch: 213, train_loss: 0.811932, val loss: 0.895675,  train_metric: -0.645 test_metric: -0.622 lr: 0.00081)\n",
            "epoch: 214, train_loss: 0.818224, val loss: 0.954288,  train_metric: -0.640 test_metric: -0.571 lr: 0.00081)\n",
            "epoch: 215, train_loss: 0.827071, val loss: 0.895567,  train_metric: -0.648 test_metric: -0.613 lr: 0.00081)\n",
            "epoch: 216, train_loss: 0.819939, val loss: 0.896246,  train_metric: -0.643 test_metric: -0.625 lr: 0.00080)\n",
            "epoch: 217, train_loss: 0.806527, val loss: 0.905943,  train_metric: -0.654 test_metric: -0.623 lr: 0.00080)\n",
            "epoch: 218, train_loss: 0.820839, val loss: 0.919969,  train_metric: -0.642 test_metric: -0.625 lr: 0.00080)\n",
            "epoch: 219, train_loss: 0.817882, val loss: 0.879781,  train_metric: -0.643 test_metric: -0.626 lr: 0.00080)\n",
            "epoch: 220, train_loss: 0.796452, val loss: 0.891244,  train_metric: -0.659 test_metric: -0.628 lr: 0.00080)\n",
            "epoch: 221, train_loss: 0.796026, val loss: 0.878977,  train_metric: -0.659 test_metric: -0.633 lr: 0.00080)\n",
            "epoch: 222, train_loss: 0.789031, val loss: 0.886927,  train_metric: -0.654 test_metric: -0.631 lr: 0.00080)\n",
            "epoch: 223, train_loss: 0.795790, val loss: 0.896548,  train_metric: -0.665 test_metric: -0.620 lr: 0.00080)\n",
            "epoch: 224, train_loss: 0.799435, val loss: 0.880630,  train_metric: -0.656 test_metric: -0.625 lr: 0.00080)\n",
            "epoch: 225, train_loss: 0.788905, val loss: 0.869727,  train_metric: -0.662 test_metric: -0.637 lr: 0.00080)\n",
            "epoch: 226, train_loss: 0.780269, val loss: 0.880012,  train_metric: -0.661 test_metric: -0.637 lr: 0.00080)\n",
            "epoch: 227, train_loss: 0.792571, val loss: 0.879764,  train_metric: -0.657 test_metric: -0.615 lr: 0.00080)\n",
            "epoch: 228, train_loss: 0.784501, val loss: 0.869929,  train_metric: -0.674 test_metric: -0.632 lr: 0.00080)\n",
            "epoch: 229, train_loss: 0.789440, val loss: 0.861062,  train_metric: -0.666 test_metric: -0.646 lr: 0.00079)\n",
            "epoch: 230, train_loss: 0.776991, val loss: 0.872740,  train_metric: -0.670 test_metric: -0.632 lr: 0.00079)\n",
            "epoch: 231, train_loss: 0.776954, val loss: 0.865281,  train_metric: -0.666 test_metric: -0.643 lr: 0.00079)\n",
            "epoch: 232, train_loss: 0.771020, val loss: 0.873931,  train_metric: -0.668 test_metric: -0.633 lr: 0.00079)\n",
            "epoch: 233, train_loss: 0.782425, val loss: 0.863960,  train_metric: -0.662 test_metric: -0.641 lr: 0.00079)\n",
            "epoch: 234, train_loss: 0.770386, val loss: 0.868080,  train_metric: -0.672 test_metric: -0.626 lr: 0.00079)\n",
            "epoch: 235, train_loss: 0.772668, val loss: 0.883881,  train_metric: -0.670 test_metric: -0.622 lr: 0.00079)\n",
            "epoch: 236, train_loss: 0.771881, val loss: 0.863672,  train_metric: -0.678 test_metric: -0.643 lr: 0.00079)\n",
            "epoch: 237, train_loss: 0.759477, val loss: 0.863652,  train_metric: -0.674 test_metric: -0.657 lr: 0.00079)\n",
            "epoch: 238, train_loss: 0.759082, val loss: 0.866997,  train_metric: -0.680 test_metric: -0.656 lr: 0.00079)\n",
            "epoch: 239, train_loss: 0.761427, val loss: 0.849550,  train_metric: -0.678 test_metric: -0.648 lr: 0.00079)\n",
            "epoch: 240, train_loss: 0.764022, val loss: 0.875101,  train_metric: -0.668 test_metric: -0.623 lr: 0.00079)\n",
            "epoch: 241, train_loss: 0.770325, val loss: 0.853734,  train_metric: -0.670 test_metric: -0.648 lr: 0.00078)\n",
            "epoch: 242, train_loss: 0.766128, val loss: 0.856419,  train_metric: -0.682 test_metric: -0.648 lr: 0.00078)\n",
            "epoch: 243, train_loss: 0.752368, val loss: 0.843245,  train_metric: -0.679 test_metric: -0.646 lr: 0.00078)\n",
            "epoch: 244, train_loss: 0.751685, val loss: 0.870607,  train_metric: -0.674 test_metric: -0.656 lr: 0.00078)\n",
            "epoch: 245, train_loss: 0.755838, val loss: 0.846418,  train_metric: -0.668 test_metric: -0.654 lr: 0.00078)\n",
            "epoch: 246, train_loss: 0.755681, val loss: 0.857300,  train_metric: -0.689 test_metric: -0.648 lr: 0.00078)\n",
            "epoch: 247, train_loss: 0.792437, val loss: 0.885636,  train_metric: -0.659 test_metric: -0.611 lr: 0.00078)\n",
            "epoch: 248, train_loss: 0.776688, val loss: 0.876950,  train_metric: -0.671 test_metric: -0.634 lr: 0.00078)\n",
            "epoch: 249, train_loss: 0.762643, val loss: 0.852457,  train_metric: -0.676 test_metric: -0.648 lr: 0.00078)\n",
            "epoch: 250, train_loss: 0.757065, val loss: 0.861423,  train_metric: -0.674 test_metric: -0.626 lr: 0.00078)\n",
            "epoch: 251, train_loss: 0.758431, val loss: 0.843202,  train_metric: -0.682 test_metric: -0.639 lr: 0.00078)\n",
            "epoch: 252, train_loss: 0.748167, val loss: 0.830790,  train_metric: -0.683 test_metric: -0.663 lr: 0.00078)\n",
            "epoch: 253, train_loss: 0.739709, val loss: 0.830474,  train_metric: -0.681 test_metric: -0.658 lr: 0.00078)\n",
            "epoch: 254, train_loss: 0.735700, val loss: 0.831804,  train_metric: -0.679 test_metric: -0.654 lr: 0.00077)\n",
            "epoch: 255, train_loss: 0.731280, val loss: 0.829790,  train_metric: -0.684 test_metric: -0.653 lr: 0.00077)\n",
            "epoch: 256, train_loss: 0.730897, val loss: 0.834868,  train_metric: -0.690 test_metric: -0.647 lr: 0.00077)\n",
            "epoch: 257, train_loss: 0.735732, val loss: 0.824497,  train_metric: -0.686 test_metric: -0.648 lr: 0.00077)\n",
            "epoch: 258, train_loss: 0.726137, val loss: 0.834023,  train_metric: -0.694 test_metric: -0.654 lr: 0.00077)\n",
            "epoch: 259, train_loss: 0.732125, val loss: 0.828289,  train_metric: -0.689 test_metric: -0.670 lr: 0.00077)\n",
            "epoch: 260, train_loss: 0.723009, val loss: 0.823902,  train_metric: -0.690 test_metric: -0.656 lr: 0.00077)\n",
            "epoch: 261, train_loss: 0.723157, val loss: 0.825107,  train_metric: -0.687 test_metric: -0.677 lr: 0.00077)\n",
            "epoch: 262, train_loss: 0.728082, val loss: 0.837499,  train_metric: -0.684 test_metric: -0.680 lr: 0.00077)\n",
            "epoch: 263, train_loss: 0.721162, val loss: 0.821460,  train_metric: -0.708 test_metric: -0.653 lr: 0.00077)\n",
            "epoch: 264, train_loss: 0.721590, val loss: 0.862264,  train_metric: -0.694 test_metric: -0.662 lr: 0.00077)\n",
            "epoch: 265, train_loss: 0.725575, val loss: 0.817227,  train_metric: -0.681 test_metric: -0.659 lr: 0.00077)\n",
            "epoch: 266, train_loss: 0.713849, val loss: 0.820461,  train_metric: -0.699 test_metric: -0.647 lr: 0.00077)\n",
            "epoch: 267, train_loss: 0.731064, val loss: 0.843779,  train_metric: -0.690 test_metric: -0.633 lr: 0.00076)\n",
            "epoch: 268, train_loss: 0.737840, val loss: 0.863980,  train_metric: -0.686 test_metric: -0.651 lr: 0.00076)\n",
            "epoch: 269, train_loss: 0.732603, val loss: 0.833947,  train_metric: -0.692 test_metric: -0.663 lr: 0.00076)\n",
            "epoch: 270, train_loss: 0.709348, val loss: 0.809373,  train_metric: -0.700 test_metric: -0.657 lr: 0.00076)\n",
            "epoch: 271, train_loss: 0.715050, val loss: 0.813250,  train_metric: -0.694 test_metric: -0.684 lr: 0.00076)\n",
            "epoch: 272, train_loss: 0.707557, val loss: 0.800678,  train_metric: -0.701 test_metric: -0.678 lr: 0.00076)\n",
            "epoch: 273, train_loss: 0.702100, val loss: 0.848290,  train_metric: -0.698 test_metric: -0.663 lr: 0.00076)\n",
            "epoch: 274, train_loss: 0.705548, val loss: 0.804392,  train_metric: -0.705 test_metric: -0.652 lr: 0.00076)\n",
            "epoch: 275, train_loss: 0.704454, val loss: 0.818599,  train_metric: -0.694 test_metric: -0.657 lr: 0.00076)\n",
            "epoch: 276, train_loss: 0.705821, val loss: 0.796122,  train_metric: -0.705 test_metric: -0.659 lr: 0.00076)\n",
            "epoch: 277, train_loss: 0.699526, val loss: 0.795066,  train_metric: -0.697 test_metric: -0.686 lr: 0.00076)\n",
            "epoch: 278, train_loss: 0.699943, val loss: 0.804065,  train_metric: -0.701 test_metric: -0.662 lr: 0.00076)\n",
            "epoch: 279, train_loss: 0.708891, val loss: 0.801293,  train_metric: -0.691 test_metric: -0.656 lr: 0.00076)\n",
            "epoch: 280, train_loss: 0.694243, val loss: 0.800787,  train_metric: -0.709 test_metric: -0.682 lr: 0.00075)\n",
            "epoch: 281, train_loss: 0.702695, val loss: 0.809410,  train_metric: -0.700 test_metric: -0.641 lr: 0.00075)\n",
            "epoch: 282, train_loss: 0.693012, val loss: 0.796939,  train_metric: -0.707 test_metric: -0.670 lr: 0.00075)\n",
            "epoch: 283, train_loss: 0.695157, val loss: 0.810211,  train_metric: -0.704 test_metric: -0.651 lr: 0.00075)\n",
            "epoch: 284, train_loss: 0.691845, val loss: 0.821689,  train_metric: -0.707 test_metric: -0.683 lr: 0.00075)\n",
            "epoch: 285, train_loss: 0.706385, val loss: 0.787841,  train_metric: -0.704 test_metric: -0.672 lr: 0.00075)\n",
            "epoch: 286, train_loss: 0.685226, val loss: 0.798069,  train_metric: -0.700 test_metric: -0.667 lr: 0.00075)\n",
            "epoch: 287, train_loss: 0.686805, val loss: 0.806658,  train_metric: -0.707 test_metric: -0.656 lr: 0.00075)\n",
            "epoch: 288, train_loss: 0.689650, val loss: 0.794311,  train_metric: -0.711 test_metric: -0.688 lr: 0.00075)\n",
            "epoch: 289, train_loss: 0.680867, val loss: 0.788978,  train_metric: -0.716 test_metric: -0.674 lr: 0.00075)\n",
            "epoch: 290, train_loss: 0.677859, val loss: 0.787572,  train_metric: -0.715 test_metric: -0.679 lr: 0.00075)\n",
            "epoch: 291, train_loss: 0.684747, val loss: 0.785713,  train_metric: -0.707 test_metric: -0.669 lr: 0.00075)\n",
            "epoch: 292, train_loss: 0.675482, val loss: 0.773850,  train_metric: -0.717 test_metric: -0.677 lr: 0.00075)\n",
            "epoch: 293, train_loss: 0.674107, val loss: 0.795488,  train_metric: -0.709 test_metric: -0.673 lr: 0.00075)\n",
            "epoch: 294, train_loss: 0.683499, val loss: 0.780920,  train_metric: -0.709 test_metric: -0.688 lr: 0.00074)\n",
            "epoch: 295, train_loss: 0.676527, val loss: 0.774122,  train_metric: -0.725 test_metric: -0.688 lr: 0.00074)\n",
            "epoch: 296, train_loss: 0.676658, val loss: 0.796438,  train_metric: -0.718 test_metric: -0.672 lr: 0.00074)\n",
            "epoch: 297, train_loss: 0.680538, val loss: 0.785551,  train_metric: -0.712 test_metric: -0.673 lr: 0.00074)\n",
            "epoch: 298, train_loss: 0.675263, val loss: 0.783043,  train_metric: -0.715 test_metric: -0.689 lr: 0.00074)\n",
            "epoch: 299, train_loss: 0.686920, val loss: 0.788358,  train_metric: -0.712 test_metric: -0.658 lr: 0.00074)\n",
            "epoch: 300, train_loss: 0.668350, val loss: 0.777794,  train_metric: -0.730 test_metric: -0.673 lr: 0.00074)\n",
            "epoch: 301, train_loss: 0.667633, val loss: 0.783839,  train_metric: -0.709 test_metric: -0.693 lr: 0.00074)\n",
            "epoch: 302, train_loss: 0.668591, val loss: 0.786507,  train_metric: -0.720 test_metric: -0.673 lr: 0.00074)\n",
            "epoch: 303, train_loss: 0.666734, val loss: 0.778770,  train_metric: -0.718 test_metric: -0.699 lr: 0.00074)\n",
            "epoch: 304, train_loss: 0.669232, val loss: 0.774482,  train_metric: -0.726 test_metric: -0.657 lr: 0.00074)\n",
            "epoch: 305, train_loss: 0.659121, val loss: 0.789757,  train_metric: -0.725 test_metric: -0.695 lr: 0.00074)\n",
            "epoch: 306, train_loss: 0.670478, val loss: 0.795954,  train_metric: -0.715 test_metric: -0.668 lr: 0.00074)\n",
            "epoch: 307, train_loss: 0.673514, val loss: 0.783826,  train_metric: -0.714 test_metric: -0.688 lr: 0.00073)\n",
            "epoch: 308, train_loss: 0.665962, val loss: 0.784796,  train_metric: -0.727 test_metric: -0.688 lr: 0.00073)\n",
            "epoch: 309, train_loss: 0.670179, val loss: 0.762919,  train_metric: -0.720 test_metric: -0.665 lr: 0.00073)\n",
            "epoch: 310, train_loss: 0.658552, val loss: 0.761066,  train_metric: -0.727 test_metric: -0.668 lr: 0.00073)\n",
            "epoch: 311, train_loss: 0.657710, val loss: 0.775488,  train_metric: -0.718 test_metric: -0.668 lr: 0.00073)\n",
            "epoch: 312, train_loss: 0.660479, val loss: 0.775303,  train_metric: -0.726 test_metric: -0.693 lr: 0.00073)\n",
            "epoch: 313, train_loss: 0.661142, val loss: 0.766493,  train_metric: -0.727 test_metric: -0.691 lr: 0.00073)\n",
            "epoch: 314, train_loss: 0.657235, val loss: 0.766675,  train_metric: -0.719 test_metric: -0.694 lr: 0.00073)\n",
            "epoch: 315, train_loss: 0.650345, val loss: 0.757733,  train_metric: -0.722 test_metric: -0.675 lr: 0.00073)\n",
            "epoch: 316, train_loss: 0.645481, val loss: 0.761363,  train_metric: -0.729 test_metric: -0.675 lr: 0.00073)\n",
            "epoch: 317, train_loss: 0.655493, val loss: 0.765576,  train_metric: -0.713 test_metric: -0.682 lr: 0.00073)\n",
            "epoch: 318, train_loss: 0.665089, val loss: 0.764260,  train_metric: -0.720 test_metric: -0.660 lr: 0.00073)\n",
            "epoch: 319, train_loss: 0.648173, val loss: 0.757390,  train_metric: -0.720 test_metric: -0.694 lr: 0.00073)\n",
            "epoch: 320, train_loss: 0.648890, val loss: 0.748466,  train_metric: -0.725 test_metric: -0.699 lr: 0.00073)\n",
            "epoch: 321, train_loss: 0.645808, val loss: 0.772245,  train_metric: -0.728 test_metric: -0.670 lr: 0.00072)\n",
            "epoch: 322, train_loss: 0.658616, val loss: 0.796662,  train_metric: -0.708 test_metric: -0.677 lr: 0.00072)\n",
            "epoch: 323, train_loss: 0.654860, val loss: 0.751050,  train_metric: -0.720 test_metric: -0.685 lr: 0.00072)\n",
            "epoch: 324, train_loss: 0.637824, val loss: 0.740040,  train_metric: -0.736 test_metric: -0.690 lr: 0.00072)\n",
            "epoch: 325, train_loss: 0.635494, val loss: 0.745872,  train_metric: -0.744 test_metric: -0.675 lr: 0.00072)\n",
            "epoch: 326, train_loss: 0.650769, val loss: 0.793360,  train_metric: -0.715 test_metric: -0.678 lr: 0.00072)\n",
            "epoch: 327, train_loss: 0.650482, val loss: 0.751282,  train_metric: -0.728 test_metric: -0.703 lr: 0.00072)\n",
            "epoch: 328, train_loss: 0.639354, val loss: 0.749035,  train_metric: -0.722 test_metric: -0.706 lr: 0.00072)\n",
            "epoch: 329, train_loss: 0.628228, val loss: 0.738185,  train_metric: -0.750 test_metric: -0.682 lr: 0.00072)\n",
            "epoch: 330, train_loss: 0.635275, val loss: 0.755286,  train_metric: -0.721 test_metric: -0.696 lr: 0.00072)\n",
            "epoch: 331, train_loss: 0.640580, val loss: 0.743721,  train_metric: -0.735 test_metric: -0.669 lr: 0.00072)\n",
            "epoch: 332, train_loss: 0.633741, val loss: 0.818778,  train_metric: -0.734 test_metric: -0.654 lr: 0.00072)\n",
            "epoch: 333, train_loss: 0.640531, val loss: 0.732786,  train_metric: -0.728 test_metric: -0.683 lr: 0.00072)\n",
            "epoch: 334, train_loss: 0.630561, val loss: 0.800055,  train_metric: -0.741 test_metric: -0.672 lr: 0.00072)\n",
            "epoch: 335, train_loss: 0.641893, val loss: 0.747381,  train_metric: -0.725 test_metric: -0.690 lr: 0.00071)\n",
            "epoch: 336, train_loss: 0.625415, val loss: 0.750749,  train_metric: -0.736 test_metric: -0.704 lr: 0.00071)\n",
            "epoch: 337, train_loss: 0.623320, val loss: 0.732281,  train_metric: -0.744 test_metric: -0.698 lr: 0.00071)\n",
            "epoch: 338, train_loss: 0.626614, val loss: 0.734510,  train_metric: -0.729 test_metric: -0.691 lr: 0.00071)\n",
            "epoch: 339, train_loss: 0.619251, val loss: 0.723717,  train_metric: -0.741 test_metric: -0.704 lr: 0.00071)\n",
            "epoch: 340, train_loss: 0.621872, val loss: 0.733018,  train_metric: -0.733 test_metric: -0.696 lr: 0.00071)\n",
            "epoch: 341, train_loss: 0.616835, val loss: 0.741574,  train_metric: -0.738 test_metric: -0.717 lr: 0.00071)\n",
            "epoch: 342, train_loss: 0.614008, val loss: 0.766802,  train_metric: -0.752 test_metric: -0.686 lr: 0.00071)\n",
            "epoch: 343, train_loss: 0.621497, val loss: 0.737410,  train_metric: -0.746 test_metric: -0.698 lr: 0.00071)\n",
            "epoch: 344, train_loss: 0.615245, val loss: 0.722386,  train_metric: -0.742 test_metric: -0.713 lr: 0.00071)\n",
            "epoch: 345, train_loss: 0.607794, val loss: 0.723704,  train_metric: -0.754 test_metric: -0.700 lr: 0.00071)\n",
            "epoch: 346, train_loss: 0.617916, val loss: 0.718946,  train_metric: -0.736 test_metric: -0.703 lr: 0.00071)\n",
            "epoch: 347, train_loss: 0.617082, val loss: 0.720765,  train_metric: -0.733 test_metric: -0.694 lr: 0.00071)\n",
            "epoch: 348, train_loss: 0.608780, val loss: 0.728684,  train_metric: -0.746 test_metric: -0.705 lr: 0.00071)\n",
            "epoch: 349, train_loss: 0.612848, val loss: 0.724625,  train_metric: -0.745 test_metric: -0.699 lr: 0.00070)\n",
            "epoch: 350, train_loss: 0.613241, val loss: 0.723062,  train_metric: -0.743 test_metric: -0.708 lr: 0.00070)\n",
            "epoch: 351, train_loss: 0.608559, val loss: 0.732466,  train_metric: -0.753 test_metric: -0.699 lr: 0.00070)\n",
            "epoch: 352, train_loss: 0.615458, val loss: 0.726752,  train_metric: -0.739 test_metric: -0.711 lr: 0.00070)\n",
            "epoch: 353, train_loss: 0.608765, val loss: 0.718164,  train_metric: -0.747 test_metric: -0.714 lr: 0.00070)\n",
            "epoch: 354, train_loss: 0.613397, val loss: 0.704760,  train_metric: -0.750 test_metric: -0.700 lr: 0.00070)\n",
            "epoch: 355, train_loss: 0.611244, val loss: 0.713916,  train_metric: -0.735 test_metric: -0.686 lr: 0.00070)\n",
            "epoch: 356, train_loss: 0.604819, val loss: 0.708356,  train_metric: -0.746 test_metric: -0.716 lr: 0.00070)\n",
            "epoch: 357, train_loss: 0.608072, val loss: 0.743100,  train_metric: -0.738 test_metric: -0.699 lr: 0.00070)\n",
            "epoch: 358, train_loss: 0.610565, val loss: 0.727011,  train_metric: -0.737 test_metric: -0.701 lr: 0.00070)\n",
            "epoch: 359, train_loss: 0.610825, val loss: 0.703367,  train_metric: -0.745 test_metric: -0.717 lr: 0.00070)\n",
            "epoch: 360, train_loss: 0.594151, val loss: 0.700046,  train_metric: -0.751 test_metric: -0.714 lr: 0.00070)\n",
            "epoch: 361, train_loss: 0.591590, val loss: 0.700924,  train_metric: -0.754 test_metric: -0.725 lr: 0.00070)\n",
            "epoch: 362, train_loss: 0.593407, val loss: 0.701128,  train_metric: -0.755 test_metric: -0.715 lr: 0.00070)\n",
            "epoch: 363, train_loss: 0.600460, val loss: 0.706935,  train_metric: -0.757 test_metric: -0.720 lr: 0.00069)\n",
            "epoch: 364, train_loss: 0.590574, val loss: 0.704898,  train_metric: -0.764 test_metric: -0.727 lr: 0.00069)\n",
            "epoch: 365, train_loss: 0.592793, val loss: 0.714534,  train_metric: -0.758 test_metric: -0.711 lr: 0.00069)\n",
            "epoch: 366, train_loss: 0.592036, val loss: 0.711247,  train_metric: -0.750 test_metric: -0.730 lr: 0.00069)\n",
            "epoch: 367, train_loss: 0.596256, val loss: 0.700066,  train_metric: -0.764 test_metric: -0.704 lr: 0.00069)\n",
            "epoch: 368, train_loss: 0.593900, val loss: 0.705573,  train_metric: -0.752 test_metric: -0.716 lr: 0.00069)\n",
            "epoch: 369, train_loss: 0.587330, val loss: 0.694685,  train_metric: -0.760 test_metric: -0.729 lr: 0.00069)\n",
            "epoch: 370, train_loss: 0.585246, val loss: 0.696868,  train_metric: -0.762 test_metric: -0.709 lr: 0.00069)\n",
            "epoch: 371, train_loss: 0.585053, val loss: 0.711210,  train_metric: -0.757 test_metric: -0.713 lr: 0.00069)\n",
            "epoch: 372, train_loss: 0.595498, val loss: 0.711738,  train_metric: -0.755 test_metric: -0.711 lr: 0.00069)\n",
            "epoch: 373, train_loss: 0.596357, val loss: 0.690176,  train_metric: -0.754 test_metric: -0.700 lr: 0.00069)\n",
            "epoch: 374, train_loss: 0.583606, val loss: 0.693038,  train_metric: -0.761 test_metric: -0.716 lr: 0.00069)\n",
            "epoch: 375, train_loss: 0.581981, val loss: 0.704862,  train_metric: -0.764 test_metric: -0.699 lr: 0.00069)\n",
            "epoch: 376, train_loss: 0.588208, val loss: 0.704349,  train_metric: -0.757 test_metric: -0.709 lr: 0.00069)\n",
            "epoch: 377, train_loss: 0.599976, val loss: 0.735063,  train_metric: -0.750 test_metric: -0.704 lr: 0.00069)\n",
            "epoch: 378, train_loss: 0.596741, val loss: 0.728401,  train_metric: -0.752 test_metric: -0.704 lr: 0.00068)\n",
            "epoch: 379, train_loss: 0.591408, val loss: 0.687032,  train_metric: -0.754 test_metric: -0.710 lr: 0.00068)\n",
            "epoch: 380, train_loss: 0.577586, val loss: 0.680693,  train_metric: -0.762 test_metric: -0.719 lr: 0.00068)\n",
            "epoch: 381, train_loss: 0.574383, val loss: 0.683741,  train_metric: -0.771 test_metric: -0.717 lr: 0.00068)\n",
            "epoch: 382, train_loss: 0.568061, val loss: 0.679807,  train_metric: -0.764 test_metric: -0.739 lr: 0.00068)\n",
            "epoch: 383, train_loss: 0.568269, val loss: 0.678925,  train_metric: -0.771 test_metric: -0.734 lr: 0.00068)\n",
            "epoch: 384, train_loss: 0.567188, val loss: 0.688443,  train_metric: -0.778 test_metric: -0.721 lr: 0.00068)\n",
            "epoch: 385, train_loss: 0.579774, val loss: 0.700256,  train_metric: -0.759 test_metric: -0.717 lr: 0.00068)\n",
            "epoch: 386, train_loss: 0.582013, val loss: 0.696689,  train_metric: -0.756 test_metric: -0.698 lr: 0.00068)\n",
            "epoch: 387, train_loss: 0.576353, val loss: 0.691306,  train_metric: -0.760 test_metric: -0.725 lr: 0.00068)\n",
            "epoch: 388, train_loss: 0.576698, val loss: 0.684465,  train_metric: -0.763 test_metric: -0.746 lr: 0.00068)\n",
            "epoch: 389, train_loss: 0.567775, val loss: 0.681135,  train_metric: -0.779 test_metric: -0.727 lr: 0.00068)\n",
            "epoch: 390, train_loss: 0.571190, val loss: 0.680867,  train_metric: -0.764 test_metric: -0.729 lr: 0.00068)\n",
            "epoch: 391, train_loss: 0.580202, val loss: 0.687687,  train_metric: -0.756 test_metric: -0.714 lr: 0.00068)\n",
            "epoch: 392, train_loss: 0.568939, val loss: 0.671344,  train_metric: -0.768 test_metric: -0.745 lr: 0.00067)\n",
            "epoch: 393, train_loss: 0.566352, val loss: 0.690035,  train_metric: -0.774 test_metric: -0.727 lr: 0.00067)\n",
            "epoch: 394, train_loss: 0.572711, val loss: 0.754945,  train_metric: -0.768 test_metric: -0.680 lr: 0.00067)\n",
            "epoch: 395, train_loss: 0.590145, val loss: 0.673789,  train_metric: -0.751 test_metric: -0.706 lr: 0.00067)\n",
            "epoch: 396, train_loss: 0.571440, val loss: 0.682170,  train_metric: -0.767 test_metric: -0.714 lr: 0.00067)\n",
            "epoch: 397, train_loss: 0.564997, val loss: 0.663324,  train_metric: -0.775 test_metric: -0.730 lr: 0.00067)\n",
            "epoch: 398, train_loss: 0.560995, val loss: 0.687842,  train_metric: -0.775 test_metric: -0.722 lr: 0.00067)\n",
            "epoch: 399, train_loss: 0.558675, val loss: 0.669915,  train_metric: -0.769 test_metric: -0.731 lr: 0.00067)\n",
            "epoch: 400, train_loss: 0.565566, val loss: 0.685431,  train_metric: -0.771 test_metric: -0.725 lr: 0.00067)\n",
            "epoch: 401, train_loss: 0.568454, val loss: 0.660995,  train_metric: -0.774 test_metric: -0.726 lr: 0.00067)\n",
            "epoch: 402, train_loss: 0.563956, val loss: 0.676128,  train_metric: -0.771 test_metric: -0.739 lr: 0.00067)\n",
            "epoch: 403, train_loss: 0.559632, val loss: 0.682590,  train_metric: -0.771 test_metric: -0.725 lr: 0.00067)\n",
            "epoch: 404, train_loss: 0.558113, val loss: 0.676667,  train_metric: -0.771 test_metric: -0.748 lr: 0.00067)\n",
            "epoch: 405, train_loss: 0.562034, val loss: 0.679480,  train_metric: -0.768 test_metric: -0.734 lr: 0.00067)\n",
            "epoch: 406, train_loss: 0.554565, val loss: 0.660637,  train_metric: -0.780 test_metric: -0.746 lr: 0.00067)\n",
            "epoch: 407, train_loss: 0.548672, val loss: 0.657712,  train_metric: -0.784 test_metric: -0.746 lr: 0.00066)\n",
            "epoch: 408, train_loss: 0.554410, val loss: 0.665401,  train_metric: -0.777 test_metric: -0.751 lr: 0.00066)\n",
            "epoch: 409, train_loss: 0.551258, val loss: 0.661686,  train_metric: -0.787 test_metric: -0.731 lr: 0.00066)\n",
            "epoch: 410, train_loss: 0.549450, val loss: 0.659921,  train_metric: -0.778 test_metric: -0.747 lr: 0.00066)\n",
            "epoch: 411, train_loss: 0.552525, val loss: 0.658172,  train_metric: -0.775 test_metric: -0.730 lr: 0.00066)\n",
            "epoch: 412, train_loss: 0.549184, val loss: 0.660783,  train_metric: -0.774 test_metric: -0.746 lr: 0.00066)\n",
            "epoch: 413, train_loss: 0.546231, val loss: 0.667290,  train_metric: -0.776 test_metric: -0.727 lr: 0.00066)\n",
            "epoch: 414, train_loss: 0.552605, val loss: 0.664901,  train_metric: -0.772 test_metric: -0.737 lr: 0.00066)\n",
            "epoch: 415, train_loss: 0.557746, val loss: 0.690410,  train_metric: -0.783 test_metric: -0.726 lr: 0.00066)\n",
            "epoch: 416, train_loss: 0.556667, val loss: 0.648358,  train_metric: -0.768 test_metric: -0.758 lr: 0.00066)\n",
            "epoch: 417, train_loss: 0.554008, val loss: 0.662796,  train_metric: -0.773 test_metric: -0.731 lr: 0.00066)\n",
            "epoch: 418, train_loss: 0.546327, val loss: 0.650167,  train_metric: -0.788 test_metric: -0.751 lr: 0.00066)\n",
            "epoch: 419, train_loss: 0.556487, val loss: 0.724694,  train_metric: -0.772 test_metric: -0.684 lr: 0.00066)\n",
            "epoch: 420, train_loss: 0.562615, val loss: 0.662371,  train_metric: -0.768 test_metric: -0.722 lr: 0.00066)\n",
            "epoch: 421, train_loss: 0.543017, val loss: 0.640373,  train_metric: -0.783 test_metric: -0.736 lr: 0.00066)\n",
            "epoch: 422, train_loss: 0.544854, val loss: 0.654381,  train_metric: -0.773 test_metric: -0.755 lr: 0.00065)\n",
            "epoch: 423, train_loss: 0.537107, val loss: 0.666238,  train_metric: -0.780 test_metric: -0.746 lr: 0.00065)\n",
            "epoch: 424, train_loss: 0.550212, val loss: 0.669344,  train_metric: -0.763 test_metric: -0.740 lr: 0.00065)\n",
            "epoch: 425, train_loss: 0.549764, val loss: 0.645278,  train_metric: -0.776 test_metric: -0.736 lr: 0.00065)\n",
            "epoch: 426, train_loss: 0.533024, val loss: 0.634406,  train_metric: -0.787 test_metric: -0.762 lr: 0.00065)\n",
            "epoch: 427, train_loss: 0.534786, val loss: 0.638167,  train_metric: -0.789 test_metric: -0.763 lr: 0.00065)\n",
            "epoch: 428, train_loss: 0.533137, val loss: 0.647527,  train_metric: -0.794 test_metric: -0.742 lr: 0.00065)\n",
            "epoch: 429, train_loss: 0.530161, val loss: 0.629753,  train_metric: -0.785 test_metric: -0.766 lr: 0.00065)\n",
            "epoch: 430, train_loss: 0.527283, val loss: 0.635212,  train_metric: -0.791 test_metric: -0.743 lr: 0.00065)\n",
            "epoch: 431, train_loss: 0.529539, val loss: 0.637049,  train_metric: -0.799 test_metric: -0.750 lr: 0.00065)\n",
            "epoch: 432, train_loss: 0.531398, val loss: 0.673353,  train_metric: -0.780 test_metric: -0.727 lr: 0.00065)\n",
            "epoch: 433, train_loss: 0.538375, val loss: 0.645464,  train_metric: -0.780 test_metric: -0.751 lr: 0.00065)\n",
            "epoch: 434, train_loss: 0.536659, val loss: 0.646495,  train_metric: -0.789 test_metric: -0.748 lr: 0.00065)\n",
            "epoch: 435, train_loss: 0.528071, val loss: 0.646467,  train_metric: -0.790 test_metric: -0.752 lr: 0.00065)\n",
            "epoch: 436, train_loss: 0.524209, val loss: 0.634645,  train_metric: -0.794 test_metric: -0.758 lr: 0.00065)\n",
            "epoch: 437, train_loss: 0.528128, val loss: 0.632843,  train_metric: -0.789 test_metric: -0.748 lr: 0.00065)\n",
            "epoch: 438, train_loss: 0.520782, val loss: 0.637271,  train_metric: -0.783 test_metric: -0.770 lr: 0.00064)\n",
            "epoch: 439, train_loss: 0.526336, val loss: 0.642329,  train_metric: -0.789 test_metric: -0.742 lr: 0.00064)\n",
            "epoch: 440, train_loss: 0.523920, val loss: 0.629858,  train_metric: -0.792 test_metric: -0.758 lr: 0.00064)\n",
            "epoch: 441, train_loss: 0.521359, val loss: 0.638534,  train_metric: -0.797 test_metric: -0.756 lr: 0.00064)\n",
            "epoch: 442, train_loss: 0.529478, val loss: 0.634948,  train_metric: -0.791 test_metric: -0.736 lr: 0.00064)\n",
            "epoch: 443, train_loss: 0.532583, val loss: 0.642361,  train_metric: -0.776 test_metric: -0.737 lr: 0.00064)\n",
            "epoch: 444, train_loss: 0.523806, val loss: 0.639574,  train_metric: -0.788 test_metric: -0.748 lr: 0.00064)\n",
            "epoch: 445, train_loss: 0.519518, val loss: 0.636809,  train_metric: -0.793 test_metric: -0.743 lr: 0.00064)\n",
            "epoch: 446, train_loss: 0.534113, val loss: 0.632702,  train_metric: -0.785 test_metric: -0.761 lr: 0.00064)\n",
            "epoch: 447, train_loss: 0.523099, val loss: 0.644215,  train_metric: -0.792 test_metric: -0.740 lr: 0.00064)\n",
            "epoch: 448, train_loss: 0.520840, val loss: 0.637563,  train_metric: -0.789 test_metric: -0.740 lr: 0.00064)\n",
            "epoch: 449, train_loss: 0.513311, val loss: 0.633691,  train_metric: -0.797 test_metric: -0.740 lr: 0.00064)\n",
            "epoch: 450, train_loss: 0.513800, val loss: 0.617948,  train_metric: -0.803 test_metric: -0.763 lr: 0.00064)\n",
            "epoch: 451, train_loss: 0.506922, val loss: 0.621217,  train_metric: -0.804 test_metric: -0.766 lr: 0.00064)\n",
            "epoch: 452, train_loss: 0.512894, val loss: 0.613302,  train_metric: -0.799 test_metric: -0.771 lr: 0.00064)\n",
            "epoch: 453, train_loss: 0.512767, val loss: 0.651785,  train_metric: -0.794 test_metric: -0.732 lr: 0.00063)\n",
            "epoch: 454, train_loss: 0.523378, val loss: 0.647263,  train_metric: -0.794 test_metric: -0.747 lr: 0.00063)\n",
            "epoch: 455, train_loss: 0.538573, val loss: 0.614784,  train_metric: -0.771 test_metric: -0.758 lr: 0.00063)\n",
            "epoch: 456, train_loss: 0.518138, val loss: 0.614897,  train_metric: -0.781 test_metric: -0.765 lr: 0.00063)\n",
            "epoch: 457, train_loss: 0.508796, val loss: 0.617640,  train_metric: -0.797 test_metric: -0.765 lr: 0.00063)\n",
            "epoch: 458, train_loss: 0.510769, val loss: 0.622806,  train_metric: -0.795 test_metric: -0.745 lr: 0.00063)\n",
            "epoch: 459, train_loss: 0.508864, val loss: 0.617182,  train_metric: -0.796 test_metric: -0.760 lr: 0.00063)\n",
            "epoch: 460, train_loss: 0.520232, val loss: 0.637588,  train_metric: -0.789 test_metric: -0.734 lr: 0.00063)\n",
            "epoch: 461, train_loss: 0.509136, val loss: 0.615643,  train_metric: -0.801 test_metric: -0.755 lr: 0.00063)\n",
            "epoch: 462, train_loss: 0.504260, val loss: 0.622650,  train_metric: -0.800 test_metric: -0.756 lr: 0.00063)\n",
            "epoch: 463, train_loss: 0.509572, val loss: 0.614548,  train_metric: -0.800 test_metric: -0.753 lr: 0.00063)\n",
            "epoch: 464, train_loss: 0.499300, val loss: 0.612566,  train_metric: -0.800 test_metric: -0.752 lr: 0.00063)\n",
            "epoch: 465, train_loss: 0.506960, val loss: 0.689436,  train_metric: -0.795 test_metric: -0.714 lr: 0.00063)\n",
            "epoch: 466, train_loss: 0.525754, val loss: 0.604662,  train_metric: -0.789 test_metric: -0.757 lr: 0.00063)\n",
            "epoch: 467, train_loss: 0.502739, val loss: 0.613459,  train_metric: -0.801 test_metric: -0.753 lr: 0.00063)\n",
            "epoch: 468, train_loss: 0.496738, val loss: 0.618151,  train_metric: -0.798 test_metric: -0.750 lr: 0.00063)\n",
            "epoch: 469, train_loss: 0.500958, val loss: 0.613259,  train_metric: -0.804 test_metric: -0.755 lr: 0.00062)\n",
            "epoch: 470, train_loss: 0.496074, val loss: 0.608889,  train_metric: -0.795 test_metric: -0.772 lr: 0.00062)\n",
            "epoch: 471, train_loss: 0.498666, val loss: 0.625624,  train_metric: -0.809 test_metric: -0.747 lr: 0.00062)\n",
            "epoch: 472, train_loss: 0.504050, val loss: 0.639734,  train_metric: -0.793 test_metric: -0.751 lr: 0.00062)\n",
            "epoch: 473, train_loss: 0.511620, val loss: 0.611546,  train_metric: -0.810 test_metric: -0.766 lr: 0.00062)\n",
            "epoch: 474, train_loss: 0.508893, val loss: 0.617670,  train_metric: -0.794 test_metric: -0.750 lr: 0.00062)\n",
            "epoch: 475, train_loss: 0.499149, val loss: 0.627475,  train_metric: -0.802 test_metric: -0.737 lr: 0.00062)\n",
            "epoch: 476, train_loss: 0.498592, val loss: 0.612091,  train_metric: -0.800 test_metric: -0.757 lr: 0.00062)\n",
            "epoch: 477, train_loss: 0.504197, val loss: 0.608721,  train_metric: -0.790 test_metric: -0.751 lr: 0.00062)\n",
            "epoch: 478, train_loss: 0.501136, val loss: 0.625984,  train_metric: -0.801 test_metric: -0.745 lr: 0.00062)\n",
            "epoch: 479, train_loss: 0.509453, val loss: 0.609872,  train_metric: -0.796 test_metric: -0.768 lr: 0.00062)\n",
            "epoch: 480, train_loss: 0.490577, val loss: 0.602941,  train_metric: -0.804 test_metric: -0.774 lr: 0.00062)\n",
            "epoch: 481, train_loss: 0.490628, val loss: 0.609532,  train_metric: -0.806 test_metric: -0.756 lr: 0.00062)\n",
            "epoch: 482, train_loss: 0.492965, val loss: 0.648157,  train_metric: -0.802 test_metric: -0.719 lr: 0.00062)\n",
            "epoch: 483, train_loss: 0.514026, val loss: 0.619065,  train_metric: -0.789 test_metric: -0.747 lr: 0.00062)\n",
            "epoch: 484, train_loss: 0.491078, val loss: 0.630668,  train_metric: -0.811 test_metric: -0.727 lr: 0.00062)\n",
            "epoch: 485, train_loss: 0.497658, val loss: 0.601218,  train_metric: -0.797 test_metric: -0.774 lr: 0.00061)\n",
            "epoch: 486, train_loss: 0.494718, val loss: 0.591429,  train_metric: -0.799 test_metric: -0.761 lr: 0.00061)\n",
            "epoch: 487, train_loss: 0.482967, val loss: 0.602392,  train_metric: -0.803 test_metric: -0.762 lr: 0.00061)\n",
            "epoch: 488, train_loss: 0.486094, val loss: 0.602153,  train_metric: -0.805 test_metric: -0.756 lr: 0.00061)\n",
            "epoch: 489, train_loss: 0.484542, val loss: 0.595218,  train_metric: -0.811 test_metric: -0.761 lr: 0.00061)\n",
            "epoch: 490, train_loss: 0.487209, val loss: 0.623675,  train_metric: -0.803 test_metric: -0.736 lr: 0.00061)\n",
            "epoch: 491, train_loss: 0.481782, val loss: 0.592197,  train_metric: -0.813 test_metric: -0.771 lr: 0.00061)\n",
            "epoch: 492, train_loss: 0.476045, val loss: 0.587704,  train_metric: -0.810 test_metric: -0.772 lr: 0.00061)\n",
            "epoch: 493, train_loss: 0.486917, val loss: 0.587037,  train_metric: -0.805 test_metric: -0.762 lr: 0.00061)\n",
            "epoch: 494, train_loss: 0.486268, val loss: 0.617625,  train_metric: -0.812 test_metric: -0.752 lr: 0.00061)\n",
            "epoch: 495, train_loss: 0.496088, val loss: 0.610316,  train_metric: -0.790 test_metric: -0.745 lr: 0.00061)\n",
            "epoch: 496, train_loss: 0.484651, val loss: 0.595954,  train_metric: -0.803 test_metric: -0.770 lr: 0.00061)\n",
            "epoch: 497, train_loss: 0.476910, val loss: 0.615362,  train_metric: -0.817 test_metric: -0.750 lr: 0.00061)\n",
            "epoch: 498, train_loss: 0.480179, val loss: 0.591360,  train_metric: -0.809 test_metric: -0.773 lr: 0.00061)\n",
            "epoch: 499, train_loss: 0.477247, val loss: 0.583471,  train_metric: -0.807 test_metric: -0.772 lr: 0.00061)\n",
            "epoch: 500, train_loss: 0.483599, val loss: 0.587000,  train_metric: -0.804 test_metric: -0.770 lr: 0.00061)\n",
            "epoch: 501, train_loss: 0.473044, val loss: 0.582875,  train_metric: -0.810 test_metric: -0.788 lr: 0.00061)\n",
            "epoch: 502, train_loss: 0.472799, val loss: 0.575917,  train_metric: -0.814 test_metric: -0.770 lr: 0.00060)\n",
            "epoch: 503, train_loss: 0.468691, val loss: 0.575447,  train_metric: -0.821 test_metric: -0.794 lr: 0.00060)\n",
            "epoch: 504, train_loss: 0.468535, val loss: 0.586885,  train_metric: -0.819 test_metric: -0.758 lr: 0.00060)\n",
            "epoch: 505, train_loss: 0.470994, val loss: 0.580915,  train_metric: -0.817 test_metric: -0.777 lr: 0.00060)\n",
            "epoch: 506, train_loss: 0.481403, val loss: 0.594245,  train_metric: -0.809 test_metric: -0.766 lr: 0.00060)\n",
            "epoch: 507, train_loss: 0.471926, val loss: 0.587302,  train_metric: -0.813 test_metric: -0.773 lr: 0.00060)\n",
            "epoch: 508, train_loss: 0.473284, val loss: 0.580840,  train_metric: -0.812 test_metric: -0.784 lr: 0.00060)\n",
            "epoch: 509, train_loss: 0.476910, val loss: 0.574707,  train_metric: -0.813 test_metric: -0.776 lr: 0.00060)\n",
            "epoch: 510, train_loss: 0.467263, val loss: 0.573809,  train_metric: -0.812 test_metric: -0.782 lr: 0.00060)\n",
            "epoch: 511, train_loss: 0.467139, val loss: 0.592858,  train_metric: -0.821 test_metric: -0.770 lr: 0.00060)\n",
            "epoch: 512, train_loss: 0.472312, val loss: 0.578873,  train_metric: -0.819 test_metric: -0.768 lr: 0.00060)\n",
            "epoch: 513, train_loss: 0.468408, val loss: 0.621009,  train_metric: -0.817 test_metric: -0.731 lr: 0.00060)\n",
            "epoch: 514, train_loss: 0.475861, val loss: 0.607143,  train_metric: -0.805 test_metric: -0.753 lr: 0.00060)\n",
            "epoch: 515, train_loss: 0.480025, val loss: 0.621217,  train_metric: -0.805 test_metric: -0.739 lr: 0.00060)\n",
            "epoch: 516, train_loss: 0.470954, val loss: 0.588044,  train_metric: -0.817 test_metric: -0.786 lr: 0.00060)\n",
            "epoch: 517, train_loss: 0.467583, val loss: 0.586738,  train_metric: -0.819 test_metric: -0.762 lr: 0.00060)\n",
            "epoch: 518, train_loss: 0.468077, val loss: 0.572864,  train_metric: -0.817 test_metric: -0.786 lr: 0.00059)\n",
            "epoch: 519, train_loss: 0.458763, val loss: 0.570051,  train_metric: -0.822 test_metric: -0.793 lr: 0.00059)\n",
            "epoch: 520, train_loss: 0.464614, val loss: 0.570687,  train_metric: -0.826 test_metric: -0.784 lr: 0.00059)\n",
            "epoch: 521, train_loss: 0.460271, val loss: 0.567906,  train_metric: -0.817 test_metric: -0.777 lr: 0.00059)\n",
            "epoch: 522, train_loss: 0.459067, val loss: 0.567658,  train_metric: -0.826 test_metric: -0.791 lr: 0.00059)\n",
            "epoch: 523, train_loss: 0.462433, val loss: 0.597040,  train_metric: -0.818 test_metric: -0.765 lr: 0.00059)\n",
            "epoch: 524, train_loss: 0.478010, val loss: 0.562154,  train_metric: -0.806 test_metric: -0.777 lr: 0.00059)\n",
            "epoch: 525, train_loss: 0.463816, val loss: 0.579211,  train_metric: -0.821 test_metric: -0.777 lr: 0.00059)\n",
            "epoch: 526, train_loss: 0.457776, val loss: 0.564897,  train_metric: -0.820 test_metric: -0.791 lr: 0.00059)\n",
            "epoch: 527, train_loss: 0.450879, val loss: 0.574943,  train_metric: -0.829 test_metric: -0.760 lr: 0.00059)\n",
            "epoch: 528, train_loss: 0.457359, val loss: 0.565305,  train_metric: -0.819 test_metric: -0.782 lr: 0.00059)\n",
            "epoch: 529, train_loss: 0.449887, val loss: 0.574233,  train_metric: -0.833 test_metric: -0.777 lr: 0.00059)\n",
            "epoch: 530, train_loss: 0.455333, val loss: 0.600668,  train_metric: -0.834 test_metric: -0.737 lr: 0.00059)\n",
            "epoch: 531, train_loss: 0.456325, val loss: 0.569359,  train_metric: -0.822 test_metric: -0.777 lr: 0.00059)\n",
            "epoch: 532, train_loss: 0.454672, val loss: 0.580920,  train_metric: -0.829 test_metric: -0.757 lr: 0.00059)\n",
            "epoch: 533, train_loss: 0.463540, val loss: 0.559186,  train_metric: -0.818 test_metric: -0.800 lr: 0.00059)\n",
            "epoch: 534, train_loss: 0.455473, val loss: 0.550600,  train_metric: -0.820 test_metric: -0.783 lr: 0.00059)\n",
            "epoch: 535, train_loss: 0.447711, val loss: 0.565723,  train_metric: -0.829 test_metric: -0.784 lr: 0.00058)\n",
            "epoch: 536, train_loss: 0.458681, val loss: 0.588028,  train_metric: -0.824 test_metric: -0.747 lr: 0.00058)\n",
            "epoch: 537, train_loss: 0.458180, val loss: 0.561871,  train_metric: -0.821 test_metric: -0.792 lr: 0.00058)\n",
            "epoch: 538, train_loss: 0.451409, val loss: 0.587219,  train_metric: -0.829 test_metric: -0.779 lr: 0.00058)\n",
            "epoch: 539, train_loss: 0.457599, val loss: 0.562565,  train_metric: -0.823 test_metric: -0.784 lr: 0.00058)\n",
            "epoch: 540, train_loss: 0.451976, val loss: 0.553963,  train_metric: -0.833 test_metric: -0.799 lr: 0.00058)\n",
            "epoch: 541, train_loss: 0.443976, val loss: 0.556084,  train_metric: -0.833 test_metric: -0.798 lr: 0.00058)\n",
            "epoch: 542, train_loss: 0.444918, val loss: 0.568763,  train_metric: -0.840 test_metric: -0.772 lr: 0.00058)\n",
            "epoch: 543, train_loss: 0.444619, val loss: 0.569064,  train_metric: -0.829 test_metric: -0.786 lr: 0.00058)\n",
            "epoch: 544, train_loss: 0.449713, val loss: 0.568275,  train_metric: -0.823 test_metric: -0.791 lr: 0.00058)\n",
            "epoch: 545, train_loss: 0.454805, val loss: 0.583446,  train_metric: -0.822 test_metric: -0.756 lr: 0.00058)\n",
            "epoch: 546, train_loss: 0.445792, val loss: 0.554308,  train_metric: -0.835 test_metric: -0.784 lr: 0.00058)\n",
            "epoch: 547, train_loss: 0.444923, val loss: 0.543365,  train_metric: -0.831 test_metric: -0.809 lr: 0.00058)\n",
            "epoch: 548, train_loss: 0.440790, val loss: 0.569035,  train_metric: -0.841 test_metric: -0.765 lr: 0.00058)\n",
            "epoch: 549, train_loss: 0.450580, val loss: 0.568414,  train_metric: -0.817 test_metric: -0.787 lr: 0.00058)\n",
            "epoch: 550, train_loss: 0.451676, val loss: 0.643822,  train_metric: -0.832 test_metric: -0.711 lr: 0.00058)\n",
            "epoch: 551, train_loss: 0.460507, val loss: 0.552979,  train_metric: -0.817 test_metric: -0.794 lr: 0.00058)\n",
            "epoch: 552, train_loss: 0.442847, val loss: 0.546158,  train_metric: -0.828 test_metric: -0.791 lr: 0.00058)\n",
            "epoch: 553, train_loss: 0.444147, val loss: 0.575325,  train_metric: -0.824 test_metric: -0.768 lr: 0.00057)\n",
            "epoch: 554, train_loss: 0.448166, val loss: 0.571090,  train_metric: -0.820 test_metric: -0.789 lr: 0.00057)\n",
            "epoch: 555, train_loss: 0.450728, val loss: 0.571171,  train_metric: -0.830 test_metric: -0.766 lr: 0.00057)\n",
            "epoch: 556, train_loss: 0.442616, val loss: 0.562176,  train_metric: -0.828 test_metric: -0.796 lr: 0.00057)\n",
            "epoch: 557, train_loss: 0.436557, val loss: 0.551165,  train_metric: -0.841 test_metric: -0.793 lr: 0.00057)\n",
            "epoch: 558, train_loss: 0.440347, val loss: 0.554968,  train_metric: -0.839 test_metric: -0.786 lr: 0.00057)\n",
            "epoch: 559, train_loss: 0.441378, val loss: 0.545964,  train_metric: -0.833 test_metric: -0.805 lr: 0.00057)\n",
            "epoch: 560, train_loss: 0.439409, val loss: 0.543622,  train_metric: -0.835 test_metric: -0.792 lr: 0.00057)\n",
            "epoch: 561, train_loss: 0.434713, val loss: 0.552221,  train_metric: -0.834 test_metric: -0.798 lr: 0.00057)\n",
            "epoch: 562, train_loss: 0.434526, val loss: 0.552994,  train_metric: -0.833 test_metric: -0.799 lr: 0.00057)\n",
            "epoch: 563, train_loss: 0.440794, val loss: 0.544058,  train_metric: -0.834 test_metric: -0.798 lr: 0.00057)\n",
            "epoch: 564, train_loss: 0.437010, val loss: 0.530736,  train_metric: -0.835 test_metric: -0.805 lr: 0.00057)\n",
            "epoch: 565, train_loss: 0.432320, val loss: 0.541890,  train_metric: -0.844 test_metric: -0.786 lr: 0.00057)\n",
            "epoch: 566, train_loss: 0.435949, val loss: 0.544993,  train_metric: -0.842 test_metric: -0.796 lr: 0.00057)\n",
            "epoch: 567, train_loss: 0.432569, val loss: 0.565314,  train_metric: -0.839 test_metric: -0.797 lr: 0.00057)\n",
            "epoch: 568, train_loss: 0.441209, val loss: 0.560488,  train_metric: -0.833 test_metric: -0.784 lr: 0.00057)\n",
            "epoch: 569, train_loss: 0.438248, val loss: 0.544113,  train_metric: -0.838 test_metric: -0.794 lr: 0.00057)\n",
            "epoch: 570, train_loss: 0.442752, val loss: 0.594730,  train_metric: -0.827 test_metric: -0.763 lr: 0.00056)\n",
            "epoch: 571, train_loss: 0.459716, val loss: 0.582103,  train_metric: -0.804 test_metric: -0.753 lr: 0.00056)\n",
            "epoch: 572, train_loss: 0.439030, val loss: 0.528830,  train_metric: -0.835 test_metric: -0.817 lr: 0.00056)\n",
            "epoch: 573, train_loss: 0.429084, val loss: 0.552584,  train_metric: -0.833 test_metric: -0.792 lr: 0.00056)\n",
            "epoch: 574, train_loss: 0.432758, val loss: 0.539119,  train_metric: -0.829 test_metric: -0.808 lr: 0.00056)\n",
            "epoch: 575, train_loss: 0.432097, val loss: 0.543494,  train_metric: -0.839 test_metric: -0.799 lr: 0.00056)\n",
            "epoch: 576, train_loss: 0.428149, val loss: 0.529806,  train_metric: -0.837 test_metric: -0.805 lr: 0.00056)\n",
            "epoch: 577, train_loss: 0.424301, val loss: 0.540630,  train_metric: -0.844 test_metric: -0.796 lr: 0.00056)\n",
            "epoch: 578, train_loss: 0.430354, val loss: 0.558955,  train_metric: -0.842 test_metric: -0.772 lr: 0.00056)\n",
            "epoch: 579, train_loss: 0.441706, val loss: 0.567701,  train_metric: -0.822 test_metric: -0.772 lr: 0.00056)\n",
            "epoch: 580, train_loss: 0.425371, val loss: 0.536705,  train_metric: -0.839 test_metric: -0.791 lr: 0.00056)\n",
            "epoch: 581, train_loss: 0.422743, val loss: 0.550407,  train_metric: -0.840 test_metric: -0.794 lr: 0.00056)\n",
            "epoch: 582, train_loss: 0.430478, val loss: 0.559039,  train_metric: -0.830 test_metric: -0.786 lr: 0.00056)\n",
            "epoch: 583, train_loss: 0.426984, val loss: 0.526265,  train_metric: -0.836 test_metric: -0.814 lr: 0.00056)\n",
            "epoch: 584, train_loss: 0.419627, val loss: 0.533713,  train_metric: -0.848 test_metric: -0.800 lr: 0.00056)\n",
            "epoch: 585, train_loss: 0.422286, val loss: 0.538611,  train_metric: -0.842 test_metric: -0.800 lr: 0.00056)\n",
            "epoch: 586, train_loss: 0.429245, val loss: 0.547166,  train_metric: -0.834 test_metric: -0.791 lr: 0.00056)\n",
            "epoch: 587, train_loss: 0.430808, val loss: 0.532379,  train_metric: -0.826 test_metric: -0.803 lr: 0.00056)\n",
            "epoch: 588, train_loss: 0.422701, val loss: 0.564724,  train_metric: -0.837 test_metric: -0.791 lr: 0.00055)\n",
            "epoch: 589, train_loss: 0.433601, val loss: 0.533646,  train_metric: -0.837 test_metric: -0.810 lr: 0.00055)\n",
            "epoch: 590, train_loss: 0.423044, val loss: 0.531428,  train_metric: -0.838 test_metric: -0.808 lr: 0.00055)\n",
            "epoch: 591, train_loss: 0.430845, val loss: 0.530617,  train_metric: -0.839 test_metric: -0.803 lr: 0.00055)\n",
            "epoch: 592, train_loss: 0.421571, val loss: 0.534915,  train_metric: -0.836 test_metric: -0.814 lr: 0.00055)\n",
            "epoch: 593, train_loss: 0.420792, val loss: 0.552427,  train_metric: -0.849 test_metric: -0.776 lr: 0.00055)\n",
            "epoch: 594, train_loss: 0.438935, val loss: 0.542558,  train_metric: -0.835 test_metric: -0.800 lr: 0.00055)\n",
            "epoch: 595, train_loss: 0.419353, val loss: 0.544687,  train_metric: -0.841 test_metric: -0.774 lr: 0.00055)\n",
            "epoch: 596, train_loss: 0.418514, val loss: 0.544100,  train_metric: -0.849 test_metric: -0.792 lr: 0.00055)\n",
            "epoch: 597, train_loss: 0.428730, val loss: 0.593199,  train_metric: -0.837 test_metric: -0.782 lr: 0.00055)\n",
            "epoch: 598, train_loss: 0.446076, val loss: 0.543112,  train_metric: -0.835 test_metric: -0.773 lr: 0.00055)\n",
            "epoch: 599, train_loss: 0.417573, val loss: 0.516529,  train_metric: -0.838 test_metric: -0.812 lr: 0.00055)\n",
            "epoch: 600, train_loss: 0.408415, val loss: 0.525910,  train_metric: -0.844 test_metric: -0.799 lr: 0.00055)\n",
            "epoch: 601, train_loss: 0.428200, val loss: 0.528210,  train_metric: -0.829 test_metric: -0.789 lr: 0.00055)\n",
            "epoch: 602, train_loss: 0.415570, val loss: 0.536668,  train_metric: -0.839 test_metric: -0.803 lr: 0.00055)\n",
            "epoch: 603, train_loss: 0.427762, val loss: 0.522760,  train_metric: -0.841 test_metric: -0.800 lr: 0.00055)\n",
            "epoch: 604, train_loss: 0.421658, val loss: 0.523355,  train_metric: -0.836 test_metric: -0.799 lr: 0.00055)\n",
            "epoch: 605, train_loss: 0.410011, val loss: 0.511414,  train_metric: -0.850 test_metric: -0.808 lr: 0.00055)\n",
            "epoch: 606, train_loss: 0.402967, val loss: 0.522297,  train_metric: -0.854 test_metric: -0.802 lr: 0.00054)\n",
            "epoch: 607, train_loss: 0.411140, val loss: 0.518074,  train_metric: -0.847 test_metric: -0.810 lr: 0.00054)\n",
            "epoch: 608, train_loss: 0.417483, val loss: 0.547903,  train_metric: -0.844 test_metric: -0.778 lr: 0.00054)\n",
            "epoch: 609, train_loss: 0.415903, val loss: 0.530277,  train_metric: -0.828 test_metric: -0.809 lr: 0.00054)\n",
            "epoch: 610, train_loss: 0.410905, val loss: 0.551270,  train_metric: -0.845 test_metric: -0.774 lr: 0.00054)\n",
            "epoch: 611, train_loss: 0.408623, val loss: 0.522834,  train_metric: -0.839 test_metric: -0.798 lr: 0.00054)\n",
            "epoch: 612, train_loss: 0.407092, val loss: 0.530397,  train_metric: -0.847 test_metric: -0.803 lr: 0.00054)\n",
            "epoch: 613, train_loss: 0.410459, val loss: 0.514943,  train_metric: -0.850 test_metric: -0.807 lr: 0.00054)\n",
            "epoch: 614, train_loss: 0.403514, val loss: 0.527389,  train_metric: -0.851 test_metric: -0.803 lr: 0.00054)\n",
            "epoch: 615, train_loss: 0.401522, val loss: 0.538950,  train_metric: -0.852 test_metric: -0.793 lr: 0.00054)\n",
            "epoch: 616, train_loss: 0.409242, val loss: 0.524340,  train_metric: -0.845 test_metric: -0.802 lr: 0.00054)\n",
            "epoch: 617, train_loss: 0.408003, val loss: 0.523588,  train_metric: -0.850 test_metric: -0.810 lr: 0.00054)\n",
            "epoch: 618, train_loss: 0.403499, val loss: 0.506840,  train_metric: -0.852 test_metric: -0.828 lr: 0.00054)\n",
            "epoch: 619, train_loss: 0.398817, val loss: 0.516995,  train_metric: -0.853 test_metric: -0.810 lr: 0.00054)\n",
            "epoch: 620, train_loss: 0.403124, val loss: 0.522613,  train_metric: -0.848 test_metric: -0.809 lr: 0.00054)\n",
            "epoch: 621, train_loss: 0.410720, val loss: 0.535021,  train_metric: -0.842 test_metric: -0.803 lr: 0.00054)\n",
            "epoch: 622, train_loss: 0.422042, val loss: 0.558428,  train_metric: -0.839 test_metric: -0.787 lr: 0.00054)\n",
            "epoch: 623, train_loss: 0.416927, val loss: 0.533787,  train_metric: -0.839 test_metric: -0.805 lr: 0.00054)\n",
            "epoch: 624, train_loss: 0.414054, val loss: 0.526482,  train_metric: -0.845 test_metric: -0.793 lr: 0.00054)\n",
            "epoch: 625, train_loss: 0.404980, val loss: 0.556399,  train_metric: -0.849 test_metric: -0.762 lr: 0.00053)\n",
            "epoch: 626, train_loss: 0.420273, val loss: 0.518212,  train_metric: -0.838 test_metric: -0.799 lr: 0.00053)\n",
            "epoch: 627, train_loss: 0.403468, val loss: 0.520014,  train_metric: -0.858 test_metric: -0.804 lr: 0.00053)\n",
            "epoch: 628, train_loss: 0.407867, val loss: 0.520264,  train_metric: -0.843 test_metric: -0.797 lr: 0.00053)\n",
            "epoch: 629, train_loss: 0.396552, val loss: 0.513817,  train_metric: -0.851 test_metric: -0.808 lr: 0.00053)\n",
            "epoch: 630, train_loss: 0.400480, val loss: 0.528493,  train_metric: -0.848 test_metric: -0.791 lr: 0.00053)\n",
            "epoch: 631, train_loss: 0.397426, val loss: 0.511200,  train_metric: -0.852 test_metric: -0.808 lr: 0.00053)\n",
            "epoch: 632, train_loss: 0.398039, val loss: 0.523227,  train_metric: -0.846 test_metric: -0.802 lr: 0.00053)\n",
            "epoch: 633, train_loss: 0.396610, val loss: 0.533772,  train_metric: -0.854 test_metric: -0.796 lr: 0.00053)\n",
            "epoch: 634, train_loss: 0.406245, val loss: 0.533263,  train_metric: -0.842 test_metric: -0.781 lr: 0.00053)\n",
            "epoch: 635, train_loss: 0.412881, val loss: 0.547006,  train_metric: -0.843 test_metric: -0.770 lr: 0.00053)\n",
            "epoch: 636, train_loss: 0.404652, val loss: 0.521531,  train_metric: -0.840 test_metric: -0.791 lr: 0.00053)\n",
            "epoch: 637, train_loss: 0.402280, val loss: 0.506201,  train_metric: -0.839 test_metric: -0.815 lr: 0.00053)\n",
            "epoch: 638, train_loss: 0.394583, val loss: 0.500470,  train_metric: -0.854 test_metric: -0.824 lr: 0.00053)\n",
            "epoch: 639, train_loss: 0.391628, val loss: 0.507506,  train_metric: -0.859 test_metric: -0.804 lr: 0.00053)\n",
            "epoch: 640, train_loss: 0.398877, val loss: 0.500586,  train_metric: -0.856 test_metric: -0.808 lr: 0.00053)\n",
            "epoch: 641, train_loss: 0.406039, val loss: 0.536069,  train_metric: -0.847 test_metric: -0.765 lr: 0.00053)\n",
            "epoch: 642, train_loss: 0.402703, val loss: 0.498919,  train_metric: -0.840 test_metric: -0.815 lr: 0.00053)\n",
            "epoch: 643, train_loss: 0.391403, val loss: 0.524336,  train_metric: -0.850 test_metric: -0.796 lr: 0.00053)\n",
            "epoch: 644, train_loss: 0.397613, val loss: 0.511476,  train_metric: -0.853 test_metric: -0.817 lr: 0.00052)\n",
            "epoch: 645, train_loss: 0.402946, val loss: 0.521487,  train_metric: -0.847 test_metric: -0.796 lr: 0.00052)\n",
            "epoch: 646, train_loss: 0.410813, val loss: 0.502815,  train_metric: -0.834 test_metric: -0.820 lr: 0.00052)\n",
            "epoch: 647, train_loss: 0.390886, val loss: 0.502874,  train_metric: -0.857 test_metric: -0.823 lr: 0.00052)\n",
            "epoch: 648, train_loss: 0.393937, val loss: 0.500254,  train_metric: -0.858 test_metric: -0.809 lr: 0.00052)\n",
            "epoch: 649, train_loss: 0.391575, val loss: 0.501278,  train_metric: -0.856 test_metric: -0.812 lr: 0.00052)\n",
            "epoch: 650, train_loss: 0.390192, val loss: 0.496034,  train_metric: -0.854 test_metric: -0.822 lr: 0.00052)\n",
            "epoch: 651, train_loss: 0.392116, val loss: 0.500505,  train_metric: -0.855 test_metric: -0.810 lr: 0.00052)\n",
            "epoch: 652, train_loss: 0.387865, val loss: 0.506616,  train_metric: -0.860 test_metric: -0.813 lr: 0.00052)\n",
            "epoch: 653, train_loss: 0.390485, val loss: 0.493157,  train_metric: -0.852 test_metric: -0.820 lr: 0.00052)\n",
            "epoch: 654, train_loss: 0.390396, val loss: 0.502427,  train_metric: -0.851 test_metric: -0.815 lr: 0.00052)\n",
            "epoch: 655, train_loss: 0.397252, val loss: 0.505708,  train_metric: -0.842 test_metric: -0.800 lr: 0.00052)\n",
            "epoch: 656, train_loss: 0.387821, val loss: 0.506485,  train_metric: -0.852 test_metric: -0.820 lr: 0.00052)\n",
            "epoch: 657, train_loss: 0.388964, val loss: 0.507351,  train_metric: -0.854 test_metric: -0.804 lr: 0.00052)\n",
            "epoch: 658, train_loss: 0.391307, val loss: 0.492181,  train_metric: -0.849 test_metric: -0.813 lr: 0.00052)\n",
            "epoch: 659, train_loss: 0.387413, val loss: 0.545710,  train_metric: -0.851 test_metric: -0.779 lr: 0.00052)\n",
            "epoch: 660, train_loss: 0.395703, val loss: 0.501440,  train_metric: -0.851 test_metric: -0.808 lr: 0.00052)\n",
            "epoch: 661, train_loss: 0.382796, val loss: 0.487540,  train_metric: -0.858 test_metric: -0.814 lr: 0.00052)\n",
            "epoch: 662, train_loss: 0.379647, val loss: 0.493859,  train_metric: -0.863 test_metric: -0.818 lr: 0.00052)\n",
            "epoch: 663, train_loss: 0.381310, val loss: 0.502744,  train_metric: -0.861 test_metric: -0.814 lr: 0.00051)\n",
            "epoch: 664, train_loss: 0.391770, val loss: 0.498841,  train_metric: -0.853 test_metric: -0.819 lr: 0.00051)\n",
            "epoch: 665, train_loss: 0.379048, val loss: 0.514208,  train_metric: -0.863 test_metric: -0.804 lr: 0.00051)\n",
            "epoch: 666, train_loss: 0.387518, val loss: 0.490087,  train_metric: -0.863 test_metric: -0.818 lr: 0.00051)\n",
            "epoch: 667, train_loss: 0.380925, val loss: 0.491994,  train_metric: -0.846 test_metric: -0.824 lr: 0.00051)\n",
            "epoch: 668, train_loss: 0.377603, val loss: 0.488714,  train_metric: -0.863 test_metric: -0.814 lr: 0.00051)\n",
            "epoch: 669, train_loss: 0.376849, val loss: 0.483379,  train_metric: -0.861 test_metric: -0.815 lr: 0.00051)\n",
            "epoch: 670, train_loss: 0.377190, val loss: 0.501714,  train_metric: -0.864 test_metric: -0.814 lr: 0.00051)\n",
            "epoch: 671, train_loss: 0.386504, val loss: 0.481811,  train_metric: -0.852 test_metric: -0.820 lr: 0.00051)\n",
            "epoch: 672, train_loss: 0.376044, val loss: 0.489637,  train_metric: -0.863 test_metric: -0.813 lr: 0.00051)\n",
            "epoch: 673, train_loss: 0.376918, val loss: 0.494657,  train_metric: -0.859 test_metric: -0.824 lr: 0.00051)\n",
            "epoch: 674, train_loss: 0.383916, val loss: 0.480589,  train_metric: -0.850 test_metric: -0.823 lr: 0.00051)\n",
            "epoch: 675, train_loss: 0.374804, val loss: 0.488233,  train_metric: -0.859 test_metric: -0.812 lr: 0.00051)\n",
            "epoch: 676, train_loss: 0.374948, val loss: 0.485110,  train_metric: -0.862 test_metric: -0.814 lr: 0.00051)\n",
            "epoch: 677, train_loss: 0.373018, val loss: 0.489195,  train_metric: -0.858 test_metric: -0.810 lr: 0.00051)\n",
            "epoch: 678, train_loss: 0.374717, val loss: 0.493554,  train_metric: -0.865 test_metric: -0.805 lr: 0.00051)\n",
            "epoch: 679, train_loss: 0.375483, val loss: 0.505755,  train_metric: -0.859 test_metric: -0.797 lr: 0.00051)\n",
            "epoch: 680, train_loss: 0.379971, val loss: 0.497307,  train_metric: -0.856 test_metric: -0.813 lr: 0.00051)\n",
            "epoch: 681, train_loss: 0.384622, val loss: 0.522089,  train_metric: -0.855 test_metric: -0.797 lr: 0.00051)\n",
            "epoch: 682, train_loss: 0.383555, val loss: 0.487409,  train_metric: -0.852 test_metric: -0.818 lr: 0.00050)\n",
            "epoch: 683, train_loss: 0.373623, val loss: 0.495332,  train_metric: -0.865 test_metric: -0.802 lr: 0.00050)\n",
            "epoch: 684, train_loss: 0.372185, val loss: 0.493044,  train_metric: -0.868 test_metric: -0.828 lr: 0.00050)\n",
            "epoch: 685, train_loss: 0.372137, val loss: 0.474414,  train_metric: -0.865 test_metric: -0.828 lr: 0.00050)\n",
            "epoch: 686, train_loss: 0.368090, val loss: 0.496908,  train_metric: -0.861 test_metric: -0.808 lr: 0.00050)\n",
            "epoch: 687, train_loss: 0.381751, val loss: 0.505648,  train_metric: -0.854 test_metric: -0.793 lr: 0.00050)\n",
            "epoch: 688, train_loss: 0.371263, val loss: 0.500812,  train_metric: -0.862 test_metric: -0.810 lr: 0.00050)\n",
            "epoch: 689, train_loss: 0.376641, val loss: 0.502148,  train_metric: -0.858 test_metric: -0.792 lr: 0.00050)\n",
            "epoch: 690, train_loss: 0.376667, val loss: 0.495400,  train_metric: -0.854 test_metric: -0.813 lr: 0.00050)\n",
            "epoch: 691, train_loss: 0.380233, val loss: 0.497882,  train_metric: -0.851 test_metric: -0.822 lr: 0.00050)\n",
            "epoch: 692, train_loss: 0.391534, val loss: 0.553869,  train_metric: -0.851 test_metric: -0.777 lr: 0.00050)\n",
            "epoch: 693, train_loss: 0.401142, val loss: 0.479153,  train_metric: -0.832 test_metric: -0.818 lr: 0.00050)\n",
            "epoch: 694, train_loss: 0.381867, val loss: 0.493830,  train_metric: -0.856 test_metric: -0.794 lr: 0.00050)\n",
            "epoch: 695, train_loss: 0.376641, val loss: 0.476929,  train_metric: -0.857 test_metric: -0.822 lr: 0.00050)\n",
            "epoch: 696, train_loss: 0.376255, val loss: 0.504309,  train_metric: -0.853 test_metric: -0.805 lr: 0.00050)\n",
            "epoch: 697, train_loss: 0.385316, val loss: 0.476606,  train_metric: -0.842 test_metric: -0.825 lr: 0.00050)\n",
            "epoch: 698, train_loss: 0.370902, val loss: 0.482220,  train_metric: -0.859 test_metric: -0.809 lr: 0.00050)\n",
            "epoch: 699, train_loss: 0.368112, val loss: 0.483023,  train_metric: -0.862 test_metric: -0.818 lr: 0.00050)\n",
            "epoch: 700, train_loss: 0.368164, val loss: 0.483131,  train_metric: -0.862 test_metric: -0.812 lr: 0.00050)\n",
            "epoch: 701, train_loss: 0.368247, val loss: 0.470153,  train_metric: -0.857 test_metric: -0.820 lr: 0.00050)\n",
            "epoch: 702, train_loss: 0.362708, val loss: 0.483171,  train_metric: -0.869 test_metric: -0.812 lr: 0.00049)\n",
            "epoch: 703, train_loss: 0.363296, val loss: 0.493410,  train_metric: -0.861 test_metric: -0.809 lr: 0.00049)\n",
            "epoch: 704, train_loss: 0.365924, val loss: 0.472922,  train_metric: -0.863 test_metric: -0.828 lr: 0.00049)\n",
            "epoch: 705, train_loss: 0.361016, val loss: 0.475673,  train_metric: -0.868 test_metric: -0.814 lr: 0.00049)\n",
            "epoch: 706, train_loss: 0.366319, val loss: 0.482527,  train_metric: -0.859 test_metric: -0.818 lr: 0.00049)\n",
            "epoch: 707, train_loss: 0.365623, val loss: 0.466636,  train_metric: -0.868 test_metric: -0.824 lr: 0.00049)\n",
            "epoch: 708, train_loss: 0.366952, val loss: 0.483516,  train_metric: -0.860 test_metric: -0.809 lr: 0.00049)\n",
            "epoch: 709, train_loss: 0.368199, val loss: 0.484846,  train_metric: -0.857 test_metric: -0.800 lr: 0.00049)\n",
            "epoch: 710, train_loss: 0.362148, val loss: 0.477674,  train_metric: -0.873 test_metric: -0.812 lr: 0.00049)\n",
            "epoch: 711, train_loss: 0.376510, val loss: 0.471116,  train_metric: -0.853 test_metric: -0.817 lr: 0.00049)\n",
            "epoch: 712, train_loss: 0.374060, val loss: 0.499085,  train_metric: -0.854 test_metric: -0.814 lr: 0.00049)\n",
            "epoch: 713, train_loss: 0.380118, val loss: 0.491130,  train_metric: -0.855 test_metric: -0.808 lr: 0.00049)\n",
            "epoch: 714, train_loss: 0.372210, val loss: 0.496627,  train_metric: -0.860 test_metric: -0.808 lr: 0.00049)\n",
            "epoch: 715, train_loss: 0.362064, val loss: 0.474018,  train_metric: -0.859 test_metric: -0.823 lr: 0.00049)\n",
            "epoch: 716, train_loss: 0.354480, val loss: 0.474203,  train_metric: -0.869 test_metric: -0.827 lr: 0.00049)\n",
            "epoch: 717, train_loss: 0.355408, val loss: 0.463315,  train_metric: -0.870 test_metric: -0.824 lr: 0.00049)\n",
            "epoch: 718, train_loss: 0.355738, val loss: 0.482013,  train_metric: -0.862 test_metric: -0.825 lr: 0.00049)\n",
            "epoch: 719, train_loss: 0.358311, val loss: 0.477314,  train_metric: -0.866 test_metric: -0.813 lr: 0.00049)\n",
            "epoch: 720, train_loss: 0.365885, val loss: 0.476601,  train_metric: -0.860 test_metric: -0.825 lr: 0.00049)\n",
            "epoch: 721, train_loss: 0.361148, val loss: 0.471477,  train_metric: -0.872 test_metric: -0.820 lr: 0.00049)\n",
            "epoch: 722, train_loss: 0.352635, val loss: 0.480317,  train_metric: -0.864 test_metric: -0.822 lr: 0.00049)\n",
            "epoch: 723, train_loss: 0.357558, val loss: 0.468735,  train_metric: -0.866 test_metric: -0.828 lr: 0.00048)\n",
            "epoch: 724, train_loss: 0.362687, val loss: 0.487555,  train_metric: -0.870 test_metric: -0.819 lr: 0.00048)\n",
            "epoch: 725, train_loss: 0.361998, val loss: 0.501145,  train_metric: -0.854 test_metric: -0.799 lr: 0.00048)\n",
            "epoch: 726, train_loss: 0.359903, val loss: 0.473207,  train_metric: -0.863 test_metric: -0.824 lr: 0.00048)\n",
            "epoch: 727, train_loss: 0.356132, val loss: 0.480566,  train_metric: -0.867 test_metric: -0.824 lr: 0.00048)\n",
            "epoch: 728, train_loss: 0.354242, val loss: 0.475101,  train_metric: -0.870 test_metric: -0.808 lr: 0.00048)\n",
            "epoch: 729, train_loss: 0.350420, val loss: 0.465591,  train_metric: -0.865 test_metric: -0.817 lr: 0.00048)\n",
            "epoch: 730, train_loss: 0.352485, val loss: 0.525854,  train_metric: -0.867 test_metric: -0.781 lr: 0.00048)\n",
            "epoch: 731, train_loss: 0.361128, val loss: 0.480396,  train_metric: -0.865 test_metric: -0.803 lr: 0.00048)\n",
            "epoch: 732, train_loss: 0.362003, val loss: 0.480239,  train_metric: -0.856 test_metric: -0.818 lr: 0.00048)\n",
            "epoch: 733, train_loss: 0.355694, val loss: 0.483568,  train_metric: -0.863 test_metric: -0.807 lr: 0.00048)\n",
            "epoch: 734, train_loss: 0.352364, val loss: 0.458267,  train_metric: -0.867 test_metric: -0.818 lr: 0.00048)\n",
            "epoch: 735, train_loss: 0.352314, val loss: 0.475702,  train_metric: -0.867 test_metric: -0.814 lr: 0.00048)\n",
            "epoch: 736, train_loss: 0.358673, val loss: 0.478280,  train_metric: -0.866 test_metric: -0.808 lr: 0.00048)\n",
            "epoch: 737, train_loss: 0.356859, val loss: 0.513979,  train_metric: -0.864 test_metric: -0.789 lr: 0.00048)\n",
            "epoch: 738, train_loss: 0.354058, val loss: 0.465227,  train_metric: -0.860 test_metric: -0.817 lr: 0.00048)\n",
            "epoch: 739, train_loss: 0.351679, val loss: 0.482549,  train_metric: -0.866 test_metric: -0.814 lr: 0.00048)\n",
            "epoch: 740, train_loss: 0.358180, val loss: 0.482547,  train_metric: -0.867 test_metric: -0.808 lr: 0.00048)\n",
            "epoch: 741, train_loss: 0.353202, val loss: 0.472281,  train_metric: -0.867 test_metric: -0.817 lr: 0.00048)\n",
            "epoch: 742, train_loss: 0.352994, val loss: 0.483079,  train_metric: -0.863 test_metric: -0.819 lr: 0.00048)\n",
            "epoch: 743, train_loss: 0.350747, val loss: 0.462351,  train_metric: -0.868 test_metric: -0.828 lr: 0.00048)\n",
            "epoch: 744, train_loss: 0.350599, val loss: 0.477156,  train_metric: -0.868 test_metric: -0.818 lr: 0.00047)\n",
            "epoch: 745, train_loss: 0.356598, val loss: 0.507835,  train_metric: -0.868 test_metric: -0.793 lr: 0.00047)\n",
            "epoch: 746, train_loss: 0.358459, val loss: 0.478671,  train_metric: -0.867 test_metric: -0.819 lr: 0.00047)\n",
            "epoch: 747, train_loss: 0.352878, val loss: 0.473665,  train_metric: -0.870 test_metric: -0.829 lr: 0.00047)\n",
            "epoch: 748, train_loss: 0.353815, val loss: 0.462344,  train_metric: -0.861 test_metric: -0.825 lr: 0.00047)\n",
            "epoch: 749, train_loss: 0.357697, val loss: 0.498961,  train_metric: -0.861 test_metric: -0.808 lr: 0.00047)\n",
            "epoch: 750, train_loss: 0.352175, val loss: 0.457079,  train_metric: -0.873 test_metric: -0.819 lr: 0.00047)\n",
            "epoch: 751, train_loss: 0.347190, val loss: 0.468077,  train_metric: -0.871 test_metric: -0.814 lr: 0.00047)\n",
            "epoch: 752, train_loss: 0.347183, val loss: 0.451160,  train_metric: -0.861 test_metric: -0.828 lr: 0.00047)\n",
            "epoch: 753, train_loss: 0.342353, val loss: 0.458670,  train_metric: -0.868 test_metric: -0.815 lr: 0.00047)\n",
            "epoch: 754, train_loss: 0.346148, val loss: 0.459532,  train_metric: -0.868 test_metric: -0.825 lr: 0.00047)\n",
            "epoch: 755, train_loss: 0.343267, val loss: 0.462635,  train_metric: -0.874 test_metric: -0.831 lr: 0.00047)\n",
            "epoch: 756, train_loss: 0.348803, val loss: 0.453539,  train_metric: -0.868 test_metric: -0.828 lr: 0.00047)\n",
            "epoch: 757, train_loss: 0.342118, val loss: 0.460762,  train_metric: -0.868 test_metric: -0.825 lr: 0.00047)\n",
            "epoch: 758, train_loss: 0.341662, val loss: 0.449129,  train_metric: -0.873 test_metric: -0.841 lr: 0.00047)\n",
            "epoch: 759, train_loss: 0.342039, val loss: 0.455474,  train_metric: -0.871 test_metric: -0.819 lr: 0.00047)\n",
            "epoch: 760, train_loss: 0.347438, val loss: 0.491806,  train_metric: -0.864 test_metric: -0.800 lr: 0.00047)\n",
            "epoch: 761, train_loss: 0.351756, val loss: 0.468899,  train_metric: -0.864 test_metric: -0.828 lr: 0.00047)\n",
            "epoch: 762, train_loss: 0.343046, val loss: 0.448828,  train_metric: -0.865 test_metric: -0.824 lr: 0.00047)\n",
            "epoch: 763, train_loss: 0.337440, val loss: 0.448383,  train_metric: -0.870 test_metric: -0.831 lr: 0.00047)\n",
            "epoch: 764, train_loss: 0.340368, val loss: 0.459049,  train_metric: -0.877 test_metric: -0.827 lr: 0.00047)\n",
            "epoch: 765, train_loss: 0.350990, val loss: 0.473125,  train_metric: -0.864 test_metric: -0.814 lr: 0.00046)\n",
            "epoch: 766, train_loss: 0.347901, val loss: 0.466849,  train_metric: -0.866 test_metric: -0.819 lr: 0.00046)\n",
            "epoch: 767, train_loss: 0.344316, val loss: 0.452036,  train_metric: -0.869 test_metric: -0.823 lr: 0.00046)\n",
            "epoch: 768, train_loss: 0.338631, val loss: 0.461762,  train_metric: -0.873 test_metric: -0.822 lr: 0.00046)\n",
            "epoch: 769, train_loss: 0.344614, val loss: 0.472961,  train_metric: -0.866 test_metric: -0.824 lr: 0.00046)\n",
            "epoch: 770, train_loss: 0.358958, val loss: 0.470007,  train_metric: -0.854 test_metric: -0.820 lr: 0.00046)\n",
            "epoch: 771, train_loss: 0.349548, val loss: 0.450214,  train_metric: -0.871 test_metric: -0.833 lr: 0.00046)\n",
            "epoch: 772, train_loss: 0.339213, val loss: 0.462794,  train_metric: -0.873 test_metric: -0.827 lr: 0.00046)\n",
            "epoch: 773, train_loss: 0.333353, val loss: 0.448570,  train_metric: -0.872 test_metric: -0.834 lr: 0.00046)\n",
            "epoch: 774, train_loss: 0.342125, val loss: 0.466837,  train_metric: -0.873 test_metric: -0.814 lr: 0.00046)\n",
            "epoch: 775, train_loss: 0.343806, val loss: 0.449158,  train_metric: -0.863 test_metric: -0.835 lr: 0.00046)\n",
            "epoch: 776, train_loss: 0.340412, val loss: 0.450382,  train_metric: -0.871 test_metric: -0.827 lr: 0.00046)\n",
            "epoch: 777, train_loss: 0.339338, val loss: 0.451752,  train_metric: -0.872 test_metric: -0.833 lr: 0.00046)\n",
            "epoch: 778, train_loss: 0.341435, val loss: 0.478030,  train_metric: -0.881 test_metric: -0.807 lr: 0.00046)\n",
            "epoch: 779, train_loss: 0.341638, val loss: 0.445372,  train_metric: -0.869 test_metric: -0.824 lr: 0.00046)\n",
            "epoch: 780, train_loss: 0.338976, val loss: 0.454439,  train_metric: -0.869 test_metric: -0.824 lr: 0.00046)\n",
            "epoch: 781, train_loss: 0.337714, val loss: 0.462536,  train_metric: -0.878 test_metric: -0.819 lr: 0.00046)\n",
            "epoch: 782, train_loss: 0.334470, val loss: 0.446607,  train_metric: -0.879 test_metric: -0.824 lr: 0.00046)\n",
            "epoch: 783, train_loss: 0.337817, val loss: 0.460430,  train_metric: -0.871 test_metric: -0.815 lr: 0.00046)\n",
            "epoch: 784, train_loss: 0.338517, val loss: 0.477381,  train_metric: -0.868 test_metric: -0.812 lr: 0.00046)\n",
            "epoch: 785, train_loss: 0.346896, val loss: 0.452534,  train_metric: -0.860 test_metric: -0.830 lr: 0.00046)\n",
            "epoch: 786, train_loss: 0.339158, val loss: 0.458760,  train_metric: -0.872 test_metric: -0.823 lr: 0.00046)\n",
            "epoch: 787, train_loss: 0.335132, val loss: 0.463936,  train_metric: -0.870 test_metric: -0.828 lr: 0.00045)\n",
            "epoch: 788, train_loss: 0.333945, val loss: 0.455550,  train_metric: -0.869 test_metric: -0.827 lr: 0.00045)\n",
            "epoch: 789, train_loss: 0.333508, val loss: 0.445024,  train_metric: -0.870 test_metric: -0.831 lr: 0.00045)\n",
            "epoch: 790, train_loss: 0.335426, val loss: 0.455985,  train_metric: -0.875 test_metric: -0.829 lr: 0.00045)\n",
            "epoch: 791, train_loss: 0.333335, val loss: 0.457729,  train_metric: -0.877 test_metric: -0.817 lr: 0.00045)\n",
            "epoch: 792, train_loss: 0.340353, val loss: 0.438122,  train_metric: -0.863 test_metric: -0.830 lr: 0.00045)\n",
            "epoch: 793, train_loss: 0.332088, val loss: 0.458227,  train_metric: -0.875 test_metric: -0.830 lr: 0.00045)\n",
            "epoch: 794, train_loss: 0.340368, val loss: 0.470238,  train_metric: -0.864 test_metric: -0.809 lr: 0.00045)\n",
            "epoch: 795, train_loss: 0.338441, val loss: 0.455691,  train_metric: -0.873 test_metric: -0.836 lr: 0.00045)\n",
            "epoch: 796, train_loss: 0.333255, val loss: 0.454695,  train_metric: -0.871 test_metric: -0.820 lr: 0.00045)\n",
            "epoch: 797, train_loss: 0.329032, val loss: 0.453452,  train_metric: -0.872 test_metric: -0.819 lr: 0.00045)\n",
            "epoch: 798, train_loss: 0.330230, val loss: 0.443770,  train_metric: -0.872 test_metric: -0.822 lr: 0.00045)\n",
            "epoch: 799, train_loss: 0.328267, val loss: 0.446233,  train_metric: -0.873 test_metric: -0.835 lr: 0.00045)\n",
            "epoch: 800, train_loss: 0.328255, val loss: 0.443771,  train_metric: -0.879 test_metric: -0.833 lr: 0.00045)\n",
            "epoch: 801, train_loss: 0.329548, val loss: 0.452715,  train_metric: -0.878 test_metric: -0.817 lr: 0.00045)\n",
            "epoch: 802, train_loss: 0.332369, val loss: 0.466560,  train_metric: -0.869 test_metric: -0.822 lr: 0.00045)\n",
            "epoch: 803, train_loss: 0.336432, val loss: 0.443021,  train_metric: -0.875 test_metric: -0.835 lr: 0.00045)\n",
            "epoch: 804, train_loss: 0.327862, val loss: 0.442526,  train_metric: -0.873 test_metric: -0.829 lr: 0.00045)\n",
            "epoch: 805, train_loss: 0.326384, val loss: 0.456356,  train_metric: -0.872 test_metric: -0.815 lr: 0.00045)\n",
            "epoch: 806, train_loss: 0.332644, val loss: 0.452114,  train_metric: -0.871 test_metric: -0.828 lr: 0.00045)\n",
            "epoch: 807, train_loss: 0.325120, val loss: 0.451199,  train_metric: -0.880 test_metric: -0.835 lr: 0.00045)\n",
            "epoch: 808, train_loss: 0.330432, val loss: 0.440174,  train_metric: -0.865 test_metric: -0.841 lr: 0.00045)\n",
            "epoch: 809, train_loss: 0.327926, val loss: 0.440507,  train_metric: -0.876 test_metric: -0.833 lr: 0.00044)\n",
            "epoch: 810, train_loss: 0.324206, val loss: 0.442296,  train_metric: -0.877 test_metric: -0.827 lr: 0.00044)\n",
            "epoch: 811, train_loss: 0.328394, val loss: 0.459754,  train_metric: -0.873 test_metric: -0.822 lr: 0.00044)\n",
            "epoch: 812, train_loss: 0.329550, val loss: 0.462254,  train_metric: -0.881 test_metric: -0.823 lr: 0.00044)\n",
            "epoch: 813, train_loss: 0.325077, val loss: 0.436405,  train_metric: -0.873 test_metric: -0.825 lr: 0.00044)\n",
            "epoch: 814, train_loss: 0.329163, val loss: 0.454597,  train_metric: -0.868 test_metric: -0.820 lr: 0.00044)\n",
            "epoch: 815, train_loss: 0.326252, val loss: 0.450041,  train_metric: -0.878 test_metric: -0.828 lr: 0.00044)\n",
            "epoch: 816, train_loss: 0.327292, val loss: 0.444579,  train_metric: -0.873 test_metric: -0.829 lr: 0.00044)\n",
            "epoch: 817, train_loss: 0.324573, val loss: 0.448272,  train_metric: -0.876 test_metric: -0.819 lr: 0.00044)\n",
            "epoch: 818, train_loss: 0.329553, val loss: 0.439624,  train_metric: -0.871 test_metric: -0.830 lr: 0.00044)\n",
            "epoch: 819, train_loss: 0.327515, val loss: 0.472448,  train_metric: -0.875 test_metric: -0.807 lr: 0.00044)\n",
            "epoch: 820, train_loss: 0.333560, val loss: 0.436620,  train_metric: -0.868 test_metric: -0.825 lr: 0.00044)\n",
            "epoch: 821, train_loss: 0.327658, val loss: 0.434660,  train_metric: -0.873 test_metric: -0.830 lr: 0.00044)\n",
            "epoch: 822, train_loss: 0.322423, val loss: 0.460592,  train_metric: -0.877 test_metric: -0.822 lr: 0.00044)\n",
            "epoch: 823, train_loss: 0.326008, val loss: 0.435263,  train_metric: -0.881 test_metric: -0.831 lr: 0.00044)\n",
            "epoch: 824, train_loss: 0.320956, val loss: 0.452439,  train_metric: -0.878 test_metric: -0.820 lr: 0.00044)\n",
            "epoch: 825, train_loss: 0.332819, val loss: 0.456553,  train_metric: -0.869 test_metric: -0.818 lr: 0.00044)\n",
            "epoch: 826, train_loss: 0.327870, val loss: 0.451111,  train_metric: -0.873 test_metric: -0.827 lr: 0.00044)\n",
            "epoch: 827, train_loss: 0.326634, val loss: 0.459960,  train_metric: -0.877 test_metric: -0.825 lr: 0.00044)\n",
            "epoch: 828, train_loss: 0.327667, val loss: 0.456609,  train_metric: -0.876 test_metric: -0.817 lr: 0.00044)\n",
            "epoch: 829, train_loss: 0.320346, val loss: 0.435963,  train_metric: -0.878 test_metric: -0.830 lr: 0.00044)\n",
            "epoch: 830, train_loss: 0.322139, val loss: 0.436059,  train_metric: -0.876 test_metric: -0.839 lr: 0.00044)\n",
            "epoch: 831, train_loss: 0.321476, val loss: 0.439815,  train_metric: -0.884 test_metric: -0.825 lr: 0.00043)\n",
            "epoch: 832, train_loss: 0.322490, val loss: 0.484553,  train_metric: -0.877 test_metric: -0.797 lr: 0.00043)\n",
            "epoch: 833, train_loss: 0.321367, val loss: 0.450748,  train_metric: -0.882 test_metric: -0.827 lr: 0.00043)\n",
            "epoch: 834, train_loss: 0.319702, val loss: 0.438229,  train_metric: -0.880 test_metric: -0.838 lr: 0.00043)\n",
            "epoch: 835, train_loss: 0.324684, val loss: 0.445546,  train_metric: -0.872 test_metric: -0.823 lr: 0.00043)\n",
            "epoch: 836, train_loss: 0.334441, val loss: 0.461517,  train_metric: -0.864 test_metric: -0.834 lr: 0.00043)\n",
            "epoch: 837, train_loss: 0.325342, val loss: 0.436201,  train_metric: -0.880 test_metric: -0.829 lr: 0.00043)\n",
            "epoch: 838, train_loss: 0.314756, val loss: 0.450534,  train_metric: -0.879 test_metric: -0.824 lr: 0.00043)\n",
            "epoch: 839, train_loss: 0.314804, val loss: 0.429046,  train_metric: -0.876 test_metric: -0.831 lr: 0.00043)\n",
            "epoch: 840, train_loss: 0.317184, val loss: 0.432761,  train_metric: -0.878 test_metric: -0.836 lr: 0.00043)\n",
            "epoch: 841, train_loss: 0.313989, val loss: 0.437092,  train_metric: -0.879 test_metric: -0.824 lr: 0.00043)\n",
            "epoch: 842, train_loss: 0.312877, val loss: 0.431271,  train_metric: -0.886 test_metric: -0.835 lr: 0.00043)\n",
            "epoch: 843, train_loss: 0.316752, val loss: 0.438401,  train_metric: -0.881 test_metric: -0.841 lr: 0.00043)\n",
            "epoch: 844, train_loss: 0.317386, val loss: 0.458890,  train_metric: -0.883 test_metric: -0.814 lr: 0.00043)\n",
            "epoch: 845, train_loss: 0.315523, val loss: 0.434932,  train_metric: -0.879 test_metric: -0.825 lr: 0.00043)\n",
            "epoch: 846, train_loss: 0.317914, val loss: 0.450513,  train_metric: -0.881 test_metric: -0.827 lr: 0.00043)\n",
            "epoch: 847, train_loss: 0.318916, val loss: 0.428352,  train_metric: -0.880 test_metric: -0.838 lr: 0.00043)\n",
            "epoch: 848, train_loss: 0.316949, val loss: 0.440192,  train_metric: -0.877 test_metric: -0.829 lr: 0.00043)\n",
            "epoch: 849, train_loss: 0.320545, val loss: 0.441006,  train_metric: -0.879 test_metric: -0.833 lr: 0.00043)\n",
            "epoch: 850, train_loss: 0.316819, val loss: 0.441626,  train_metric: -0.877 test_metric: -0.825 lr: 0.00043)\n",
            "epoch: 851, train_loss: 0.310575, val loss: 0.438917,  train_metric: -0.882 test_metric: -0.829 lr: 0.00043)\n",
            "epoch: 852, train_loss: 0.313896, val loss: 0.438573,  train_metric: -0.885 test_metric: -0.829 lr: 0.00043)\n",
            "epoch: 853, train_loss: 0.312907, val loss: 0.446015,  train_metric: -0.878 test_metric: -0.823 lr: 0.00043)\n",
            "epoch: 854, train_loss: 0.317482, val loss: 0.450644,  train_metric: -0.880 test_metric: -0.827 lr: 0.00043)\n",
            "epoch: 855, train_loss: 0.312948, val loss: 0.430852,  train_metric: -0.881 test_metric: -0.833 lr: 0.00042)\n",
            "epoch: 856, train_loss: 0.311329, val loss: 0.450158,  train_metric: -0.885 test_metric: -0.825 lr: 0.00042)\n",
            "epoch: 857, train_loss: 0.309475, val loss: 0.433944,  train_metric: -0.884 test_metric: -0.824 lr: 0.00042)\n",
            "epoch: 858, train_loss: 0.314151, val loss: 0.446398,  train_metric: -0.883 test_metric: -0.814 lr: 0.00042)\n",
            "epoch: 859, train_loss: 0.317286, val loss: 0.458745,  train_metric: -0.878 test_metric: -0.827 lr: 0.00042)\n",
            "epoch: 860, train_loss: 0.322178, val loss: 0.452956,  train_metric: -0.872 test_metric: -0.838 lr: 0.00042)\n",
            "epoch: 861, train_loss: 0.318784, val loss: 0.439302,  train_metric: -0.875 test_metric: -0.815 lr: 0.00042)\n",
            "epoch: 862, train_loss: 0.312851, val loss: 0.442415,  train_metric: -0.883 test_metric: -0.834 lr: 0.00042)\n",
            "epoch: 863, train_loss: 0.311097, val loss: 0.435352,  train_metric: -0.878 test_metric: -0.829 lr: 0.00042)\n",
            "epoch: 864, train_loss: 0.314001, val loss: 0.434795,  train_metric: -0.877 test_metric: -0.830 lr: 0.00042)\n",
            "epoch: 865, train_loss: 0.320097, val loss: 0.440558,  train_metric: -0.877 test_metric: -0.828 lr: 0.00042)\n",
            "epoch: 866, train_loss: 0.317792, val loss: 0.437160,  train_metric: -0.872 test_metric: -0.830 lr: 0.00042)\n",
            "epoch: 867, train_loss: 0.311528, val loss: 0.430352,  train_metric: -0.883 test_metric: -0.840 lr: 0.00042)\n",
            "epoch: 868, train_loss: 0.308162, val loss: 0.438845,  train_metric: -0.885 test_metric: -0.828 lr: 0.00042)\n",
            "epoch: 869, train_loss: 0.311609, val loss: 0.429285,  train_metric: -0.887 test_metric: -0.838 lr: 0.00042)\n",
            "epoch: 870, train_loss: 0.306112, val loss: 0.426098,  train_metric: -0.878 test_metric: -0.844 lr: 0.00042)\n",
            "epoch: 871, train_loss: 0.306193, val loss: 0.432845,  train_metric: -0.885 test_metric: -0.830 lr: 0.00042)\n",
            "epoch: 872, train_loss: 0.309374, val loss: 0.432494,  train_metric: -0.883 test_metric: -0.833 lr: 0.00042)\n",
            "epoch: 873, train_loss: 0.306371, val loss: 0.437138,  train_metric: -0.883 test_metric: -0.827 lr: 0.00042)\n",
            "epoch: 874, train_loss: 0.307896, val loss: 0.452512,  train_metric: -0.882 test_metric: -0.824 lr: 0.00042)\n",
            "epoch: 875, train_loss: 0.314066, val loss: 0.438703,  train_metric: -0.875 test_metric: -0.830 lr: 0.00042)\n",
            "epoch: 876, train_loss: 0.308845, val loss: 0.429023,  train_metric: -0.882 test_metric: -0.828 lr: 0.00042)\n",
            "epoch: 877, train_loss: 0.304800, val loss: 0.424434,  train_metric: -0.888 test_metric: -0.841 lr: 0.00042)\n",
            "epoch: 878, train_loss: 0.303438, val loss: 0.434372,  train_metric: -0.889 test_metric: -0.830 lr: 0.00042)\n",
            "epoch: 879, train_loss: 0.311551, val loss: 0.428169,  train_metric: -0.878 test_metric: -0.833 lr: 0.00041)\n",
            "epoch: 880, train_loss: 0.310517, val loss: 0.426840,  train_metric: -0.881 test_metric: -0.845 lr: 0.00041)\n",
            "epoch: 881, train_loss: 0.305642, val loss: 0.422499,  train_metric: -0.889 test_metric: -0.840 lr: 0.00041)\n",
            "epoch: 882, train_loss: 0.304017, val loss: 0.432182,  train_metric: -0.884 test_metric: -0.828 lr: 0.00041)\n",
            "epoch: 883, train_loss: 0.303273, val loss: 0.428628,  train_metric: -0.883 test_metric: -0.838 lr: 0.00041)\n",
            "epoch: 884, train_loss: 0.305164, val loss: 0.424981,  train_metric: -0.891 test_metric: -0.838 lr: 0.00041)\n",
            "epoch: 885, train_loss: 0.306082, val loss: 0.428037,  train_metric: -0.883 test_metric: -0.831 lr: 0.00041)\n",
            "epoch: 886, train_loss: 0.303648, val loss: 0.430190,  train_metric: -0.886 test_metric: -0.828 lr: 0.00041)\n",
            "epoch: 887, train_loss: 0.321221, val loss: 0.446106,  train_metric: -0.873 test_metric: -0.822 lr: 0.00041)\n",
            "epoch: 888, train_loss: 0.305635, val loss: 0.438532,  train_metric: -0.883 test_metric: -0.836 lr: 0.00041)\n",
            "epoch: 889, train_loss: 0.311363, val loss: 0.420621,  train_metric: -0.886 test_metric: -0.833 lr: 0.00041)\n",
            "epoch: 890, train_loss: 0.307678, val loss: 0.462402,  train_metric: -0.874 test_metric: -0.817 lr: 0.00041)\n",
            "epoch: 891, train_loss: 0.316713, val loss: 0.437244,  train_metric: -0.879 test_metric: -0.830 lr: 0.00041)\n",
            "epoch: 892, train_loss: 0.306001, val loss: 0.429866,  train_metric: -0.887 test_metric: -0.829 lr: 0.00041)\n",
            "epoch: 893, train_loss: 0.307440, val loss: 0.430184,  train_metric: -0.884 test_metric: -0.829 lr: 0.00041)\n",
            "epoch: 894, train_loss: 0.303868, val loss: 0.432522,  train_metric: -0.888 test_metric: -0.834 lr: 0.00041)\n",
            "epoch: 895, train_loss: 0.302798, val loss: 0.431401,  train_metric: -0.881 test_metric: -0.830 lr: 0.00041)\n",
            "epoch: 896, train_loss: 0.301444, val loss: 0.449173,  train_metric: -0.879 test_metric: -0.815 lr: 0.00041)\n",
            "epoch: 897, train_loss: 0.306337, val loss: 0.432075,  train_metric: -0.879 test_metric: -0.834 lr: 0.00041)\n",
            "epoch: 898, train_loss: 0.296910, val loss: 0.428182,  train_metric: -0.890 test_metric: -0.830 lr: 0.00041)\n",
            "epoch: 899, train_loss: 0.297459, val loss: 0.428894,  train_metric: -0.888 test_metric: -0.829 lr: 0.00041)\n",
            "epoch: 900, train_loss: 0.299694, val loss: 0.427816,  train_metric: -0.884 test_metric: -0.831 lr: 0.00041)\n",
            "epoch: 901, train_loss: 0.296006, val loss: 0.423967,  train_metric: -0.889 test_metric: -0.828 lr: 0.00041)\n",
            "epoch: 902, train_loss: 0.305043, val loss: 0.436081,  train_metric: -0.885 test_metric: -0.834 lr: 0.00041)\n",
            "epoch: 903, train_loss: 0.298104, val loss: 0.419862,  train_metric: -0.893 test_metric: -0.839 lr: 0.00040)\n",
            "epoch: 904, train_loss: 0.300298, val loss: 0.449695,  train_metric: -0.883 test_metric: -0.824 lr: 0.00040)\n",
            "epoch: 905, train_loss: 0.315526, val loss: 0.419053,  train_metric: -0.880 test_metric: -0.840 lr: 0.00040)\n",
            "epoch: 906, train_loss: 0.307953, val loss: 0.431964,  train_metric: -0.882 test_metric: -0.834 lr: 0.00040)\n",
            "epoch: 907, train_loss: 0.301883, val loss: 0.430924,  train_metric: -0.884 test_metric: -0.836 lr: 0.00040)\n",
            "epoch: 908, train_loss: 0.305502, val loss: 0.444647,  train_metric: -0.878 test_metric: -0.823 lr: 0.00040)\n",
            "epoch: 909, train_loss: 0.295569, val loss: 0.418239,  train_metric: -0.892 test_metric: -0.839 lr: 0.00040)\n",
            "epoch: 910, train_loss: 0.301829, val loss: 0.427175,  train_metric: -0.881 test_metric: -0.841 lr: 0.00040)\n",
            "epoch: 911, train_loss: 0.298241, val loss: 0.426658,  train_metric: -0.888 test_metric: -0.845 lr: 0.00040)\n",
            "epoch: 912, train_loss: 0.302158, val loss: 0.425232,  train_metric: -0.883 test_metric: -0.831 lr: 0.00040)\n",
            "epoch: 913, train_loss: 0.302516, val loss: 0.435464,  train_metric: -0.877 test_metric: -0.833 lr: 0.00040)\n",
            "epoch: 914, train_loss: 0.300293, val loss: 0.420663,  train_metric: -0.885 test_metric: -0.833 lr: 0.00040)\n",
            "epoch: 915, train_loss: 0.293990, val loss: 0.429743,  train_metric: -0.891 test_metric: -0.833 lr: 0.00040)\n",
            "epoch: 916, train_loss: 0.303237, val loss: 0.416515,  train_metric: -0.877 test_metric: -0.840 lr: 0.00040)\n",
            "epoch: 917, train_loss: 0.292968, val loss: 0.421155,  train_metric: -0.885 test_metric: -0.839 lr: 0.00040)\n",
            "epoch: 918, train_loss: 0.300667, val loss: 0.422151,  train_metric: -0.883 test_metric: -0.834 lr: 0.00040)\n",
            "epoch: 919, train_loss: 0.293694, val loss: 0.423195,  train_metric: -0.888 test_metric: -0.844 lr: 0.00040)\n",
            "epoch: 920, train_loss: 0.302466, val loss: 0.424112,  train_metric: -0.884 test_metric: -0.839 lr: 0.00040)\n",
            "epoch: 921, train_loss: 0.296867, val loss: 0.419200,  train_metric: -0.883 test_metric: -0.838 lr: 0.00040)\n",
            "epoch: 922, train_loss: 0.295045, val loss: 0.443210,  train_metric: -0.885 test_metric: -0.828 lr: 0.00040)\n",
            "epoch: 923, train_loss: 0.307466, val loss: 0.431050,  train_metric: -0.880 test_metric: -0.824 lr: 0.00040)\n",
            "epoch: 924, train_loss: 0.307790, val loss: 0.439702,  train_metric: -0.873 test_metric: -0.838 lr: 0.00040)\n",
            "epoch: 925, train_loss: 0.308554, val loss: 0.433499,  train_metric: -0.879 test_metric: -0.828 lr: 0.00040)\n",
            "epoch: 926, train_loss: 0.299823, val loss: 0.425312,  train_metric: -0.888 test_metric: -0.833 lr: 0.00040)\n",
            "epoch: 927, train_loss: 0.304348, val loss: 0.423819,  train_metric: -0.885 test_metric: -0.836 lr: 0.00040)\n",
            "epoch: 928, train_loss: 0.314329, val loss: 0.436933,  train_metric: -0.875 test_metric: -0.825 lr: 0.00039)\n",
            "epoch: 929, train_loss: 0.309575, val loss: 0.458987,  train_metric: -0.876 test_metric: -0.813 lr: 0.00039)\n",
            "epoch: 930, train_loss: 0.310907, val loss: 0.451590,  train_metric: -0.880 test_metric: -0.838 lr: 0.00039)\n",
            "epoch: 931, train_loss: 0.309272, val loss: 0.420520,  train_metric: -0.879 test_metric: -0.848 lr: 0.00039)\n",
            "epoch: 932, train_loss: 0.290548, val loss: 0.421121,  train_metric: -0.894 test_metric: -0.840 lr: 0.00039)\n",
            "epoch: 933, train_loss: 0.295638, val loss: 0.415948,  train_metric: -0.882 test_metric: -0.834 lr: 0.00039)\n",
            "epoch: 934, train_loss: 0.291706, val loss: 0.410140,  train_metric: -0.888 test_metric: -0.840 lr: 0.00039)\n",
            "epoch: 935, train_loss: 0.291603, val loss: 0.416827,  train_metric: -0.890 test_metric: -0.844 lr: 0.00039)\n",
            "epoch: 936, train_loss: 0.290679, val loss: 0.414776,  train_metric: -0.890 test_metric: -0.839 lr: 0.00039)\n",
            "epoch: 937, train_loss: 0.290405, val loss: 0.418463,  train_metric: -0.892 test_metric: -0.843 lr: 0.00039)\n",
            "epoch: 938, train_loss: 0.295514, val loss: 0.405687,  train_metric: -0.881 test_metric: -0.834 lr: 0.00039)\n",
            "epoch: 939, train_loss: 0.296280, val loss: 0.431847,  train_metric: -0.885 test_metric: -0.829 lr: 0.00039)\n",
            "epoch: 940, train_loss: 0.297385, val loss: 0.420137,  train_metric: -0.888 test_metric: -0.836 lr: 0.00039)\n",
            "epoch: 941, train_loss: 0.294281, val loss: 0.420911,  train_metric: -0.884 test_metric: -0.849 lr: 0.00039)\n",
            "epoch: 942, train_loss: 0.291828, val loss: 0.414629,  train_metric: -0.886 test_metric: -0.840 lr: 0.00039)\n",
            "epoch: 943, train_loss: 0.292367, val loss: 0.417380,  train_metric: -0.892 test_metric: -0.844 lr: 0.00039)\n",
            "epoch: 944, train_loss: 0.300766, val loss: 0.427229,  train_metric: -0.880 test_metric: -0.830 lr: 0.00039)\n",
            "epoch: 945, train_loss: 0.299499, val loss: 0.419696,  train_metric: -0.883 test_metric: -0.840 lr: 0.00039)\n",
            "epoch: 946, train_loss: 0.288280, val loss: 0.415589,  train_metric: -0.890 test_metric: -0.839 lr: 0.00039)\n",
            "epoch: 947, train_loss: 0.287754, val loss: 0.421654,  train_metric: -0.893 test_metric: -0.834 lr: 0.00039)\n",
            "epoch: 948, train_loss: 0.292790, val loss: 0.450975,  train_metric: -0.893 test_metric: -0.823 lr: 0.00039)\n",
            "epoch: 949, train_loss: 0.297305, val loss: 0.426992,  train_metric: -0.887 test_metric: -0.829 lr: 0.00039)\n",
            "epoch: 950, train_loss: 0.297990, val loss: 0.431148,  train_metric: -0.888 test_metric: -0.843 lr: 0.00039)\n",
            "epoch: 951, train_loss: 0.307168, val loss: 0.408553,  train_metric: -0.875 test_metric: -0.846 lr: 0.00039)\n",
            "epoch: 952, train_loss: 0.299181, val loss: 0.424649,  train_metric: -0.881 test_metric: -0.838 lr: 0.00039)\n",
            "epoch: 953, train_loss: 0.298545, val loss: 0.405212,  train_metric: -0.886 test_metric: -0.848 lr: 0.00039)\n",
            "epoch: 954, train_loss: 0.302736, val loss: 0.427499,  train_metric: -0.874 test_metric: -0.835 lr: 0.00038)\n",
            "epoch: 955, train_loss: 0.302756, val loss: 0.441972,  train_metric: -0.879 test_metric: -0.831 lr: 0.00038)\n",
            "epoch: 956, train_loss: 0.304392, val loss: 0.426581,  train_metric: -0.881 test_metric: -0.838 lr: 0.00038)\n",
            "epoch: 957, train_loss: 0.290505, val loss: 0.424933,  train_metric: -0.890 test_metric: -0.825 lr: 0.00038)\n",
            "epoch: 958, train_loss: 0.289889, val loss: 0.414278,  train_metric: -0.884 test_metric: -0.845 lr: 0.00038)\n",
            "epoch: 959, train_loss: 0.287800, val loss: 0.410488,  train_metric: -0.888 test_metric: -0.844 lr: 0.00038)\n",
            "epoch: 960, train_loss: 0.289627, val loss: 0.409301,  train_metric: -0.892 test_metric: -0.841 lr: 0.00038)\n",
            "epoch: 961, train_loss: 0.292456, val loss: 0.408685,  train_metric: -0.890 test_metric: -0.848 lr: 0.00038)\n",
            "epoch: 962, train_loss: 0.290828, val loss: 0.409158,  train_metric: -0.889 test_metric: -0.841 lr: 0.00038)\n",
            "epoch: 963, train_loss: 0.284179, val loss: 0.408782,  train_metric: -0.900 test_metric: -0.845 lr: 0.00038)\n",
            "epoch: 964, train_loss: 0.288376, val loss: 0.411410,  train_metric: -0.896 test_metric: -0.848 lr: 0.00038)\n",
            "epoch: 965, train_loss: 0.285989, val loss: 0.408304,  train_metric: -0.884 test_metric: -0.841 lr: 0.00038)\n",
            "epoch: 966, train_loss: 0.290763, val loss: 0.408814,  train_metric: -0.894 test_metric: -0.845 lr: 0.00038)\n",
            "epoch: 967, train_loss: 0.287008, val loss: 0.407199,  train_metric: -0.889 test_metric: -0.845 lr: 0.00038)\n",
            "epoch: 968, train_loss: 0.286838, val loss: 0.415338,  train_metric: -0.893 test_metric: -0.840 lr: 0.00038)\n",
            "epoch: 969, train_loss: 0.290221, val loss: 0.438650,  train_metric: -0.890 test_metric: -0.824 lr: 0.00038)\n",
            "epoch: 970, train_loss: 0.301121, val loss: 0.414048,  train_metric: -0.883 test_metric: -0.841 lr: 0.00038)\n",
            "epoch: 971, train_loss: 0.286926, val loss: 0.416509,  train_metric: -0.892 test_metric: -0.831 lr: 0.00038)\n",
            "epoch: 972, train_loss: 0.283115, val loss: 0.433402,  train_metric: -0.891 test_metric: -0.827 lr: 0.00038)\n",
            "epoch: 973, train_loss: 0.288629, val loss: 0.410126,  train_metric: -0.887 test_metric: -0.836 lr: 0.00038)\n",
            "epoch: 974, train_loss: 0.283989, val loss: 0.407164,  train_metric: -0.891 test_metric: -0.851 lr: 0.00038)\n",
            "epoch: 975, train_loss: 0.283295, val loss: 0.405582,  train_metric: -0.890 test_metric: -0.839 lr: 0.00038)\n",
            "epoch: 976, train_loss: 0.286951, val loss: 0.434689,  train_metric: -0.887 test_metric: -0.844 lr: 0.00038)\n",
            "epoch: 977, train_loss: 0.304648, val loss: 0.474726,  train_metric: -0.881 test_metric: -0.814 lr: 0.00038)\n",
            "epoch: 978, train_loss: 0.303858, val loss: 0.440553,  train_metric: -0.879 test_metric: -0.814 lr: 0.00038)\n",
            "epoch: 979, train_loss: 0.297707, val loss: 0.445006,  train_metric: -0.887 test_metric: -0.823 lr: 0.00038)\n",
            "epoch: 980, train_loss: 0.300242, val loss: 0.421055,  train_metric: -0.886 test_metric: -0.839 lr: 0.00037)\n",
            "epoch: 981, train_loss: 0.285555, val loss: 0.401706,  train_metric: -0.892 test_metric: -0.844 lr: 0.00037)\n",
            "epoch: 982, train_loss: 0.282342, val loss: 0.411585,  train_metric: -0.888 test_metric: -0.846 lr: 0.00037)\n",
            "epoch: 983, train_loss: 0.277928, val loss: 0.416424,  train_metric: -0.894 test_metric: -0.833 lr: 0.00037)\n",
            "epoch: 984, train_loss: 0.281203, val loss: 0.411925,  train_metric: -0.895 test_metric: -0.836 lr: 0.00037)\n",
            "epoch: 985, train_loss: 0.284849, val loss: 0.411041,  train_metric: -0.894 test_metric: -0.840 lr: 0.00037)\n",
            "epoch: 986, train_loss: 0.285376, val loss: 0.403576,  train_metric: -0.890 test_metric: -0.854 lr: 0.00037)\n",
            "epoch: 987, train_loss: 0.280898, val loss: 0.422311,  train_metric: -0.893 test_metric: -0.843 lr: 0.00037)\n",
            "epoch: 988, train_loss: 0.290722, val loss: 0.423472,  train_metric: -0.883 test_metric: -0.823 lr: 0.00037)\n",
            "epoch: 989, train_loss: 0.288902, val loss: 0.435416,  train_metric: -0.884 test_metric: -0.830 lr: 0.00037)\n",
            "epoch: 990, train_loss: 0.293277, val loss: 0.408176,  train_metric: -0.883 test_metric: -0.843 lr: 0.00037)\n",
            "epoch: 991, train_loss: 0.286212, val loss: 0.422226,  train_metric: -0.892 test_metric: -0.838 lr: 0.00037)\n",
            "epoch: 992, train_loss: 0.282243, val loss: 0.409532,  train_metric: -0.889 test_metric: -0.843 lr: 0.00037)\n",
            "epoch: 993, train_loss: 0.288631, val loss: 0.406790,  train_metric: -0.888 test_metric: -0.846 lr: 0.00037)\n",
            "epoch: 994, train_loss: 0.277877, val loss: 0.421615,  train_metric: -0.897 test_metric: -0.835 lr: 0.00037)\n",
            "epoch: 995, train_loss: 0.280940, val loss: 0.422375,  train_metric: -0.895 test_metric: -0.840 lr: 0.00037)\n",
            "epoch: 996, train_loss: 0.276337, val loss: 0.405916,  train_metric: -0.894 test_metric: -0.845 lr: 0.00037)\n",
            "epoch: 997, train_loss: 0.283179, val loss: 0.413132,  train_metric: -0.897 test_metric: -0.851 lr: 0.00037)\n",
            "epoch: 998, train_loss: 0.280708, val loss: 0.399883,  train_metric: -0.895 test_metric: -0.849 lr: 0.00037)\n",
            "epoch: 999, train_loss: 0.278100, val loss: 0.406027,  train_metric: -0.893 test_metric: -0.841 lr: 0.00037)\n",
            "epoch: 1000, train_loss: 0.275239, val loss: 0.403750,  train_metric: -0.899 test_metric: -0.854 lr: 0.00037)\n",
            "epoch: 1001, train_loss: 0.276814, val loss: 0.428426,  train_metric: -0.901 test_metric: -0.831 lr: 0.00037)\n",
            "epoch: 1002, train_loss: 0.283523, val loss: 0.401746,  train_metric: -0.889 test_metric: -0.848 lr: 0.00037)\n",
            "epoch: 1003, train_loss: 0.274218, val loss: 0.415308,  train_metric: -0.896 test_metric: -0.836 lr: 0.00037)\n",
            "epoch: 1004, train_loss: 0.278269, val loss: 0.404650,  train_metric: -0.895 test_metric: -0.851 lr: 0.00037)\n",
            "epoch: 1005, train_loss: 0.290876, val loss: 0.422634,  train_metric: -0.886 test_metric: -0.825 lr: 0.00037)\n",
            "epoch: 1006, train_loss: 0.285016, val loss: 0.405841,  train_metric: -0.888 test_metric: -0.845 lr: 0.00037)\n",
            "epoch: 1007, train_loss: 0.274449, val loss: 0.404873,  train_metric: -0.896 test_metric: -0.844 lr: 0.00036)\n",
            "epoch: 1008, train_loss: 0.277081, val loss: 0.403697,  train_metric: -0.896 test_metric: -0.853 lr: 0.00036)\n",
            "epoch: 1009, train_loss: 0.273972, val loss: 0.402465,  train_metric: -0.894 test_metric: -0.844 lr: 0.00036)\n",
            "epoch: 1010, train_loss: 0.275684, val loss: 0.409548,  train_metric: -0.894 test_metric: -0.844 lr: 0.00036)\n",
            "epoch: 1011, train_loss: 0.284978, val loss: 0.418299,  train_metric: -0.887 test_metric: -0.833 lr: 0.00036)\n",
            "epoch: 1012, train_loss: 0.288800, val loss: 0.431329,  train_metric: -0.890 test_metric: -0.825 lr: 0.00036)\n",
            "epoch: 1013, train_loss: 0.289209, val loss: 0.403899,  train_metric: -0.886 test_metric: -0.846 lr: 0.00036)\n",
            "epoch: 1014, train_loss: 0.273494, val loss: 0.408041,  train_metric: -0.897 test_metric: -0.845 lr: 0.00036)\n",
            "epoch: 1015, train_loss: 0.276289, val loss: 0.407144,  train_metric: -0.894 test_metric: -0.846 lr: 0.00036)\n",
            "epoch: 1016, train_loss: 0.271992, val loss: 0.408264,  train_metric: -0.896 test_metric: -0.845 lr: 0.00036)\n",
            "epoch: 1017, train_loss: 0.275492, val loss: 0.399747,  train_metric: -0.897 test_metric: -0.846 lr: 0.00036)\n",
            "epoch: 1018, train_loss: 0.276726, val loss: 0.403408,  train_metric: -0.897 test_metric: -0.853 lr: 0.00036)\n",
            "epoch: 1019, train_loss: 0.276901, val loss: 0.418521,  train_metric: -0.894 test_metric: -0.836 lr: 0.00036)\n",
            "epoch: 1020, train_loss: 0.276977, val loss: 0.406069,  train_metric: -0.898 test_metric: -0.843 lr: 0.00036)\n",
            "epoch: 1021, train_loss: 0.273717, val loss: 0.398427,  train_metric: -0.894 test_metric: -0.848 lr: 0.00036)\n",
            "epoch: 1022, train_loss: 0.278512, val loss: 0.406029,  train_metric: -0.891 test_metric: -0.851 lr: 0.00036)\n",
            "epoch: 1023, train_loss: 0.277770, val loss: 0.399063,  train_metric: -0.894 test_metric: -0.856 lr: 0.00036)\n",
            "epoch: 1024, train_loss: 0.275614, val loss: 0.416269,  train_metric: -0.896 test_metric: -0.844 lr: 0.00036)\n",
            "epoch: 1025, train_loss: 0.275311, val loss: 0.403559,  train_metric: -0.896 test_metric: -0.841 lr: 0.00036)\n",
            "epoch: 1026, train_loss: 0.271494, val loss: 0.400942,  train_metric: -0.894 test_metric: -0.849 lr: 0.00036)\n",
            "epoch: 1027, train_loss: 0.274213, val loss: 0.410151,  train_metric: -0.896 test_metric: -0.846 lr: 0.00036)\n",
            "epoch: 1028, train_loss: 0.275187, val loss: 0.398472,  train_metric: -0.891 test_metric: -0.854 lr: 0.00036)\n",
            "epoch: 1029, train_loss: 0.274166, val loss: 0.401919,  train_metric: -0.895 test_metric: -0.855 lr: 0.00036)\n",
            "epoch: 1030, train_loss: 0.270696, val loss: 0.406985,  train_metric: -0.899 test_metric: -0.848 lr: 0.00036)\n",
            "epoch: 1031, train_loss: 0.275851, val loss: 0.405818,  train_metric: -0.891 test_metric: -0.848 lr: 0.00036)\n",
            "epoch: 1032, train_loss: 0.277729, val loss: 0.395661,  train_metric: -0.889 test_metric: -0.850 lr: 0.00036)\n",
            "epoch: 1033, train_loss: 0.271229, val loss: 0.407906,  train_metric: -0.903 test_metric: -0.844 lr: 0.00036)\n",
            "epoch: 1034, train_loss: 0.277685, val loss: 0.410021,  train_metric: -0.892 test_metric: -0.833 lr: 0.00036)\n",
            "epoch: 1035, train_loss: 0.276724, val loss: 0.440940,  train_metric: -0.893 test_metric: -0.822 lr: 0.00035)\n",
            "epoch: 1036, train_loss: 0.276346, val loss: 0.424500,  train_metric: -0.892 test_metric: -0.831 lr: 0.00035)\n",
            "epoch: 1037, train_loss: 0.283265, val loss: 0.452320,  train_metric: -0.889 test_metric: -0.819 lr: 0.00035)\n",
            "epoch: 1038, train_loss: 0.290177, val loss: 0.424081,  train_metric: -0.879 test_metric: -0.827 lr: 0.00035)\n",
            "epoch: 1039, train_loss: 0.280872, val loss: 0.416321,  train_metric: -0.891 test_metric: -0.839 lr: 0.00035)\n",
            "epoch: 1040, train_loss: 0.271308, val loss: 0.400678,  train_metric: -0.894 test_metric: -0.840 lr: 0.00035)\n",
            "epoch: 1041, train_loss: 0.267854, val loss: 0.393784,  train_metric: -0.904 test_metric: -0.855 lr: 0.00035)\n",
            "epoch: 1042, train_loss: 0.267431, val loss: 0.392364,  train_metric: -0.898 test_metric: -0.850 lr: 0.00035)\n",
            "epoch: 1043, train_loss: 0.268124, val loss: 0.401396,  train_metric: -0.902 test_metric: -0.844 lr: 0.00035)\n",
            "epoch: 1044, train_loss: 0.268872, val loss: 0.394146,  train_metric: -0.896 test_metric: -0.849 lr: 0.00035)\n",
            "epoch: 1045, train_loss: 0.266835, val loss: 0.403164,  train_metric: -0.901 test_metric: -0.845 lr: 0.00035)\n",
            "epoch: 1046, train_loss: 0.270954, val loss: 0.393990,  train_metric: -0.893 test_metric: -0.851 lr: 0.00035)\n",
            "epoch: 1047, train_loss: 0.272694, val loss: 0.421956,  train_metric: -0.897 test_metric: -0.824 lr: 0.00035)\n",
            "epoch: 1048, train_loss: 0.271738, val loss: 0.410950,  train_metric: -0.897 test_metric: -0.843 lr: 0.00035)\n",
            "epoch: 1049, train_loss: 0.269997, val loss: 0.394165,  train_metric: -0.898 test_metric: -0.844 lr: 0.00035)\n",
            "epoch: 1050, train_loss: 0.267685, val loss: 0.396240,  train_metric: -0.902 test_metric: -0.850 lr: 0.00035)\n",
            "epoch: 1051, train_loss: 0.266644, val loss: 0.402664,  train_metric: -0.901 test_metric: -0.851 lr: 0.00035)\n",
            "epoch: 1052, train_loss: 0.270387, val loss: 0.417658,  train_metric: -0.897 test_metric: -0.846 lr: 0.00035)\n",
            "epoch: 1053, train_loss: 0.280070, val loss: 0.403563,  train_metric: -0.890 test_metric: -0.845 lr: 0.00035)\n",
            "epoch: 1054, train_loss: 0.272221, val loss: 0.402791,  train_metric: -0.894 test_metric: -0.843 lr: 0.00035)\n",
            "epoch: 1055, train_loss: 0.270380, val loss: 0.399817,  train_metric: -0.898 test_metric: -0.844 lr: 0.00035)\n",
            "epoch: 1056, train_loss: 0.267995, val loss: 0.403274,  train_metric: -0.901 test_metric: -0.843 lr: 0.00035)\n",
            "epoch: 1057, train_loss: 0.265492, val loss: 0.408540,  train_metric: -0.902 test_metric: -0.840 lr: 0.00035)\n",
            "epoch: 1058, train_loss: 0.269037, val loss: 0.405597,  train_metric: -0.896 test_metric: -0.844 lr: 0.00035)\n",
            "epoch: 1059, train_loss: 0.264365, val loss: 0.409208,  train_metric: -0.897 test_metric: -0.839 lr: 0.00035)\n",
            "epoch: 1060, train_loss: 0.269633, val loss: 0.425855,  train_metric: -0.902 test_metric: -0.829 lr: 0.00035)\n",
            "epoch: 1061, train_loss: 0.270180, val loss: 0.409713,  train_metric: -0.892 test_metric: -0.843 lr: 0.00035)\n",
            "epoch: 1062, train_loss: 0.268329, val loss: 0.427003,  train_metric: -0.901 test_metric: -0.831 lr: 0.00035)\n",
            "epoch: 1063, train_loss: 0.275922, val loss: 0.407367,  train_metric: -0.898 test_metric: -0.850 lr: 0.00034)\n",
            "epoch: 1064, train_loss: 0.262655, val loss: 0.396637,  train_metric: -0.897 test_metric: -0.841 lr: 0.00034)\n",
            "epoch: 1065, train_loss: 0.268555, val loss: 0.442473,  train_metric: -0.898 test_metric: -0.825 lr: 0.00034)\n",
            "epoch: 1066, train_loss: 0.284169, val loss: 0.402923,  train_metric: -0.888 test_metric: -0.846 lr: 0.00034)\n",
            "epoch: 1067, train_loss: 0.269544, val loss: 0.397564,  train_metric: -0.895 test_metric: -0.857 lr: 0.00034)\n",
            "epoch: 1068, train_loss: 0.269151, val loss: 0.406930,  train_metric: -0.898 test_metric: -0.843 lr: 0.00034)\n",
            "epoch: 1069, train_loss: 0.270232, val loss: 0.402944,  train_metric: -0.896 test_metric: -0.841 lr: 0.00034)\n",
            "epoch: 1070, train_loss: 0.265026, val loss: 0.405860,  train_metric: -0.899 test_metric: -0.844 lr: 0.00034)\n",
            "epoch: 1071, train_loss: 0.267318, val loss: 0.396889,  train_metric: -0.895 test_metric: -0.859 lr: 0.00034)\n",
            "epoch: 1072, train_loss: 0.264670, val loss: 0.400142,  train_metric: -0.903 test_metric: -0.844 lr: 0.00034)\n",
            "epoch: 1073, train_loss: 0.269459, val loss: 0.403554,  train_metric: -0.901 test_metric: -0.844 lr: 0.00034)\n",
            "epoch: 1074, train_loss: 0.267239, val loss: 0.399581,  train_metric: -0.902 test_metric: -0.848 lr: 0.00034)\n",
            "epoch: 1075, train_loss: 0.266013, val loss: 0.400118,  train_metric: -0.902 test_metric: -0.853 lr: 0.00034)\n",
            "epoch: 1076, train_loss: 0.266803, val loss: 0.407743,  train_metric: -0.896 test_metric: -0.851 lr: 0.00034)\n",
            "epoch: 1077, train_loss: 0.267180, val loss: 0.394222,  train_metric: -0.896 test_metric: -0.850 lr: 0.00034)\n",
            "epoch: 1078, train_loss: 0.263451, val loss: 0.393392,  train_metric: -0.905 test_metric: -0.862 lr: 0.00034)\n",
            "epoch: 1079, train_loss: 0.264461, val loss: 0.407386,  train_metric: -0.898 test_metric: -0.843 lr: 0.00034)\n",
            "epoch: 1080, train_loss: 0.265860, val loss: 0.396080,  train_metric: -0.907 test_metric: -0.845 lr: 0.00034)\n",
            "epoch: 1081, train_loss: 0.263362, val loss: 0.402979,  train_metric: -0.901 test_metric: -0.854 lr: 0.00034)\n",
            "epoch: 1082, train_loss: 0.266294, val loss: 0.404605,  train_metric: -0.900 test_metric: -0.838 lr: 0.00034)\n",
            "epoch: 1083, train_loss: 0.266913, val loss: 0.410345,  train_metric: -0.896 test_metric: -0.841 lr: 0.00034)\n",
            "epoch: 1084, train_loss: 0.271319, val loss: 0.399913,  train_metric: -0.891 test_metric: -0.851 lr: 0.00034)\n",
            "epoch: 1085, train_loss: 0.267768, val loss: 0.403602,  train_metric: -0.893 test_metric: -0.850 lr: 0.00034)\n",
            "epoch: 1086, train_loss: 0.273493, val loss: 0.387442,  train_metric: -0.893 test_metric: -0.853 lr: 0.00034)\n",
            "epoch: 1087, train_loss: 0.272615, val loss: 0.401123,  train_metric: -0.893 test_metric: -0.840 lr: 0.00034)\n",
            "epoch: 1088, train_loss: 0.262733, val loss: 0.393529,  train_metric: -0.903 test_metric: -0.843 lr: 0.00034)\n",
            "epoch: 1089, train_loss: 0.261345, val loss: 0.401139,  train_metric: -0.898 test_metric: -0.844 lr: 0.00034)\n",
            "epoch: 1090, train_loss: 0.265775, val loss: 0.408386,  train_metric: -0.897 test_metric: -0.830 lr: 0.00034)\n",
            "epoch: 1091, train_loss: 0.268909, val loss: 0.394315,  train_metric: -0.892 test_metric: -0.850 lr: 0.00034)\n",
            "epoch: 1092, train_loss: 0.263642, val loss: 0.419091,  train_metric: -0.899 test_metric: -0.835 lr: 0.00034)\n",
            "epoch: 1093, train_loss: 0.266229, val loss: 0.392459,  train_metric: -0.899 test_metric: -0.866 lr: 0.00033)\n",
            "epoch: 1094, train_loss: 0.263212, val loss: 0.399102,  train_metric: -0.906 test_metric: -0.848 lr: 0.00033)\n",
            "epoch: 1095, train_loss: 0.268103, val loss: 0.402758,  train_metric: -0.895 test_metric: -0.850 lr: 0.00033)\n",
            "epoch: 1096, train_loss: 0.265974, val loss: 0.439313,  train_metric: -0.900 test_metric: -0.820 lr: 0.00033)\n",
            "epoch: 1097, train_loss: 0.269531, val loss: 0.400321,  train_metric: -0.898 test_metric: -0.844 lr: 0.00033)\n",
            "epoch: 1098, train_loss: 0.259720, val loss: 0.392070,  train_metric: -0.901 test_metric: -0.860 lr: 0.00033)\n",
            "epoch: 1099, train_loss: 0.259621, val loss: 0.405007,  train_metric: -0.902 test_metric: -0.835 lr: 0.00033)\n",
            "epoch: 1100, train_loss: 0.263735, val loss: 0.388464,  train_metric: -0.896 test_metric: -0.855 lr: 0.00033)\n",
            "epoch: 1101, train_loss: 0.260322, val loss: 0.400942,  train_metric: -0.899 test_metric: -0.849 lr: 0.00033)\n",
            "epoch: 1102, train_loss: 0.261088, val loss: 0.396069,  train_metric: -0.901 test_metric: -0.853 lr: 0.00033)\n",
            "epoch: 1103, train_loss: 0.258879, val loss: 0.388839,  train_metric: -0.902 test_metric: -0.851 lr: 0.00033)\n",
            "epoch: 1104, train_loss: 0.260226, val loss: 0.389498,  train_metric: -0.901 test_metric: -0.851 lr: 0.00033)\n",
            "epoch: 1105, train_loss: 0.260004, val loss: 0.382177,  train_metric: -0.897 test_metric: -0.861 lr: 0.00033)\n",
            "epoch: 1106, train_loss: 0.256038, val loss: 0.394114,  train_metric: -0.904 test_metric: -0.851 lr: 0.00033)\n",
            "epoch: 1107, train_loss: 0.257245, val loss: 0.405253,  train_metric: -0.907 test_metric: -0.845 lr: 0.00033)\n",
            "epoch: 1108, train_loss: 0.260509, val loss: 0.398379,  train_metric: -0.903 test_metric: -0.844 lr: 0.00033)\n",
            "epoch: 1109, train_loss: 0.260810, val loss: 0.402929,  train_metric: -0.900 test_metric: -0.848 lr: 0.00033)\n",
            "epoch: 1110, train_loss: 0.261012, val loss: 0.390689,  train_metric: -0.895 test_metric: -0.856 lr: 0.00033)\n",
            "epoch: 1111, train_loss: 0.260394, val loss: 0.425093,  train_metric: -0.907 test_metric: -0.835 lr: 0.00033)\n",
            "epoch: 1112, train_loss: 0.266004, val loss: 0.396660,  train_metric: -0.902 test_metric: -0.845 lr: 0.00033)\n",
            "epoch: 1113, train_loss: 0.260401, val loss: 0.392812,  train_metric: -0.902 test_metric: -0.861 lr: 0.00033)\n",
            "epoch: 1114, train_loss: 0.260275, val loss: 0.389507,  train_metric: -0.901 test_metric: -0.859 lr: 0.00033)\n",
            "epoch: 1115, train_loss: 0.256209, val loss: 0.386870,  train_metric: -0.901 test_metric: -0.857 lr: 0.00033)\n",
            "epoch: 1116, train_loss: 0.256262, val loss: 0.387965,  train_metric: -0.907 test_metric: -0.864 lr: 0.00033)\n",
            "epoch: 1117, train_loss: 0.259299, val loss: 0.382770,  train_metric: -0.905 test_metric: -0.856 lr: 0.00033)\n",
            "epoch: 1118, train_loss: 0.257904, val loss: 0.389167,  train_metric: -0.902 test_metric: -0.865 lr: 0.00033)\n",
            "epoch: 1119, train_loss: 0.258726, val loss: 0.392201,  train_metric: -0.904 test_metric: -0.848 lr: 0.00033)\n",
            "epoch: 1120, train_loss: 0.271680, val loss: 0.407559,  train_metric: -0.889 test_metric: -0.843 lr: 0.00033)\n",
            "epoch: 1121, train_loss: 0.264592, val loss: 0.408341,  train_metric: -0.896 test_metric: -0.843 lr: 0.00033)\n",
            "epoch: 1122, train_loss: 0.261417, val loss: 0.383742,  train_metric: -0.901 test_metric: -0.856 lr: 0.00033)\n",
            "epoch: 1123, train_loss: 0.263731, val loss: 0.413363,  train_metric: -0.897 test_metric: -0.839 lr: 0.00032)\n",
            "epoch: 1124, train_loss: 0.263190, val loss: 0.409902,  train_metric: -0.899 test_metric: -0.838 lr: 0.00032)\n",
            "epoch: 1125, train_loss: 0.259667, val loss: 0.399352,  train_metric: -0.898 test_metric: -0.855 lr: 0.00032)\n",
            "epoch: 1126, train_loss: 0.258493, val loss: 0.403636,  train_metric: -0.904 test_metric: -0.843 lr: 0.00032)\n",
            "epoch: 1127, train_loss: 0.260270, val loss: 0.386309,  train_metric: -0.900 test_metric: -0.862 lr: 0.00032)\n",
            "epoch: 1128, train_loss: 0.260183, val loss: 0.392037,  train_metric: -0.909 test_metric: -0.850 lr: 0.00032)\n",
            "epoch: 1129, train_loss: 0.254559, val loss: 0.393871,  train_metric: -0.908 test_metric: -0.849 lr: 0.00032)\n",
            "epoch: 1130, train_loss: 0.256996, val loss: 0.388125,  train_metric: -0.902 test_metric: -0.862 lr: 0.00032)\n",
            "epoch: 1131, train_loss: 0.258130, val loss: 0.401024,  train_metric: -0.902 test_metric: -0.841 lr: 0.00032)\n",
            "epoch: 1132, train_loss: 0.258772, val loss: 0.410996,  train_metric: -0.906 test_metric: -0.841 lr: 0.00032)\n",
            "epoch: 1133, train_loss: 0.255533, val loss: 0.392464,  train_metric: -0.912 test_metric: -0.856 lr: 0.00032)\n",
            "epoch: 1134, train_loss: 0.253958, val loss: 0.412109,  train_metric: -0.907 test_metric: -0.830 lr: 0.00032)\n",
            "epoch: 1135, train_loss: 0.257154, val loss: 0.386821,  train_metric: -0.903 test_metric: -0.856 lr: 0.00032)\n",
            "epoch: 1136, train_loss: 0.251754, val loss: 0.397367,  train_metric: -0.907 test_metric: -0.853 lr: 0.00032)\n",
            "epoch: 1137, train_loss: 0.261959, val loss: 0.399994,  train_metric: -0.900 test_metric: -0.855 lr: 0.00032)\n",
            "epoch: 1138, train_loss: 0.258096, val loss: 0.394175,  train_metric: -0.908 test_metric: -0.851 lr: 0.00032)\n",
            "epoch: 1139, train_loss: 0.256065, val loss: 0.390146,  train_metric: -0.905 test_metric: -0.860 lr: 0.00032)\n",
            "epoch: 1140, train_loss: 0.251229, val loss: 0.382001,  train_metric: -0.911 test_metric: -0.866 lr: 0.00032)\n",
            "epoch: 1141, train_loss: 0.254735, val loss: 0.398829,  train_metric: -0.905 test_metric: -0.857 lr: 0.00032)\n",
            "epoch: 1142, train_loss: 0.255043, val loss: 0.392751,  train_metric: -0.906 test_metric: -0.857 lr: 0.00032)\n",
            "epoch: 1143, train_loss: 0.260063, val loss: 0.377227,  train_metric: -0.905 test_metric: -0.862 lr: 0.00032)\n",
            "epoch: 1144, train_loss: 0.253836, val loss: 0.386025,  train_metric: -0.904 test_metric: -0.865 lr: 0.00032)\n",
            "epoch: 1145, train_loss: 0.254726, val loss: 0.419976,  train_metric: -0.906 test_metric: -0.830 lr: 0.00032)\n",
            "epoch: 1146, train_loss: 0.256348, val loss: 0.391774,  train_metric: -0.910 test_metric: -0.853 lr: 0.00032)\n",
            "epoch: 1147, train_loss: 0.253490, val loss: 0.393371,  train_metric: -0.903 test_metric: -0.846 lr: 0.00032)\n",
            "epoch: 1148, train_loss: 0.251976, val loss: 0.400301,  train_metric: -0.903 test_metric: -0.841 lr: 0.00032)\n",
            "epoch: 1149, train_loss: 0.260122, val loss: 0.401913,  train_metric: -0.909 test_metric: -0.851 lr: 0.00032)\n",
            "epoch: 1150, train_loss: 0.257787, val loss: 0.385719,  train_metric: -0.906 test_metric: -0.859 lr: 0.00032)\n",
            "epoch: 1151, train_loss: 0.251284, val loss: 0.386010,  train_metric: -0.912 test_metric: -0.860 lr: 0.00032)\n",
            "epoch: 1152, train_loss: 0.249959, val loss: 0.390253,  train_metric: -0.904 test_metric: -0.856 lr: 0.00032)\n",
            "epoch: 1153, train_loss: 0.251719, val loss: 0.398109,  train_metric: -0.910 test_metric: -0.848 lr: 0.00032)\n",
            "epoch: 1154, train_loss: 0.260969, val loss: 0.402524,  train_metric: -0.903 test_metric: -0.851 lr: 0.00031)\n",
            "epoch: 1155, train_loss: 0.258006, val loss: 0.382619,  train_metric: -0.908 test_metric: -0.860 lr: 0.00031)\n",
            "epoch: 1156, train_loss: 0.252812, val loss: 0.385898,  train_metric: -0.901 test_metric: -0.861 lr: 0.00031)\n",
            "epoch: 1157, train_loss: 0.251756, val loss: 0.387580,  train_metric: -0.912 test_metric: -0.853 lr: 0.00031)\n",
            "epoch: 1158, train_loss: 0.251929, val loss: 0.420573,  train_metric: -0.906 test_metric: -0.839 lr: 0.00031)\n",
            "epoch: 1159, train_loss: 0.257710, val loss: 0.401640,  train_metric: -0.908 test_metric: -0.841 lr: 0.00031)\n",
            "epoch: 1160, train_loss: 0.250507, val loss: 0.415118,  train_metric: -0.904 test_metric: -0.835 lr: 0.00031)\n",
            "epoch: 1161, train_loss: 0.258222, val loss: 0.388717,  train_metric: -0.901 test_metric: -0.855 lr: 0.00031)\n",
            "epoch: 1162, train_loss: 0.257076, val loss: 0.382977,  train_metric: -0.899 test_metric: -0.859 lr: 0.00031)\n",
            "epoch: 1163, train_loss: 0.263897, val loss: 0.391588,  train_metric: -0.900 test_metric: -0.865 lr: 0.00031)\n",
            "epoch: 1164, train_loss: 0.258623, val loss: 0.397639,  train_metric: -0.898 test_metric: -0.861 lr: 0.00031)\n",
            "epoch: 1165, train_loss: 0.258393, val loss: 0.388499,  train_metric: -0.900 test_metric: -0.859 lr: 0.00031)\n",
            "epoch: 1166, train_loss: 0.254099, val loss: 0.377809,  train_metric: -0.903 test_metric: -0.870 lr: 0.00031)\n",
            "epoch: 1167, train_loss: 0.247314, val loss: 0.384420,  train_metric: -0.909 test_metric: -0.860 lr: 0.00031)\n",
            "epoch: 1168, train_loss: 0.250264, val loss: 0.387801,  train_metric: -0.907 test_metric: -0.854 lr: 0.00031)\n",
            "epoch: 1169, train_loss: 0.252986, val loss: 0.386051,  train_metric: -0.907 test_metric: -0.860 lr: 0.00031)\n",
            "epoch: 1170, train_loss: 0.248916, val loss: 0.389175,  train_metric: -0.911 test_metric: -0.861 lr: 0.00031)\n",
            "epoch: 1171, train_loss: 0.249979, val loss: 0.393662,  train_metric: -0.912 test_metric: -0.848 lr: 0.00031)\n",
            "epoch: 1172, train_loss: 0.248764, val loss: 0.380930,  train_metric: -0.907 test_metric: -0.864 lr: 0.00031)\n",
            "epoch: 1173, train_loss: 0.251521, val loss: 0.378039,  train_metric: -0.907 test_metric: -0.866 lr: 0.00031)\n",
            "epoch: 1174, train_loss: 0.247198, val loss: 0.381328,  train_metric: -0.903 test_metric: -0.866 lr: 0.00031)\n",
            "epoch: 1175, train_loss: 0.249469, val loss: 0.383934,  train_metric: -0.911 test_metric: -0.860 lr: 0.00031)\n",
            "epoch: 1176, train_loss: 0.247204, val loss: 0.381570,  train_metric: -0.905 test_metric: -0.870 lr: 0.00031)\n",
            "epoch: 1177, train_loss: 0.248022, val loss: 0.393531,  train_metric: -0.908 test_metric: -0.859 lr: 0.00031)\n",
            "epoch: 1178, train_loss: 0.252625, val loss: 0.384417,  train_metric: -0.907 test_metric: -0.855 lr: 0.00031)\n",
            "epoch: 1179, train_loss: 0.250191, val loss: 0.389476,  train_metric: -0.911 test_metric: -0.851 lr: 0.00031)\n",
            "epoch: 1180, train_loss: 0.253765, val loss: 0.377652,  train_metric: -0.907 test_metric: -0.870 lr: 0.00031)\n",
            "epoch: 1181, train_loss: 0.255433, val loss: 0.424213,  train_metric: -0.899 test_metric: -0.829 lr: 0.00031)\n",
            "epoch: 1182, train_loss: 0.256510, val loss: 0.385231,  train_metric: -0.898 test_metric: -0.859 lr: 0.00031)\n",
            "epoch: 1183, train_loss: 0.254953, val loss: 0.388435,  train_metric: -0.908 test_metric: -0.853 lr: 0.00031)\n",
            "epoch: 1184, train_loss: 0.252832, val loss: 0.384887,  train_metric: -0.900 test_metric: -0.856 lr: 0.00031)\n",
            "epoch: 1185, train_loss: 0.246801, val loss: 0.383562,  train_metric: -0.913 test_metric: -0.855 lr: 0.00031)\n",
            "epoch: 1186, train_loss: 0.248636, val loss: 0.382722,  train_metric: -0.905 test_metric: -0.861 lr: 0.00030)\n",
            "epoch: 1187, train_loss: 0.251802, val loss: 0.376830,  train_metric: -0.906 test_metric: -0.865 lr: 0.00030)\n",
            "epoch: 1188, train_loss: 0.252751, val loss: 0.391781,  train_metric: -0.906 test_metric: -0.862 lr: 0.00030)\n",
            "epoch: 1189, train_loss: 0.249996, val loss: 0.377893,  train_metric: -0.905 test_metric: -0.855 lr: 0.00030)\n",
            "epoch: 1190, train_loss: 0.247459, val loss: 0.390381,  train_metric: -0.915 test_metric: -0.859 lr: 0.00030)\n",
            "epoch: 1191, train_loss: 0.246108, val loss: 0.387977,  train_metric: -0.914 test_metric: -0.844 lr: 0.00030)\n",
            "epoch: 1192, train_loss: 0.252468, val loss: 0.393857,  train_metric: -0.902 test_metric: -0.857 lr: 0.00030)\n",
            "epoch: 1193, train_loss: 0.245832, val loss: 0.400402,  train_metric: -0.911 test_metric: -0.836 lr: 0.00030)\n",
            "epoch: 1194, train_loss: 0.253123, val loss: 0.395250,  train_metric: -0.907 test_metric: -0.849 lr: 0.00030)\n",
            "epoch: 1195, train_loss: 0.243886, val loss: 0.387977,  train_metric: -0.909 test_metric: -0.848 lr: 0.00030)\n",
            "epoch: 1196, train_loss: 0.247140, val loss: 0.386797,  train_metric: -0.907 test_metric: -0.856 lr: 0.00030)\n",
            "epoch: 1197, train_loss: 0.248764, val loss: 0.383217,  train_metric: -0.903 test_metric: -0.859 lr: 0.00030)\n",
            "epoch: 1198, train_loss: 0.252861, val loss: 0.397638,  train_metric: -0.899 test_metric: -0.841 lr: 0.00030)\n",
            "epoch: 1199, train_loss: 0.259892, val loss: 0.378494,  train_metric: -0.895 test_metric: -0.865 lr: 0.00030)\n",
            "epoch: 1200, train_loss: 0.250728, val loss: 0.380120,  train_metric: -0.910 test_metric: -0.874 lr: 0.00030)\n",
            "epoch: 1201, train_loss: 0.253944, val loss: 0.378661,  train_metric: -0.904 test_metric: -0.872 lr: 0.00030)\n",
            "epoch: 1202, train_loss: 0.246767, val loss: 0.374371,  train_metric: -0.911 test_metric: -0.869 lr: 0.00030)\n",
            "epoch: 1203, train_loss: 0.244863, val loss: 0.382085,  train_metric: -0.912 test_metric: -0.871 lr: 0.00030)\n",
            "epoch: 1204, train_loss: 0.245478, val loss: 0.383782,  train_metric: -0.905 test_metric: -0.855 lr: 0.00030)\n",
            "epoch: 1205, train_loss: 0.244979, val loss: 0.396272,  train_metric: -0.909 test_metric: -0.844 lr: 0.00030)\n",
            "epoch: 1206, train_loss: 0.258553, val loss: 0.388246,  train_metric: -0.902 test_metric: -0.857 lr: 0.00030)\n",
            "epoch: 1207, train_loss: 0.247421, val loss: 0.384652,  train_metric: -0.906 test_metric: -0.848 lr: 0.00030)\n",
            "epoch: 1208, train_loss: 0.247316, val loss: 0.378271,  train_metric: -0.910 test_metric: -0.865 lr: 0.00030)\n",
            "epoch: 1209, train_loss: 0.242555, val loss: 0.385666,  train_metric: -0.909 test_metric: -0.860 lr: 0.00030)\n",
            "epoch: 1210, train_loss: 0.243058, val loss: 0.387063,  train_metric: -0.912 test_metric: -0.850 lr: 0.00030)\n",
            "epoch: 1211, train_loss: 0.240863, val loss: 0.389086,  train_metric: -0.908 test_metric: -0.853 lr: 0.00030)\n",
            "epoch: 1212, train_loss: 0.247134, val loss: 0.421141,  train_metric: -0.910 test_metric: -0.834 lr: 0.00030)\n",
            "epoch: 1213, train_loss: 0.256082, val loss: 0.408489,  train_metric: -0.903 test_metric: -0.845 lr: 0.00030)\n",
            "epoch: 1214, train_loss: 0.251283, val loss: 0.406996,  train_metric: -0.912 test_metric: -0.844 lr: 0.00030)\n",
            "epoch: 1215, train_loss: 0.256441, val loss: 0.394050,  train_metric: -0.902 test_metric: -0.853 lr: 0.00030)\n",
            "epoch: 1216, train_loss: 0.247149, val loss: 0.388277,  train_metric: -0.909 test_metric: -0.865 lr: 0.00030)\n",
            "epoch: 1217, train_loss: 0.252894, val loss: 0.380363,  train_metric: -0.903 test_metric: -0.865 lr: 0.00030)\n",
            "epoch: 1218, train_loss: 0.243709, val loss: 0.370782,  train_metric: -0.911 test_metric: -0.862 lr: 0.00030)\n",
            "epoch: 1219, train_loss: 0.241810, val loss: 0.380705,  train_metric: -0.910 test_metric: -0.870 lr: 0.00030)\n",
            "epoch: 1220, train_loss: 0.242297, val loss: 0.372202,  train_metric: -0.910 test_metric: -0.865 lr: 0.00029)\n",
            "epoch: 1221, train_loss: 0.240292, val loss: 0.402637,  train_metric: -0.912 test_metric: -0.840 lr: 0.00029)\n",
            "epoch: 1222, train_loss: 0.252945, val loss: 0.410792,  train_metric: -0.900 test_metric: -0.839 lr: 0.00029)\n",
            "epoch: 1223, train_loss: 0.252218, val loss: 0.407084,  train_metric: -0.905 test_metric: -0.843 lr: 0.00029)\n",
            "epoch: 1224, train_loss: 0.248216, val loss: 0.377967,  train_metric: -0.904 test_metric: -0.864 lr: 0.00029)\n",
            "epoch: 1225, train_loss: 0.245059, val loss: 0.376999,  train_metric: -0.903 test_metric: -0.866 lr: 0.00029)\n",
            "epoch: 1226, train_loss: 0.241889, val loss: 0.382255,  train_metric: -0.911 test_metric: -0.857 lr: 0.00029)\n",
            "epoch: 1227, train_loss: 0.244316, val loss: 0.377739,  train_metric: -0.906 test_metric: -0.867 lr: 0.00029)\n",
            "epoch: 1228, train_loss: 0.241097, val loss: 0.378809,  train_metric: -0.914 test_metric: -0.854 lr: 0.00029)\n",
            "epoch: 1229, train_loss: 0.237471, val loss: 0.379657,  train_metric: -0.913 test_metric: -0.861 lr: 0.00029)\n",
            "epoch: 1230, train_loss: 0.239998, val loss: 0.386679,  train_metric: -0.913 test_metric: -0.853 lr: 0.00029)\n",
            "epoch: 1231, train_loss: 0.241437, val loss: 0.389919,  train_metric: -0.908 test_metric: -0.853 lr: 0.00029)\n",
            "epoch: 1232, train_loss: 0.244615, val loss: 0.403631,  train_metric: -0.907 test_metric: -0.851 lr: 0.00029)\n",
            "epoch: 1233, train_loss: 0.251103, val loss: 0.388721,  train_metric: -0.897 test_metric: -0.851 lr: 0.00029)\n",
            "epoch: 1234, train_loss: 0.244172, val loss: 0.385963,  train_metric: -0.907 test_metric: -0.853 lr: 0.00029)\n",
            "epoch: 1235, train_loss: 0.250785, val loss: 0.405426,  train_metric: -0.909 test_metric: -0.848 lr: 0.00029)\n",
            "epoch: 1236, train_loss: 0.246089, val loss: 0.379756,  train_metric: -0.909 test_metric: -0.870 lr: 0.00029)\n",
            "epoch: 1237, train_loss: 0.239961, val loss: 0.382559,  train_metric: -0.917 test_metric: -0.866 lr: 0.00029)\n",
            "epoch: 1238, train_loss: 0.238714, val loss: 0.376900,  train_metric: -0.914 test_metric: -0.872 lr: 0.00029)\n",
            "epoch: 1239, train_loss: 0.238744, val loss: 0.377706,  train_metric: -0.913 test_metric: -0.861 lr: 0.00029)\n",
            "epoch: 1240, train_loss: 0.244273, val loss: 0.391035,  train_metric: -0.912 test_metric: -0.851 lr: 0.00029)\n",
            "epoch: 1241, train_loss: 0.245924, val loss: 0.398163,  train_metric: -0.906 test_metric: -0.850 lr: 0.00029)\n",
            "epoch: 1242, train_loss: 0.249599, val loss: 0.394066,  train_metric: -0.907 test_metric: -0.849 lr: 0.00029)\n",
            "epoch: 1243, train_loss: 0.249214, val loss: 0.376037,  train_metric: -0.906 test_metric: -0.867 lr: 0.00029)\n",
            "epoch: 1244, train_loss: 0.239348, val loss: 0.370790,  train_metric: -0.914 test_metric: -0.870 lr: 0.00029)\n",
            "epoch: 1245, train_loss: 0.234288, val loss: 0.371506,  train_metric: -0.917 test_metric: -0.872 lr: 0.00029)\n",
            "epoch: 1246, train_loss: 0.237140, val loss: 0.376124,  train_metric: -0.918 test_metric: -0.861 lr: 0.00029)\n",
            "epoch: 1247, train_loss: 0.243574, val loss: 0.386746,  train_metric: -0.910 test_metric: -0.856 lr: 0.00029)\n",
            "epoch: 1248, train_loss: 0.240925, val loss: 0.376728,  train_metric: -0.912 test_metric: -0.871 lr: 0.00029)\n",
            "epoch: 1249, train_loss: 0.240942, val loss: 0.389248,  train_metric: -0.910 test_metric: -0.860 lr: 0.00029)\n",
            "epoch: 1250, train_loss: 0.238876, val loss: 0.378205,  train_metric: -0.915 test_metric: -0.870 lr: 0.00029)\n",
            "epoch: 1251, train_loss: 0.246687, val loss: 0.419092,  train_metric: -0.906 test_metric: -0.835 lr: 0.00029)\n",
            "epoch: 1252, train_loss: 0.248836, val loss: 0.385924,  train_metric: -0.902 test_metric: -0.854 lr: 0.00029)\n",
            "epoch: 1253, train_loss: 0.241038, val loss: 0.379766,  train_metric: -0.909 test_metric: -0.860 lr: 0.00029)\n",
            "epoch: 1254, train_loss: 0.236203, val loss: 0.371262,  train_metric: -0.915 test_metric: -0.866 lr: 0.00028)\n",
            "epoch: 1255, train_loss: 0.244665, val loss: 0.383501,  train_metric: -0.913 test_metric: -0.856 lr: 0.00028)\n",
            "epoch: 1256, train_loss: 0.244586, val loss: 0.382336,  train_metric: -0.908 test_metric: -0.865 lr: 0.00028)\n",
            "epoch: 1257, train_loss: 0.240961, val loss: 0.383007,  train_metric: -0.911 test_metric: -0.857 lr: 0.00028)\n",
            "epoch: 1258, train_loss: 0.241872, val loss: 0.372738,  train_metric: -0.911 test_metric: -0.866 lr: 0.00028)\n",
            "epoch: 1259, train_loss: 0.238382, val loss: 0.380057,  train_metric: -0.914 test_metric: -0.865 lr: 0.00028)\n",
            "epoch: 1260, train_loss: 0.237864, val loss: 0.367558,  train_metric: -0.912 test_metric: -0.871 lr: 0.00028)\n",
            "epoch: 1261, train_loss: 0.236709, val loss: 0.382045,  train_metric: -0.914 test_metric: -0.857 lr: 0.00028)\n",
            "epoch: 1262, train_loss: 0.238367, val loss: 0.381520,  train_metric: -0.911 test_metric: -0.870 lr: 0.00028)\n",
            "epoch: 1263, train_loss: 0.244749, val loss: 0.399798,  train_metric: -0.907 test_metric: -0.845 lr: 0.00028)\n",
            "epoch: 1264, train_loss: 0.248165, val loss: 0.382670,  train_metric: -0.902 test_metric: -0.871 lr: 0.00028)\n",
            "epoch: 1265, train_loss: 0.248016, val loss: 0.371304,  train_metric: -0.910 test_metric: -0.864 lr: 0.00028)\n",
            "epoch: 1266, train_loss: 0.244352, val loss: 0.373759,  train_metric: -0.905 test_metric: -0.875 lr: 0.00028)\n",
            "epoch: 1267, train_loss: 0.242663, val loss: 0.369511,  train_metric: -0.912 test_metric: -0.871 lr: 0.00028)\n",
            "epoch: 1268, train_loss: 0.238026, val loss: 0.373845,  train_metric: -0.916 test_metric: -0.869 lr: 0.00028)\n",
            "epoch: 1269, train_loss: 0.241886, val loss: 0.374316,  train_metric: -0.910 test_metric: -0.881 lr: 0.00028)\n",
            "epoch: 1270, train_loss: 0.247228, val loss: 0.374327,  train_metric: -0.910 test_metric: -0.870 lr: 0.00028)\n",
            "epoch: 1271, train_loss: 0.242948, val loss: 0.372530,  train_metric: -0.910 test_metric: -0.870 lr: 0.00028)\n",
            "epoch: 1272, train_loss: 0.237295, val loss: 0.375399,  train_metric: -0.909 test_metric: -0.865 lr: 0.00028)\n",
            "epoch: 1273, train_loss: 0.232442, val loss: 0.370343,  train_metric: -0.917 test_metric: -0.881 lr: 0.00028)\n",
            "epoch: 1274, train_loss: 0.234357, val loss: 0.370215,  train_metric: -0.925 test_metric: -0.870 lr: 0.00028)\n",
            "epoch: 1275, train_loss: 0.234142, val loss: 0.370831,  train_metric: -0.911 test_metric: -0.872 lr: 0.00028)\n",
            "epoch: 1276, train_loss: 0.238617, val loss: 0.378512,  train_metric: -0.913 test_metric: -0.871 lr: 0.00028)\n",
            "epoch: 1277, train_loss: 0.239752, val loss: 0.371686,  train_metric: -0.912 test_metric: -0.871 lr: 0.00028)\n",
            "epoch: 1278, train_loss: 0.231702, val loss: 0.368445,  train_metric: -0.916 test_metric: -0.875 lr: 0.00028)\n",
            "epoch: 1279, train_loss: 0.234281, val loss: 0.391546,  train_metric: -0.918 test_metric: -0.854 lr: 0.00028)\n",
            "epoch: 1280, train_loss: 0.242478, val loss: 0.367132,  train_metric: -0.909 test_metric: -0.866 lr: 0.00028)\n",
            "epoch: 1281, train_loss: 0.243410, val loss: 0.374298,  train_metric: -0.913 test_metric: -0.860 lr: 0.00028)\n",
            "epoch: 1282, train_loss: 0.232949, val loss: 0.366738,  train_metric: -0.911 test_metric: -0.869 lr: 0.00028)\n",
            "epoch: 1283, train_loss: 0.233790, val loss: 0.373585,  train_metric: -0.916 test_metric: -0.861 lr: 0.00028)\n",
            "epoch: 1284, train_loss: 0.234875, val loss: 0.378917,  train_metric: -0.919 test_metric: -0.872 lr: 0.00028)\n",
            "epoch: 1285, train_loss: 0.234291, val loss: 0.375226,  train_metric: -0.918 test_metric: -0.869 lr: 0.00028)\n",
            "epoch: 1286, train_loss: 0.239639, val loss: 0.389158,  train_metric: -0.912 test_metric: -0.859 lr: 0.00028)\n",
            "epoch: 1287, train_loss: 0.248299, val loss: 0.400883,  train_metric: -0.906 test_metric: -0.850 lr: 0.00028)\n",
            "epoch: 1288, train_loss: 0.246369, val loss: 0.382021,  train_metric: -0.906 test_metric: -0.857 lr: 0.00028)\n",
            "epoch: 1289, train_loss: 0.232388, val loss: 0.378751,  train_metric: -0.919 test_metric: -0.860 lr: 0.00028)\n",
            "epoch: 1290, train_loss: 0.233795, val loss: 0.374023,  train_metric: -0.918 test_metric: -0.861 lr: 0.00027)\n",
            "epoch: 1291, train_loss: 0.238679, val loss: 0.385215,  train_metric: -0.913 test_metric: -0.864 lr: 0.00027)\n",
            "epoch: 1292, train_loss: 0.242135, val loss: 0.372047,  train_metric: -0.913 test_metric: -0.861 lr: 0.00027)\n",
            "epoch: 1293, train_loss: 0.242767, val loss: 0.371551,  train_metric: -0.910 test_metric: -0.870 lr: 0.00027)\n",
            "epoch: 1294, train_loss: 0.246740, val loss: 0.382356,  train_metric: -0.907 test_metric: -0.862 lr: 0.00027)\n",
            "epoch: 1295, train_loss: 0.238726, val loss: 0.366317,  train_metric: -0.910 test_metric: -0.872 lr: 0.00027)\n",
            "epoch: 1296, train_loss: 0.233031, val loss: 0.368462,  train_metric: -0.912 test_metric: -0.877 lr: 0.00027)\n",
            "epoch: 1297, train_loss: 0.234303, val loss: 0.372286,  train_metric: -0.920 test_metric: -0.874 lr: 0.00027)\n",
            "epoch: 1298, train_loss: 0.231612, val loss: 0.382447,  train_metric: -0.921 test_metric: -0.859 lr: 0.00027)\n",
            "epoch: 1299, train_loss: 0.236336, val loss: 0.388375,  train_metric: -0.915 test_metric: -0.860 lr: 0.00027)\n",
            "epoch: 1300, train_loss: 0.236038, val loss: 0.367220,  train_metric: -0.910 test_metric: -0.870 lr: 0.00027)\n",
            "epoch: 1301, train_loss: 0.233013, val loss: 0.383770,  train_metric: -0.913 test_metric: -0.857 lr: 0.00027)\n",
            "epoch: 1302, train_loss: 0.243556, val loss: 0.422152,  train_metric: -0.905 test_metric: -0.827 lr: 0.00027)\n",
            "epoch: 1303, train_loss: 0.257601, val loss: 0.423481,  train_metric: -0.905 test_metric: -0.835 lr: 0.00027)\n",
            "epoch: 1304, train_loss: 0.251308, val loss: 0.399145,  train_metric: -0.915 test_metric: -0.844 lr: 0.00027)\n",
            "epoch: 1305, train_loss: 0.239966, val loss: 0.387726,  train_metric: -0.910 test_metric: -0.864 lr: 0.00027)\n",
            "epoch: 1306, train_loss: 0.233363, val loss: 0.375555,  train_metric: -0.911 test_metric: -0.864 lr: 0.00027)\n",
            "epoch: 1307, train_loss: 0.229825, val loss: 0.364965,  train_metric: -0.918 test_metric: -0.874 lr: 0.00027)\n",
            "epoch: 1308, train_loss: 0.230695, val loss: 0.373806,  train_metric: -0.917 test_metric: -0.865 lr: 0.00027)\n",
            "epoch: 1309, train_loss: 0.229521, val loss: 0.368630,  train_metric: -0.916 test_metric: -0.875 lr: 0.00027)\n",
            "epoch: 1310, train_loss: 0.229612, val loss: 0.372070,  train_metric: -0.915 test_metric: -0.871 lr: 0.00027)\n",
            "epoch: 1311, train_loss: 0.229826, val loss: 0.371469,  train_metric: -0.915 test_metric: -0.862 lr: 0.00027)\n",
            "epoch: 1312, train_loss: 0.228807, val loss: 0.381229,  train_metric: -0.921 test_metric: -0.870 lr: 0.00027)\n",
            "epoch: 1313, train_loss: 0.237181, val loss: 0.369938,  train_metric: -0.914 test_metric: -0.869 lr: 0.00027)\n",
            "epoch: 1314, train_loss: 0.233385, val loss: 0.372674,  train_metric: -0.915 test_metric: -0.879 lr: 0.00027)\n",
            "epoch: 1315, train_loss: 0.235735, val loss: 0.369781,  train_metric: -0.914 test_metric: -0.874 lr: 0.00027)\n",
            "epoch: 1316, train_loss: 0.228717, val loss: 0.369434,  train_metric: -0.927 test_metric: -0.870 lr: 0.00027)\n",
            "epoch: 1317, train_loss: 0.228785, val loss: 0.363148,  train_metric: -0.919 test_metric: -0.871 lr: 0.00027)\n",
            "epoch: 1318, train_loss: 0.229284, val loss: 0.383439,  train_metric: -0.925 test_metric: -0.856 lr: 0.00027)\n",
            "epoch: 1319, train_loss: 0.230841, val loss: 0.371077,  train_metric: -0.912 test_metric: -0.870 lr: 0.00027)\n",
            "epoch: 1320, train_loss: 0.227829, val loss: 0.371193,  train_metric: -0.917 test_metric: -0.862 lr: 0.00027)\n",
            "epoch: 1321, train_loss: 0.229372, val loss: 0.368918,  train_metric: -0.916 test_metric: -0.871 lr: 0.00027)\n",
            "epoch: 1322, train_loss: 0.232917, val loss: 0.366276,  train_metric: -0.920 test_metric: -0.876 lr: 0.00027)\n",
            "epoch: 1323, train_loss: 0.231397, val loss: 0.371001,  train_metric: -0.919 test_metric: -0.872 lr: 0.00027)\n",
            "epoch: 1324, train_loss: 0.228785, val loss: 0.371124,  train_metric: -0.919 test_metric: -0.875 lr: 0.00027)\n",
            "epoch: 1325, train_loss: 0.232714, val loss: 0.391878,  train_metric: -0.911 test_metric: -0.855 lr: 0.00027)\n",
            "epoch: 1326, train_loss: 0.231304, val loss: 0.380736,  train_metric: -0.915 test_metric: -0.862 lr: 0.00027)\n",
            "epoch: 1327, train_loss: 0.230210, val loss: 0.377369,  train_metric: -0.916 test_metric: -0.866 lr: 0.00026)\n",
            "epoch: 1328, train_loss: 0.234820, val loss: 0.377976,  train_metric: -0.913 test_metric: -0.867 lr: 0.00026)\n",
            "epoch: 1329, train_loss: 0.232564, val loss: 0.380948,  train_metric: -0.914 test_metric: -0.859 lr: 0.00026)\n",
            "epoch: 1330, train_loss: 0.229162, val loss: 0.367829,  train_metric: -0.919 test_metric: -0.869 lr: 0.00026)\n",
            "epoch: 1331, train_loss: 0.228195, val loss: 0.384505,  train_metric: -0.915 test_metric: -0.861 lr: 0.00026)\n",
            "epoch: 1332, train_loss: 0.234679, val loss: 0.366819,  train_metric: -0.914 test_metric: -0.877 lr: 0.00026)\n",
            "epoch: 1333, train_loss: 0.227575, val loss: 0.373714,  train_metric: -0.922 test_metric: -0.865 lr: 0.00026)\n",
            "epoch: 1334, train_loss: 0.229993, val loss: 0.364397,  train_metric: -0.919 test_metric: -0.876 lr: 0.00026)\n",
            "epoch: 1335, train_loss: 0.229245, val loss: 0.365604,  train_metric: -0.918 test_metric: -0.877 lr: 0.00026)\n",
            "epoch: 1336, train_loss: 0.227403, val loss: 0.381137,  train_metric: -0.917 test_metric: -0.851 lr: 0.00026)\n",
            "epoch: 1337, train_loss: 0.231443, val loss: 0.362157,  train_metric: -0.918 test_metric: -0.871 lr: 0.00026)\n",
            "epoch: 1338, train_loss: 0.228835, val loss: 0.377555,  train_metric: -0.913 test_metric: -0.866 lr: 0.00026)\n",
            "epoch: 1339, train_loss: 0.228688, val loss: 0.366864,  train_metric: -0.915 test_metric: -0.881 lr: 0.00026)\n",
            "epoch: 1340, train_loss: 0.229676, val loss: 0.360646,  train_metric: -0.919 test_metric: -0.877 lr: 0.00026)\n",
            "epoch: 1341, train_loss: 0.229108, val loss: 0.363168,  train_metric: -0.919 test_metric: -0.882 lr: 0.00026)\n",
            "epoch: 1342, train_loss: 0.226920, val loss: 0.371356,  train_metric: -0.916 test_metric: -0.862 lr: 0.00026)\n",
            "epoch: 1343, train_loss: 0.225878, val loss: 0.365597,  train_metric: -0.918 test_metric: -0.882 lr: 0.00026)\n",
            "epoch: 1344, train_loss: 0.226982, val loss: 0.364699,  train_metric: -0.922 test_metric: -0.872 lr: 0.00026)\n",
            "epoch: 1345, train_loss: 0.229345, val loss: 0.373264,  train_metric: -0.915 test_metric: -0.870 lr: 0.00026)\n",
            "epoch: 1346, train_loss: 0.225293, val loss: 0.369485,  train_metric: -0.921 test_metric: -0.870 lr: 0.00026)\n",
            "epoch: 1347, train_loss: 0.226478, val loss: 0.369747,  train_metric: -0.917 test_metric: -0.869 lr: 0.00026)\n",
            "epoch: 1348, train_loss: 0.231837, val loss: 0.369573,  train_metric: -0.916 test_metric: -0.877 lr: 0.00026)\n",
            "epoch: 1349, train_loss: 0.242099, val loss: 0.371747,  train_metric: -0.917 test_metric: -0.869 lr: 0.00026)\n",
            "epoch: 1350, train_loss: 0.230818, val loss: 0.368225,  train_metric: -0.916 test_metric: -0.874 lr: 0.00026)\n",
            "epoch: 1351, train_loss: 0.228601, val loss: 0.368648,  train_metric: -0.918 test_metric: -0.872 lr: 0.00026)\n",
            "epoch: 1352, train_loss: 0.224761, val loss: 0.370955,  train_metric: -0.922 test_metric: -0.880 lr: 0.00026)\n",
            "epoch: 1353, train_loss: 0.232263, val loss: 0.376485,  train_metric: -0.917 test_metric: -0.857 lr: 0.00026)\n",
            "epoch: 1354, train_loss: 0.228127, val loss: 0.369734,  train_metric: -0.913 test_metric: -0.872 lr: 0.00026)\n",
            "epoch: 1355, train_loss: 0.224903, val loss: 0.361624,  train_metric: -0.920 test_metric: -0.875 lr: 0.00026)\n",
            "epoch: 1356, train_loss: 0.227911, val loss: 0.372853,  train_metric: -0.918 test_metric: -0.877 lr: 0.00026)\n",
            "epoch: 1357, train_loss: 0.226226, val loss: 0.367554,  train_metric: -0.916 test_metric: -0.867 lr: 0.00026)\n",
            "epoch: 1358, train_loss: 0.225457, val loss: 0.364865,  train_metric: -0.918 test_metric: -0.876 lr: 0.00026)\n",
            "epoch: 1359, train_loss: 0.226234, val loss: 0.363602,  train_metric: -0.926 test_metric: -0.870 lr: 0.00026)\n",
            "epoch: 1360, train_loss: 0.225808, val loss: 0.367285,  train_metric: -0.924 test_metric: -0.874 lr: 0.00026)\n",
            "epoch: 1361, train_loss: 0.226476, val loss: 0.370408,  train_metric: -0.918 test_metric: -0.870 lr: 0.00026)\n",
            "epoch: 1362, train_loss: 0.231388, val loss: 0.368895,  train_metric: -0.917 test_metric: -0.872 lr: 0.00026)\n",
            "epoch: 1363, train_loss: 0.230671, val loss: 0.365547,  train_metric: -0.913 test_metric: -0.876 lr: 0.00026)\n",
            "epoch: 1364, train_loss: 0.227407, val loss: 0.375801,  train_metric: -0.925 test_metric: -0.877 lr: 0.00026)\n",
            "epoch: 1365, train_loss: 0.233348, val loss: 0.371042,  train_metric: -0.921 test_metric: -0.864 lr: 0.00025)\n",
            "epoch: 1366, train_loss: 0.229464, val loss: 0.383996,  train_metric: -0.918 test_metric: -0.866 lr: 0.00025)\n",
            "epoch: 1367, train_loss: 0.232003, val loss: 0.368347,  train_metric: -0.916 test_metric: -0.865 lr: 0.00025)\n",
            "epoch: 1368, train_loss: 0.225776, val loss: 0.369011,  train_metric: -0.915 test_metric: -0.872 lr: 0.00025)\n",
            "epoch: 1369, train_loss: 0.226154, val loss: 0.363932,  train_metric: -0.918 test_metric: -0.881 lr: 0.00025)\n",
            "epoch: 1370, train_loss: 0.225869, val loss: 0.369069,  train_metric: -0.924 test_metric: -0.869 lr: 0.00025)\n",
            "epoch: 1371, train_loss: 0.225540, val loss: 0.364436,  train_metric: -0.923 test_metric: -0.880 lr: 0.00025)\n",
            "epoch: 1372, train_loss: 0.227231, val loss: 0.373268,  train_metric: -0.921 test_metric: -0.870 lr: 0.00025)\n",
            "epoch: 1373, train_loss: 0.231624, val loss: 0.381185,  train_metric: -0.915 test_metric: -0.859 lr: 0.00025)\n",
            "epoch: 1374, train_loss: 0.236778, val loss: 0.366855,  train_metric: -0.906 test_metric: -0.881 lr: 0.00025)\n",
            "epoch: 1375, train_loss: 0.230072, val loss: 0.378082,  train_metric: -0.919 test_metric: -0.854 lr: 0.00025)\n",
            "epoch: 1376, train_loss: 0.228053, val loss: 0.371286,  train_metric: -0.915 test_metric: -0.874 lr: 0.00025)\n",
            "epoch: 1377, train_loss: 0.227279, val loss: 0.361299,  train_metric: -0.918 test_metric: -0.885 lr: 0.00025)\n",
            "epoch: 1378, train_loss: 0.230230, val loss: 0.371507,  train_metric: -0.913 test_metric: -0.876 lr: 0.00025)\n",
            "epoch: 1379, train_loss: 0.226308, val loss: 0.371093,  train_metric: -0.916 test_metric: -0.869 lr: 0.00025)\n",
            "epoch: 1380, train_loss: 0.228415, val loss: 0.387262,  train_metric: -0.914 test_metric: -0.862 lr: 0.00025)\n",
            "epoch: 1381, train_loss: 0.236777, val loss: 0.364837,  train_metric: -0.912 test_metric: -0.872 lr: 0.00025)\n",
            "epoch: 1382, train_loss: 0.223995, val loss: 0.372279,  train_metric: -0.925 test_metric: -0.870 lr: 0.00025)\n",
            "epoch: 1383, train_loss: 0.224869, val loss: 0.362677,  train_metric: -0.922 test_metric: -0.880 lr: 0.00025)\n",
            "epoch: 1384, train_loss: 0.225073, val loss: 0.377253,  train_metric: -0.917 test_metric: -0.867 lr: 0.00025)\n",
            "epoch: 1385, train_loss: 0.225688, val loss: 0.364616,  train_metric: -0.922 test_metric: -0.875 lr: 0.00025)\n",
            "epoch: 1386, train_loss: 0.224078, val loss: 0.362174,  train_metric: -0.916 test_metric: -0.880 lr: 0.00025)\n",
            "epoch: 1387, train_loss: 0.225001, val loss: 0.365130,  train_metric: -0.918 test_metric: -0.880 lr: 0.00025)\n",
            "epoch: 1388, train_loss: 0.219988, val loss: 0.379863,  train_metric: -0.927 test_metric: -0.854 lr: 0.00025)\n",
            "epoch: 1389, train_loss: 0.225639, val loss: 0.374572,  train_metric: -0.917 test_metric: -0.872 lr: 0.00025)\n",
            "epoch: 1390, train_loss: 0.223292, val loss: 0.375124,  train_metric: -0.921 test_metric: -0.864 lr: 0.00025)\n",
            "epoch: 1391, train_loss: 0.224644, val loss: 0.365551,  train_metric: -0.917 test_metric: -0.869 lr: 0.00025)\n",
            "epoch: 1392, train_loss: 0.222498, val loss: 0.366814,  train_metric: -0.915 test_metric: -0.875 lr: 0.00025)\n",
            "epoch: 1393, train_loss: 0.223648, val loss: 0.371948,  train_metric: -0.920 test_metric: -0.870 lr: 0.00025)\n",
            "epoch: 1394, train_loss: 0.223320, val loss: 0.372255,  train_metric: -0.915 test_metric: -0.865 lr: 0.00025)\n",
            "epoch: 1395, train_loss: 0.222802, val loss: 0.362251,  train_metric: -0.921 test_metric: -0.872 lr: 0.00025)\n",
            "epoch: 1396, train_loss: 0.220542, val loss: 0.359760,  train_metric: -0.925 test_metric: -0.882 lr: 0.00025)\n",
            "epoch: 1397, train_loss: 0.219390, val loss: 0.364398,  train_metric: -0.923 test_metric: -0.879 lr: 0.00025)\n",
            "epoch: 1398, train_loss: 0.219704, val loss: 0.371345,  train_metric: -0.923 test_metric: -0.871 lr: 0.00025)\n",
            "epoch: 1399, train_loss: 0.221265, val loss: 0.383654,  train_metric: -0.920 test_metric: -0.867 lr: 0.00025)\n",
            "epoch: 1400, train_loss: 0.225189, val loss: 0.365829,  train_metric: -0.918 test_metric: -0.875 lr: 0.00025)\n",
            "epoch: 1401, train_loss: 0.220842, val loss: 0.362913,  train_metric: -0.920 test_metric: -0.876 lr: 0.00025)\n",
            "epoch: 1402, train_loss: 0.221583, val loss: 0.359847,  train_metric: -0.922 test_metric: -0.877 lr: 0.00025)\n",
            "epoch: 1403, train_loss: 0.221585, val loss: 0.359172,  train_metric: -0.921 test_metric: -0.884 lr: 0.00025)\n",
            "epoch: 1404, train_loss: 0.221645, val loss: 0.368443,  train_metric: -0.923 test_metric: -0.867 lr: 0.00025)\n",
            "epoch: 1405, train_loss: 0.218760, val loss: 0.375106,  train_metric: -0.922 test_metric: -0.872 lr: 0.00024)\n",
            "epoch: 1406, train_loss: 0.229625, val loss: 0.364186,  train_metric: -0.918 test_metric: -0.872 lr: 0.00024)\n",
            "epoch: 1407, train_loss: 0.232009, val loss: 0.363961,  train_metric: -0.913 test_metric: -0.881 lr: 0.00024)\n",
            "epoch: 1408, train_loss: 0.221919, val loss: 0.361443,  train_metric: -0.926 test_metric: -0.871 lr: 0.00024)\n",
            "epoch: 1409, train_loss: 0.225868, val loss: 0.372654,  train_metric: -0.919 test_metric: -0.871 lr: 0.00024)\n",
            "epoch: 1410, train_loss: 0.221940, val loss: 0.366015,  train_metric: -0.920 test_metric: -0.871 lr: 0.00024)\n",
            "epoch: 1411, train_loss: 0.220634, val loss: 0.374015,  train_metric: -0.921 test_metric: -0.869 lr: 0.00024)\n",
            "epoch: 1412, train_loss: 0.226486, val loss: 0.368639,  train_metric: -0.911 test_metric: -0.871 lr: 0.00024)\n",
            "epoch: 1413, train_loss: 0.221484, val loss: 0.362862,  train_metric: -0.919 test_metric: -0.884 lr: 0.00024)\n",
            "epoch: 1414, train_loss: 0.226738, val loss: 0.360701,  train_metric: -0.918 test_metric: -0.874 lr: 0.00024)\n",
            "epoch: 1415, train_loss: 0.221776, val loss: 0.366174,  train_metric: -0.921 test_metric: -0.876 lr: 0.00024)\n",
            "epoch: 1416, train_loss: 0.220194, val loss: 0.364883,  train_metric: -0.922 test_metric: -0.871 lr: 0.00024)\n",
            "epoch: 1417, train_loss: 0.222780, val loss: 0.361904,  train_metric: -0.921 test_metric: -0.876 lr: 0.00024)\n",
            "epoch: 1418, train_loss: 0.220688, val loss: 0.356572,  train_metric: -0.923 test_metric: -0.882 lr: 0.00024)\n",
            "epoch: 1419, train_loss: 0.222199, val loss: 0.361767,  train_metric: -0.921 test_metric: -0.872 lr: 0.00024)\n",
            "epoch: 1420, train_loss: 0.220041, val loss: 0.381843,  train_metric: -0.921 test_metric: -0.870 lr: 0.00024)\n",
            "epoch: 1421, train_loss: 0.222504, val loss: 0.368575,  train_metric: -0.927 test_metric: -0.869 lr: 0.00024)\n",
            "epoch: 1422, train_loss: 0.220054, val loss: 0.359172,  train_metric: -0.921 test_metric: -0.875 lr: 0.00024)\n",
            "epoch: 1423, train_loss: 0.218543, val loss: 0.373099,  train_metric: -0.926 test_metric: -0.862 lr: 0.00024)\n",
            "epoch: 1424, train_loss: 0.221787, val loss: 0.375499,  train_metric: -0.918 test_metric: -0.866 lr: 0.00024)\n",
            "epoch: 1425, train_loss: 0.215815, val loss: 0.361234,  train_metric: -0.931 test_metric: -0.880 lr: 0.00024)\n",
            "epoch: 1426, train_loss: 0.224315, val loss: 0.371254,  train_metric: -0.923 test_metric: -0.865 lr: 0.00024)\n",
            "epoch: 1427, train_loss: 0.225398, val loss: 0.361720,  train_metric: -0.919 test_metric: -0.876 lr: 0.00024)\n",
            "epoch: 1428, train_loss: 0.217085, val loss: 0.368665,  train_metric: -0.925 test_metric: -0.871 lr: 0.00024)\n",
            "epoch: 1429, train_loss: 0.217512, val loss: 0.365295,  train_metric: -0.922 test_metric: -0.874 lr: 0.00024)\n",
            "epoch: 1430, train_loss: 0.215458, val loss: 0.360833,  train_metric: -0.926 test_metric: -0.872 lr: 0.00024)\n",
            "epoch: 1431, train_loss: 0.219197, val loss: 0.362259,  train_metric: -0.915 test_metric: -0.879 lr: 0.00024)\n",
            "epoch: 1432, train_loss: 0.221150, val loss: 0.370984,  train_metric: -0.931 test_metric: -0.862 lr: 0.00024)\n",
            "epoch: 1433, train_loss: 0.227150, val loss: 0.428020,  train_metric: -0.917 test_metric: -0.845 lr: 0.00024)\n",
            "epoch: 1434, train_loss: 0.243364, val loss: 0.383511,  train_metric: -0.902 test_metric: -0.848 lr: 0.00024)\n",
            "epoch: 1435, train_loss: 0.233823, val loss: 0.372055,  train_metric: -0.906 test_metric: -0.879 lr: 0.00024)\n",
            "epoch: 1436, train_loss: 0.224145, val loss: 0.359023,  train_metric: -0.915 test_metric: -0.877 lr: 0.00024)\n",
            "epoch: 1437, train_loss: 0.218849, val loss: 0.362959,  train_metric: -0.927 test_metric: -0.870 lr: 0.00024)\n",
            "epoch: 1438, train_loss: 0.219647, val loss: 0.362139,  train_metric: -0.926 test_metric: -0.884 lr: 0.00024)\n",
            "epoch: 1439, train_loss: 0.223218, val loss: 0.355587,  train_metric: -0.920 test_metric: -0.885 lr: 0.00024)\n",
            "epoch: 1440, train_loss: 0.219908, val loss: 0.360901,  train_metric: -0.923 test_metric: -0.881 lr: 0.00024)\n",
            "epoch: 1441, train_loss: 0.220580, val loss: 0.366239,  train_metric: -0.920 test_metric: -0.875 lr: 0.00024)\n",
            "epoch: 1442, train_loss: 0.220035, val loss: 0.367450,  train_metric: -0.924 test_metric: -0.875 lr: 0.00024)\n",
            "epoch: 1443, train_loss: 0.218294, val loss: 0.371544,  train_metric: -0.924 test_metric: -0.872 lr: 0.00024)\n",
            "epoch: 1444, train_loss: 0.219457, val loss: 0.374450,  train_metric: -0.923 test_metric: -0.861 lr: 0.00024)\n",
            "epoch: 1445, train_loss: 0.227063, val loss: 0.355352,  train_metric: -0.917 test_metric: -0.879 lr: 0.00024)\n",
            "epoch: 1446, train_loss: 0.220260, val loss: 0.358437,  train_metric: -0.923 test_metric: -0.885 lr: 0.00024)\n",
            "epoch: 1447, train_loss: 0.220426, val loss: 0.357890,  train_metric: -0.921 test_metric: -0.875 lr: 0.00023)\n",
            "epoch: 1448, train_loss: 0.218872, val loss: 0.358294,  train_metric: -0.923 test_metric: -0.882 lr: 0.00023)\n",
            "epoch: 1449, train_loss: 0.223333, val loss: 0.373451,  train_metric: -0.919 test_metric: -0.856 lr: 0.00023)\n",
            "epoch: 1450, train_loss: 0.219022, val loss: 0.359455,  train_metric: -0.919 test_metric: -0.876 lr: 0.00023)\n",
            "epoch: 1451, train_loss: 0.216394, val loss: 0.357713,  train_metric: -0.929 test_metric: -0.881 lr: 0.00023)\n",
            "epoch: 1452, train_loss: 0.215823, val loss: 0.360398,  train_metric: -0.921 test_metric: -0.870 lr: 0.00023)\n",
            "epoch: 1453, train_loss: 0.216786, val loss: 0.375206,  train_metric: -0.923 test_metric: -0.869 lr: 0.00023)\n",
            "epoch: 1454, train_loss: 0.220156, val loss: 0.374811,  train_metric: -0.925 test_metric: -0.857 lr: 0.00023)\n",
            "epoch: 1455, train_loss: 0.222143, val loss: 0.378472,  train_metric: -0.917 test_metric: -0.860 lr: 0.00023)\n",
            "epoch: 1456, train_loss: 0.216463, val loss: 0.358998,  train_metric: -0.922 test_metric: -0.884 lr: 0.00023)\n",
            "epoch: 1457, train_loss: 0.217186, val loss: 0.364900,  train_metric: -0.927 test_metric: -0.872 lr: 0.00023)\n",
            "epoch: 1458, train_loss: 0.219220, val loss: 0.378712,  train_metric: -0.922 test_metric: -0.875 lr: 0.00023)\n",
            "epoch: 1459, train_loss: 0.221276, val loss: 0.382807,  train_metric: -0.928 test_metric: -0.853 lr: 0.00023)\n",
            "epoch: 1460, train_loss: 0.218508, val loss: 0.376014,  train_metric: -0.923 test_metric: -0.869 lr: 0.00023)\n",
            "epoch: 1461, train_loss: 0.219790, val loss: 0.366378,  train_metric: -0.929 test_metric: -0.869 lr: 0.00023)\n",
            "epoch: 1462, train_loss: 0.218796, val loss: 0.361886,  train_metric: -0.919 test_metric: -0.871 lr: 0.00023)\n",
            "epoch: 1463, train_loss: 0.219641, val loss: 0.358403,  train_metric: -0.922 test_metric: -0.876 lr: 0.00023)\n",
            "epoch: 1464, train_loss: 0.217241, val loss: 0.374473,  train_metric: -0.926 test_metric: -0.872 lr: 0.00023)\n",
            "epoch: 1465, train_loss: 0.221336, val loss: 0.371202,  train_metric: -0.922 test_metric: -0.874 lr: 0.00023)\n",
            "epoch: 1466, train_loss: 0.219023, val loss: 0.359811,  train_metric: -0.919 test_metric: -0.876 lr: 0.00023)\n",
            "epoch: 1467, train_loss: 0.217343, val loss: 0.357976,  train_metric: -0.923 test_metric: -0.880 lr: 0.00023)\n",
            "epoch: 1468, train_loss: 0.218224, val loss: 0.362158,  train_metric: -0.923 test_metric: -0.875 lr: 0.00023)\n",
            "epoch: 1469, train_loss: 0.215805, val loss: 0.366066,  train_metric: -0.923 test_metric: -0.876 lr: 0.00023)\n",
            "epoch: 1470, train_loss: 0.214398, val loss: 0.365268,  train_metric: -0.924 test_metric: -0.874 lr: 0.00023)\n",
            "epoch: 1471, train_loss: 0.220357, val loss: 0.358164,  train_metric: -0.920 test_metric: -0.865 lr: 0.00023)\n",
            "epoch: 1472, train_loss: 0.214833, val loss: 0.359199,  train_metric: -0.927 test_metric: -0.880 lr: 0.00023)\n",
            "epoch: 1473, train_loss: 0.214160, val loss: 0.359398,  train_metric: -0.927 test_metric: -0.875 lr: 0.00023)\n",
            "epoch: 1474, train_loss: 0.213328, val loss: 0.356906,  train_metric: -0.928 test_metric: -0.893 lr: 0.00023)\n",
            "epoch: 1475, train_loss: 0.216613, val loss: 0.362748,  train_metric: -0.926 test_metric: -0.875 lr: 0.00023)\n",
            "epoch: 1476, train_loss: 0.218464, val loss: 0.352640,  train_metric: -0.921 test_metric: -0.875 lr: 0.00023)\n",
            "epoch: 1477, train_loss: 0.214981, val loss: 0.359280,  train_metric: -0.927 test_metric: -0.886 lr: 0.00023)\n",
            "epoch: 1478, train_loss: 0.217341, val loss: 0.378343,  train_metric: -0.921 test_metric: -0.851 lr: 0.00023)\n",
            "epoch: 1479, train_loss: 0.221117, val loss: 0.359580,  train_metric: -0.919 test_metric: -0.885 lr: 0.00023)\n",
            "epoch: 1480, train_loss: 0.218189, val loss: 0.360380,  train_metric: -0.919 test_metric: -0.880 lr: 0.00023)\n",
            "epoch: 1481, train_loss: 0.217227, val loss: 0.362865,  train_metric: -0.929 test_metric: -0.880 lr: 0.00023)\n",
            "epoch: 1482, train_loss: 0.215121, val loss: 0.354914,  train_metric: -0.925 test_metric: -0.877 lr: 0.00023)\n",
            "epoch: 1483, train_loss: 0.212764, val loss: 0.357351,  train_metric: -0.929 test_metric: -0.879 lr: 0.00023)\n",
            "epoch: 1484, train_loss: 0.222146, val loss: 0.363885,  train_metric: -0.923 test_metric: -0.875 lr: 0.00023)\n",
            "epoch: 1485, train_loss: 0.216976, val loss: 0.379480,  train_metric: -0.919 test_metric: -0.854 lr: 0.00023)\n",
            "epoch: 1486, train_loss: 0.217647, val loss: 0.358145,  train_metric: -0.920 test_metric: -0.885 lr: 0.00023)\n",
            "epoch: 1487, train_loss: 0.216310, val loss: 0.353687,  train_metric: -0.925 test_metric: -0.880 lr: 0.00023)\n",
            "epoch: 1488, train_loss: 0.215580, val loss: 0.365848,  train_metric: -0.925 test_metric: -0.864 lr: 0.00023)\n",
            "epoch: 1489, train_loss: 0.212019, val loss: 0.354426,  train_metric: -0.922 test_metric: -0.879 lr: 0.00023)\n",
            "epoch: 1490, train_loss: 0.212353, val loss: 0.380894,  train_metric: -0.927 test_metric: -0.856 lr: 0.00022)\n",
            "epoch: 1491, train_loss: 0.221421, val loss: 0.376311,  train_metric: -0.917 test_metric: -0.867 lr: 0.00022)\n",
            "epoch: 1492, train_loss: 0.215151, val loss: 0.391842,  train_metric: -0.924 test_metric: -0.853 lr: 0.00022)\n",
            "epoch: 1493, train_loss: 0.215296, val loss: 0.357263,  train_metric: -0.924 test_metric: -0.877 lr: 0.00022)\n",
            "epoch: 1494, train_loss: 0.210249, val loss: 0.366450,  train_metric: -0.934 test_metric: -0.861 lr: 0.00022)\n",
            "epoch: 1495, train_loss: 0.216559, val loss: 0.357144,  train_metric: -0.924 test_metric: -0.881 lr: 0.00022)\n",
            "epoch: 1496, train_loss: 0.214844, val loss: 0.367594,  train_metric: -0.929 test_metric: -0.872 lr: 0.00022)\n",
            "epoch: 1497, train_loss: 0.214015, val loss: 0.358528,  train_metric: -0.927 test_metric: -0.877 lr: 0.00022)\n",
            "epoch: 1498, train_loss: 0.212168, val loss: 0.357783,  train_metric: -0.920 test_metric: -0.879 lr: 0.00022)\n",
            "epoch: 1499, train_loss: 0.223033, val loss: 0.362596,  train_metric: -0.917 test_metric: -0.875 lr: 0.00022)\n",
            "epoch: 1500, train_loss: 0.223397, val loss: 0.358808,  train_metric: -0.918 test_metric: -0.877 lr: 0.00022)\n",
            "epoch: 1501, train_loss: 0.214500, val loss: 0.357064,  train_metric: -0.930 test_metric: -0.880 lr: 0.00022)\n",
            "epoch: 1502, train_loss: 0.218524, val loss: 0.358757,  train_metric: -0.920 test_metric: -0.875 lr: 0.00022)\n",
            "epoch: 1503, train_loss: 0.214096, val loss: 0.357501,  train_metric: -0.921 test_metric: -0.875 lr: 0.00022)\n",
            "epoch: 1504, train_loss: 0.212451, val loss: 0.353708,  train_metric: -0.929 test_metric: -0.879 lr: 0.00022)\n",
            "epoch: 1505, train_loss: 0.216091, val loss: 0.383685,  train_metric: -0.928 test_metric: -0.856 lr: 0.00022)\n",
            "epoch: 1506, train_loss: 0.212035, val loss: 0.362869,  train_metric: -0.928 test_metric: -0.879 lr: 0.00022)\n",
            "epoch: 1507, train_loss: 0.213077, val loss: 0.366831,  train_metric: -0.926 test_metric: -0.879 lr: 0.00022)\n",
            "epoch: 1508, train_loss: 0.214063, val loss: 0.350298,  train_metric: -0.921 test_metric: -0.875 lr: 0.00022)\n",
            "epoch: 1509, train_loss: 0.209783, val loss: 0.366985,  train_metric: -0.931 test_metric: -0.876 lr: 0.00022)\n",
            "epoch: 1510, train_loss: 0.216791, val loss: 0.366611,  train_metric: -0.927 test_metric: -0.869 lr: 0.00022)\n",
            "epoch: 1511, train_loss: 0.218195, val loss: 0.362772,  train_metric: -0.921 test_metric: -0.877 lr: 0.00022)\n",
            "epoch: 1512, train_loss: 0.212020, val loss: 0.356913,  train_metric: -0.923 test_metric: -0.872 lr: 0.00022)\n",
            "epoch: 1513, train_loss: 0.209173, val loss: 0.356459,  train_metric: -0.930 test_metric: -0.871 lr: 0.00022)\n",
            "epoch: 1514, train_loss: 0.210340, val loss: 0.365832,  train_metric: -0.926 test_metric: -0.877 lr: 0.00022)\n",
            "epoch: 1515, train_loss: 0.210576, val loss: 0.359880,  train_metric: -0.929 test_metric: -0.870 lr: 0.00022)\n",
            "epoch: 1516, train_loss: 0.213464, val loss: 0.354567,  train_metric: -0.921 test_metric: -0.886 lr: 0.00022)\n",
            "epoch: 1517, train_loss: 0.213266, val loss: 0.361036,  train_metric: -0.930 test_metric: -0.879 lr: 0.00022)\n",
            "epoch: 1518, train_loss: 0.211171, val loss: 0.362965,  train_metric: -0.924 test_metric: -0.865 lr: 0.00022)\n",
            "epoch: 1519, train_loss: 0.210287, val loss: 0.369063,  train_metric: -0.926 test_metric: -0.865 lr: 0.00022)\n",
            "epoch: 1520, train_loss: 0.216166, val loss: 0.360603,  train_metric: -0.921 test_metric: -0.875 lr: 0.00022)\n",
            "epoch: 1521, train_loss: 0.214521, val loss: 0.355222,  train_metric: -0.930 test_metric: -0.879 lr: 0.00022)\n",
            "epoch: 1522, train_loss: 0.217281, val loss: 0.351637,  train_metric: -0.923 test_metric: -0.881 lr: 0.00022)\n",
            "epoch: 1523, train_loss: 0.212377, val loss: 0.371567,  train_metric: -0.930 test_metric: -0.869 lr: 0.00022)\n",
            "epoch: 1524, train_loss: 0.212068, val loss: 0.361831,  train_metric: -0.926 test_metric: -0.872 lr: 0.00022)\n",
            "epoch: 1525, train_loss: 0.210516, val loss: 0.359661,  train_metric: -0.929 test_metric: -0.874 lr: 0.00022)\n",
            "epoch: 1526, train_loss: 0.212615, val loss: 0.354695,  train_metric: -0.929 test_metric: -0.888 lr: 0.00022)\n",
            "epoch: 1527, train_loss: 0.210967, val loss: 0.354693,  train_metric: -0.930 test_metric: -0.879 lr: 0.00022)\n",
            "epoch: 1528, train_loss: 0.216212, val loss: 0.352044,  train_metric: -0.926 test_metric: -0.875 lr: 0.00022)\n",
            "epoch: 1529, train_loss: 0.210510, val loss: 0.364148,  train_metric: -0.926 test_metric: -0.880 lr: 0.00022)\n",
            "epoch: 1530, train_loss: 0.216149, val loss: 0.355412,  train_metric: -0.922 test_metric: -0.879 lr: 0.00022)\n",
            "epoch: 1531, train_loss: 0.216282, val loss: 0.374154,  train_metric: -0.921 test_metric: -0.860 lr: 0.00022)\n",
            "epoch: 1532, train_loss: 0.214955, val loss: 0.357894,  train_metric: -0.922 test_metric: -0.881 lr: 0.00022)\n",
            "epoch: 1533, train_loss: 0.220743, val loss: 0.352950,  train_metric: -0.923 test_metric: -0.879 lr: 0.00022)\n",
            "epoch: 1534, train_loss: 0.216836, val loss: 0.361016,  train_metric: -0.923 test_metric: -0.882 lr: 0.00022)\n",
            "epoch: 1535, train_loss: 0.213801, val loss: 0.369481,  train_metric: -0.927 test_metric: -0.859 lr: 0.00022)\n",
            "epoch: 1536, train_loss: 0.221262, val loss: 0.359201,  train_metric: -0.921 test_metric: -0.886 lr: 0.00021)\n",
            "epoch: 1537, train_loss: 0.216007, val loss: 0.350765,  train_metric: -0.923 test_metric: -0.886 lr: 0.00021)\n",
            "epoch: 1538, train_loss: 0.208109, val loss: 0.372915,  train_metric: -0.932 test_metric: -0.862 lr: 0.00021)\n",
            "epoch: 1539, train_loss: 0.213808, val loss: 0.350998,  train_metric: -0.926 test_metric: -0.887 lr: 0.00021)\n",
            "epoch: 1540, train_loss: 0.210138, val loss: 0.355860,  train_metric: -0.927 test_metric: -0.881 lr: 0.00021)\n",
            "epoch: 1541, train_loss: 0.209005, val loss: 0.354915,  train_metric: -0.928 test_metric: -0.886 lr: 0.00021)\n",
            "epoch: 1542, train_loss: 0.209525, val loss: 0.373345,  train_metric: -0.932 test_metric: -0.859 lr: 0.00021)\n",
            "epoch: 1543, train_loss: 0.211337, val loss: 0.360946,  train_metric: -0.926 test_metric: -0.871 lr: 0.00021)\n",
            "epoch: 1544, train_loss: 0.208438, val loss: 0.350438,  train_metric: -0.928 test_metric: -0.884 lr: 0.00021)\n",
            "epoch: 1545, train_loss: 0.209412, val loss: 0.366595,  train_metric: -0.928 test_metric: -0.866 lr: 0.00021)\n",
            "epoch: 1546, train_loss: 0.210596, val loss: 0.356997,  train_metric: -0.930 test_metric: -0.875 lr: 0.00021)\n",
            "epoch: 1547, train_loss: 0.208044, val loss: 0.353827,  train_metric: -0.927 test_metric: -0.882 lr: 0.00021)\n",
            "epoch: 1548, train_loss: 0.206857, val loss: 0.358155,  train_metric: -0.931 test_metric: -0.880 lr: 0.00021)\n",
            "epoch: 1549, train_loss: 0.208833, val loss: 0.356117,  train_metric: -0.928 test_metric: -0.872 lr: 0.00021)\n",
            "epoch: 1550, train_loss: 0.208234, val loss: 0.364333,  train_metric: -0.929 test_metric: -0.875 lr: 0.00021)\n",
            "epoch: 1551, train_loss: 0.208513, val loss: 0.370059,  train_metric: -0.930 test_metric: -0.865 lr: 0.00021)\n",
            "epoch: 1552, train_loss: 0.210371, val loss: 0.377589,  train_metric: -0.926 test_metric: -0.856 lr: 0.00021)\n",
            "epoch: 1553, train_loss: 0.214594, val loss: 0.376389,  train_metric: -0.923 test_metric: -0.867 lr: 0.00021)\n",
            "epoch: 1554, train_loss: 0.213162, val loss: 0.383055,  train_metric: -0.925 test_metric: -0.860 lr: 0.00021)\n",
            "epoch: 1555, train_loss: 0.217215, val loss: 0.359548,  train_metric: -0.918 test_metric: -0.862 lr: 0.00021)\n",
            "epoch: 1556, train_loss: 0.209320, val loss: 0.359338,  train_metric: -0.927 test_metric: -0.872 lr: 0.00021)\n",
            "epoch: 1557, train_loss: 0.209794, val loss: 0.360969,  train_metric: -0.926 test_metric: -0.876 lr: 0.00021)\n",
            "epoch: 1558, train_loss: 0.212186, val loss: 0.362033,  train_metric: -0.926 test_metric: -0.872 lr: 0.00021)\n",
            "epoch: 1559, train_loss: 0.208083, val loss: 0.359081,  train_metric: -0.931 test_metric: -0.879 lr: 0.00021)\n",
            "epoch: 1560, train_loss: 0.208497, val loss: 0.355232,  train_metric: -0.926 test_metric: -0.874 lr: 0.00021)\n",
            "epoch: 1561, train_loss: 0.209066, val loss: 0.353661,  train_metric: -0.928 test_metric: -0.880 lr: 0.00021)\n",
            "epoch: 1562, train_loss: 0.206096, val loss: 0.351132,  train_metric: -0.931 test_metric: -0.874 lr: 0.00021)\n",
            "epoch: 1563, train_loss: 0.210054, val loss: 0.356299,  train_metric: -0.926 test_metric: -0.882 lr: 0.00021)\n",
            "epoch: 1564, train_loss: 0.208579, val loss: 0.360991,  train_metric: -0.930 test_metric: -0.860 lr: 0.00021)\n",
            "epoch: 1565, train_loss: 0.209265, val loss: 0.352775,  train_metric: -0.923 test_metric: -0.879 lr: 0.00021)\n",
            "epoch: 1566, train_loss: 0.207783, val loss: 0.350258,  train_metric: -0.931 test_metric: -0.880 lr: 0.00021)\n",
            "epoch: 1567, train_loss: 0.206810, val loss: 0.351238,  train_metric: -0.927 test_metric: -0.881 lr: 0.00021)\n",
            "epoch: 1568, train_loss: 0.210906, val loss: 0.353882,  train_metric: -0.926 test_metric: -0.881 lr: 0.00021)\n",
            "epoch: 1569, train_loss: 0.211442, val loss: 0.349588,  train_metric: -0.929 test_metric: -0.885 lr: 0.00021)\n",
            "epoch: 1570, train_loss: 0.205691, val loss: 0.360406,  train_metric: -0.930 test_metric: -0.877 lr: 0.00021)\n",
            "epoch: 1571, train_loss: 0.206139, val loss: 0.360212,  train_metric: -0.926 test_metric: -0.881 lr: 0.00021)\n",
            "epoch: 1572, train_loss: 0.212472, val loss: 0.352617,  train_metric: -0.928 test_metric: -0.880 lr: 0.00021)\n",
            "epoch: 1573, train_loss: 0.207361, val loss: 0.349021,  train_metric: -0.932 test_metric: -0.880 lr: 0.00021)\n",
            "epoch: 1574, train_loss: 0.207346, val loss: 0.362173,  train_metric: -0.927 test_metric: -0.880 lr: 0.00021)\n",
            "epoch: 1575, train_loss: 0.212434, val loss: 0.355815,  train_metric: -0.934 test_metric: -0.874 lr: 0.00021)\n",
            "epoch: 1576, train_loss: 0.209093, val loss: 0.363408,  train_metric: -0.925 test_metric: -0.870 lr: 0.00021)\n",
            "epoch: 1577, train_loss: 0.215985, val loss: 0.361723,  train_metric: -0.923 test_metric: -0.877 lr: 0.00021)\n",
            "epoch: 1578, train_loss: 0.211032, val loss: 0.361174,  train_metric: -0.925 test_metric: -0.874 lr: 0.00021)\n",
            "epoch: 1579, train_loss: 0.206796, val loss: 0.366021,  train_metric: -0.928 test_metric: -0.861 lr: 0.00021)\n",
            "epoch: 1580, train_loss: 0.206202, val loss: 0.358522,  train_metric: -0.926 test_metric: -0.875 lr: 0.00021)\n",
            "epoch: 1581, train_loss: 0.204884, val loss: 0.358937,  train_metric: -0.932 test_metric: -0.876 lr: 0.00021)\n",
            "epoch: 1582, train_loss: 0.205601, val loss: 0.354380,  train_metric: -0.931 test_metric: -0.882 lr: 0.00021)\n",
            "epoch: 1583, train_loss: 0.204989, val loss: 0.350625,  train_metric: -0.932 test_metric: -0.879 lr: 0.00020)\n",
            "epoch: 1584, train_loss: 0.206634, val loss: 0.357534,  train_metric: -0.928 test_metric: -0.884 lr: 0.00020)\n",
            "epoch: 1585, train_loss: 0.206627, val loss: 0.353216,  train_metric: -0.927 test_metric: -0.879 lr: 0.00020)\n",
            "epoch: 1586, train_loss: 0.206371, val loss: 0.354054,  train_metric: -0.929 test_metric: -0.882 lr: 0.00020)\n",
            "epoch: 1587, train_loss: 0.210341, val loss: 0.359489,  train_metric: -0.926 test_metric: -0.874 lr: 0.00020)\n",
            "epoch: 1588, train_loss: 0.207497, val loss: 0.348805,  train_metric: -0.933 test_metric: -0.888 lr: 0.00020)\n",
            "epoch: 1589, train_loss: 0.208385, val loss: 0.355167,  train_metric: -0.925 test_metric: -0.882 lr: 0.00020)\n",
            "epoch: 1590, train_loss: 0.203783, val loss: 0.356218,  train_metric: -0.936 test_metric: -0.879 lr: 0.00020)\n",
            "epoch: 1591, train_loss: 0.206609, val loss: 0.363789,  train_metric: -0.929 test_metric: -0.876 lr: 0.00020)\n",
            "epoch: 1592, train_loss: 0.206813, val loss: 0.376447,  train_metric: -0.930 test_metric: -0.869 lr: 0.00020)\n",
            "epoch: 1593, train_loss: 0.206466, val loss: 0.361375,  train_metric: -0.930 test_metric: -0.870 lr: 0.00020)\n",
            "epoch: 1594, train_loss: 0.205646, val loss: 0.360584,  train_metric: -0.932 test_metric: -0.875 lr: 0.00020)\n",
            "epoch: 1595, train_loss: 0.206926, val loss: 0.356547,  train_metric: -0.929 test_metric: -0.882 lr: 0.00020)\n",
            "epoch: 1596, train_loss: 0.205216, val loss: 0.354378,  train_metric: -0.928 test_metric: -0.880 lr: 0.00020)\n",
            "epoch: 1597, train_loss: 0.208463, val loss: 0.364426,  train_metric: -0.927 test_metric: -0.879 lr: 0.00020)\n",
            "epoch: 1598, train_loss: 0.205471, val loss: 0.351624,  train_metric: -0.929 test_metric: -0.885 lr: 0.00020)\n",
            "epoch: 1599, train_loss: 0.205153, val loss: 0.361343,  train_metric: -0.932 test_metric: -0.866 lr: 0.00020)\n",
            "epoch: 1600, train_loss: 0.208064, val loss: 0.356048,  train_metric: -0.928 test_metric: -0.872 lr: 0.00020)\n",
            "epoch: 1601, train_loss: 0.211078, val loss: 0.372283,  train_metric: -0.921 test_metric: -0.872 lr: 0.00020)\n",
            "epoch: 1602, train_loss: 0.210884, val loss: 0.368258,  train_metric: -0.923 test_metric: -0.869 lr: 0.00020)\n",
            "epoch: 1603, train_loss: 0.204832, val loss: 0.350851,  train_metric: -0.932 test_metric: -0.882 lr: 0.00020)\n",
            "epoch: 1604, train_loss: 0.209733, val loss: 0.364282,  train_metric: -0.926 test_metric: -0.871 lr: 0.00020)\n",
            "epoch: 1605, train_loss: 0.208440, val loss: 0.372687,  train_metric: -0.927 test_metric: -0.862 lr: 0.00020)\n",
            "epoch: 1606, train_loss: 0.203856, val loss: 0.350567,  train_metric: -0.930 test_metric: -0.893 lr: 0.00020)\n",
            "epoch: 1607, train_loss: 0.205998, val loss: 0.354822,  train_metric: -0.936 test_metric: -0.880 lr: 0.00020)\n",
            "epoch: 1608, train_loss: 0.205256, val loss: 0.365817,  train_metric: -0.928 test_metric: -0.866 lr: 0.00020)\n",
            "epoch: 1609, train_loss: 0.209044, val loss: 0.348438,  train_metric: -0.925 test_metric: -0.880 lr: 0.00020)\n",
            "epoch: 1610, train_loss: 0.203155, val loss: 0.359880,  train_metric: -0.933 test_metric: -0.879 lr: 0.00020)\n",
            "epoch: 1611, train_loss: 0.207713, val loss: 0.349180,  train_metric: -0.931 test_metric: -0.884 lr: 0.00020)\n",
            "epoch: 1612, train_loss: 0.201652, val loss: 0.355054,  train_metric: -0.934 test_metric: -0.874 lr: 0.00020)\n",
            "epoch: 1613, train_loss: 0.201867, val loss: 0.353975,  train_metric: -0.930 test_metric: -0.886 lr: 0.00020)\n",
            "epoch: 1614, train_loss: 0.203580, val loss: 0.356369,  train_metric: -0.931 test_metric: -0.879 lr: 0.00020)\n",
            "epoch: 1615, train_loss: 0.205030, val loss: 0.349863,  train_metric: -0.931 test_metric: -0.877 lr: 0.00020)\n",
            "epoch: 1616, train_loss: 0.203445, val loss: 0.356853,  train_metric: -0.932 test_metric: -0.879 lr: 0.00020)\n",
            "epoch: 1617, train_loss: 0.204018, val loss: 0.364110,  train_metric: -0.934 test_metric: -0.865 lr: 0.00020)\n",
            "epoch: 1618, train_loss: 0.205794, val loss: 0.345329,  train_metric: -0.929 test_metric: -0.891 lr: 0.00020)\n",
            "epoch: 1619, train_loss: 0.204034, val loss: 0.350556,  train_metric: -0.930 test_metric: -0.880 lr: 0.00020)\n",
            "epoch: 1620, train_loss: 0.205752, val loss: 0.356417,  train_metric: -0.927 test_metric: -0.874 lr: 0.00020)\n",
            "epoch: 1621, train_loss: 0.205674, val loss: 0.368585,  train_metric: -0.928 test_metric: -0.875 lr: 0.00020)\n",
            "epoch: 1622, train_loss: 0.204616, val loss: 0.353517,  train_metric: -0.927 test_metric: -0.885 lr: 0.00020)\n",
            "epoch: 1623, train_loss: 0.208124, val loss: 0.366833,  train_metric: -0.927 test_metric: -0.861 lr: 0.00020)\n",
            "epoch: 1624, train_loss: 0.214205, val loss: 0.355682,  train_metric: -0.921 test_metric: -0.875 lr: 0.00020)\n",
            "epoch: 1625, train_loss: 0.204733, val loss: 0.352255,  train_metric: -0.927 test_metric: -0.882 lr: 0.00020)\n",
            "epoch: 1626, train_loss: 0.202551, val loss: 0.353695,  train_metric: -0.930 test_metric: -0.882 lr: 0.00020)\n",
            "epoch: 1627, train_loss: 0.204165, val loss: 0.349434,  train_metric: -0.931 test_metric: -0.882 lr: 0.00020)\n",
            "epoch: 1628, train_loss: 0.201830, val loss: 0.350477,  train_metric: -0.934 test_metric: -0.879 lr: 0.00020)\n",
            "epoch: 1629, train_loss: 0.203642, val loss: 0.354561,  train_metric: -0.930 test_metric: -0.872 lr: 0.00020)\n",
            "epoch: 1630, train_loss: 0.200768, val loss: 0.358464,  train_metric: -0.932 test_metric: -0.877 lr: 0.00020)\n",
            "epoch: 1631, train_loss: 0.200902, val loss: 0.350803,  train_metric: -0.936 test_metric: -0.875 lr: 0.00020)\n",
            "epoch: 1632, train_loss: 0.203643, val loss: 0.363371,  train_metric: -0.934 test_metric: -0.874 lr: 0.00020)\n",
            "epoch: 1633, train_loss: 0.205377, val loss: 0.358632,  train_metric: -0.929 test_metric: -0.871 lr: 0.00019)\n",
            "epoch: 1634, train_loss: 0.203549, val loss: 0.352307,  train_metric: -0.935 test_metric: -0.884 lr: 0.00019)\n",
            "epoch: 1635, train_loss: 0.200877, val loss: 0.352127,  train_metric: -0.931 test_metric: -0.881 lr: 0.00019)\n",
            "epoch: 1636, train_loss: 0.206921, val loss: 0.364142,  train_metric: -0.935 test_metric: -0.870 lr: 0.00019)\n",
            "epoch: 1637, train_loss: 0.204271, val loss: 0.361133,  train_metric: -0.925 test_metric: -0.876 lr: 0.00019)\n",
            "epoch: 1638, train_loss: 0.202743, val loss: 0.344817,  train_metric: -0.927 test_metric: -0.886 lr: 0.00019)\n",
            "epoch: 1639, train_loss: 0.205195, val loss: 0.353663,  train_metric: -0.924 test_metric: -0.882 lr: 0.00019)\n",
            "epoch: 1640, train_loss: 0.207064, val loss: 0.359928,  train_metric: -0.927 test_metric: -0.869 lr: 0.00019)\n",
            "epoch: 1641, train_loss: 0.204582, val loss: 0.349612,  train_metric: -0.925 test_metric: -0.882 lr: 0.00019)\n",
            "epoch: 1642, train_loss: 0.202611, val loss: 0.354461,  train_metric: -0.932 test_metric: -0.872 lr: 0.00019)\n",
            "epoch: 1643, train_loss: 0.201853, val loss: 0.355189,  train_metric: -0.932 test_metric: -0.887 lr: 0.00019)\n",
            "epoch: 1644, train_loss: 0.203600, val loss: 0.358210,  train_metric: -0.932 test_metric: -0.874 lr: 0.00019)\n",
            "epoch: 1645, train_loss: 0.205414, val loss: 0.348527,  train_metric: -0.922 test_metric: -0.886 lr: 0.00019)\n",
            "epoch: 1646, train_loss: 0.201993, val loss: 0.347617,  train_metric: -0.936 test_metric: -0.886 lr: 0.00019)\n",
            "epoch: 1647, train_loss: 0.200547, val loss: 0.352876,  train_metric: -0.934 test_metric: -0.887 lr: 0.00019)\n",
            "epoch: 1648, train_loss: 0.201220, val loss: 0.347196,  train_metric: -0.932 test_metric: -0.892 lr: 0.00019)\n",
            "epoch: 1649, train_loss: 0.211950, val loss: 0.386856,  train_metric: -0.928 test_metric: -0.853 lr: 0.00019)\n",
            "epoch: 1650, train_loss: 0.214700, val loss: 0.351194,  train_metric: -0.926 test_metric: -0.890 lr: 0.00019)\n",
            "epoch: 1651, train_loss: 0.203895, val loss: 0.347349,  train_metric: -0.932 test_metric: -0.886 lr: 0.00019)\n",
            "epoch: 1652, train_loss: 0.202367, val loss: 0.351363,  train_metric: -0.927 test_metric: -0.872 lr: 0.00019)\n",
            "epoch: 1653, train_loss: 0.200468, val loss: 0.355561,  train_metric: -0.934 test_metric: -0.874 lr: 0.00019)\n",
            "epoch: 1654, train_loss: 0.200468, val loss: 0.354034,  train_metric: -0.929 test_metric: -0.879 lr: 0.00019)\n",
            "epoch: 1655, train_loss: 0.203991, val loss: 0.352423,  train_metric: -0.931 test_metric: -0.876 lr: 0.00019)\n",
            "epoch: 1656, train_loss: 0.198585, val loss: 0.356566,  train_metric: -0.937 test_metric: -0.874 lr: 0.00019)\n",
            "epoch: 1657, train_loss: 0.201045, val loss: 0.349082,  train_metric: -0.938 test_metric: -0.887 lr: 0.00019)\n",
            "epoch: 1658, train_loss: 0.199704, val loss: 0.350675,  train_metric: -0.930 test_metric: -0.880 lr: 0.00019)\n",
            "epoch: 1659, train_loss: 0.199376, val loss: 0.350887,  train_metric: -0.933 test_metric: -0.885 lr: 0.00019)\n",
            "epoch: 1660, train_loss: 0.199116, val loss: 0.361574,  train_metric: -0.934 test_metric: -0.877 lr: 0.00019)\n",
            "epoch: 1661, train_loss: 0.205078, val loss: 0.354823,  train_metric: -0.929 test_metric: -0.880 lr: 0.00019)\n",
            "epoch: 1662, train_loss: 0.200312, val loss: 0.348607,  train_metric: -0.936 test_metric: -0.891 lr: 0.00019)\n",
            "epoch: 1663, train_loss: 0.200063, val loss: 0.351988,  train_metric: -0.932 test_metric: -0.882 lr: 0.00019)\n",
            "epoch: 1664, train_loss: 0.199153, val loss: 0.351425,  train_metric: -0.937 test_metric: -0.879 lr: 0.00019)\n",
            "epoch: 1665, train_loss: 0.201523, val loss: 0.357166,  train_metric: -0.930 test_metric: -0.876 lr: 0.00019)\n",
            "epoch: 1666, train_loss: 0.206257, val loss: 0.363612,  train_metric: -0.930 test_metric: -0.864 lr: 0.00019)\n",
            "epoch: 1667, train_loss: 0.207387, val loss: 0.349768,  train_metric: -0.924 test_metric: -0.884 lr: 0.00019)\n",
            "epoch: 1668, train_loss: 0.206059, val loss: 0.358540,  train_metric: -0.926 test_metric: -0.884 lr: 0.00019)\n",
            "epoch: 1669, train_loss: 0.201470, val loss: 0.351109,  train_metric: -0.932 test_metric: -0.879 lr: 0.00019)\n",
            "epoch: 1670, train_loss: 0.201031, val loss: 0.345691,  train_metric: -0.929 test_metric: -0.884 lr: 0.00019)\n",
            "epoch: 1671, train_loss: 0.199688, val loss: 0.349259,  train_metric: -0.936 test_metric: -0.879 lr: 0.00019)\n",
            "epoch: 1672, train_loss: 0.198798, val loss: 0.353003,  train_metric: -0.934 test_metric: -0.876 lr: 0.00019)\n",
            "epoch: 1673, train_loss: 0.199928, val loss: 0.346296,  train_metric: -0.930 test_metric: -0.884 lr: 0.00019)\n",
            "epoch: 1674, train_loss: 0.200390, val loss: 0.352884,  train_metric: -0.933 test_metric: -0.877 lr: 0.00019)\n",
            "epoch: 1675, train_loss: 0.198167, val loss: 0.353749,  train_metric: -0.933 test_metric: -0.882 lr: 0.00019)\n",
            "epoch: 1676, train_loss: 0.199963, val loss: 0.351897,  train_metric: -0.934 test_metric: -0.877 lr: 0.00019)\n",
            "epoch: 1677, train_loss: 0.200099, val loss: 0.348155,  train_metric: -0.936 test_metric: -0.879 lr: 0.00019)\n",
            "epoch: 1678, train_loss: 0.202292, val loss: 0.349598,  train_metric: -0.932 test_metric: -0.881 lr: 0.00019)\n",
            "epoch: 1679, train_loss: 0.199151, val loss: 0.350117,  train_metric: -0.926 test_metric: -0.888 lr: 0.00019)\n",
            "epoch: 1680, train_loss: 0.199749, val loss: 0.357383,  train_metric: -0.937 test_metric: -0.875 lr: 0.00019)\n",
            "epoch: 1681, train_loss: 0.200788, val loss: 0.355580,  train_metric: -0.929 test_metric: -0.881 lr: 0.00019)\n",
            "epoch: 1682, train_loss: 0.207385, val loss: 0.356746,  train_metric: -0.930 test_metric: -0.872 lr: 0.00019)\n",
            "epoch: 1683, train_loss: 0.196691, val loss: 0.350928,  train_metric: -0.930 test_metric: -0.885 lr: 0.00019)\n",
            "epoch: 1684, train_loss: 0.197915, val loss: 0.351321,  train_metric: -0.939 test_metric: -0.880 lr: 0.00019)\n",
            "epoch: 1685, train_loss: 0.199123, val loss: 0.347181,  train_metric: -0.937 test_metric: -0.885 lr: 0.00019)\n",
            "epoch: 1686, train_loss: 0.198642, val loss: 0.364625,  train_metric: -0.932 test_metric: -0.870 lr: 0.00018)\n",
            "epoch: 1687, train_loss: 0.205448, val loss: 0.360841,  train_metric: -0.927 test_metric: -0.867 lr: 0.00018)\n",
            "epoch: 1688, train_loss: 0.199338, val loss: 0.357946,  train_metric: -0.931 test_metric: -0.880 lr: 0.00018)\n",
            "epoch: 1689, train_loss: 0.202172, val loss: 0.350022,  train_metric: -0.928 test_metric: -0.884 lr: 0.00018)\n",
            "epoch: 1690, train_loss: 0.203192, val loss: 0.348696,  train_metric: -0.931 test_metric: -0.884 lr: 0.00018)\n",
            "epoch: 1691, train_loss: 0.198656, val loss: 0.350661,  train_metric: -0.931 test_metric: -0.880 lr: 0.00018)\n",
            "epoch: 1692, train_loss: 0.198891, val loss: 0.356098,  train_metric: -0.930 test_metric: -0.875 lr: 0.00018)\n",
            "epoch: 1693, train_loss: 0.203570, val loss: 0.356685,  train_metric: -0.923 test_metric: -0.872 lr: 0.00018)\n",
            "epoch: 1694, train_loss: 0.197757, val loss: 0.352139,  train_metric: -0.939 test_metric: -0.880 lr: 0.00018)\n",
            "epoch: 1695, train_loss: 0.199736, val loss: 0.353803,  train_metric: -0.928 test_metric: -0.882 lr: 0.00018)\n",
            "epoch: 1696, train_loss: 0.198511, val loss: 0.351192,  train_metric: -0.934 test_metric: -0.874 lr: 0.00018)\n",
            "epoch: 1697, train_loss: 0.200840, val loss: 0.356391,  train_metric: -0.935 test_metric: -0.871 lr: 0.00018)\n",
            "epoch: 1698, train_loss: 0.198687, val loss: 0.365495,  train_metric: -0.929 test_metric: -0.865 lr: 0.00018)\n",
            "epoch: 1699, train_loss: 0.199447, val loss: 0.355587,  train_metric: -0.935 test_metric: -0.881 lr: 0.00018)\n",
            "epoch: 1700, train_loss: 0.197554, val loss: 0.349248,  train_metric: -0.934 test_metric: -0.879 lr: 0.00018)\n",
            "epoch: 1701, train_loss: 0.196986, val loss: 0.348429,  train_metric: -0.932 test_metric: -0.879 lr: 0.00018)\n",
            "epoch: 1702, train_loss: 0.197906, val loss: 0.357223,  train_metric: -0.932 test_metric: -0.876 lr: 0.00018)\n",
            "epoch: 1703, train_loss: 0.198022, val loss: 0.351546,  train_metric: -0.936 test_metric: -0.886 lr: 0.00018)\n",
            "epoch: 1704, train_loss: 0.204029, val loss: 0.353020,  train_metric: -0.931 test_metric: -0.885 lr: 0.00018)\n",
            "epoch: 1705, train_loss: 0.197570, val loss: 0.357317,  train_metric: -0.934 test_metric: -0.876 lr: 0.00018)\n",
            "epoch: 1706, train_loss: 0.195111, val loss: 0.355941,  train_metric: -0.940 test_metric: -0.874 lr: 0.00018)\n",
            "epoch: 1707, train_loss: 0.202427, val loss: 0.345058,  train_metric: -0.930 test_metric: -0.887 lr: 0.00018)\n",
            "epoch: 1708, train_loss: 0.197201, val loss: 0.349540,  train_metric: -0.932 test_metric: -0.877 lr: 0.00018)\n",
            "epoch: 1709, train_loss: 0.199702, val loss: 0.353001,  train_metric: -0.936 test_metric: -0.884 lr: 0.00018)\n",
            "epoch: 1710, train_loss: 0.198622, val loss: 0.356592,  train_metric: -0.934 test_metric: -0.880 lr: 0.00018)\n",
            "epoch: 1711, train_loss: 0.200114, val loss: 0.344390,  train_metric: -0.931 test_metric: -0.881 lr: 0.00018)\n",
            "epoch: 1712, train_loss: 0.202030, val loss: 0.343972,  train_metric: -0.930 test_metric: -0.887 lr: 0.00018)\n",
            "epoch: 1713, train_loss: 0.198829, val loss: 0.346647,  train_metric: -0.930 test_metric: -0.882 lr: 0.00018)\n",
            "epoch: 1714, train_loss: 0.197677, val loss: 0.350723,  train_metric: -0.935 test_metric: -0.881 lr: 0.00018)\n",
            "epoch: 1715, train_loss: 0.199005, val loss: 0.360429,  train_metric: -0.931 test_metric: -0.879 lr: 0.00018)\n",
            "epoch: 1716, train_loss: 0.197704, val loss: 0.370092,  train_metric: -0.938 test_metric: -0.862 lr: 0.00018)\n",
            "epoch: 1717, train_loss: 0.198840, val loss: 0.346858,  train_metric: -0.931 test_metric: -0.879 lr: 0.00018)\n",
            "epoch: 1718, train_loss: 0.197188, val loss: 0.348905,  train_metric: -0.939 test_metric: -0.888 lr: 0.00018)\n",
            "epoch: 1719, train_loss: 0.197739, val loss: 0.372075,  train_metric: -0.935 test_metric: -0.861 lr: 0.00018)\n",
            "epoch: 1720, train_loss: 0.204432, val loss: 0.354645,  train_metric: -0.925 test_metric: -0.882 lr: 0.00018)\n",
            "epoch: 1721, train_loss: 0.196373, val loss: 0.379388,  train_metric: -0.938 test_metric: -0.855 lr: 0.00018)\n",
            "epoch: 1722, train_loss: 0.206060, val loss: 0.349026,  train_metric: -0.932 test_metric: -0.876 lr: 0.00018)\n",
            "epoch: 1723, train_loss: 0.198240, val loss: 0.350725,  train_metric: -0.934 test_metric: -0.876 lr: 0.00018)\n",
            "epoch: 1724, train_loss: 0.196586, val loss: 0.347750,  train_metric: -0.933 test_metric: -0.882 lr: 0.00018)\n",
            "epoch: 1725, train_loss: 0.195395, val loss: 0.348113,  train_metric: -0.942 test_metric: -0.887 lr: 0.00018)\n",
            "epoch: 1726, train_loss: 0.197620, val loss: 0.360444,  train_metric: -0.934 test_metric: -0.870 lr: 0.00018)\n",
            "epoch: 1727, train_loss: 0.199011, val loss: 0.346867,  train_metric: -0.934 test_metric: -0.885 lr: 0.00018)\n",
            "epoch: 1728, train_loss: 0.195166, val loss: 0.346864,  train_metric: -0.942 test_metric: -0.882 lr: 0.00018)\n",
            "epoch: 1729, train_loss: 0.197077, val loss: 0.349387,  train_metric: -0.932 test_metric: -0.887 lr: 0.00018)\n",
            "epoch: 1730, train_loss: 0.196942, val loss: 0.347802,  train_metric: -0.937 test_metric: -0.885 lr: 0.00018)\n",
            "epoch: 1731, train_loss: 0.200038, val loss: 0.344908,  train_metric: -0.930 test_metric: -0.887 lr: 0.00018)\n",
            "epoch: 1732, train_loss: 0.198470, val loss: 0.348448,  train_metric: -0.935 test_metric: -0.895 lr: 0.00018)\n",
            "epoch: 1733, train_loss: 0.195873, val loss: 0.347553,  train_metric: -0.938 test_metric: -0.881 lr: 0.00018)\n",
            "epoch: 1734, train_loss: 0.195597, val loss: 0.349358,  train_metric: -0.933 test_metric: -0.891 lr: 0.00018)\n",
            "epoch: 1735, train_loss: 0.194015, val loss: 0.343800,  train_metric: -0.939 test_metric: -0.890 lr: 0.00018)\n",
            "epoch: 1736, train_loss: 0.197322, val loss: 0.356232,  train_metric: -0.936 test_metric: -0.879 lr: 0.00018)\n",
            "epoch: 1737, train_loss: 0.202074, val loss: 0.353213,  train_metric: -0.929 test_metric: -0.876 lr: 0.00018)\n",
            "epoch: 1738, train_loss: 0.201748, val loss: 0.345083,  train_metric: -0.929 test_metric: -0.891 lr: 0.00018)\n",
            "epoch: 1739, train_loss: 0.198400, val loss: 0.346222,  train_metric: -0.934 test_metric: -0.881 lr: 0.00018)\n",
            "epoch: 1740, train_loss: 0.197902, val loss: 0.348689,  train_metric: -0.931 test_metric: -0.887 lr: 0.00018)\n",
            "epoch: 1741, train_loss: 0.197407, val loss: 0.347308,  train_metric: -0.933 test_metric: -0.882 lr: 0.00018)\n",
            "epoch: 1742, train_loss: 0.194427, val loss: 0.347912,  train_metric: -0.936 test_metric: -0.877 lr: 0.00017)\n",
            "epoch: 1743, train_loss: 0.197291, val loss: 0.346050,  train_metric: -0.929 test_metric: -0.886 lr: 0.00017)\n",
            "epoch: 1744, train_loss: 0.199255, val loss: 0.355358,  train_metric: -0.935 test_metric: -0.881 lr: 0.00017)\n",
            "epoch: 1745, train_loss: 0.196131, val loss: 0.350357,  train_metric: -0.936 test_metric: -0.871 lr: 0.00017)\n",
            "epoch: 1746, train_loss: 0.193838, val loss: 0.346210,  train_metric: -0.936 test_metric: -0.891 lr: 0.00017)\n",
            "epoch: 1747, train_loss: 0.195763, val loss: 0.349347,  train_metric: -0.935 test_metric: -0.877 lr: 0.00017)\n",
            "epoch: 1748, train_loss: 0.193938, val loss: 0.351374,  train_metric: -0.936 test_metric: -0.879 lr: 0.00017)\n",
            "epoch: 1749, train_loss: 0.194487, val loss: 0.348118,  train_metric: -0.938 test_metric: -0.885 lr: 0.00017)\n",
            "epoch: 1750, train_loss: 0.196870, val loss: 0.355478,  train_metric: -0.930 test_metric: -0.885 lr: 0.00017)\n",
            "epoch: 1751, train_loss: 0.198563, val loss: 0.350485,  train_metric: -0.931 test_metric: -0.881 lr: 0.00017)\n",
            "epoch: 1752, train_loss: 0.199563, val loss: 0.350410,  train_metric: -0.926 test_metric: -0.870 lr: 0.00017)\n",
            "epoch: 1753, train_loss: 0.193825, val loss: 0.343420,  train_metric: -0.936 test_metric: -0.890 lr: 0.00017)\n",
            "epoch: 1754, train_loss: 0.194950, val loss: 0.347257,  train_metric: -0.935 test_metric: -0.890 lr: 0.00017)\n",
            "epoch: 1755, train_loss: 0.193996, val loss: 0.349356,  train_metric: -0.939 test_metric: -0.874 lr: 0.00017)\n",
            "epoch: 1756, train_loss: 0.196108, val loss: 0.347484,  train_metric: -0.932 test_metric: -0.874 lr: 0.00017)\n",
            "epoch: 1757, train_loss: 0.193563, val loss: 0.355135,  train_metric: -0.939 test_metric: -0.877 lr: 0.00017)\n",
            "epoch: 1758, train_loss: 0.194588, val loss: 0.349870,  train_metric: -0.936 test_metric: -0.881 lr: 0.00017)\n",
            "epoch: 1759, train_loss: 0.194014, val loss: 0.358821,  train_metric: -0.935 test_metric: -0.871 lr: 0.00017)\n",
            "epoch: 1760, train_loss: 0.195616, val loss: 0.348556,  train_metric: -0.929 test_metric: -0.874 lr: 0.00017)\n",
            "epoch: 1761, train_loss: 0.192192, val loss: 0.348521,  train_metric: -0.936 test_metric: -0.887 lr: 0.00017)\n",
            "epoch: 1762, train_loss: 0.196828, val loss: 0.353499,  train_metric: -0.936 test_metric: -0.882 lr: 0.00017)\n",
            "epoch: 1763, train_loss: 0.196897, val loss: 0.345082,  train_metric: -0.936 test_metric: -0.887 lr: 0.00017)\n",
            "epoch: 1764, train_loss: 0.193385, val loss: 0.355235,  train_metric: -0.937 test_metric: -0.872 lr: 0.00017)\n",
            "epoch: 1765, train_loss: 0.192118, val loss: 0.347272,  train_metric: -0.935 test_metric: -0.881 lr: 0.00017)\n",
            "epoch: 1766, train_loss: 0.192504, val loss: 0.351048,  train_metric: -0.940 test_metric: -0.885 lr: 0.00017)\n",
            "epoch: 1767, train_loss: 0.200514, val loss: 0.362511,  train_metric: -0.930 test_metric: -0.881 lr: 0.00017)\n",
            "epoch: 1768, train_loss: 0.197483, val loss: 0.358773,  train_metric: -0.936 test_metric: -0.872 lr: 0.00017)\n",
            "epoch: 1769, train_loss: 0.195512, val loss: 0.346986,  train_metric: -0.930 test_metric: -0.880 lr: 0.00017)\n",
            "epoch: 1770, train_loss: 0.195078, val loss: 0.343789,  train_metric: -0.933 test_metric: -0.887 lr: 0.00017)\n",
            "epoch: 1771, train_loss: 0.194208, val loss: 0.370253,  train_metric: -0.939 test_metric: -0.864 lr: 0.00017)\n",
            "epoch: 1772, train_loss: 0.198609, val loss: 0.348328,  train_metric: -0.930 test_metric: -0.886 lr: 0.00017)\n",
            "epoch: 1773, train_loss: 0.192756, val loss: 0.348883,  train_metric: -0.939 test_metric: -0.884 lr: 0.00017)\n",
            "epoch: 1774, train_loss: 0.192739, val loss: 0.352962,  train_metric: -0.937 test_metric: -0.871 lr: 0.00017)\n",
            "epoch: 1775, train_loss: 0.194297, val loss: 0.354665,  train_metric: -0.931 test_metric: -0.876 lr: 0.00017)\n",
            "epoch: 1776, train_loss: 0.192400, val loss: 0.354597,  train_metric: -0.940 test_metric: -0.872 lr: 0.00017)\n",
            "epoch: 1777, train_loss: 0.196763, val loss: 0.363400,  train_metric: -0.930 test_metric: -0.888 lr: 0.00017)\n",
            "epoch: 1778, train_loss: 0.202117, val loss: 0.349727,  train_metric: -0.926 test_metric: -0.893 lr: 0.00017)\n",
            "epoch: 1779, train_loss: 0.196297, val loss: 0.342366,  train_metric: -0.938 test_metric: -0.891 lr: 0.00017)\n",
            "epoch: 1780, train_loss: 0.192746, val loss: 0.351309,  train_metric: -0.939 test_metric: -0.884 lr: 0.00017)\n",
            "epoch: 1781, train_loss: 0.194454, val loss: 0.346944,  train_metric: -0.930 test_metric: -0.879 lr: 0.00017)\n",
            "epoch: 1782, train_loss: 0.195445, val loss: 0.347812,  train_metric: -0.932 test_metric: -0.879 lr: 0.00017)\n",
            "epoch: 1783, train_loss: 0.196009, val loss: 0.353045,  train_metric: -0.935 test_metric: -0.881 lr: 0.00017)\n",
            "epoch: 1784, train_loss: 0.194052, val loss: 0.366406,  train_metric: -0.939 test_metric: -0.866 lr: 0.00017)\n",
            "epoch: 1785, train_loss: 0.203614, val loss: 0.348334,  train_metric: -0.928 test_metric: -0.879 lr: 0.00017)\n",
            "epoch: 1786, train_loss: 0.193721, val loss: 0.348238,  train_metric: -0.933 test_metric: -0.882 lr: 0.00017)\n",
            "epoch: 1787, train_loss: 0.194620, val loss: 0.352891,  train_metric: -0.935 test_metric: -0.876 lr: 0.00017)\n",
            "epoch: 1788, train_loss: 0.193889, val loss: 0.355766,  train_metric: -0.940 test_metric: -0.870 lr: 0.00017)\n",
            "epoch: 1789, train_loss: 0.194042, val loss: 0.346783,  train_metric: -0.934 test_metric: -0.888 lr: 0.00017)\n",
            "epoch: 1790, train_loss: 0.191661, val loss: 0.348687,  train_metric: -0.938 test_metric: -0.882 lr: 0.00017)\n",
            "epoch: 1791, train_loss: 0.194882, val loss: 0.356031,  train_metric: -0.934 test_metric: -0.871 lr: 0.00017)\n",
            "epoch: 1792, train_loss: 0.194374, val loss: 0.367877,  train_metric: -0.936 test_metric: -0.860 lr: 0.00017)\n",
            "epoch: 1793, train_loss: 0.198178, val loss: 0.345883,  train_metric: -0.933 test_metric: -0.875 lr: 0.00017)\n",
            "epoch: 1794, train_loss: 0.194477, val loss: 0.352316,  train_metric: -0.938 test_metric: -0.885 lr: 0.00017)\n",
            "epoch: 1795, train_loss: 0.194246, val loss: 0.351987,  train_metric: -0.939 test_metric: -0.881 lr: 0.00017)\n",
            "epoch: 1796, train_loss: 0.195907, val loss: 0.341194,  train_metric: -0.931 test_metric: -0.892 lr: 0.00017)\n",
            "epoch: 1797, train_loss: 0.191496, val loss: 0.346294,  train_metric: -0.942 test_metric: -0.874 lr: 0.00017)\n",
            "epoch: 1798, train_loss: 0.191927, val loss: 0.350240,  train_metric: -0.935 test_metric: -0.884 lr: 0.00017)\n",
            "epoch: 1799, train_loss: 0.191674, val loss: 0.349117,  train_metric: -0.937 test_metric: -0.884 lr: 0.00017)\n",
            "epoch: 1800, train_loss: 0.190405, val loss: 0.344382,  train_metric: -0.938 test_metric: -0.881 lr: 0.00016)\n",
            "epoch: 1801, train_loss: 0.192549, val loss: 0.349898,  train_metric: -0.940 test_metric: -0.880 lr: 0.00016)\n",
            "epoch: 1802, train_loss: 0.192930, val loss: 0.347540,  train_metric: -0.930 test_metric: -0.884 lr: 0.00016)\n",
            "epoch: 1803, train_loss: 0.193215, val loss: 0.344365,  train_metric: -0.940 test_metric: -0.885 lr: 0.00016)\n",
            "epoch: 1804, train_loss: 0.193358, val loss: 0.348121,  train_metric: -0.935 test_metric: -0.877 lr: 0.00016)\n",
            "epoch: 1805, train_loss: 0.191102, val loss: 0.349102,  train_metric: -0.936 test_metric: -0.887 lr: 0.00016)\n",
            "epoch: 1806, train_loss: 0.191986, val loss: 0.342807,  train_metric: -0.934 test_metric: -0.890 lr: 0.00016)\n",
            "epoch: 1807, train_loss: 0.192660, val loss: 0.344750,  train_metric: -0.936 test_metric: -0.885 lr: 0.00016)\n",
            "epoch: 1808, train_loss: 0.190062, val loss: 0.347357,  train_metric: -0.941 test_metric: -0.880 lr: 0.00016)\n",
            "epoch: 1809, train_loss: 0.192160, val loss: 0.352537,  train_metric: -0.935 test_metric: -0.872 lr: 0.00016)\n",
            "epoch: 1810, train_loss: 0.192247, val loss: 0.351769,  train_metric: -0.939 test_metric: -0.875 lr: 0.00016)\n",
            "epoch: 1811, train_loss: 0.192431, val loss: 0.352842,  train_metric: -0.932 test_metric: -0.872 lr: 0.00016)\n",
            "epoch: 1812, train_loss: 0.192452, val loss: 0.348048,  train_metric: -0.936 test_metric: -0.887 lr: 0.00016)\n",
            "epoch: 1813, train_loss: 0.191414, val loss: 0.343628,  train_metric: -0.943 test_metric: -0.888 lr: 0.00016)\n",
            "epoch: 1814, train_loss: 0.194056, val loss: 0.344614,  train_metric: -0.938 test_metric: -0.884 lr: 0.00016)\n",
            "epoch: 1815, train_loss: 0.191178, val loss: 0.348404,  train_metric: -0.936 test_metric: -0.881 lr: 0.00016)\n",
            "epoch: 1816, train_loss: 0.192951, val loss: 0.341001,  train_metric: -0.936 test_metric: -0.888 lr: 0.00016)\n",
            "epoch: 1817, train_loss: 0.192148, val loss: 0.349875,  train_metric: -0.941 test_metric: -0.875 lr: 0.00016)\n",
            "epoch: 1818, train_loss: 0.194733, val loss: 0.351191,  train_metric: -0.936 test_metric: -0.879 lr: 0.00016)\n",
            "epoch: 1819, train_loss: 0.191887, val loss: 0.347294,  train_metric: -0.936 test_metric: -0.876 lr: 0.00016)\n",
            "epoch: 1820, train_loss: 0.190774, val loss: 0.352477,  train_metric: -0.940 test_metric: -0.881 lr: 0.00016)\n",
            "epoch: 1821, train_loss: 0.189753, val loss: 0.344838,  train_metric: -0.939 test_metric: -0.891 lr: 0.00016)\n",
            "epoch: 1822, train_loss: 0.190474, val loss: 0.342859,  train_metric: -0.936 test_metric: -0.888 lr: 0.00016)\n",
            "epoch: 1823, train_loss: 0.190309, val loss: 0.353856,  train_metric: -0.943 test_metric: -0.876 lr: 0.00016)\n",
            "epoch: 1824, train_loss: 0.191126, val loss: 0.345647,  train_metric: -0.934 test_metric: -0.880 lr: 0.00016)\n",
            "epoch: 1825, train_loss: 0.192094, val loss: 0.344578,  train_metric: -0.937 test_metric: -0.895 lr: 0.00016)\n",
            "epoch: 1826, train_loss: 0.193529, val loss: 0.355942,  train_metric: -0.934 test_metric: -0.875 lr: 0.00016)\n",
            "epoch: 1827, train_loss: 0.192164, val loss: 0.352321,  train_metric: -0.938 test_metric: -0.871 lr: 0.00016)\n",
            "epoch: 1828, train_loss: 0.192954, val loss: 0.353755,  train_metric: -0.935 test_metric: -0.884 lr: 0.00016)\n",
            "epoch: 1829, train_loss: 0.191875, val loss: 0.351654,  train_metric: -0.936 test_metric: -0.879 lr: 0.00016)\n",
            "epoch: 1830, train_loss: 0.191444, val loss: 0.344841,  train_metric: -0.938 test_metric: -0.888 lr: 0.00016)\n",
            "epoch: 1831, train_loss: 0.192761, val loss: 0.344860,  train_metric: -0.937 test_metric: -0.884 lr: 0.00016)\n",
            "epoch: 1832, train_loss: 0.192122, val loss: 0.350802,  train_metric: -0.938 test_metric: -0.872 lr: 0.00016)\n",
            "epoch: 1833, train_loss: 0.191227, val loss: 0.346053,  train_metric: -0.936 test_metric: -0.886 lr: 0.00016)\n",
            "epoch: 1834, train_loss: 0.190395, val loss: 0.347191,  train_metric: -0.939 test_metric: -0.880 lr: 0.00016)\n",
            "epoch: 1835, train_loss: 0.189389, val loss: 0.344981,  train_metric: -0.938 test_metric: -0.884 lr: 0.00016)\n",
            "epoch: 1836, train_loss: 0.190016, val loss: 0.348409,  train_metric: -0.938 test_metric: -0.875 lr: 0.00016)\n",
            "epoch: 1837, train_loss: 0.190654, val loss: 0.348733,  train_metric: -0.940 test_metric: -0.882 lr: 0.00016)\n",
            "epoch: 1838, train_loss: 0.187913, val loss: 0.357152,  train_metric: -0.940 test_metric: -0.869 lr: 0.00016)\n",
            "epoch: 1839, train_loss: 0.193319, val loss: 0.343127,  train_metric: -0.934 test_metric: -0.885 lr: 0.00016)\n",
            "epoch: 1840, train_loss: 0.190565, val loss: 0.340334,  train_metric: -0.937 test_metric: -0.888 lr: 0.00016)\n",
            "epoch: 1841, train_loss: 0.189108, val loss: 0.352591,  train_metric: -0.938 test_metric: -0.872 lr: 0.00016)\n",
            "epoch: 1842, train_loss: 0.190430, val loss: 0.344132,  train_metric: -0.939 test_metric: -0.887 lr: 0.00016)\n",
            "epoch: 1843, train_loss: 0.191237, val loss: 0.342265,  train_metric: -0.936 test_metric: -0.886 lr: 0.00016)\n",
            "epoch: 1844, train_loss: 0.189949, val loss: 0.358209,  train_metric: -0.943 test_metric: -0.874 lr: 0.00016)\n",
            "epoch: 1845, train_loss: 0.190392, val loss: 0.346129,  train_metric: -0.932 test_metric: -0.881 lr: 0.00016)\n",
            "epoch: 1846, train_loss: 0.188030, val loss: 0.349308,  train_metric: -0.945 test_metric: -0.880 lr: 0.00016)\n",
            "epoch: 1847, train_loss: 0.190624, val loss: 0.348702,  train_metric: -0.939 test_metric: -0.877 lr: 0.00016)\n",
            "epoch: 1848, train_loss: 0.191630, val loss: 0.348134,  train_metric: -0.934 test_metric: -0.880 lr: 0.00016)\n",
            "epoch: 1849, train_loss: 0.189601, val loss: 0.340986,  train_metric: -0.936 test_metric: -0.886 lr: 0.00016)\n",
            "epoch: 1850, train_loss: 0.189382, val loss: 0.346065,  train_metric: -0.935 test_metric: -0.880 lr: 0.00016)\n",
            "epoch: 1851, train_loss: 0.190742, val loss: 0.364960,  train_metric: -0.937 test_metric: -0.866 lr: 0.00016)\n",
            "epoch: 1852, train_loss: 0.189788, val loss: 0.352122,  train_metric: -0.937 test_metric: -0.872 lr: 0.00016)\n",
            "epoch: 1853, train_loss: 0.190706, val loss: 0.351267,  train_metric: -0.936 test_metric: -0.880 lr: 0.00016)\n",
            "epoch: 1854, train_loss: 0.187369, val loss: 0.344942,  train_metric: -0.941 test_metric: -0.879 lr: 0.00016)\n",
            "epoch: 1855, train_loss: 0.188115, val loss: 0.343512,  train_metric: -0.943 test_metric: -0.886 lr: 0.00016)\n",
            "epoch: 1856, train_loss: 0.191856, val loss: 0.346077,  train_metric: -0.933 test_metric: -0.880 lr: 0.00016)\n",
            "epoch: 1857, train_loss: 0.190971, val loss: 0.358055,  train_metric: -0.936 test_metric: -0.882 lr: 0.00016)\n",
            "epoch: 1858, train_loss: 0.199193, val loss: 0.339434,  train_metric: -0.923 test_metric: -0.888 lr: 0.00016)\n",
            "epoch: 1859, train_loss: 0.191604, val loss: 0.351514,  train_metric: -0.939 test_metric: -0.871 lr: 0.00016)\n",
            "epoch: 1860, train_loss: 0.191550, val loss: 0.342654,  train_metric: -0.936 test_metric: -0.885 lr: 0.00016)\n",
            "epoch: 1861, train_loss: 0.193403, val loss: 0.351057,  train_metric: -0.935 test_metric: -0.875 lr: 0.00016)\n",
            "epoch: 1862, train_loss: 0.189896, val loss: 0.350322,  train_metric: -0.943 test_metric: -0.879 lr: 0.00016)\n",
            "epoch: 1863, train_loss: 0.195829, val loss: 0.348571,  train_metric: -0.929 test_metric: -0.884 lr: 0.00015)\n",
            "epoch: 1864, train_loss: 0.191634, val loss: 0.339382,  train_metric: -0.938 test_metric: -0.888 lr: 0.00015)\n",
            "epoch: 1865, train_loss: 0.191880, val loss: 0.360834,  train_metric: -0.940 test_metric: -0.866 lr: 0.00015)\n",
            "epoch: 1866, train_loss: 0.193276, val loss: 0.349577,  train_metric: -0.936 test_metric: -0.881 lr: 0.00015)\n",
            "epoch: 1867, train_loss: 0.190746, val loss: 0.350111,  train_metric: -0.936 test_metric: -0.881 lr: 0.00015)\n",
            "epoch: 1868, train_loss: 0.186248, val loss: 0.344850,  train_metric: -0.937 test_metric: -0.890 lr: 0.00015)\n",
            "epoch: 1869, train_loss: 0.191410, val loss: 0.342968,  train_metric: -0.937 test_metric: -0.888 lr: 0.00015)\n",
            "epoch: 1870, train_loss: 0.188914, val loss: 0.343968,  train_metric: -0.933 test_metric: -0.887 lr: 0.00015)\n",
            "epoch: 1871, train_loss: 0.187261, val loss: 0.348562,  train_metric: -0.943 test_metric: -0.884 lr: 0.00015)\n",
            "epoch: 1872, train_loss: 0.189832, val loss: 0.345741,  train_metric: -0.938 test_metric: -0.879 lr: 0.00015)\n",
            "epoch: 1873, train_loss: 0.188852, val loss: 0.350248,  train_metric: -0.937 test_metric: -0.872 lr: 0.00015)\n",
            "epoch: 1874, train_loss: 0.188468, val loss: 0.345511,  train_metric: -0.937 test_metric: -0.885 lr: 0.00015)\n",
            "epoch: 1875, train_loss: 0.188583, val loss: 0.343524,  train_metric: -0.938 test_metric: -0.886 lr: 0.00015)\n",
            "epoch: 1876, train_loss: 0.189557, val loss: 0.347774,  train_metric: -0.939 test_metric: -0.881 lr: 0.00015)\n",
            "epoch: 1877, train_loss: 0.189245, val loss: 0.347239,  train_metric: -0.936 test_metric: -0.879 lr: 0.00015)\n",
            "epoch: 1878, train_loss: 0.188682, val loss: 0.346186,  train_metric: -0.941 test_metric: -0.880 lr: 0.00015)\n",
            "epoch: 1879, train_loss: 0.187473, val loss: 0.347850,  train_metric: -0.939 test_metric: -0.888 lr: 0.00015)\n",
            "epoch: 1880, train_loss: 0.187544, val loss: 0.340210,  train_metric: -0.943 test_metric: -0.888 lr: 0.00015)\n",
            "epoch: 1881, train_loss: 0.188857, val loss: 0.347254,  train_metric: -0.938 test_metric: -0.876 lr: 0.00015)\n",
            "epoch: 1882, train_loss: 0.186633, val loss: 0.344242,  train_metric: -0.938 test_metric: -0.888 lr: 0.00015)\n",
            "epoch: 1883, train_loss: 0.185216, val loss: 0.345062,  train_metric: -0.943 test_metric: -0.885 lr: 0.00015)\n",
            "epoch: 1884, train_loss: 0.187409, val loss: 0.345583,  train_metric: -0.939 test_metric: -0.885 lr: 0.00015)\n",
            "epoch: 1885, train_loss: 0.186624, val loss: 0.341153,  train_metric: -0.939 test_metric: -0.885 lr: 0.00015)\n",
            "epoch: 1886, train_loss: 0.187142, val loss: 0.344575,  train_metric: -0.941 test_metric: -0.877 lr: 0.00015)\n",
            "epoch: 1887, train_loss: 0.188443, val loss: 0.350905,  train_metric: -0.942 test_metric: -0.875 lr: 0.00015)\n",
            "epoch: 1888, train_loss: 0.186633, val loss: 0.346156,  train_metric: -0.936 test_metric: -0.882 lr: 0.00015)\n",
            "epoch: 1889, train_loss: 0.188513, val loss: 0.344186,  train_metric: -0.937 test_metric: -0.884 lr: 0.00015)\n",
            "epoch: 1890, train_loss: 0.191356, val loss: 0.356959,  train_metric: -0.934 test_metric: -0.869 lr: 0.00015)\n",
            "epoch: 1891, train_loss: 0.187776, val loss: 0.347232,  train_metric: -0.939 test_metric: -0.886 lr: 0.00015)\n",
            "epoch: 1892, train_loss: 0.185145, val loss: 0.338819,  train_metric: -0.942 test_metric: -0.891 lr: 0.00015)\n",
            "epoch: 1893, train_loss: 0.187391, val loss: 0.345043,  train_metric: -0.942 test_metric: -0.890 lr: 0.00015)\n",
            "epoch: 1894, train_loss: 0.187990, val loss: 0.343265,  train_metric: -0.937 test_metric: -0.890 lr: 0.00015)\n",
            "epoch: 1895, train_loss: 0.185665, val loss: 0.349684,  train_metric: -0.944 test_metric: -0.875 lr: 0.00015)\n",
            "epoch: 1896, train_loss: 0.186759, val loss: 0.341249,  train_metric: -0.937 test_metric: -0.887 lr: 0.00015)\n",
            "epoch: 1897, train_loss: 0.185976, val loss: 0.349644,  train_metric: -0.940 test_metric: -0.881 lr: 0.00015)\n",
            "epoch: 1898, train_loss: 0.187351, val loss: 0.350650,  train_metric: -0.943 test_metric: -0.877 lr: 0.00015)\n",
            "epoch: 1899, train_loss: 0.187116, val loss: 0.348651,  train_metric: -0.940 test_metric: -0.884 lr: 0.00015)\n",
            "epoch: 1900, train_loss: 0.186113, val loss: 0.341444,  train_metric: -0.942 test_metric: -0.886 lr: 0.00015)\n",
            "epoch: 1901, train_loss: 0.187780, val loss: 0.346580,  train_metric: -0.939 test_metric: -0.886 lr: 0.00015)\n",
            "epoch: 1902, train_loss: 0.188085, val loss: 0.349288,  train_metric: -0.937 test_metric: -0.876 lr: 0.00015)\n",
            "epoch: 1903, train_loss: 0.190721, val loss: 0.339458,  train_metric: -0.935 test_metric: -0.887 lr: 0.00015)\n",
            "epoch: 1904, train_loss: 0.190537, val loss: 0.353412,  train_metric: -0.937 test_metric: -0.870 lr: 0.00015)\n",
            "epoch: 1905, train_loss: 0.189203, val loss: 0.343230,  train_metric: -0.934 test_metric: -0.879 lr: 0.00015)\n",
            "epoch: 1906, train_loss: 0.185969, val loss: 0.341500,  train_metric: -0.939 test_metric: -0.882 lr: 0.00015)\n",
            "epoch: 1907, train_loss: 0.185394, val loss: 0.349543,  train_metric: -0.939 test_metric: -0.887 lr: 0.00015)\n",
            "epoch: 1908, train_loss: 0.189490, val loss: 0.348269,  train_metric: -0.935 test_metric: -0.884 lr: 0.00015)\n",
            "epoch: 1909, train_loss: 0.191286, val loss: 0.352261,  train_metric: -0.936 test_metric: -0.870 lr: 0.00015)\n",
            "epoch: 1910, train_loss: 0.192653, val loss: 0.355165,  train_metric: -0.933 test_metric: -0.869 lr: 0.00015)\n",
            "epoch: 1911, train_loss: 0.187295, val loss: 0.347622,  train_metric: -0.939 test_metric: -0.885 lr: 0.00015)\n",
            "epoch: 1912, train_loss: 0.186397, val loss: 0.345503,  train_metric: -0.939 test_metric: -0.887 lr: 0.00015)\n",
            "epoch: 1913, train_loss: 0.187668, val loss: 0.352403,  train_metric: -0.938 test_metric: -0.880 lr: 0.00015)\n",
            "epoch: 1914, train_loss: 0.191114, val loss: 0.345675,  train_metric: -0.936 test_metric: -0.881 lr: 0.00015)\n",
            "epoch: 1915, train_loss: 0.187236, val loss: 0.352199,  train_metric: -0.943 test_metric: -0.872 lr: 0.00015)\n",
            "epoch: 1916, train_loss: 0.191526, val loss: 0.355488,  train_metric: -0.934 test_metric: -0.867 lr: 0.00015)\n",
            "epoch: 1917, train_loss: 0.187150, val loss: 0.344273,  train_metric: -0.939 test_metric: -0.876 lr: 0.00015)\n",
            "epoch: 1918, train_loss: 0.188205, val loss: 0.361089,  train_metric: -0.936 test_metric: -0.869 lr: 0.00015)\n",
            "epoch: 1919, train_loss: 0.189178, val loss: 0.342128,  train_metric: -0.934 test_metric: -0.881 lr: 0.00015)\n",
            "epoch: 1920, train_loss: 0.185629, val loss: 0.348982,  train_metric: -0.940 test_metric: -0.881 lr: 0.00015)\n",
            "epoch: 1921, train_loss: 0.186280, val loss: 0.344321,  train_metric: -0.940 test_metric: -0.880 lr: 0.00015)\n",
            "epoch: 1922, train_loss: 0.186637, val loss: 0.360196,  train_metric: -0.943 test_metric: -0.880 lr: 0.00015)\n",
            "epoch: 1923, train_loss: 0.192539, val loss: 0.348057,  train_metric: -0.937 test_metric: -0.887 lr: 0.00015)\n",
            "epoch: 1924, train_loss: 0.191775, val loss: 0.349103,  train_metric: -0.935 test_metric: -0.872 lr: 0.00015)\n",
            "epoch: 1925, train_loss: 0.185860, val loss: 0.352936,  train_metric: -0.940 test_metric: -0.872 lr: 0.00015)\n",
            "epoch: 1926, train_loss: 0.186638, val loss: 0.340699,  train_metric: -0.937 test_metric: -0.885 lr: 0.00015)\n",
            "epoch: 1927, train_loss: 0.183339, val loss: 0.353302,  train_metric: -0.940 test_metric: -0.870 lr: 0.00015)\n",
            "epoch: 1928, train_loss: 0.185229, val loss: 0.341838,  train_metric: -0.940 test_metric: -0.886 lr: 0.00015)\n",
            "epoch: 1929, train_loss: 0.187551, val loss: 0.340468,  train_metric: -0.942 test_metric: -0.887 lr: 0.00015)\n",
            "epoch: 1930, train_loss: 0.184485, val loss: 0.339331,  train_metric: -0.940 test_metric: -0.888 lr: 0.00014)\n",
            "epoch: 1931, train_loss: 0.187146, val loss: 0.343746,  train_metric: -0.941 test_metric: -0.881 lr: 0.00014)\n",
            "epoch: 1932, train_loss: 0.189118, val loss: 0.351408,  train_metric: -0.934 test_metric: -0.871 lr: 0.00014)\n",
            "epoch: 1933, train_loss: 0.185925, val loss: 0.349985,  train_metric: -0.939 test_metric: -0.874 lr: 0.00014)\n",
            "epoch: 1934, train_loss: 0.187429, val loss: 0.346980,  train_metric: -0.938 test_metric: -0.875 lr: 0.00014)\n",
            "epoch: 1935, train_loss: 0.190820, val loss: 0.348067,  train_metric: -0.935 test_metric: -0.885 lr: 0.00014)\n",
            "epoch: 1936, train_loss: 0.187679, val loss: 0.349117,  train_metric: -0.942 test_metric: -0.879 lr: 0.00014)\n",
            "epoch: 1937, train_loss: 0.185419, val loss: 0.341743,  train_metric: -0.938 test_metric: -0.886 lr: 0.00014)\n",
            "epoch: 1938, train_loss: 0.184802, val loss: 0.336377,  train_metric: -0.939 test_metric: -0.891 lr: 0.00014)\n",
            "epoch: 1939, train_loss: 0.185030, val loss: 0.340225,  train_metric: -0.943 test_metric: -0.886 lr: 0.00014)\n",
            "epoch: 1940, train_loss: 0.185324, val loss: 0.346318,  train_metric: -0.945 test_metric: -0.875 lr: 0.00014)\n",
            "epoch: 1941, train_loss: 0.186027, val loss: 0.354366,  train_metric: -0.936 test_metric: -0.875 lr: 0.00014)\n",
            "epoch: 1942, train_loss: 0.188144, val loss: 0.338933,  train_metric: -0.936 test_metric: -0.892 lr: 0.00014)\n",
            "epoch: 1943, train_loss: 0.183879, val loss: 0.345695,  train_metric: -0.939 test_metric: -0.881 lr: 0.00014)\n",
            "epoch: 1944, train_loss: 0.183481, val loss: 0.342002,  train_metric: -0.936 test_metric: -0.885 lr: 0.00014)\n",
            "epoch: 1945, train_loss: 0.182806, val loss: 0.339833,  train_metric: -0.945 test_metric: -0.888 lr: 0.00014)\n",
            "epoch: 1946, train_loss: 0.184075, val loss: 0.342168,  train_metric: -0.942 test_metric: -0.884 lr: 0.00014)\n",
            "epoch: 1947, train_loss: 0.185181, val loss: 0.347564,  train_metric: -0.935 test_metric: -0.888 lr: 0.00014)\n",
            "epoch: 1948, train_loss: 0.184944, val loss: 0.339285,  train_metric: -0.939 test_metric: -0.886 lr: 0.00014)\n",
            "epoch: 1949, train_loss: 0.184433, val loss: 0.353963,  train_metric: -0.938 test_metric: -0.882 lr: 0.00014)\n",
            "epoch: 1950, train_loss: 0.187133, val loss: 0.344569,  train_metric: -0.936 test_metric: -0.885 lr: 0.00014)\n",
            "epoch: 1951, train_loss: 0.185620, val loss: 0.341828,  train_metric: -0.943 test_metric: -0.887 lr: 0.00014)\n",
            "epoch: 1952, train_loss: 0.185707, val loss: 0.340575,  train_metric: -0.936 test_metric: -0.884 lr: 0.00014)\n",
            "epoch: 1953, train_loss: 0.186296, val loss: 0.343212,  train_metric: -0.935 test_metric: -0.891 lr: 0.00014)\n",
            "epoch: 1954, train_loss: 0.185659, val loss: 0.343289,  train_metric: -0.939 test_metric: -0.888 lr: 0.00014)\n",
            "epoch: 1955, train_loss: 0.185860, val loss: 0.347465,  train_metric: -0.940 test_metric: -0.877 lr: 0.00014)\n",
            "epoch: 1956, train_loss: 0.183740, val loss: 0.348350,  train_metric: -0.938 test_metric: -0.877 lr: 0.00014)\n",
            "epoch: 1957, train_loss: 0.182709, val loss: 0.340410,  train_metric: -0.942 test_metric: -0.885 lr: 0.00014)\n",
            "epoch: 1958, train_loss: 0.182071, val loss: 0.342716,  train_metric: -0.944 test_metric: -0.884 lr: 0.00014)\n",
            "epoch: 1959, train_loss: 0.185123, val loss: 0.337907,  train_metric: -0.942 test_metric: -0.892 lr: 0.00014)\n",
            "epoch: 1960, train_loss: 0.188374, val loss: 0.343846,  train_metric: -0.937 test_metric: -0.887 lr: 0.00014)\n",
            "epoch: 1961, train_loss: 0.183831, val loss: 0.341044,  train_metric: -0.938 test_metric: -0.885 lr: 0.00014)\n",
            "epoch: 1962, train_loss: 0.183830, val loss: 0.347084,  train_metric: -0.939 test_metric: -0.886 lr: 0.00014)\n",
            "epoch: 1963, train_loss: 0.183941, val loss: 0.372028,  train_metric: -0.946 test_metric: -0.859 lr: 0.00014)\n",
            "epoch: 1964, train_loss: 0.191869, val loss: 0.350161,  train_metric: -0.932 test_metric: -0.875 lr: 0.00014)\n",
            "epoch: 1965, train_loss: 0.187426, val loss: 0.348144,  train_metric: -0.936 test_metric: -0.882 lr: 0.00014)\n",
            "epoch: 1966, train_loss: 0.186737, val loss: 0.345036,  train_metric: -0.936 test_metric: -0.893 lr: 0.00014)\n",
            "epoch: 1967, train_loss: 0.184512, val loss: 0.343499,  train_metric: -0.946 test_metric: -0.882 lr: 0.00014)\n",
            "epoch: 1968, train_loss: 0.184576, val loss: 0.349586,  train_metric: -0.940 test_metric: -0.875 lr: 0.00014)\n",
            "epoch: 1969, train_loss: 0.185195, val loss: 0.344564,  train_metric: -0.935 test_metric: -0.875 lr: 0.00014)\n",
            "epoch: 1970, train_loss: 0.184460, val loss: 0.340747,  train_metric: -0.936 test_metric: -0.888 lr: 0.00014)\n",
            "epoch: 1971, train_loss: 0.181746, val loss: 0.342474,  train_metric: -0.944 test_metric: -0.886 lr: 0.00014)\n",
            "epoch: 1972, train_loss: 0.185607, val loss: 0.351640,  train_metric: -0.937 test_metric: -0.881 lr: 0.00014)\n",
            "epoch: 1973, train_loss: 0.188688, val loss: 0.347870,  train_metric: -0.932 test_metric: -0.884 lr: 0.00014)\n",
            "epoch: 1974, train_loss: 0.187733, val loss: 0.342642,  train_metric: -0.936 test_metric: -0.882 lr: 0.00014)\n",
            "epoch: 1975, train_loss: 0.185046, val loss: 0.357616,  train_metric: -0.937 test_metric: -0.871 lr: 0.00014)\n",
            "epoch: 1976, train_loss: 0.187736, val loss: 0.338755,  train_metric: -0.936 test_metric: -0.895 lr: 0.00014)\n",
            "epoch: 1977, train_loss: 0.187051, val loss: 0.345919,  train_metric: -0.938 test_metric: -0.879 lr: 0.00014)\n",
            "epoch: 1978, train_loss: 0.184687, val loss: 0.357652,  train_metric: -0.938 test_metric: -0.871 lr: 0.00014)\n",
            "epoch: 1979, train_loss: 0.186134, val loss: 0.354069,  train_metric: -0.935 test_metric: -0.874 lr: 0.00014)\n",
            "epoch: 1980, train_loss: 0.183929, val loss: 0.342679,  train_metric: -0.940 test_metric: -0.880 lr: 0.00014)\n",
            "epoch: 1981, train_loss: 0.185205, val loss: 0.339618,  train_metric: -0.938 test_metric: -0.895 lr: 0.00014)\n",
            "epoch: 1982, train_loss: 0.185320, val loss: 0.340463,  train_metric: -0.939 test_metric: -0.891 lr: 0.00014)\n",
            "epoch: 1983, train_loss: 0.183841, val loss: 0.337079,  train_metric: -0.944 test_metric: -0.897 lr: 0.00014)\n",
            "epoch: 1984, train_loss: 0.182853, val loss: 0.354919,  train_metric: -0.945 test_metric: -0.870 lr: 0.00014)\n",
            "epoch: 1985, train_loss: 0.185729, val loss: 0.350452,  train_metric: -0.939 test_metric: -0.876 lr: 0.00014)\n",
            "epoch: 1986, train_loss: 0.185770, val loss: 0.360026,  train_metric: -0.937 test_metric: -0.869 lr: 0.00014)\n",
            "epoch: 1987, train_loss: 0.184570, val loss: 0.347575,  train_metric: -0.940 test_metric: -0.879 lr: 0.00014)\n",
            "epoch: 1988, train_loss: 0.181197, val loss: 0.342247,  train_metric: -0.944 test_metric: -0.880 lr: 0.00014)\n",
            "epoch: 1989, train_loss: 0.182494, val loss: 0.340361,  train_metric: -0.941 test_metric: -0.886 lr: 0.00014)\n",
            "epoch: 1990, train_loss: 0.184152, val loss: 0.350993,  train_metric: -0.940 test_metric: -0.887 lr: 0.00014)\n",
            "epoch: 1991, train_loss: 0.186123, val loss: 0.341665,  train_metric: -0.938 test_metric: -0.888 lr: 0.00014)\n",
            "epoch: 1992, train_loss: 0.183474, val loss: 0.342273,  train_metric: -0.942 test_metric: -0.882 lr: 0.00014)\n",
            "epoch: 1993, train_loss: 0.182076, val loss: 0.354583,  train_metric: -0.943 test_metric: -0.870 lr: 0.00014)\n",
            "epoch: 1994, train_loss: 0.182634, val loss: 0.351246,  train_metric: -0.940 test_metric: -0.885 lr: 0.00014)\n",
            "epoch: 1995, train_loss: 0.185131, val loss: 0.339834,  train_metric: -0.945 test_metric: -0.881 lr: 0.00014)\n",
            "epoch: 1996, train_loss: 0.184061, val loss: 0.339082,  train_metric: -0.938 test_metric: -0.893 lr: 0.00014)\n",
            "epoch: 1997, train_loss: 0.181686, val loss: 0.342181,  train_metric: -0.939 test_metric: -0.890 lr: 0.00014)\n",
            "epoch: 1998, train_loss: 0.184263, val loss: 0.337338,  train_metric: -0.942 test_metric: -0.892 lr: 0.00014)\n",
            "epoch: 1999, train_loss: 0.185332, val loss: 0.339362,  train_metric: -0.936 test_metric: -0.891 lr: 0.00014)\n",
            "epoch: 2000, train_loss: 0.181337, val loss: 0.353883,  train_metric: -0.944 test_metric: -0.872 lr: 0.00014)\n",
            "epoch: 2001, train_loss: 0.183992, val loss: 0.349402,  train_metric: -0.940 test_metric: -0.881 lr: 0.00013)\n",
            "epoch: 2002, train_loss: 0.184266, val loss: 0.342628,  train_metric: -0.936 test_metric: -0.884 lr: 0.00013)\n",
            "epoch: 2003, train_loss: 0.182114, val loss: 0.342535,  train_metric: -0.941 test_metric: -0.885 lr: 0.00013)\n",
            "epoch: 2004, train_loss: 0.180437, val loss: 0.337180,  train_metric: -0.945 test_metric: -0.888 lr: 0.00013)\n",
            "epoch: 2005, train_loss: 0.180770, val loss: 0.341293,  train_metric: -0.945 test_metric: -0.879 lr: 0.00013)\n",
            "epoch: 2006, train_loss: 0.180730, val loss: 0.344088,  train_metric: -0.943 test_metric: -0.885 lr: 0.00013)\n",
            "epoch: 2007, train_loss: 0.180281, val loss: 0.352853,  train_metric: -0.946 test_metric: -0.872 lr: 0.00013)\n",
            "epoch: 2008, train_loss: 0.186582, val loss: 0.347524,  train_metric: -0.936 test_metric: -0.882 lr: 0.00013)\n",
            "epoch: 2009, train_loss: 0.184266, val loss: 0.346219,  train_metric: -0.938 test_metric: -0.886 lr: 0.00013)\n",
            "epoch: 2010, train_loss: 0.182407, val loss: 0.346362,  train_metric: -0.939 test_metric: -0.892 lr: 0.00013)\n",
            "epoch: 2011, train_loss: 0.185637, val loss: 0.347171,  train_metric: -0.943 test_metric: -0.882 lr: 0.00013)\n",
            "epoch: 2012, train_loss: 0.184440, val loss: 0.344939,  train_metric: -0.940 test_metric: -0.886 lr: 0.00013)\n",
            "epoch: 2013, train_loss: 0.181159, val loss: 0.340765,  train_metric: -0.944 test_metric: -0.881 lr: 0.00013)\n",
            "epoch: 2014, train_loss: 0.180630, val loss: 0.348532,  train_metric: -0.941 test_metric: -0.885 lr: 0.00013)\n",
            "epoch: 2015, train_loss: 0.180773, val loss: 0.350060,  train_metric: -0.944 test_metric: -0.874 lr: 0.00013)\n",
            "epoch: 2016, train_loss: 0.183962, val loss: 0.348800,  train_metric: -0.938 test_metric: -0.879 lr: 0.00013)\n",
            "epoch: 2017, train_loss: 0.182146, val loss: 0.347380,  train_metric: -0.942 test_metric: -0.875 lr: 0.00013)\n",
            "epoch: 2018, train_loss: 0.182523, val loss: 0.342328,  train_metric: -0.938 test_metric: -0.880 lr: 0.00013)\n",
            "epoch: 2019, train_loss: 0.181322, val loss: 0.345863,  train_metric: -0.947 test_metric: -0.885 lr: 0.00013)\n",
            "epoch: 2020, train_loss: 0.181060, val loss: 0.339772,  train_metric: -0.946 test_metric: -0.880 lr: 0.00013)\n",
            "epoch: 2021, train_loss: 0.183551, val loss: 0.346153,  train_metric: -0.938 test_metric: -0.882 lr: 0.00013)\n",
            "epoch: 2022, train_loss: 0.180626, val loss: 0.347474,  train_metric: -0.942 test_metric: -0.877 lr: 0.00013)\n",
            "epoch: 2023, train_loss: 0.181877, val loss: 0.340847,  train_metric: -0.941 test_metric: -0.882 lr: 0.00013)\n",
            "epoch: 2024, train_loss: 0.181213, val loss: 0.345807,  train_metric: -0.940 test_metric: -0.882 lr: 0.00013)\n",
            "epoch: 2025, train_loss: 0.180572, val loss: 0.340800,  train_metric: -0.944 test_metric: -0.884 lr: 0.00013)\n",
            "epoch: 2026, train_loss: 0.180728, val loss: 0.342659,  train_metric: -0.942 test_metric: -0.882 lr: 0.00013)\n",
            "epoch: 2027, train_loss: 0.182798, val loss: 0.338368,  train_metric: -0.939 test_metric: -0.888 lr: 0.00013)\n",
            "epoch: 2028, train_loss: 0.180537, val loss: 0.342379,  train_metric: -0.947 test_metric: -0.892 lr: 0.00013)\n",
            "epoch: 2029, train_loss: 0.187109, val loss: 0.339933,  train_metric: -0.940 test_metric: -0.888 lr: 0.00013)\n",
            "epoch: 2030, train_loss: 0.185797, val loss: 0.350925,  train_metric: -0.938 test_metric: -0.872 lr: 0.00013)\n",
            "epoch: 2031, train_loss: 0.188251, val loss: 0.343526,  train_metric: -0.938 test_metric: -0.881 lr: 0.00013)\n",
            "epoch: 2032, train_loss: 0.182847, val loss: 0.340995,  train_metric: -0.936 test_metric: -0.885 lr: 0.00013)\n",
            "epoch: 2033, train_loss: 0.183049, val loss: 0.340006,  train_metric: -0.940 test_metric: -0.881 lr: 0.00013)\n",
            "epoch: 2034, train_loss: 0.181827, val loss: 0.349985,  train_metric: -0.943 test_metric: -0.879 lr: 0.00013)\n",
            "epoch: 2035, train_loss: 0.181162, val loss: 0.343986,  train_metric: -0.943 test_metric: -0.882 lr: 0.00013)\n",
            "epoch: 2036, train_loss: 0.183546, val loss: 0.351533,  train_metric: -0.938 test_metric: -0.876 lr: 0.00013)\n",
            "epoch: 2037, train_loss: 0.183323, val loss: 0.349428,  train_metric: -0.941 test_metric: -0.871 lr: 0.00013)\n",
            "epoch: 2038, train_loss: 0.182499, val loss: 0.344418,  train_metric: -0.938 test_metric: -0.882 lr: 0.00013)\n",
            "epoch: 2039, train_loss: 0.181483, val loss: 0.338950,  train_metric: -0.945 test_metric: -0.895 lr: 0.00013)\n",
            "epoch: 2040, train_loss: 0.182404, val loss: 0.336938,  train_metric: -0.943 test_metric: -0.882 lr: 0.00013)\n",
            "epoch: 2041, train_loss: 0.181323, val loss: 0.344608,  train_metric: -0.940 test_metric: -0.888 lr: 0.00013)\n",
            "epoch: 2042, train_loss: 0.180080, val loss: 0.339051,  train_metric: -0.946 test_metric: -0.885 lr: 0.00013)\n",
            "epoch: 2043, train_loss: 0.178760, val loss: 0.342248,  train_metric: -0.943 test_metric: -0.882 lr: 0.00013)\n",
            "epoch: 2044, train_loss: 0.178937, val loss: 0.337330,  train_metric: -0.942 test_metric: -0.887 lr: 0.00013)\n",
            "epoch: 2045, train_loss: 0.179536, val loss: 0.343655,  train_metric: -0.945 test_metric: -0.890 lr: 0.00013)\n",
            "epoch: 2046, train_loss: 0.181854, val loss: 0.353333,  train_metric: -0.940 test_metric: -0.884 lr: 0.00013)\n",
            "epoch: 2047, train_loss: 0.182529, val loss: 0.338806,  train_metric: -0.937 test_metric: -0.891 lr: 0.00013)\n",
            "epoch: 2048, train_loss: 0.180859, val loss: 0.337494,  train_metric: -0.949 test_metric: -0.890 lr: 0.00013)\n",
            "epoch: 2049, train_loss: 0.179018, val loss: 0.354384,  train_metric: -0.943 test_metric: -0.874 lr: 0.00013)\n",
            "epoch: 2050, train_loss: 0.180273, val loss: 0.355947,  train_metric: -0.944 test_metric: -0.872 lr: 0.00013)\n",
            "epoch: 2051, train_loss: 0.181418, val loss: 0.344139,  train_metric: -0.940 test_metric: -0.884 lr: 0.00013)\n",
            "epoch: 2052, train_loss: 0.180784, val loss: 0.352263,  train_metric: -0.944 test_metric: -0.874 lr: 0.00013)\n",
            "epoch: 2053, train_loss: 0.179846, val loss: 0.339862,  train_metric: -0.942 test_metric: -0.885 lr: 0.00013)\n",
            "epoch: 2054, train_loss: 0.182032, val loss: 0.344660,  train_metric: -0.944 test_metric: -0.892 lr: 0.00013)\n",
            "epoch: 2055, train_loss: 0.181530, val loss: 0.341813,  train_metric: -0.938 test_metric: -0.887 lr: 0.00013)\n",
            "epoch: 2056, train_loss: 0.183078, val loss: 0.341398,  train_metric: -0.938 test_metric: -0.893 lr: 0.00013)\n",
            "epoch: 2057, train_loss: 0.181416, val loss: 0.343025,  train_metric: -0.939 test_metric: -0.881 lr: 0.00013)\n",
            "epoch: 2058, train_loss: 0.179660, val loss: 0.337325,  train_metric: -0.943 test_metric: -0.888 lr: 0.00013)\n",
            "epoch: 2059, train_loss: 0.180704, val loss: 0.345421,  train_metric: -0.942 test_metric: -0.880 lr: 0.00013)\n",
            "epoch: 2060, train_loss: 0.180177, val loss: 0.346435,  train_metric: -0.942 test_metric: -0.879 lr: 0.00013)\n",
            "epoch: 2061, train_loss: 0.181439, val loss: 0.342192,  train_metric: -0.945 test_metric: -0.881 lr: 0.00013)\n",
            "epoch: 2062, train_loss: 0.180084, val loss: 0.357394,  train_metric: -0.948 test_metric: -0.870 lr: 0.00013)\n",
            "epoch: 2063, train_loss: 0.183835, val loss: 0.344292,  train_metric: -0.934 test_metric: -0.888 lr: 0.00013)\n",
            "epoch: 2064, train_loss: 0.180838, val loss: 0.338575,  train_metric: -0.943 test_metric: -0.884 lr: 0.00013)\n",
            "epoch: 2065, train_loss: 0.179743, val loss: 0.340101,  train_metric: -0.945 test_metric: -0.882 lr: 0.00013)\n",
            "epoch: 2066, train_loss: 0.178163, val loss: 0.351149,  train_metric: -0.941 test_metric: -0.875 lr: 0.00013)\n",
            "epoch: 2067, train_loss: 0.179579, val loss: 0.344225,  train_metric: -0.939 test_metric: -0.880 lr: 0.00013)\n",
            "epoch: 2068, train_loss: 0.181782, val loss: 0.336234,  train_metric: -0.942 test_metric: -0.891 lr: 0.00013)\n",
            "epoch: 2069, train_loss: 0.177945, val loss: 0.359912,  train_metric: -0.944 test_metric: -0.875 lr: 0.00013)\n",
            "epoch: 2070, train_loss: 0.184996, val loss: 0.335932,  train_metric: -0.937 test_metric: -0.886 lr: 0.00013)\n",
            "epoch: 2071, train_loss: 0.182063, val loss: 0.342924,  train_metric: -0.942 test_metric: -0.886 lr: 0.00013)\n",
            "epoch: 2072, train_loss: 0.181162, val loss: 0.342508,  train_metric: -0.939 test_metric: -0.888 lr: 0.00013)\n",
            "epoch: 2073, train_loss: 0.180267, val loss: 0.341165,  train_metric: -0.947 test_metric: -0.872 lr: 0.00013)\n",
            "epoch: 2074, train_loss: 0.180516, val loss: 0.349615,  train_metric: -0.942 test_metric: -0.874 lr: 0.00013)\n",
            "epoch: 2075, train_loss: 0.178654, val loss: 0.344679,  train_metric: -0.940 test_metric: -0.884 lr: 0.00013)\n",
            "epoch: 2076, train_loss: 0.181301, val loss: 0.341792,  train_metric: -0.941 test_metric: -0.886 lr: 0.00013)\n",
            "epoch: 2077, train_loss: 0.178990, val loss: 0.342169,  train_metric: -0.940 test_metric: -0.886 lr: 0.00013)\n",
            "epoch: 2078, train_loss: 0.180088, val loss: 0.335643,  train_metric: -0.941 test_metric: -0.886 lr: 0.00012)\n",
            "epoch: 2079, train_loss: 0.178414, val loss: 0.348428,  train_metric: -0.950 test_metric: -0.874 lr: 0.00012)\n",
            "epoch: 2080, train_loss: 0.179176, val loss: 0.339312,  train_metric: -0.944 test_metric: -0.890 lr: 0.00012)\n",
            "epoch: 2081, train_loss: 0.180004, val loss: 0.345448,  train_metric: -0.943 test_metric: -0.887 lr: 0.00012)\n",
            "epoch: 2082, train_loss: 0.179223, val loss: 0.339037,  train_metric: -0.945 test_metric: -0.888 lr: 0.00012)\n",
            "epoch: 2083, train_loss: 0.178188, val loss: 0.343170,  train_metric: -0.949 test_metric: -0.888 lr: 0.00012)\n",
            "epoch: 2084, train_loss: 0.184911, val loss: 0.346010,  train_metric: -0.940 test_metric: -0.877 lr: 0.00012)\n",
            "epoch: 2085, train_loss: 0.180139, val loss: 0.347416,  train_metric: -0.941 test_metric: -0.879 lr: 0.00012)\n",
            "epoch: 2086, train_loss: 0.180040, val loss: 0.343254,  train_metric: -0.940 test_metric: -0.885 lr: 0.00012)\n",
            "epoch: 2087, train_loss: 0.178363, val loss: 0.363096,  train_metric: -0.944 test_metric: -0.866 lr: 0.00012)\n",
            "epoch: 2088, train_loss: 0.186279, val loss: 0.366200,  train_metric: -0.937 test_metric: -0.861 lr: 0.00012)\n",
            "epoch: 2089, train_loss: 0.180729, val loss: 0.354452,  train_metric: -0.940 test_metric: -0.870 lr: 0.00012)\n",
            "epoch: 2090, train_loss: 0.182656, val loss: 0.341763,  train_metric: -0.941 test_metric: -0.882 lr: 0.00012)\n",
            "epoch: 2091, train_loss: 0.180076, val loss: 0.345488,  train_metric: -0.942 test_metric: -0.891 lr: 0.00012)\n",
            "epoch: 2092, train_loss: 0.179622, val loss: 0.337501,  train_metric: -0.946 test_metric: -0.891 lr: 0.00012)\n",
            "epoch: 2093, train_loss: 0.179738, val loss: 0.346451,  train_metric: -0.943 test_metric: -0.885 lr: 0.00012)\n",
            "epoch: 2094, train_loss: 0.181178, val loss: 0.348209,  train_metric: -0.941 test_metric: -0.884 lr: 0.00012)\n",
            "epoch: 2095, train_loss: 0.180251, val loss: 0.339199,  train_metric: -0.939 test_metric: -0.893 lr: 0.00012)\n",
            "epoch: 2096, train_loss: 0.180810, val loss: 0.343369,  train_metric: -0.945 test_metric: -0.893 lr: 0.00012)\n",
            "epoch: 2097, train_loss: 0.181493, val loss: 0.344100,  train_metric: -0.938 test_metric: -0.880 lr: 0.00012)\n",
            "epoch: 2098, train_loss: 0.178029, val loss: 0.339962,  train_metric: -0.942 test_metric: -0.891 lr: 0.00012)\n",
            "epoch: 2099, train_loss: 0.181273, val loss: 0.351743,  train_metric: -0.941 test_metric: -0.886 lr: 0.00012)\n",
            "epoch: 2100, train_loss: 0.181031, val loss: 0.335533,  train_metric: -0.940 test_metric: -0.890 lr: 0.00012)\n",
            "epoch: 2101, train_loss: 0.178562, val loss: 0.342136,  train_metric: -0.947 test_metric: -0.885 lr: 0.00012)\n",
            "epoch: 2102, train_loss: 0.178666, val loss: 0.342622,  train_metric: -0.943 test_metric: -0.891 lr: 0.00012)\n",
            "epoch: 2103, train_loss: 0.183273, val loss: 0.338393,  train_metric: -0.937 test_metric: -0.898 lr: 0.00012)\n",
            "epoch: 2104, train_loss: 0.181662, val loss: 0.352523,  train_metric: -0.937 test_metric: -0.884 lr: 0.00012)\n",
            "epoch: 2105, train_loss: 0.182233, val loss: 0.342237,  train_metric: -0.942 test_metric: -0.875 lr: 0.00012)\n",
            "epoch: 2106, train_loss: 0.182394, val loss: 0.340032,  train_metric: -0.939 test_metric: -0.881 lr: 0.00012)\n",
            "epoch: 2107, train_loss: 0.177365, val loss: 0.340555,  train_metric: -0.942 test_metric: -0.886 lr: 0.00012)\n",
            "epoch: 2108, train_loss: 0.177652, val loss: 0.336551,  train_metric: -0.950 test_metric: -0.887 lr: 0.00012)\n",
            "epoch: 2109, train_loss: 0.177571, val loss: 0.340511,  train_metric: -0.944 test_metric: -0.887 lr: 0.00012)\n",
            "epoch: 2110, train_loss: 0.177708, val loss: 0.338027,  train_metric: -0.943 test_metric: -0.895 lr: 0.00012)\n",
            "epoch: 2111, train_loss: 0.179466, val loss: 0.343342,  train_metric: -0.943 test_metric: -0.885 lr: 0.00012)\n",
            "epoch: 2112, train_loss: 0.177467, val loss: 0.338313,  train_metric: -0.945 test_metric: -0.886 lr: 0.00012)\n",
            "epoch: 2113, train_loss: 0.177390, val loss: 0.348373,  train_metric: -0.945 test_metric: -0.885 lr: 0.00012)\n",
            "epoch: 2114, train_loss: 0.181700, val loss: 0.336972,  train_metric: -0.940 test_metric: -0.888 lr: 0.00012)\n",
            "epoch: 2115, train_loss: 0.177709, val loss: 0.339476,  train_metric: -0.943 test_metric: -0.882 lr: 0.00012)\n",
            "epoch: 2116, train_loss: 0.178334, val loss: 0.347770,  train_metric: -0.943 test_metric: -0.879 lr: 0.00012)\n",
            "epoch: 2117, train_loss: 0.177218, val loss: 0.337381,  train_metric: -0.943 test_metric: -0.885 lr: 0.00012)\n",
            "epoch: 2118, train_loss: 0.177115, val loss: 0.348560,  train_metric: -0.946 test_metric: -0.874 lr: 0.00012)\n",
            "epoch: 2119, train_loss: 0.179173, val loss: 0.337735,  train_metric: -0.948 test_metric: -0.887 lr: 0.00012)\n",
            "epoch: 2120, train_loss: 0.176985, val loss: 0.343814,  train_metric: -0.939 test_metric: -0.881 lr: 0.00012)\n",
            "epoch: 2121, train_loss: 0.179004, val loss: 0.341142,  train_metric: -0.942 test_metric: -0.885 lr: 0.00012)\n",
            "epoch: 2122, train_loss: 0.177553, val loss: 0.338167,  train_metric: -0.947 test_metric: -0.893 lr: 0.00012)\n",
            "epoch: 2123, train_loss: 0.177218, val loss: 0.341067,  train_metric: -0.944 test_metric: -0.884 lr: 0.00012)\n",
            "epoch: 2124, train_loss: 0.179236, val loss: 0.335665,  train_metric: -0.938 test_metric: -0.888 lr: 0.00012)\n",
            "epoch: 2125, train_loss: 0.179699, val loss: 0.339448,  train_metric: -0.947 test_metric: -0.888 lr: 0.00012)\n",
            "epoch: 2126, train_loss: 0.176730, val loss: 0.340960,  train_metric: -0.946 test_metric: -0.887 lr: 0.00012)\n",
            "epoch: 2127, train_loss: 0.179137, val loss: 0.343897,  train_metric: -0.936 test_metric: -0.891 lr: 0.00012)\n",
            "epoch: 2128, train_loss: 0.178170, val loss: 0.337228,  train_metric: -0.947 test_metric: -0.893 lr: 0.00012)\n",
            "epoch: 2129, train_loss: 0.178213, val loss: 0.342430,  train_metric: -0.944 test_metric: -0.880 lr: 0.00012)\n",
            "epoch: 2130, train_loss: 0.180769, val loss: 0.338871,  train_metric: -0.941 test_metric: -0.886 lr: 0.00012)\n",
            "epoch: 2131, train_loss: 0.178797, val loss: 0.344269,  train_metric: -0.946 test_metric: -0.884 lr: 0.00012)\n",
            "epoch: 2132, train_loss: 0.177264, val loss: 0.339116,  train_metric: -0.943 test_metric: -0.886 lr: 0.00012)\n",
            "epoch: 2133, train_loss: 0.176120, val loss: 0.345140,  train_metric: -0.943 test_metric: -0.880 lr: 0.00012)\n",
            "epoch: 2134, train_loss: 0.178231, val loss: 0.344789,  train_metric: -0.943 test_metric: -0.879 lr: 0.00012)\n",
            "epoch: 2135, train_loss: 0.176691, val loss: 0.348226,  train_metric: -0.940 test_metric: -0.875 lr: 0.00012)\n",
            "epoch: 2136, train_loss: 0.180141, val loss: 0.346630,  train_metric: -0.942 test_metric: -0.882 lr: 0.00012)\n",
            "epoch: 2137, train_loss: 0.175630, val loss: 0.334887,  train_metric: -0.946 test_metric: -0.888 lr: 0.00012)\n",
            "epoch: 2138, train_loss: 0.175554, val loss: 0.339317,  train_metric: -0.948 test_metric: -0.890 lr: 0.00012)\n",
            "epoch: 2139, train_loss: 0.176715, val loss: 0.342933,  train_metric: -0.942 test_metric: -0.884 lr: 0.00012)\n",
            "epoch: 2140, train_loss: 0.178434, val loss: 0.340029,  train_metric: -0.942 test_metric: -0.884 lr: 0.00012)\n",
            "epoch: 2141, train_loss: 0.178009, val loss: 0.343837,  train_metric: -0.939 test_metric: -0.887 lr: 0.00012)\n",
            "epoch: 2142, train_loss: 0.178019, val loss: 0.337205,  train_metric: -0.946 test_metric: -0.890 lr: 0.00012)\n",
            "epoch: 2143, train_loss: 0.178798, val loss: 0.336898,  train_metric: -0.943 test_metric: -0.891 lr: 0.00012)\n",
            "epoch: 2144, train_loss: 0.177115, val loss: 0.342332,  train_metric: -0.947 test_metric: -0.895 lr: 0.00012)\n",
            "epoch: 2145, train_loss: 0.177647, val loss: 0.342860,  train_metric: -0.945 test_metric: -0.880 lr: 0.00012)\n",
            "epoch: 2146, train_loss: 0.179111, val loss: 0.341188,  train_metric: -0.943 test_metric: -0.888 lr: 0.00012)\n",
            "epoch: 2147, train_loss: 0.176436, val loss: 0.348927,  train_metric: -0.946 test_metric: -0.884 lr: 0.00012)\n",
            "epoch: 2148, train_loss: 0.177372, val loss: 0.341252,  train_metric: -0.943 test_metric: -0.885 lr: 0.00012)\n",
            "epoch: 2149, train_loss: 0.176265, val loss: 0.339346,  train_metric: -0.945 test_metric: -0.888 lr: 0.00012)\n",
            "epoch: 2150, train_loss: 0.174555, val loss: 0.345822,  train_metric: -0.948 test_metric: -0.875 lr: 0.00012)\n",
            "epoch: 2151, train_loss: 0.177332, val loss: 0.342951,  train_metric: -0.941 test_metric: -0.876 lr: 0.00012)\n",
            "epoch: 2152, train_loss: 0.179364, val loss: 0.335925,  train_metric: -0.941 test_metric: -0.887 lr: 0.00012)\n",
            "epoch: 2153, train_loss: 0.176242, val loss: 0.343583,  train_metric: -0.945 test_metric: -0.890 lr: 0.00012)\n",
            "epoch: 2154, train_loss: 0.176657, val loss: 0.335845,  train_metric: -0.944 test_metric: -0.892 lr: 0.00012)\n",
            "epoch: 2155, train_loss: 0.174493, val loss: 0.338008,  train_metric: -0.943 test_metric: -0.890 lr: 0.00012)\n",
            "epoch: 2156, train_loss: 0.176195, val loss: 0.336152,  train_metric: -0.945 test_metric: -0.886 lr: 0.00012)\n",
            "epoch: 2157, train_loss: 0.176622, val loss: 0.343907,  train_metric: -0.947 test_metric: -0.890 lr: 0.00012)\n",
            "epoch: 2158, train_loss: 0.177157, val loss: 0.340951,  train_metric: -0.943 test_metric: -0.892 lr: 0.00012)\n",
            "epoch: 2159, train_loss: 0.179595, val loss: 0.344948,  train_metric: -0.938 test_metric: -0.876 lr: 0.00012)\n",
            "epoch: 2160, train_loss: 0.175879, val loss: 0.342562,  train_metric: -0.948 test_metric: -0.890 lr: 0.00012)\n",
            "epoch: 2161, train_loss: 0.176089, val loss: 0.341251,  train_metric: -0.948 test_metric: -0.884 lr: 0.00011)\n",
            "epoch: 2162, train_loss: 0.176933, val loss: 0.343820,  train_metric: -0.938 test_metric: -0.881 lr: 0.00011)\n",
            "epoch: 2163, train_loss: 0.176496, val loss: 0.344695,  train_metric: -0.943 test_metric: -0.877 lr: 0.00011)\n",
            "epoch: 2164, train_loss: 0.177049, val loss: 0.347141,  train_metric: -0.940 test_metric: -0.877 lr: 0.00011)\n",
            "epoch: 2165, train_loss: 0.177123, val loss: 0.351990,  train_metric: -0.939 test_metric: -0.872 lr: 0.00011)\n",
            "epoch: 2166, train_loss: 0.182097, val loss: 0.359620,  train_metric: -0.935 test_metric: -0.866 lr: 0.00011)\n",
            "epoch: 2167, train_loss: 0.179163, val loss: 0.346520,  train_metric: -0.940 test_metric: -0.876 lr: 0.00011)\n",
            "epoch: 2168, train_loss: 0.176481, val loss: 0.342962,  train_metric: -0.943 test_metric: -0.881 lr: 0.00011)\n",
            "epoch: 2169, train_loss: 0.177682, val loss: 0.338975,  train_metric: -0.946 test_metric: -0.885 lr: 0.00011)\n",
            "epoch: 2170, train_loss: 0.176993, val loss: 0.374624,  train_metric: -0.943 test_metric: -0.861 lr: 0.00011)\n",
            "epoch: 2171, train_loss: 0.182808, val loss: 0.353530,  train_metric: -0.939 test_metric: -0.867 lr: 0.00011)\n",
            "epoch: 2172, train_loss: 0.179820, val loss: 0.351563,  train_metric: -0.937 test_metric: -0.875 lr: 0.00011)\n",
            "epoch: 2173, train_loss: 0.177451, val loss: 0.352694,  train_metric: -0.942 test_metric: -0.874 lr: 0.00011)\n",
            "epoch: 2174, train_loss: 0.179360, val loss: 0.353404,  train_metric: -0.940 test_metric: -0.871 lr: 0.00011)\n",
            "epoch: 2175, train_loss: 0.177245, val loss: 0.349663,  train_metric: -0.940 test_metric: -0.879 lr: 0.00011)\n",
            "epoch: 2176, train_loss: 0.176957, val loss: 0.344657,  train_metric: -0.943 test_metric: -0.876 lr: 0.00011)\n",
            "epoch: 2177, train_loss: 0.175630, val loss: 0.340166,  train_metric: -0.940 test_metric: -0.884 lr: 0.00011)\n",
            "epoch: 2178, train_loss: 0.174041, val loss: 0.351077,  train_metric: -0.951 test_metric: -0.874 lr: 0.00011)\n",
            "epoch: 2179, train_loss: 0.176484, val loss: 0.338501,  train_metric: -0.944 test_metric: -0.885 lr: 0.00011)\n",
            "epoch: 2180, train_loss: 0.178827, val loss: 0.351273,  train_metric: -0.945 test_metric: -0.874 lr: 0.00011)\n",
            "epoch: 2181, train_loss: 0.176889, val loss: 0.345850,  train_metric: -0.944 test_metric: -0.877 lr: 0.00011)\n",
            "epoch: 2182, train_loss: 0.175330, val loss: 0.353028,  train_metric: -0.946 test_metric: -0.872 lr: 0.00011)\n",
            "epoch: 2183, train_loss: 0.175743, val loss: 0.340345,  train_metric: -0.946 test_metric: -0.882 lr: 0.00011)\n",
            "epoch: 2184, train_loss: 0.173635, val loss: 0.340238,  train_metric: -0.949 test_metric: -0.891 lr: 0.00011)\n",
            "epoch: 2185, train_loss: 0.175243, val loss: 0.337748,  train_metric: -0.947 test_metric: -0.895 lr: 0.00011)\n",
            "epoch: 2186, train_loss: 0.175798, val loss: 0.337320,  train_metric: -0.942 test_metric: -0.892 lr: 0.00011)\n",
            "epoch: 2187, train_loss: 0.175391, val loss: 0.338327,  train_metric: -0.951 test_metric: -0.891 lr: 0.00011)\n",
            "epoch: 2188, train_loss: 0.174668, val loss: 0.338664,  train_metric: -0.945 test_metric: -0.884 lr: 0.00011)\n",
            "epoch: 2189, train_loss: 0.175224, val loss: 0.337718,  train_metric: -0.944 test_metric: -0.893 lr: 0.00011)\n",
            "epoch: 2190, train_loss: 0.174049, val loss: 0.340695,  train_metric: -0.944 test_metric: -0.888 lr: 0.00011)\n",
            "epoch: 2191, train_loss: 0.175801, val loss: 0.336528,  train_metric: -0.946 test_metric: -0.891 lr: 0.00011)\n",
            "epoch: 2192, train_loss: 0.176407, val loss: 0.339744,  train_metric: -0.945 test_metric: -0.877 lr: 0.00011)\n",
            "epoch: 2193, train_loss: 0.177949, val loss: 0.342573,  train_metric: -0.942 test_metric: -0.884 lr: 0.00011)\n",
            "epoch: 2194, train_loss: 0.174173, val loss: 0.337755,  train_metric: -0.945 test_metric: -0.887 lr: 0.00011)\n",
            "epoch: 2195, train_loss: 0.174930, val loss: 0.346148,  train_metric: -0.949 test_metric: -0.882 lr: 0.00011)\n",
            "epoch: 2196, train_loss: 0.177426, val loss: 0.338446,  train_metric: -0.940 test_metric: -0.881 lr: 0.00011)\n",
            "epoch: 2197, train_loss: 0.176590, val loss: 0.342148,  train_metric: -0.942 test_metric: -0.884 lr: 0.00011)\n",
            "epoch: 2198, train_loss: 0.176597, val loss: 0.338481,  train_metric: -0.943 test_metric: -0.886 lr: 0.00011)\n",
            "epoch: 2199, train_loss: 0.173937, val loss: 0.336716,  train_metric: -0.948 test_metric: -0.890 lr: 0.00011)\n",
            "epoch: 2200, train_loss: 0.175029, val loss: 0.342117,  train_metric: -0.943 test_metric: -0.881 lr: 0.00011)\n",
            "epoch: 2201, train_loss: 0.175008, val loss: 0.343520,  train_metric: -0.945 test_metric: -0.882 lr: 0.00011)\n",
            "epoch: 2202, train_loss: 0.175086, val loss: 0.341057,  train_metric: -0.944 test_metric: -0.886 lr: 0.00011)\n",
            "epoch: 2203, train_loss: 0.173796, val loss: 0.340007,  train_metric: -0.951 test_metric: -0.885 lr: 0.00011)\n",
            "epoch: 2204, train_loss: 0.173835, val loss: 0.337755,  train_metric: -0.946 test_metric: -0.887 lr: 0.00011)\n",
            "epoch: 2205, train_loss: 0.175805, val loss: 0.336392,  train_metric: -0.945 test_metric: -0.892 lr: 0.00011)\n",
            "epoch: 2206, train_loss: 0.176565, val loss: 0.339226,  train_metric: -0.946 test_metric: -0.888 lr: 0.00011)\n",
            "epoch: 2207, train_loss: 0.173804, val loss: 0.340122,  train_metric: -0.947 test_metric: -0.884 lr: 0.00011)\n",
            "epoch: 2208, train_loss: 0.173595, val loss: 0.336312,  train_metric: -0.945 test_metric: -0.887 lr: 0.00011)\n",
            "epoch: 2209, train_loss: 0.174736, val loss: 0.336172,  train_metric: -0.943 test_metric: -0.890 lr: 0.00011)\n",
            "epoch: 2210, train_loss: 0.173999, val loss: 0.337434,  train_metric: -0.948 test_metric: -0.886 lr: 0.00011)\n",
            "epoch: 2211, train_loss: 0.172998, val loss: 0.351694,  train_metric: -0.947 test_metric: -0.872 lr: 0.00011)\n",
            "epoch: 2212, train_loss: 0.176146, val loss: 0.340138,  train_metric: -0.942 test_metric: -0.882 lr: 0.00011)\n",
            "epoch: 2213, train_loss: 0.174003, val loss: 0.350460,  train_metric: -0.947 test_metric: -0.871 lr: 0.00011)\n",
            "epoch: 2214, train_loss: 0.177958, val loss: 0.353517,  train_metric: -0.943 test_metric: -0.875 lr: 0.00011)\n",
            "epoch: 2215, train_loss: 0.178198, val loss: 0.346881,  train_metric: -0.936 test_metric: -0.877 lr: 0.00011)\n",
            "epoch: 2216, train_loss: 0.173404, val loss: 0.336990,  train_metric: -0.945 test_metric: -0.887 lr: 0.00011)\n",
            "epoch: 2217, train_loss: 0.174824, val loss: 0.337468,  train_metric: -0.943 test_metric: -0.890 lr: 0.00011)\n",
            "epoch: 2218, train_loss: 0.174162, val loss: 0.341703,  train_metric: -0.943 test_metric: -0.886 lr: 0.00011)\n",
            "epoch: 2219, train_loss: 0.174209, val loss: 0.344424,  train_metric: -0.949 test_metric: -0.877 lr: 0.00011)\n",
            "epoch: 2220, train_loss: 0.175340, val loss: 0.337109,  train_metric: -0.942 test_metric: -0.888 lr: 0.00011)\n",
            "epoch: 2221, train_loss: 0.176239, val loss: 0.335776,  train_metric: -0.940 test_metric: -0.893 lr: 0.00011)\n",
            "epoch: 2222, train_loss: 0.173541, val loss: 0.337066,  train_metric: -0.952 test_metric: -0.893 lr: 0.00011)\n",
            "epoch: 2223, train_loss: 0.175358, val loss: 0.344223,  train_metric: -0.946 test_metric: -0.888 lr: 0.00011)\n",
            "epoch: 2224, train_loss: 0.173932, val loss: 0.341250,  train_metric: -0.947 test_metric: -0.885 lr: 0.00011)\n",
            "epoch: 2225, train_loss: 0.175138, val loss: 0.338009,  train_metric: -0.941 test_metric: -0.891 lr: 0.00011)\n",
            "epoch: 2226, train_loss: 0.177148, val loss: 0.337477,  train_metric: -0.945 test_metric: -0.891 lr: 0.00011)\n",
            "epoch: 2227, train_loss: 0.175795, val loss: 0.335335,  train_metric: -0.940 test_metric: -0.897 lr: 0.00011)\n",
            "epoch: 2228, train_loss: 0.173204, val loss: 0.336866,  train_metric: -0.948 test_metric: -0.884 lr: 0.00011)\n",
            "epoch: 2229, train_loss: 0.173422, val loss: 0.340143,  train_metric: -0.943 test_metric: -0.886 lr: 0.00011)\n",
            "epoch: 2230, train_loss: 0.173787, val loss: 0.333515,  train_metric: -0.949 test_metric: -0.893 lr: 0.00011)\n",
            "epoch: 2231, train_loss: 0.175848, val loss: 0.338112,  train_metric: -0.943 test_metric: -0.896 lr: 0.00011)\n",
            "epoch: 2232, train_loss: 0.176887, val loss: 0.336140,  train_metric: -0.943 test_metric: -0.888 lr: 0.00011)\n",
            "epoch: 2233, train_loss: 0.173494, val loss: 0.340591,  train_metric: -0.945 test_metric: -0.886 lr: 0.00011)\n",
            "epoch: 2234, train_loss: 0.172932, val loss: 0.340728,  train_metric: -0.946 test_metric: -0.884 lr: 0.00011)\n",
            "epoch: 2235, train_loss: 0.174708, val loss: 0.341017,  train_metric: -0.946 test_metric: -0.881 lr: 0.00011)\n",
            "epoch: 2236, train_loss: 0.174544, val loss: 0.344675,  train_metric: -0.947 test_metric: -0.881 lr: 0.00011)\n",
            "epoch: 2237, train_loss: 0.174142, val loss: 0.342738,  train_metric: -0.945 test_metric: -0.881 lr: 0.00011)\n",
            "epoch: 2238, train_loss: 0.174033, val loss: 0.337596,  train_metric: -0.946 test_metric: -0.885 lr: 0.00011)\n",
            "epoch: 2239, train_loss: 0.172771, val loss: 0.340657,  train_metric: -0.944 test_metric: -0.891 lr: 0.00011)\n",
            "epoch: 2240, train_loss: 0.175587, val loss: 0.338274,  train_metric: -0.946 test_metric: -0.885 lr: 0.00011)\n",
            "epoch: 2241, train_loss: 0.173178, val loss: 0.340034,  train_metric: -0.949 test_metric: -0.892 lr: 0.00011)\n",
            "epoch: 2242, train_loss: 0.174375, val loss: 0.353201,  train_metric: -0.946 test_metric: -0.871 lr: 0.00011)\n",
            "epoch: 2243, train_loss: 0.177496, val loss: 0.345865,  train_metric: -0.941 test_metric: -0.871 lr: 0.00011)\n",
            "epoch: 2244, train_loss: 0.174969, val loss: 0.339615,  train_metric: -0.943 test_metric: -0.881 lr: 0.00011)\n",
            "epoch: 2245, train_loss: 0.173429, val loss: 0.348441,  train_metric: -0.945 test_metric: -0.880 lr: 0.00011)\n",
            "epoch: 2246, train_loss: 0.176884, val loss: 0.337678,  train_metric: -0.941 test_metric: -0.885 lr: 0.00011)\n",
            "epoch: 2247, train_loss: 0.172488, val loss: 0.350312,  train_metric: -0.948 test_metric: -0.875 lr: 0.00011)\n",
            "epoch: 2248, train_loss: 0.174644, val loss: 0.339306,  train_metric: -0.943 test_metric: -0.881 lr: 0.00011)\n",
            "epoch: 2249, train_loss: 0.173291, val loss: 0.341278,  train_metric: -0.946 test_metric: -0.887 lr: 0.00011)\n",
            "epoch: 2250, train_loss: 0.176126, val loss: 0.339798,  train_metric: -0.939 test_metric: -0.892 lr: 0.00011)\n",
            "epoch: 2251, train_loss: 0.173310, val loss: 0.338878,  train_metric: -0.949 test_metric: -0.886 lr: 0.00011)\n",
            "epoch: 2252, train_loss: 0.172273, val loss: 0.342307,  train_metric: -0.945 test_metric: -0.882 lr: 0.00010)\n",
            "epoch: 2253, train_loss: 0.172725, val loss: 0.339909,  train_metric: -0.949 test_metric: -0.884 lr: 0.00010)\n",
            "epoch: 2254, train_loss: 0.175886, val loss: 0.341843,  train_metric: -0.942 test_metric: -0.891 lr: 0.00010)\n",
            "epoch: 2255, train_loss: 0.176169, val loss: 0.340898,  train_metric: -0.944 test_metric: -0.893 lr: 0.00010)\n",
            "epoch: 2256, train_loss: 0.177537, val loss: 0.346116,  train_metric: -0.940 test_metric: -0.885 lr: 0.00010)\n",
            "epoch: 2257, train_loss: 0.175432, val loss: 0.339378,  train_metric: -0.943 test_metric: -0.893 lr: 0.00010)\n",
            "epoch: 2258, train_loss: 0.174531, val loss: 0.335320,  train_metric: -0.941 test_metric: -0.895 lr: 0.00010)\n",
            "epoch: 2259, train_loss: 0.173869, val loss: 0.337157,  train_metric: -0.950 test_metric: -0.895 lr: 0.00010)\n",
            "epoch: 2260, train_loss: 0.173566, val loss: 0.344047,  train_metric: -0.946 test_metric: -0.890 lr: 0.00010)\n",
            "epoch: 2261, train_loss: 0.173213, val loss: 0.341524,  train_metric: -0.943 test_metric: -0.893 lr: 0.00010)\n",
            "epoch: 2262, train_loss: 0.176286, val loss: 0.338639,  train_metric: -0.944 test_metric: -0.890 lr: 0.00010)\n",
            "epoch: 2263, train_loss: 0.174083, val loss: 0.339369,  train_metric: -0.948 test_metric: -0.896 lr: 0.00010)\n",
            "epoch: 2264, train_loss: 0.178119, val loss: 0.350303,  train_metric: -0.944 test_metric: -0.884 lr: 0.00010)\n",
            "epoch: 2265, train_loss: 0.174889, val loss: 0.339204,  train_metric: -0.945 test_metric: -0.890 lr: 0.00010)\n",
            "epoch: 2266, train_loss: 0.174011, val loss: 0.341221,  train_metric: -0.944 test_metric: -0.885 lr: 0.00010)\n",
            "epoch: 2267, train_loss: 0.171371, val loss: 0.337922,  train_metric: -0.949 test_metric: -0.896 lr: 0.00010)\n",
            "epoch: 2268, train_loss: 0.172780, val loss: 0.335120,  train_metric: -0.946 test_metric: -0.886 lr: 0.00010)\n",
            "epoch: 2269, train_loss: 0.172120, val loss: 0.335627,  train_metric: -0.947 test_metric: -0.890 lr: 0.00010)\n",
            "epoch: 2270, train_loss: 0.171686, val loss: 0.341922,  train_metric: -0.948 test_metric: -0.885 lr: 0.00010)\n",
            "epoch: 2271, train_loss: 0.171952, val loss: 0.338924,  train_metric: -0.948 test_metric: -0.881 lr: 0.00010)\n",
            "epoch: 2272, train_loss: 0.171830, val loss: 0.343870,  train_metric: -0.944 test_metric: -0.884 lr: 0.00010)\n",
            "epoch: 2273, train_loss: 0.173785, val loss: 0.341965,  train_metric: -0.944 test_metric: -0.879 lr: 0.00010)\n",
            "epoch: 2274, train_loss: 0.176098, val loss: 0.351388,  train_metric: -0.943 test_metric: -0.880 lr: 0.00010)\n",
            "epoch: 2275, train_loss: 0.176781, val loss: 0.342653,  train_metric: -0.938 test_metric: -0.876 lr: 0.00010)\n",
            "epoch: 2276, train_loss: 0.172944, val loss: 0.339941,  train_metric: -0.948 test_metric: -0.888 lr: 0.00010)\n",
            "epoch: 2277, train_loss: 0.172355, val loss: 0.345229,  train_metric: -0.942 test_metric: -0.880 lr: 0.00010)\n",
            "epoch: 2278, train_loss: 0.172272, val loss: 0.344148,  train_metric: -0.943 test_metric: -0.882 lr: 0.00010)\n",
            "epoch: 2279, train_loss: 0.170925, val loss: 0.338848,  train_metric: -0.949 test_metric: -0.890 lr: 0.00010)\n",
            "epoch: 2280, train_loss: 0.172388, val loss: 0.341650,  train_metric: -0.948 test_metric: -0.887 lr: 0.00010)\n",
            "epoch: 2281, train_loss: 0.173841, val loss: 0.336594,  train_metric: -0.945 test_metric: -0.890 lr: 0.00010)\n",
            "epoch: 2282, train_loss: 0.176011, val loss: 0.345166,  train_metric: -0.939 test_metric: -0.881 lr: 0.00010)\n",
            "epoch: 2283, train_loss: 0.172872, val loss: 0.343390,  train_metric: -0.945 test_metric: -0.877 lr: 0.00010)\n",
            "epoch: 2284, train_loss: 0.174773, val loss: 0.341344,  train_metric: -0.940 test_metric: -0.877 lr: 0.00010)\n",
            "epoch: 2285, train_loss: 0.173354, val loss: 0.341422,  train_metric: -0.944 test_metric: -0.891 lr: 0.00010)\n",
            "epoch: 2286, train_loss: 0.177264, val loss: 0.346976,  train_metric: -0.940 test_metric: -0.886 lr: 0.00010)\n",
            "epoch: 2287, train_loss: 0.175728, val loss: 0.340634,  train_metric: -0.940 test_metric: -0.891 lr: 0.00010)\n",
            "epoch: 2288, train_loss: 0.174826, val loss: 0.339577,  train_metric: -0.943 test_metric: -0.890 lr: 0.00010)\n",
            "epoch: 2289, train_loss: 0.173502, val loss: 0.337341,  train_metric: -0.947 test_metric: -0.891 lr: 0.00010)\n",
            "epoch: 2290, train_loss: 0.172359, val loss: 0.337197,  train_metric: -0.947 test_metric: -0.896 lr: 0.00010)\n",
            "epoch: 2291, train_loss: 0.174209, val loss: 0.339917,  train_metric: -0.944 test_metric: -0.880 lr: 0.00010)\n",
            "epoch: 2292, train_loss: 0.173541, val loss: 0.343156,  train_metric: -0.940 test_metric: -0.886 lr: 0.00010)\n",
            "epoch: 2293, train_loss: 0.173274, val loss: 0.340193,  train_metric: -0.946 test_metric: -0.888 lr: 0.00010)\n",
            "epoch: 2294, train_loss: 0.171194, val loss: 0.334422,  train_metric: -0.947 test_metric: -0.895 lr: 0.00010)\n",
            "epoch: 2295, train_loss: 0.171158, val loss: 0.344358,  train_metric: -0.948 test_metric: -0.884 lr: 0.00010)\n",
            "epoch: 2296, train_loss: 0.172450, val loss: 0.337367,  train_metric: -0.944 test_metric: -0.888 lr: 0.00010)\n",
            "epoch: 2297, train_loss: 0.172421, val loss: 0.332685,  train_metric: -0.948 test_metric: -0.897 lr: 0.00010)\n",
            "epoch: 2298, train_loss: 0.173952, val loss: 0.340142,  train_metric: -0.946 test_metric: -0.891 lr: 0.00010)\n",
            "epoch: 2299, train_loss: 0.173345, val loss: 0.341589,  train_metric: -0.948 test_metric: -0.886 lr: 0.00010)\n",
            "epoch: 2300, train_loss: 0.173090, val loss: 0.340326,  train_metric: -0.944 test_metric: -0.882 lr: 0.00010)\n",
            "epoch: 2301, train_loss: 0.173903, val loss: 0.339909,  train_metric: -0.944 test_metric: -0.882 lr: 0.00010)\n",
            "epoch: 2302, train_loss: 0.173019, val loss: 0.340006,  train_metric: -0.948 test_metric: -0.888 lr: 0.00010)\n",
            "epoch: 2303, train_loss: 0.170279, val loss: 0.336181,  train_metric: -0.948 test_metric: -0.890 lr: 0.00010)\n",
            "epoch: 2304, train_loss: 0.170493, val loss: 0.339275,  train_metric: -0.948 test_metric: -0.882 lr: 0.00010)\n",
            "epoch: 2305, train_loss: 0.171045, val loss: 0.347406,  train_metric: -0.949 test_metric: -0.876 lr: 0.00010)\n",
            "epoch: 2306, train_loss: 0.177890, val loss: 0.344506,  train_metric: -0.941 test_metric: -0.882 lr: 0.00010)\n",
            "epoch: 2307, train_loss: 0.172842, val loss: 0.343644,  train_metric: -0.945 test_metric: -0.888 lr: 0.00010)\n",
            "epoch: 2308, train_loss: 0.175788, val loss: 0.337594,  train_metric: -0.948 test_metric: -0.892 lr: 0.00010)\n",
            "epoch: 2309, train_loss: 0.171589, val loss: 0.342424,  train_metric: -0.944 test_metric: -0.887 lr: 0.00010)\n",
            "epoch: 2310, train_loss: 0.172730, val loss: 0.337853,  train_metric: -0.949 test_metric: -0.887 lr: 0.00010)\n",
            "epoch: 2311, train_loss: 0.175888, val loss: 0.340394,  train_metric: -0.944 test_metric: -0.885 lr: 0.00010)\n",
            "epoch: 2312, train_loss: 0.173962, val loss: 0.338174,  train_metric: -0.943 test_metric: -0.882 lr: 0.00010)\n",
            "epoch: 2313, train_loss: 0.171032, val loss: 0.333556,  train_metric: -0.947 test_metric: -0.892 lr: 0.00010)\n",
            "epoch: 2314, train_loss: 0.172028, val loss: 0.340522,  train_metric: -0.948 test_metric: -0.890 lr: 0.00010)\n",
            "epoch: 2315, train_loss: 0.171519, val loss: 0.338630,  train_metric: -0.946 test_metric: -0.886 lr: 0.00010)\n",
            "epoch: 2316, train_loss: 0.169980, val loss: 0.336280,  train_metric: -0.952 test_metric: -0.884 lr: 0.00010)\n",
            "epoch: 2317, train_loss: 0.170506, val loss: 0.335071,  train_metric: -0.950 test_metric: -0.895 lr: 0.00010)\n",
            "epoch: 2318, train_loss: 0.170698, val loss: 0.343965,  train_metric: -0.948 test_metric: -0.893 lr: 0.00010)\n",
            "epoch: 2319, train_loss: 0.172373, val loss: 0.343330,  train_metric: -0.948 test_metric: -0.881 lr: 0.00010)\n",
            "epoch: 2320, train_loss: 0.171091, val loss: 0.339293,  train_metric: -0.945 test_metric: -0.890 lr: 0.00010)\n",
            "epoch: 2321, train_loss: 0.172109, val loss: 0.343762,  train_metric: -0.946 test_metric: -0.885 lr: 0.00010)\n",
            "epoch: 2322, train_loss: 0.169773, val loss: 0.337551,  train_metric: -0.948 test_metric: -0.888 lr: 0.00010)\n",
            "epoch: 2323, train_loss: 0.170628, val loss: 0.337468,  train_metric: -0.953 test_metric: -0.888 lr: 0.00010)\n",
            "epoch: 2324, train_loss: 0.169963, val loss: 0.336174,  train_metric: -0.947 test_metric: -0.888 lr: 0.00010)\n",
            "epoch: 2325, train_loss: 0.170944, val loss: 0.334449,  train_metric: -0.951 test_metric: -0.897 lr: 0.00010)\n",
            "epoch: 2326, train_loss: 0.171301, val loss: 0.338990,  train_metric: -0.953 test_metric: -0.888 lr: 0.00010)\n",
            "epoch: 2327, train_loss: 0.171612, val loss: 0.347421,  train_metric: -0.945 test_metric: -0.875 lr: 0.00010)\n",
            "epoch: 2328, train_loss: 0.173376, val loss: 0.336051,  train_metric: -0.943 test_metric: -0.890 lr: 0.00010)\n",
            "epoch: 2329, train_loss: 0.173427, val loss: 0.337072,  train_metric: -0.945 test_metric: -0.888 lr: 0.00010)\n",
            "epoch: 2330, train_loss: 0.171700, val loss: 0.348334,  train_metric: -0.950 test_metric: -0.875 lr: 0.00010)\n",
            "epoch: 2331, train_loss: 0.172900, val loss: 0.340509,  train_metric: -0.945 test_metric: -0.884 lr: 0.00010)\n",
            "epoch: 2332, train_loss: 0.169979, val loss: 0.334639,  train_metric: -0.947 test_metric: -0.885 lr: 0.00010)\n",
            "epoch: 2333, train_loss: 0.170596, val loss: 0.338933,  train_metric: -0.948 test_metric: -0.886 lr: 0.00010)\n",
            "epoch: 2334, train_loss: 0.170347, val loss: 0.336243,  train_metric: -0.953 test_metric: -0.893 lr: 0.00010)\n",
            "epoch: 2335, train_loss: 0.169651, val loss: 0.340832,  train_metric: -0.950 test_metric: -0.887 lr: 0.00010)\n",
            "epoch: 2336, train_loss: 0.169866, val loss: 0.335960,  train_metric: -0.946 test_metric: -0.887 lr: 0.00010)\n",
            "epoch: 2337, train_loss: 0.171540, val loss: 0.344046,  train_metric: -0.946 test_metric: -0.885 lr: 0.00010)\n",
            "epoch: 2338, train_loss: 0.170119, val loss: 0.340762,  train_metric: -0.946 test_metric: -0.887 lr: 0.00010)\n",
            "epoch: 2339, train_loss: 0.172298, val loss: 0.339480,  train_metric: -0.953 test_metric: -0.885 lr: 0.00010)\n",
            "epoch: 2340, train_loss: 0.171041, val loss: 0.341973,  train_metric: -0.951 test_metric: -0.892 lr: 0.00010)\n",
            "epoch: 2341, train_loss: 0.173649, val loss: 0.341945,  train_metric: -0.947 test_metric: -0.888 lr: 0.00010)\n",
            "epoch: 2342, train_loss: 0.170437, val loss: 0.340104,  train_metric: -0.946 test_metric: -0.885 lr: 0.00010)\n",
            "epoch: 2343, train_loss: 0.171350, val loss: 0.335355,  train_metric: -0.945 test_metric: -0.890 lr: 0.00010)\n",
            "epoch: 2344, train_loss: 0.171574, val loss: 0.333888,  train_metric: -0.944 test_metric: -0.893 lr: 0.00010)\n",
            "epoch: 2345, train_loss: 0.170767, val loss: 0.338150,  train_metric: -0.950 test_metric: -0.888 lr: 0.00010)\n",
            "epoch: 2346, train_loss: 0.169695, val loss: 0.342615,  train_metric: -0.949 test_metric: -0.880 lr: 0.00010)\n",
            "epoch: 2347, train_loss: 0.171284, val loss: 0.337649,  train_metric: -0.950 test_metric: -0.891 lr: 0.00010)\n",
            "epoch: 2348, train_loss: 0.173328, val loss: 0.357385,  train_metric: -0.945 test_metric: -0.880 lr: 0.00010)\n",
            "epoch: 2349, train_loss: 0.182157, val loss: 0.337082,  train_metric: -0.933 test_metric: -0.888 lr: 0.00010)\n",
            "epoch: 2350, train_loss: 0.171296, val loss: 0.339018,  train_metric: -0.946 test_metric: -0.886 lr: 0.00010)\n",
            "epoch: 2351, train_loss: 0.170588, val loss: 0.343937,  train_metric: -0.950 test_metric: -0.882 lr: 0.00010)\n",
            "epoch: 2352, train_loss: 0.171573, val loss: 0.336821,  train_metric: -0.947 test_metric: -0.888 lr: 0.00009)\n",
            "epoch: 2353, train_loss: 0.172350, val loss: 0.339328,  train_metric: -0.943 test_metric: -0.888 lr: 0.00009)\n",
            "epoch: 2354, train_loss: 0.171857, val loss: 0.333431,  train_metric: -0.948 test_metric: -0.890 lr: 0.00009)\n",
            "epoch: 2355, train_loss: 0.172850, val loss: 0.341577,  train_metric: -0.946 test_metric: -0.891 lr: 0.00009)\n",
            "epoch: 2356, train_loss: 0.172364, val loss: 0.334709,  train_metric: -0.945 test_metric: -0.891 lr: 0.00009)\n",
            "epoch: 2357, train_loss: 0.169495, val loss: 0.336159,  train_metric: -0.948 test_metric: -0.891 lr: 0.00009)\n",
            "epoch: 2358, train_loss: 0.169857, val loss: 0.333802,  train_metric: -0.949 test_metric: -0.891 lr: 0.00009)\n",
            "epoch: 2359, train_loss: 0.169319, val loss: 0.337293,  train_metric: -0.948 test_metric: -0.887 lr: 0.00009)\n",
            "epoch: 2360, train_loss: 0.171876, val loss: 0.336501,  train_metric: -0.948 test_metric: -0.893 lr: 0.00009)\n",
            "epoch: 2361, train_loss: 0.170299, val loss: 0.340248,  train_metric: -0.949 test_metric: -0.886 lr: 0.00009)\n",
            "epoch: 2362, train_loss: 0.170215, val loss: 0.337378,  train_metric: -0.950 test_metric: -0.895 lr: 0.00009)\n",
            "epoch: 2363, train_loss: 0.168761, val loss: 0.337613,  train_metric: -0.952 test_metric: -0.884 lr: 0.00009)\n",
            "epoch: 2364, train_loss: 0.168142, val loss: 0.335773,  train_metric: -0.947 test_metric: -0.890 lr: 0.00009)\n",
            "epoch: 2365, train_loss: 0.173765, val loss: 0.339750,  train_metric: -0.945 test_metric: -0.893 lr: 0.00009)\n",
            "epoch: 2366, train_loss: 0.169346, val loss: 0.338356,  train_metric: -0.948 test_metric: -0.892 lr: 0.00009)\n",
            "epoch: 2367, train_loss: 0.169401, val loss: 0.342930,  train_metric: -0.946 test_metric: -0.881 lr: 0.00009)\n",
            "epoch: 2368, train_loss: 0.169732, val loss: 0.339298,  train_metric: -0.949 test_metric: -0.886 lr: 0.00009)\n",
            "epoch: 2369, train_loss: 0.168932, val loss: 0.343502,  train_metric: -0.951 test_metric: -0.879 lr: 0.00009)\n",
            "epoch: 2370, train_loss: 0.169983, val loss: 0.344771,  train_metric: -0.948 test_metric: -0.875 lr: 0.00009)\n",
            "epoch: 2371, train_loss: 0.169959, val loss: 0.342394,  train_metric: -0.948 test_metric: -0.881 lr: 0.00009)\n",
            "epoch: 2372, train_loss: 0.173818, val loss: 0.343752,  train_metric: -0.942 test_metric: -0.879 lr: 0.00009)\n",
            "epoch: 2373, train_loss: 0.169927, val loss: 0.346065,  train_metric: -0.945 test_metric: -0.871 lr: 0.00009)\n",
            "epoch: 2374, train_loss: 0.171436, val loss: 0.340039,  train_metric: -0.945 test_metric: -0.884 lr: 0.00009)\n",
            "epoch: 2375, train_loss: 0.170108, val loss: 0.342863,  train_metric: -0.945 test_metric: -0.881 lr: 0.00009)\n",
            "epoch: 2376, train_loss: 0.170506, val loss: 0.339466,  train_metric: -0.947 test_metric: -0.884 lr: 0.00009)\n",
            "epoch: 2377, train_loss: 0.170858, val loss: 0.340546,  train_metric: -0.947 test_metric: -0.879 lr: 0.00009)\n",
            "epoch: 2378, train_loss: 0.169373, val loss: 0.346879,  train_metric: -0.948 test_metric: -0.881 lr: 0.00009)\n",
            "epoch: 2379, train_loss: 0.170044, val loss: 0.339189,  train_metric: -0.944 test_metric: -0.885 lr: 0.00009)\n",
            "epoch: 2380, train_loss: 0.175172, val loss: 0.345279,  train_metric: -0.946 test_metric: -0.876 lr: 0.00009)\n",
            "epoch: 2381, train_loss: 0.171043, val loss: 0.355051,  train_metric: -0.948 test_metric: -0.872 lr: 0.00009)\n",
            "epoch: 2382, train_loss: 0.171904, val loss: 0.341405,  train_metric: -0.946 test_metric: -0.879 lr: 0.00009)\n",
            "epoch: 2383, train_loss: 0.169419, val loss: 0.333941,  train_metric: -0.945 test_metric: -0.890 lr: 0.00009)\n",
            "epoch: 2384, train_loss: 0.170144, val loss: 0.351104,  train_metric: -0.947 test_metric: -0.875 lr: 0.00009)\n",
            "epoch: 2385, train_loss: 0.171355, val loss: 0.336084,  train_metric: -0.945 test_metric: -0.881 lr: 0.00009)\n",
            "epoch: 2386, train_loss: 0.168747, val loss: 0.339397,  train_metric: -0.951 test_metric: -0.886 lr: 0.00009)\n",
            "epoch: 2387, train_loss: 0.168996, val loss: 0.338932,  train_metric: -0.950 test_metric: -0.893 lr: 0.00009)\n",
            "epoch: 2388, train_loss: 0.169143, val loss: 0.340249,  train_metric: -0.951 test_metric: -0.885 lr: 0.00009)\n",
            "epoch: 2389, train_loss: 0.168175, val loss: 0.339326,  train_metric: -0.944 test_metric: -0.890 lr: 0.00009)\n",
            "epoch: 2390, train_loss: 0.173547, val loss: 0.334587,  train_metric: -0.950 test_metric: -0.895 lr: 0.00009)\n",
            "epoch: 2391, train_loss: 0.170318, val loss: 0.342176,  train_metric: -0.945 test_metric: -0.886 lr: 0.00009)\n",
            "epoch: 2392, train_loss: 0.168965, val loss: 0.336915,  train_metric: -0.946 test_metric: -0.893 lr: 0.00009)\n",
            "epoch: 2393, train_loss: 0.170000, val loss: 0.334443,  train_metric: -0.946 test_metric: -0.890 lr: 0.00009)\n",
            "epoch: 2394, train_loss: 0.169321, val loss: 0.335153,  train_metric: -0.951 test_metric: -0.892 lr: 0.00009)\n",
            "epoch: 2395, train_loss: 0.167679, val loss: 0.338005,  train_metric: -0.949 test_metric: -0.884 lr: 0.00009)\n",
            "epoch: 2396, train_loss: 0.167833, val loss: 0.334374,  train_metric: -0.947 test_metric: -0.890 lr: 0.00009)\n",
            "epoch: 2397, train_loss: 0.167432, val loss: 0.338847,  train_metric: -0.949 test_metric: -0.887 lr: 0.00009)\n",
            "epoch: 2398, train_loss: 0.168132, val loss: 0.337080,  train_metric: -0.953 test_metric: -0.895 lr: 0.00009)\n",
            "epoch: 2399, train_loss: 0.168525, val loss: 0.341267,  train_metric: -0.946 test_metric: -0.884 lr: 0.00009)\n",
            "epoch: 2400, train_loss: 0.169118, val loss: 0.350819,  train_metric: -0.949 test_metric: -0.871 lr: 0.00009)\n",
            "epoch: 2401, train_loss: 0.170452, val loss: 0.347206,  train_metric: -0.948 test_metric: -0.876 lr: 0.00009)\n",
            "epoch: 2402, train_loss: 0.171631, val loss: 0.341614,  train_metric: -0.945 test_metric: -0.879 lr: 0.00009)\n",
            "epoch: 2403, train_loss: 0.169053, val loss: 0.339504,  train_metric: -0.947 test_metric: -0.885 lr: 0.00009)\n",
            "epoch: 2404, train_loss: 0.170134, val loss: 0.339694,  train_metric: -0.946 test_metric: -0.884 lr: 0.00009)\n",
            "epoch: 2405, train_loss: 0.168036, val loss: 0.335411,  train_metric: -0.948 test_metric: -0.888 lr: 0.00009)\n",
            "epoch: 2406, train_loss: 0.168927, val loss: 0.335577,  train_metric: -0.952 test_metric: -0.886 lr: 0.00009)\n",
            "epoch: 2407, train_loss: 0.169073, val loss: 0.337862,  train_metric: -0.946 test_metric: -0.890 lr: 0.00009)\n",
            "epoch: 2408, train_loss: 0.169110, val loss: 0.335071,  train_metric: -0.948 test_metric: -0.893 lr: 0.00009)\n",
            "epoch: 2409, train_loss: 0.168158, val loss: 0.339817,  train_metric: -0.951 test_metric: -0.885 lr: 0.00009)\n",
            "epoch: 2410, train_loss: 0.170881, val loss: 0.336162,  train_metric: -0.946 test_metric: -0.886 lr: 0.00009)\n",
            "epoch: 2411, train_loss: 0.169749, val loss: 0.338807,  train_metric: -0.945 test_metric: -0.884 lr: 0.00009)\n",
            "epoch: 2412, train_loss: 0.168269, val loss: 0.338612,  train_metric: -0.952 test_metric: -0.890 lr: 0.00009)\n",
            "epoch: 2413, train_loss: 0.168086, val loss: 0.333182,  train_metric: -0.947 test_metric: -0.893 lr: 0.00009)\n",
            "epoch: 2414, train_loss: 0.167263, val loss: 0.339802,  train_metric: -0.949 test_metric: -0.890 lr: 0.00009)\n",
            "epoch: 2415, train_loss: 0.169379, val loss: 0.336942,  train_metric: -0.950 test_metric: -0.890 lr: 0.00009)\n",
            "epoch: 2416, train_loss: 0.167545, val loss: 0.336442,  train_metric: -0.950 test_metric: -0.892 lr: 0.00009)\n",
            "epoch: 2417, train_loss: 0.168625, val loss: 0.335686,  train_metric: -0.948 test_metric: -0.891 lr: 0.00009)\n",
            "epoch: 2418, train_loss: 0.168880, val loss: 0.349926,  train_metric: -0.951 test_metric: -0.876 lr: 0.00009)\n",
            "epoch: 2419, train_loss: 0.171379, val loss: 0.342795,  train_metric: -0.944 test_metric: -0.881 lr: 0.00009)\n",
            "epoch: 2420, train_loss: 0.169317, val loss: 0.345404,  train_metric: -0.944 test_metric: -0.877 lr: 0.00009)\n",
            "epoch: 2421, train_loss: 0.169622, val loss: 0.339501,  train_metric: -0.947 test_metric: -0.888 lr: 0.00009)\n",
            "epoch: 2422, train_loss: 0.169857, val loss: 0.335608,  train_metric: -0.949 test_metric: -0.886 lr: 0.00009)\n",
            "epoch: 2423, train_loss: 0.168179, val loss: 0.333505,  train_metric: -0.948 test_metric: -0.895 lr: 0.00009)\n",
            "epoch: 2424, train_loss: 0.169240, val loss: 0.341893,  train_metric: -0.946 test_metric: -0.890 lr: 0.00009)\n",
            "epoch: 2425, train_loss: 0.169738, val loss: 0.336514,  train_metric: -0.947 test_metric: -0.893 lr: 0.00009)\n",
            "epoch: 2426, train_loss: 0.169349, val loss: 0.334443,  train_metric: -0.949 test_metric: -0.893 lr: 0.00009)\n",
            "epoch: 2427, train_loss: 0.169567, val loss: 0.342353,  train_metric: -0.947 test_metric: -0.884 lr: 0.00009)\n",
            "epoch: 2428, train_loss: 0.169846, val loss: 0.341056,  train_metric: -0.946 test_metric: -0.882 lr: 0.00009)\n",
            "epoch: 2429, train_loss: 0.168427, val loss: 0.332084,  train_metric: -0.951 test_metric: -0.891 lr: 0.00009)\n",
            "epoch: 2430, train_loss: 0.168654, val loss: 0.333940,  train_metric: -0.949 test_metric: -0.897 lr: 0.00009)\n",
            "epoch: 2431, train_loss: 0.168912, val loss: 0.343161,  train_metric: -0.947 test_metric: -0.880 lr: 0.00009)\n",
            "epoch: 2432, train_loss: 0.172518, val loss: 0.346229,  train_metric: -0.945 test_metric: -0.876 lr: 0.00009)\n",
            "epoch: 2433, train_loss: 0.170126, val loss: 0.338929,  train_metric: -0.947 test_metric: -0.880 lr: 0.00009)\n",
            "epoch: 2434, train_loss: 0.167913, val loss: 0.333318,  train_metric: -0.947 test_metric: -0.888 lr: 0.00009)\n",
            "epoch: 2435, train_loss: 0.168379, val loss: 0.342996,  train_metric: -0.952 test_metric: -0.877 lr: 0.00009)\n",
            "epoch: 2436, train_loss: 0.168271, val loss: 0.343416,  train_metric: -0.948 test_metric: -0.879 lr: 0.00009)\n",
            "epoch: 2437, train_loss: 0.171811, val loss: 0.340384,  train_metric: -0.945 test_metric: -0.886 lr: 0.00009)\n",
            "epoch: 2438, train_loss: 0.168430, val loss: 0.334334,  train_metric: -0.950 test_metric: -0.893 lr: 0.00009)\n",
            "epoch: 2439, train_loss: 0.167419, val loss: 0.342029,  train_metric: -0.952 test_metric: -0.882 lr: 0.00009)\n",
            "epoch: 2440, train_loss: 0.171807, val loss: 0.340291,  train_metric: -0.942 test_metric: -0.880 lr: 0.00009)\n",
            "epoch: 2441, train_loss: 0.170493, val loss: 0.343865,  train_metric: -0.949 test_metric: -0.879 lr: 0.00009)\n",
            "epoch: 2442, train_loss: 0.168441, val loss: 0.339911,  train_metric: -0.946 test_metric: -0.882 lr: 0.00009)\n",
            "epoch: 2443, train_loss: 0.168651, val loss: 0.347770,  train_metric: -0.946 test_metric: -0.875 lr: 0.00009)\n",
            "epoch: 2444, train_loss: 0.168847, val loss: 0.332813,  train_metric: -0.945 test_metric: -0.898 lr: 0.00009)\n",
            "epoch: 2445, train_loss: 0.168384, val loss: 0.334257,  train_metric: -0.951 test_metric: -0.892 lr: 0.00009)\n",
            "epoch: 2446, train_loss: 0.168000, val loss: 0.343242,  train_metric: -0.947 test_metric: -0.881 lr: 0.00009)\n",
            "epoch: 2447, train_loss: 0.168562, val loss: 0.335783,  train_metric: -0.952 test_metric: -0.892 lr: 0.00009)\n",
            "epoch: 2448, train_loss: 0.167290, val loss: 0.338893,  train_metric: -0.947 test_metric: -0.892 lr: 0.00009)\n",
            "epoch: 2449, train_loss: 0.170764, val loss: 0.334186,  train_metric: -0.945 test_metric: -0.890 lr: 0.00009)\n",
            "epoch: 2450, train_loss: 0.169332, val loss: 0.336219,  train_metric: -0.944 test_metric: -0.888 lr: 0.00009)\n",
            "epoch: 2451, train_loss: 0.166257, val loss: 0.337263,  train_metric: -0.950 test_metric: -0.891 lr: 0.00009)\n",
            "epoch: 2452, train_loss: 0.167978, val loss: 0.335587,  train_metric: -0.948 test_metric: -0.885 lr: 0.00009)\n",
            "epoch: 2453, train_loss: 0.167349, val loss: 0.334729,  train_metric: -0.950 test_metric: -0.890 lr: 0.00009)\n",
            "epoch: 2454, train_loss: 0.167821, val loss: 0.339135,  train_metric: -0.947 test_metric: -0.893 lr: 0.00009)\n",
            "epoch: 2455, train_loss: 0.168571, val loss: 0.330526,  train_metric: -0.947 test_metric: -0.896 lr: 0.00009)\n",
            "epoch: 2456, train_loss: 0.168128, val loss: 0.335971,  train_metric: -0.949 test_metric: -0.893 lr: 0.00009)\n",
            "epoch: 2457, train_loss: 0.168534, val loss: 0.339666,  train_metric: -0.945 test_metric: -0.890 lr: 0.00009)\n",
            "epoch: 2458, train_loss: 0.168249, val loss: 0.333345,  train_metric: -0.944 test_metric: -0.886 lr: 0.00009)\n",
            "epoch: 2459, train_loss: 0.167411, val loss: 0.340020,  train_metric: -0.952 test_metric: -0.888 lr: 0.00009)\n",
            "epoch: 2460, train_loss: 0.169174, val loss: 0.343126,  train_metric: -0.949 test_metric: -0.884 lr: 0.00009)\n",
            "epoch: 2461, train_loss: 0.167121, val loss: 0.336790,  train_metric: -0.952 test_metric: -0.888 lr: 0.00009)\n",
            "epoch: 2462, train_loss: 0.166454, val loss: 0.340199,  train_metric: -0.949 test_metric: -0.885 lr: 0.00009)\n",
            "epoch: 2463, train_loss: 0.166396, val loss: 0.336883,  train_metric: -0.952 test_metric: -0.887 lr: 0.00008)\n",
            "epoch: 2464, train_loss: 0.167239, val loss: 0.340115,  train_metric: -0.948 test_metric: -0.890 lr: 0.00008)\n",
            "epoch: 2465, train_loss: 0.168815, val loss: 0.331831,  train_metric: -0.954 test_metric: -0.895 lr: 0.00008)\n",
            "epoch: 2466, train_loss: 0.167291, val loss: 0.336336,  train_metric: -0.951 test_metric: -0.888 lr: 0.00008)\n",
            "epoch: 2467, train_loss: 0.165719, val loss: 0.335462,  train_metric: -0.948 test_metric: -0.891 lr: 0.00008)\n",
            "epoch: 2468, train_loss: 0.167795, val loss: 0.334733,  train_metric: -0.951 test_metric: -0.892 lr: 0.00008)\n",
            "epoch: 2469, train_loss: 0.166419, val loss: 0.334688,  train_metric: -0.952 test_metric: -0.891 lr: 0.00008)\n",
            "epoch: 2470, train_loss: 0.166193, val loss: 0.333384,  train_metric: -0.948 test_metric: -0.895 lr: 0.00008)\n",
            "epoch: 2471, train_loss: 0.166588, val loss: 0.337798,  train_metric: -0.955 test_metric: -0.888 lr: 0.00008)\n",
            "epoch: 2472, train_loss: 0.166363, val loss: 0.333896,  train_metric: -0.953 test_metric: -0.892 lr: 0.00008)\n",
            "epoch: 2473, train_loss: 0.167501, val loss: 0.333841,  train_metric: -0.947 test_metric: -0.891 lr: 0.00008)\n",
            "epoch: 2474, train_loss: 0.169197, val loss: 0.337667,  train_metric: -0.945 test_metric: -0.892 lr: 0.00008)\n",
            "epoch: 2475, train_loss: 0.166035, val loss: 0.336772,  train_metric: -0.949 test_metric: -0.885 lr: 0.00008)\n",
            "epoch: 2476, train_loss: 0.167061, val loss: 0.337533,  train_metric: -0.945 test_metric: -0.885 lr: 0.00008)\n",
            "epoch: 2477, train_loss: 0.166958, val loss: 0.336151,  train_metric: -0.949 test_metric: -0.892 lr: 0.00008)\n",
            "epoch: 2478, train_loss: 0.165732, val loss: 0.348559,  train_metric: -0.953 test_metric: -0.876 lr: 0.00008)\n",
            "epoch: 2479, train_loss: 0.168228, val loss: 0.342771,  train_metric: -0.946 test_metric: -0.880 lr: 0.00008)\n",
            "epoch: 2480, train_loss: 0.170394, val loss: 0.347862,  train_metric: -0.945 test_metric: -0.881 lr: 0.00008)\n",
            "epoch: 2481, train_loss: 0.167975, val loss: 0.345474,  train_metric: -0.946 test_metric: -0.881 lr: 0.00008)\n",
            "epoch: 2482, train_loss: 0.167138, val loss: 0.336533,  train_metric: -0.949 test_metric: -0.880 lr: 0.00008)\n",
            "epoch: 2483, train_loss: 0.165545, val loss: 0.340728,  train_metric: -0.951 test_metric: -0.885 lr: 0.00008)\n",
            "epoch: 2484, train_loss: 0.167422, val loss: 0.336183,  train_metric: -0.947 test_metric: -0.887 lr: 0.00008)\n",
            "epoch: 2485, train_loss: 0.167506, val loss: 0.339665,  train_metric: -0.948 test_metric: -0.884 lr: 0.00008)\n",
            "epoch: 2486, train_loss: 0.165564, val loss: 0.334243,  train_metric: -0.951 test_metric: -0.896 lr: 0.00008)\n",
            "epoch: 2487, train_loss: 0.166666, val loss: 0.337551,  train_metric: -0.950 test_metric: -0.890 lr: 0.00008)\n",
            "epoch: 2488, train_loss: 0.165321, val loss: 0.334792,  train_metric: -0.950 test_metric: -0.895 lr: 0.00008)\n",
            "epoch: 2489, train_loss: 0.167415, val loss: 0.340310,  train_metric: -0.948 test_metric: -0.893 lr: 0.00008)\n",
            "epoch: 2490, train_loss: 0.167588, val loss: 0.336155,  train_metric: -0.951 test_metric: -0.892 lr: 0.00008)\n",
            "epoch: 2491, train_loss: 0.169202, val loss: 0.335113,  train_metric: -0.943 test_metric: -0.887 lr: 0.00008)\n",
            "epoch: 2492, train_loss: 0.167139, val loss: 0.344155,  train_metric: -0.950 test_metric: -0.880 lr: 0.00008)\n",
            "epoch: 2493, train_loss: 0.165947, val loss: 0.337868,  train_metric: -0.947 test_metric: -0.890 lr: 0.00008)\n",
            "epoch: 2494, train_loss: 0.167461, val loss: 0.334836,  train_metric: -0.948 test_metric: -0.893 lr: 0.00008)\n",
            "epoch: 2495, train_loss: 0.168305, val loss: 0.337964,  train_metric: -0.952 test_metric: -0.891 lr: 0.00008)\n",
            "epoch: 2496, train_loss: 0.167055, val loss: 0.334794,  train_metric: -0.952 test_metric: -0.891 lr: 0.00008)\n",
            "epoch: 2497, train_loss: 0.165863, val loss: 0.337339,  train_metric: -0.951 test_metric: -0.884 lr: 0.00008)\n",
            "epoch: 2498, train_loss: 0.168393, val loss: 0.341396,  train_metric: -0.947 test_metric: -0.879 lr: 0.00008)\n",
            "epoch: 2499, train_loss: 0.167176, val loss: 0.339389,  train_metric: -0.949 test_metric: -0.887 lr: 0.00008)\n",
            "epoch: 2500, train_loss: 0.167946, val loss: 0.347003,  train_metric: -0.948 test_metric: -0.877 lr: 0.00008)\n",
            "epoch: 2501, train_loss: 0.167908, val loss: 0.334301,  train_metric: -0.948 test_metric: -0.895 lr: 0.00008)\n",
            "epoch: 2502, train_loss: 0.166356, val loss: 0.334683,  train_metric: -0.949 test_metric: -0.895 lr: 0.00008)\n",
            "epoch: 2503, train_loss: 0.167826, val loss: 0.337394,  train_metric: -0.948 test_metric: -0.896 lr: 0.00008)\n",
            "epoch: 2504, train_loss: 0.169425, val loss: 0.342792,  train_metric: -0.943 test_metric: -0.890 lr: 0.00008)\n",
            "epoch: 2505, train_loss: 0.166648, val loss: 0.336627,  train_metric: -0.949 test_metric: -0.893 lr: 0.00008)\n",
            "epoch: 2506, train_loss: 0.167085, val loss: 0.331132,  train_metric: -0.946 test_metric: -0.895 lr: 0.00008)\n",
            "epoch: 2507, train_loss: 0.167393, val loss: 0.339047,  train_metric: -0.949 test_metric: -0.884 lr: 0.00008)\n",
            "epoch: 2508, train_loss: 0.166591, val loss: 0.336753,  train_metric: -0.946 test_metric: -0.890 lr: 0.00008)\n",
            "epoch: 2509, train_loss: 0.165521, val loss: 0.336192,  train_metric: -0.952 test_metric: -0.886 lr: 0.00008)\n",
            "epoch: 2510, train_loss: 0.165198, val loss: 0.337000,  train_metric: -0.950 test_metric: -0.890 lr: 0.00008)\n",
            "epoch: 2511, train_loss: 0.165797, val loss: 0.334892,  train_metric: -0.952 test_metric: -0.886 lr: 0.00008)\n",
            "epoch: 2512, train_loss: 0.166372, val loss: 0.333195,  train_metric: -0.950 test_metric: -0.896 lr: 0.00008)\n",
            "epoch: 2513, train_loss: 0.165670, val loss: 0.336110,  train_metric: -0.949 test_metric: -0.885 lr: 0.00008)\n",
            "epoch: 2514, train_loss: 0.167137, val loss: 0.342717,  train_metric: -0.949 test_metric: -0.879 lr: 0.00008)\n",
            "epoch: 2515, train_loss: 0.168609, val loss: 0.335637,  train_metric: -0.944 test_metric: -0.885 lr: 0.00008)\n",
            "epoch: 2516, train_loss: 0.168721, val loss: 0.337880,  train_metric: -0.941 test_metric: -0.885 lr: 0.00008)\n",
            "epoch: 2517, train_loss: 0.167398, val loss: 0.337335,  train_metric: -0.949 test_metric: -0.890 lr: 0.00008)\n",
            "epoch: 2518, train_loss: 0.168250, val loss: 0.334254,  train_metric: -0.944 test_metric: -0.887 lr: 0.00008)\n",
            "epoch: 2519, train_loss: 0.167683, val loss: 0.336710,  train_metric: -0.945 test_metric: -0.891 lr: 0.00008)\n",
            "epoch: 2520, train_loss: 0.167718, val loss: 0.344931,  train_metric: -0.948 test_metric: -0.884 lr: 0.00008)\n",
            "epoch: 2521, train_loss: 0.167521, val loss: 0.333729,  train_metric: -0.951 test_metric: -0.891 lr: 0.00008)\n",
            "epoch: 2522, train_loss: 0.166093, val loss: 0.331818,  train_metric: -0.953 test_metric: -0.895 lr: 0.00008)\n",
            "epoch: 2523, train_loss: 0.165446, val loss: 0.335055,  train_metric: -0.951 test_metric: -0.891 lr: 0.00008)\n",
            "epoch: 2524, train_loss: 0.165206, val loss: 0.335882,  train_metric: -0.949 test_metric: -0.892 lr: 0.00008)\n",
            "epoch: 2525, train_loss: 0.165185, val loss: 0.333695,  train_metric: -0.954 test_metric: -0.897 lr: 0.00008)\n",
            "epoch: 2526, train_loss: 0.167332, val loss: 0.337747,  train_metric: -0.946 test_metric: -0.888 lr: 0.00008)\n",
            "epoch: 2527, train_loss: 0.164620, val loss: 0.334398,  train_metric: -0.952 test_metric: -0.892 lr: 0.00008)\n",
            "epoch: 2528, train_loss: 0.167206, val loss: 0.345517,  train_metric: -0.948 test_metric: -0.879 lr: 0.00008)\n",
            "epoch: 2529, train_loss: 0.165669, val loss: 0.336290,  train_metric: -0.950 test_metric: -0.891 lr: 0.00008)\n",
            "epoch: 2530, train_loss: 0.165717, val loss: 0.336413,  train_metric: -0.946 test_metric: -0.890 lr: 0.00008)\n",
            "epoch: 2531, train_loss: 0.167435, val loss: 0.341404,  train_metric: -0.948 test_metric: -0.893 lr: 0.00008)\n",
            "epoch: 2532, train_loss: 0.168601, val loss: 0.337355,  train_metric: -0.948 test_metric: -0.892 lr: 0.00008)\n",
            "epoch: 2533, train_loss: 0.173106, val loss: 0.333980,  train_metric: -0.940 test_metric: -0.893 lr: 0.00008)\n",
            "epoch: 2534, train_loss: 0.169045, val loss: 0.335297,  train_metric: -0.948 test_metric: -0.897 lr: 0.00008)\n",
            "epoch: 2535, train_loss: 0.165331, val loss: 0.340557,  train_metric: -0.951 test_metric: -0.885 lr: 0.00008)\n",
            "epoch: 2536, train_loss: 0.165688, val loss: 0.339657,  train_metric: -0.950 test_metric: -0.879 lr: 0.00008)\n",
            "epoch: 2537, train_loss: 0.165768, val loss: 0.335873,  train_metric: -0.948 test_metric: -0.887 lr: 0.00008)\n",
            "epoch: 2538, train_loss: 0.165969, val loss: 0.340990,  train_metric: -0.949 test_metric: -0.882 lr: 0.00008)\n",
            "epoch: 2539, train_loss: 0.166961, val loss: 0.342663,  train_metric: -0.951 test_metric: -0.881 lr: 0.00008)\n",
            "epoch: 2540, train_loss: 0.166172, val loss: 0.337743,  train_metric: -0.949 test_metric: -0.884 lr: 0.00008)\n",
            "epoch: 2541, train_loss: 0.166923, val loss: 0.344786,  train_metric: -0.948 test_metric: -0.875 lr: 0.00008)\n",
            "epoch: 2542, train_loss: 0.167261, val loss: 0.339983,  train_metric: -0.948 test_metric: -0.890 lr: 0.00008)\n",
            "epoch: 2543, train_loss: 0.165291, val loss: 0.335526,  train_metric: -0.945 test_metric: -0.897 lr: 0.00008)\n",
            "epoch: 2544, train_loss: 0.167352, val loss: 0.334555,  train_metric: -0.951 test_metric: -0.895 lr: 0.00008)\n",
            "epoch: 2545, train_loss: 0.166009, val loss: 0.335623,  train_metric: -0.953 test_metric: -0.893 lr: 0.00008)\n",
            "epoch: 2546, train_loss: 0.166510, val loss: 0.336544,  train_metric: -0.947 test_metric: -0.891 lr: 0.00008)\n",
            "epoch: 2547, train_loss: 0.165111, val loss: 0.335029,  train_metric: -0.950 test_metric: -0.887 lr: 0.00008)\n",
            "epoch: 2548, train_loss: 0.165892, val loss: 0.337795,  train_metric: -0.948 test_metric: -0.895 lr: 0.00008)\n",
            "epoch: 2549, train_loss: 0.166800, val loss: 0.334688,  train_metric: -0.950 test_metric: -0.892 lr: 0.00008)\n",
            "epoch: 2550, train_loss: 0.165922, val loss: 0.334682,  train_metric: -0.948 test_metric: -0.896 lr: 0.00008)\n",
            "epoch: 2551, train_loss: 0.164531, val loss: 0.339193,  train_metric: -0.955 test_metric: -0.888 lr: 0.00008)\n",
            "epoch: 2552, train_loss: 0.165724, val loss: 0.335037,  train_metric: -0.953 test_metric: -0.884 lr: 0.00008)\n",
            "epoch: 2553, train_loss: 0.165902, val loss: 0.344134,  train_metric: -0.953 test_metric: -0.882 lr: 0.00008)\n",
            "epoch: 2554, train_loss: 0.165459, val loss: 0.335626,  train_metric: -0.949 test_metric: -0.892 lr: 0.00008)\n",
            "epoch: 2555, train_loss: 0.164619, val loss: 0.343337,  train_metric: -0.953 test_metric: -0.879 lr: 0.00008)\n",
            "epoch: 2556, train_loss: 0.166846, val loss: 0.343310,  train_metric: -0.948 test_metric: -0.885 lr: 0.00008)\n",
            "epoch: 2557, train_loss: 0.166764, val loss: 0.341021,  train_metric: -0.946 test_metric: -0.881 lr: 0.00008)\n",
            "epoch: 2558, train_loss: 0.166047, val loss: 0.336255,  train_metric: -0.944 test_metric: -0.887 lr: 0.00008)\n",
            "epoch: 2559, train_loss: 0.165293, val loss: 0.337566,  train_metric: -0.952 test_metric: -0.895 lr: 0.00008)\n",
            "epoch: 2560, train_loss: 0.164342, val loss: 0.333666,  train_metric: -0.952 test_metric: -0.898 lr: 0.00008)\n",
            "epoch: 2561, train_loss: 0.164455, val loss: 0.338970,  train_metric: -0.949 test_metric: -0.888 lr: 0.00008)\n",
            "epoch: 2562, train_loss: 0.165915, val loss: 0.345751,  train_metric: -0.948 test_metric: -0.876 lr: 0.00008)\n",
            "epoch: 2563, train_loss: 0.165645, val loss: 0.342659,  train_metric: -0.949 test_metric: -0.880 lr: 0.00008)\n",
            "epoch: 2564, train_loss: 0.166065, val loss: 0.337696,  train_metric: -0.950 test_metric: -0.888 lr: 0.00008)\n",
            "epoch: 2565, train_loss: 0.165051, val loss: 0.341725,  train_metric: -0.952 test_metric: -0.882 lr: 0.00008)\n",
            "epoch: 2566, train_loss: 0.165917, val loss: 0.334192,  train_metric: -0.946 test_metric: -0.891 lr: 0.00008)\n",
            "epoch: 2567, train_loss: 0.164047, val loss: 0.338209,  train_metric: -0.950 test_metric: -0.892 lr: 0.00008)\n",
            "epoch: 2568, train_loss: 0.165313, val loss: 0.333963,  train_metric: -0.949 test_metric: -0.888 lr: 0.00008)\n",
            "epoch: 2569, train_loss: 0.166072, val loss: 0.339906,  train_metric: -0.948 test_metric: -0.887 lr: 0.00008)\n",
            "epoch: 2570, train_loss: 0.164903, val loss: 0.340513,  train_metric: -0.952 test_metric: -0.888 lr: 0.00008)\n",
            "epoch: 2571, train_loss: 0.166141, val loss: 0.348832,  train_metric: -0.946 test_metric: -0.875 lr: 0.00008)\n",
            "epoch: 2572, train_loss: 0.167927, val loss: 0.347328,  train_metric: -0.945 test_metric: -0.875 lr: 0.00008)\n",
            "epoch: 2573, train_loss: 0.169100, val loss: 0.345006,  train_metric: -0.946 test_metric: -0.877 lr: 0.00008)\n",
            "epoch: 2574, train_loss: 0.168166, val loss: 0.336238,  train_metric: -0.945 test_metric: -0.886 lr: 0.00008)\n",
            "epoch: 2575, train_loss: 0.169119, val loss: 0.341465,  train_metric: -0.940 test_metric: -0.880 lr: 0.00008)\n",
            "epoch: 2576, train_loss: 0.164080, val loss: 0.340348,  train_metric: -0.954 test_metric: -0.885 lr: 0.00008)\n",
            "epoch: 2577, train_loss: 0.165254, val loss: 0.332959,  train_metric: -0.951 test_metric: -0.897 lr: 0.00008)\n",
            "epoch: 2578, train_loss: 0.165073, val loss: 0.338218,  train_metric: -0.949 test_metric: -0.892 lr: 0.00008)\n",
            "epoch: 2579, train_loss: 0.163952, val loss: 0.332211,  train_metric: -0.953 test_metric: -0.892 lr: 0.00008)\n",
            "epoch: 2580, train_loss: 0.165359, val loss: 0.346242,  train_metric: -0.953 test_metric: -0.884 lr: 0.00008)\n",
            "epoch: 2581, train_loss: 0.164952, val loss: 0.340984,  train_metric: -0.949 test_metric: -0.881 lr: 0.00008)\n",
            "epoch: 2582, train_loss: 0.165551, val loss: 0.337481,  train_metric: -0.951 test_metric: -0.885 lr: 0.00008)\n",
            "epoch: 2583, train_loss: 0.165249, val loss: 0.338193,  train_metric: -0.949 test_metric: -0.892 lr: 0.00008)\n",
            "epoch: 2584, train_loss: 0.164788, val loss: 0.332383,  train_metric: -0.946 test_metric: -0.891 lr: 0.00008)\n",
            "epoch: 2585, train_loss: 0.165775, val loss: 0.337329,  train_metric: -0.953 test_metric: -0.886 lr: 0.00008)\n",
            "epoch: 2586, train_loss: 0.164282, val loss: 0.334054,  train_metric: -0.950 test_metric: -0.890 lr: 0.00008)\n",
            "epoch: 2587, train_loss: 0.164023, val loss: 0.341458,  train_metric: -0.954 test_metric: -0.887 lr: 0.00008)\n",
            "epoch: 2588, train_loss: 0.165752, val loss: 0.332325,  train_metric: -0.949 test_metric: -0.896 lr: 0.00007)\n",
            "epoch: 2589, train_loss: 0.166533, val loss: 0.336668,  train_metric: -0.949 test_metric: -0.895 lr: 0.00007)\n",
            "epoch: 2590, train_loss: 0.166857, val loss: 0.333692,  train_metric: -0.951 test_metric: -0.888 lr: 0.00007)\n",
            "epoch: 2591, train_loss: 0.165215, val loss: 0.339907,  train_metric: -0.953 test_metric: -0.886 lr: 0.00007)\n",
            "epoch: 2592, train_loss: 0.164164, val loss: 0.337668,  train_metric: -0.949 test_metric: -0.885 lr: 0.00007)\n",
            "epoch: 2593, train_loss: 0.163862, val loss: 0.334039,  train_metric: -0.948 test_metric: -0.891 lr: 0.00007)\n",
            "epoch: 2594, train_loss: 0.163971, val loss: 0.342494,  train_metric: -0.954 test_metric: -0.890 lr: 0.00007)\n",
            "epoch: 2595, train_loss: 0.165879, val loss: 0.334824,  train_metric: -0.949 test_metric: -0.893 lr: 0.00007)\n",
            "epoch: 2596, train_loss: 0.164270, val loss: 0.334337,  train_metric: -0.953 test_metric: -0.890 lr: 0.00007)\n",
            "epoch: 2597, train_loss: 0.163880, val loss: 0.341997,  train_metric: -0.949 test_metric: -0.880 lr: 0.00007)\n",
            "epoch: 2598, train_loss: 0.166307, val loss: 0.342808,  train_metric: -0.946 test_metric: -0.882 lr: 0.00007)\n",
            "epoch: 2599, train_loss: 0.166265, val loss: 0.339177,  train_metric: -0.948 test_metric: -0.880 lr: 0.00007)\n",
            "epoch: 2600, train_loss: 0.164033, val loss: 0.338085,  train_metric: -0.953 test_metric: -0.887 lr: 0.00007)\n",
            "epoch: 2601, train_loss: 0.162936, val loss: 0.332913,  train_metric: -0.948 test_metric: -0.895 lr: 0.00007)\n",
            "epoch: 2602, train_loss: 0.165275, val loss: 0.340990,  train_metric: -0.952 test_metric: -0.884 lr: 0.00007)\n",
            "epoch: 2603, train_loss: 0.164054, val loss: 0.333023,  train_metric: -0.951 test_metric: -0.888 lr: 0.00007)\n",
            "epoch: 2604, train_loss: 0.163316, val loss: 0.337075,  train_metric: -0.951 test_metric: -0.890 lr: 0.00007)\n",
            "epoch: 2605, train_loss: 0.163503, val loss: 0.337212,  train_metric: -0.951 test_metric: -0.887 lr: 0.00007)\n",
            "epoch: 2606, train_loss: 0.164574, val loss: 0.332921,  train_metric: -0.952 test_metric: -0.893 lr: 0.00007)\n",
            "epoch: 2607, train_loss: 0.164143, val loss: 0.339394,  train_metric: -0.953 test_metric: -0.887 lr: 0.00007)\n",
            "epoch: 2608, train_loss: 0.166065, val loss: 0.340176,  train_metric: -0.949 test_metric: -0.881 lr: 0.00007)\n",
            "epoch: 2609, train_loss: 0.164560, val loss: 0.332284,  train_metric: -0.947 test_metric: -0.898 lr: 0.00007)\n",
            "epoch: 2610, train_loss: 0.164178, val loss: 0.336648,  train_metric: -0.953 test_metric: -0.890 lr: 0.00007)\n",
            "epoch: 2611, train_loss: 0.165197, val loss: 0.339063,  train_metric: -0.948 test_metric: -0.885 lr: 0.00007)\n",
            "epoch: 2612, train_loss: 0.164594, val loss: 0.333961,  train_metric: -0.951 test_metric: -0.887 lr: 0.00007)\n",
            "epoch: 2613, train_loss: 0.164744, val loss: 0.339972,  train_metric: -0.948 test_metric: -0.881 lr: 0.00007)\n",
            "epoch: 2614, train_loss: 0.163181, val loss: 0.333778,  train_metric: -0.948 test_metric: -0.891 lr: 0.00007)\n",
            "epoch: 2615, train_loss: 0.165024, val loss: 0.336686,  train_metric: -0.951 test_metric: -0.892 lr: 0.00007)\n",
            "epoch: 2616, train_loss: 0.164875, val loss: 0.336034,  train_metric: -0.947 test_metric: -0.892 lr: 0.00007)\n",
            "epoch: 2617, train_loss: 0.166729, val loss: 0.335141,  train_metric: -0.949 test_metric: -0.884 lr: 0.00007)\n",
            "epoch: 2618, train_loss: 0.165692, val loss: 0.349312,  train_metric: -0.951 test_metric: -0.875 lr: 0.00007)\n",
            "epoch: 2619, train_loss: 0.165167, val loss: 0.333081,  train_metric: -0.947 test_metric: -0.891 lr: 0.00007)\n",
            "epoch: 2620, train_loss: 0.164356, val loss: 0.340025,  train_metric: -0.948 test_metric: -0.888 lr: 0.00007)\n",
            "epoch: 2621, train_loss: 0.163885, val loss: 0.338534,  train_metric: -0.950 test_metric: -0.882 lr: 0.00007)\n",
            "epoch: 2622, train_loss: 0.165170, val loss: 0.337187,  train_metric: -0.945 test_metric: -0.895 lr: 0.00007)\n",
            "epoch: 2623, train_loss: 0.164642, val loss: 0.332394,  train_metric: -0.948 test_metric: -0.893 lr: 0.00007)\n",
            "epoch: 2624, train_loss: 0.164031, val loss: 0.335744,  train_metric: -0.945 test_metric: -0.882 lr: 0.00007)\n",
            "epoch: 2625, train_loss: 0.164136, val loss: 0.332959,  train_metric: -0.950 test_metric: -0.896 lr: 0.00007)\n",
            "epoch: 2626, train_loss: 0.163734, val loss: 0.335399,  train_metric: -0.954 test_metric: -0.891 lr: 0.00007)\n",
            "epoch: 2627, train_loss: 0.166392, val loss: 0.342834,  train_metric: -0.948 test_metric: -0.879 lr: 0.00007)\n",
            "epoch: 2628, train_loss: 0.165188, val loss: 0.332530,  train_metric: -0.948 test_metric: -0.896 lr: 0.00007)\n",
            "epoch: 2629, train_loss: 0.163356, val loss: 0.339521,  train_metric: -0.951 test_metric: -0.888 lr: 0.00007)\n",
            "epoch: 2630, train_loss: 0.165116, val loss: 0.333510,  train_metric: -0.949 test_metric: -0.890 lr: 0.00007)\n",
            "epoch: 2631, train_loss: 0.163576, val loss: 0.336111,  train_metric: -0.952 test_metric: -0.893 lr: 0.00007)\n",
            "epoch: 2632, train_loss: 0.163567, val loss: 0.334591,  train_metric: -0.953 test_metric: -0.888 lr: 0.00007)\n",
            "epoch: 2633, train_loss: 0.162647, val loss: 0.339560,  train_metric: -0.951 test_metric: -0.882 lr: 0.00007)\n",
            "epoch: 2634, train_loss: 0.163283, val loss: 0.336462,  train_metric: -0.951 test_metric: -0.891 lr: 0.00007)\n",
            "epoch: 2635, train_loss: 0.163262, val loss: 0.335603,  train_metric: -0.953 test_metric: -0.886 lr: 0.00007)\n",
            "epoch: 2636, train_loss: 0.162571, val loss: 0.341139,  train_metric: -0.952 test_metric: -0.881 lr: 0.00007)\n",
            "epoch: 2637, train_loss: 0.164628, val loss: 0.341757,  train_metric: -0.949 test_metric: -0.876 lr: 0.00007)\n",
            "epoch: 2638, train_loss: 0.164394, val loss: 0.332635,  train_metric: -0.945 test_metric: -0.900 lr: 0.00007)\n",
            "epoch: 2639, train_loss: 0.165450, val loss: 0.334142,  train_metric: -0.951 test_metric: -0.890 lr: 0.00007)\n",
            "epoch: 2640, train_loss: 0.163700, val loss: 0.343662,  train_metric: -0.949 test_metric: -0.881 lr: 0.00007)\n",
            "epoch: 2641, train_loss: 0.163221, val loss: 0.333339,  train_metric: -0.949 test_metric: -0.893 lr: 0.00007)\n",
            "epoch: 2642, train_loss: 0.163665, val loss: 0.340225,  train_metric: -0.956 test_metric: -0.881 lr: 0.00007)\n",
            "epoch: 2643, train_loss: 0.166143, val loss: 0.337960,  train_metric: -0.947 test_metric: -0.893 lr: 0.00007)\n",
            "epoch: 2644, train_loss: 0.165587, val loss: 0.334419,  train_metric: -0.952 test_metric: -0.892 lr: 0.00007)\n",
            "epoch: 2645, train_loss: 0.163922, val loss: 0.337109,  train_metric: -0.947 test_metric: -0.887 lr: 0.00007)\n",
            "epoch: 2646, train_loss: 0.162972, val loss: 0.335043,  train_metric: -0.953 test_metric: -0.888 lr: 0.00007)\n",
            "epoch: 2647, train_loss: 0.163138, val loss: 0.336750,  train_metric: -0.954 test_metric: -0.896 lr: 0.00007)\n",
            "epoch: 2648, train_loss: 0.164094, val loss: 0.331981,  train_metric: -0.951 test_metric: -0.896 lr: 0.00007)\n",
            "epoch: 2649, train_loss: 0.163355, val loss: 0.340535,  train_metric: -0.952 test_metric: -0.887 lr: 0.00007)\n",
            "epoch: 2650, train_loss: 0.163686, val loss: 0.334318,  train_metric: -0.953 test_metric: -0.884 lr: 0.00007)\n",
            "epoch: 2651, train_loss: 0.163048, val loss: 0.338896,  train_metric: -0.949 test_metric: -0.884 lr: 0.00007)\n",
            "epoch: 2652, train_loss: 0.165232, val loss: 0.335604,  train_metric: -0.949 test_metric: -0.890 lr: 0.00007)\n",
            "epoch: 2653, train_loss: 0.163615, val loss: 0.337084,  train_metric: -0.951 test_metric: -0.887 lr: 0.00007)\n",
            "epoch: 2654, train_loss: 0.163354, val loss: 0.336669,  train_metric: -0.948 test_metric: -0.888 lr: 0.00007)\n",
            "epoch: 2655, train_loss: 0.162607, val loss: 0.332797,  train_metric: -0.948 test_metric: -0.895 lr: 0.00007)\n",
            "epoch: 2656, train_loss: 0.163778, val loss: 0.335240,  train_metric: -0.956 test_metric: -0.891 lr: 0.00007)\n",
            "epoch: 2657, train_loss: 0.164506, val loss: 0.335767,  train_metric: -0.951 test_metric: -0.888 lr: 0.00007)\n",
            "epoch: 2658, train_loss: 0.163364, val loss: 0.331447,  train_metric: -0.950 test_metric: -0.897 lr: 0.00007)\n",
            "epoch: 2659, train_loss: 0.163630, val loss: 0.334681,  train_metric: -0.951 test_metric: -0.891 lr: 0.00007)\n",
            "epoch: 2660, train_loss: 0.163237, val loss: 0.340268,  train_metric: -0.949 test_metric: -0.885 lr: 0.00007)\n",
            "epoch: 2661, train_loss: 0.162879, val loss: 0.336034,  train_metric: -0.951 test_metric: -0.886 lr: 0.00007)\n",
            "epoch: 2662, train_loss: 0.164341, val loss: 0.334439,  train_metric: -0.946 test_metric: -0.893 lr: 0.00007)\n",
            "epoch: 2663, train_loss: 0.163727, val loss: 0.338690,  train_metric: -0.952 test_metric: -0.884 lr: 0.00007)\n",
            "epoch: 2664, train_loss: 0.163493, val loss: 0.338024,  train_metric: -0.947 test_metric: -0.884 lr: 0.00007)\n",
            "epoch: 2665, train_loss: 0.163056, val loss: 0.339091,  train_metric: -0.952 test_metric: -0.886 lr: 0.00007)\n",
            "epoch: 2666, train_loss: 0.163505, val loss: 0.337176,  train_metric: -0.948 test_metric: -0.885 lr: 0.00007)\n",
            "epoch: 2667, train_loss: 0.162111, val loss: 0.334162,  train_metric: -0.952 test_metric: -0.892 lr: 0.00007)\n",
            "epoch: 2668, train_loss: 0.162115, val loss: 0.332937,  train_metric: -0.953 test_metric: -0.893 lr: 0.00007)\n",
            "epoch: 2669, train_loss: 0.163188, val loss: 0.333430,  train_metric: -0.955 test_metric: -0.893 lr: 0.00007)\n",
            "epoch: 2670, train_loss: 0.162154, val loss: 0.332244,  train_metric: -0.949 test_metric: -0.895 lr: 0.00007)\n",
            "epoch: 2671, train_loss: 0.162756, val loss: 0.335497,  train_metric: -0.951 test_metric: -0.893 lr: 0.00007)\n",
            "epoch: 2672, train_loss: 0.163932, val loss: 0.348124,  train_metric: -0.951 test_metric: -0.872 lr: 0.00007)\n",
            "epoch: 2673, train_loss: 0.164918, val loss: 0.334274,  train_metric: -0.953 test_metric: -0.887 lr: 0.00007)\n",
            "epoch: 2674, train_loss: 0.161586, val loss: 0.333343,  train_metric: -0.954 test_metric: -0.891 lr: 0.00007)\n",
            "epoch: 2675, train_loss: 0.162822, val loss: 0.336678,  train_metric: -0.949 test_metric: -0.887 lr: 0.00007)\n",
            "epoch: 2676, train_loss: 0.162753, val loss: 0.333995,  train_metric: -0.948 test_metric: -0.892 lr: 0.00007)\n",
            "epoch: 2677, train_loss: 0.163254, val loss: 0.340431,  train_metric: -0.953 test_metric: -0.880 lr: 0.00007)\n",
            "epoch: 2678, train_loss: 0.164881, val loss: 0.336779,  train_metric: -0.946 test_metric: -0.884 lr: 0.00007)\n",
            "epoch: 2679, train_loss: 0.162728, val loss: 0.343443,  train_metric: -0.952 test_metric: -0.876 lr: 0.00007)\n",
            "epoch: 2680, train_loss: 0.164233, val loss: 0.341897,  train_metric: -0.949 test_metric: -0.881 lr: 0.00007)\n",
            "epoch: 2681, train_loss: 0.161986, val loss: 0.331783,  train_metric: -0.953 test_metric: -0.896 lr: 0.00007)\n",
            "epoch: 2682, train_loss: 0.162686, val loss: 0.336960,  train_metric: -0.951 test_metric: -0.895 lr: 0.00007)\n",
            "epoch: 2683, train_loss: 0.162978, val loss: 0.332407,  train_metric: -0.951 test_metric: -0.893 lr: 0.00007)\n",
            "epoch: 2684, train_loss: 0.162129, val loss: 0.334348,  train_metric: -0.954 test_metric: -0.891 lr: 0.00007)\n",
            "epoch: 2685, train_loss: 0.161791, val loss: 0.333278,  train_metric: -0.956 test_metric: -0.896 lr: 0.00007)\n",
            "epoch: 2686, train_loss: 0.161863, val loss: 0.333268,  train_metric: -0.954 test_metric: -0.895 lr: 0.00007)\n",
            "epoch: 2687, train_loss: 0.162102, val loss: 0.338429,  train_metric: -0.950 test_metric: -0.887 lr: 0.00007)\n",
            "epoch: 2688, train_loss: 0.162505, val loss: 0.335406,  train_metric: -0.951 test_metric: -0.895 lr: 0.00007)\n",
            "epoch: 2689, train_loss: 0.161875, val loss: 0.334481,  train_metric: -0.951 test_metric: -0.892 lr: 0.00007)\n",
            "epoch: 2690, train_loss: 0.161598, val loss: 0.339362,  train_metric: -0.949 test_metric: -0.880 lr: 0.00007)\n",
            "epoch: 2691, train_loss: 0.162961, val loss: 0.334414,  train_metric: -0.953 test_metric: -0.886 lr: 0.00007)\n",
            "epoch: 2692, train_loss: 0.162687, val loss: 0.336966,  train_metric: -0.952 test_metric: -0.892 lr: 0.00007)\n",
            "epoch: 2693, train_loss: 0.163666, val loss: 0.334593,  train_metric: -0.949 test_metric: -0.898 lr: 0.00007)\n",
            "epoch: 2694, train_loss: 0.163824, val loss: 0.338826,  train_metric: -0.948 test_metric: -0.885 lr: 0.00007)\n",
            "epoch: 2695, train_loss: 0.163236, val loss: 0.337627,  train_metric: -0.954 test_metric: -0.887 lr: 0.00007)\n",
            "epoch: 2696, train_loss: 0.163230, val loss: 0.337968,  train_metric: -0.950 test_metric: -0.890 lr: 0.00007)\n",
            "epoch: 2697, train_loss: 0.162776, val loss: 0.332845,  train_metric: -0.949 test_metric: -0.895 lr: 0.00007)\n",
            "epoch: 2698, train_loss: 0.161433, val loss: 0.335412,  train_metric: -0.951 test_metric: -0.887 lr: 0.00007)\n",
            "epoch: 2699, train_loss: 0.163434, val loss: 0.339086,  train_metric: -0.954 test_metric: -0.885 lr: 0.00007)\n",
            "epoch: 2700, train_loss: 0.163182, val loss: 0.337580,  train_metric: -0.949 test_metric: -0.888 lr: 0.00007)\n",
            "epoch: 2701, train_loss: 0.162151, val loss: 0.335000,  train_metric: -0.954 test_metric: -0.895 lr: 0.00007)\n",
            "epoch: 2702, train_loss: 0.162271, val loss: 0.340153,  train_metric: -0.955 test_metric: -0.879 lr: 0.00007)\n",
            "epoch: 2703, train_loss: 0.165105, val loss: 0.339199,  train_metric: -0.946 test_metric: -0.886 lr: 0.00007)\n",
            "epoch: 2704, train_loss: 0.162481, val loss: 0.338421,  train_metric: -0.954 test_metric: -0.881 lr: 0.00007)\n",
            "epoch: 2705, train_loss: 0.162640, val loss: 0.335712,  train_metric: -0.950 test_metric: -0.891 lr: 0.00007)\n",
            "epoch: 2706, train_loss: 0.164493, val loss: 0.337357,  train_metric: -0.948 test_metric: -0.891 lr: 0.00007)\n",
            "epoch: 2707, train_loss: 0.163524, val loss: 0.337654,  train_metric: -0.949 test_metric: -0.890 lr: 0.00007)\n",
            "epoch: 2708, train_loss: 0.162803, val loss: 0.331361,  train_metric: -0.952 test_metric: -0.893 lr: 0.00007)\n",
            "epoch: 2709, train_loss: 0.162016, val loss: 0.336379,  train_metric: -0.952 test_metric: -0.890 lr: 0.00007)\n",
            "epoch: 2710, train_loss: 0.163195, val loss: 0.335149,  train_metric: -0.953 test_metric: -0.891 lr: 0.00007)\n",
            "epoch: 2711, train_loss: 0.162646, val loss: 0.339554,  train_metric: -0.953 test_metric: -0.887 lr: 0.00007)\n",
            "epoch: 2712, train_loss: 0.164797, val loss: 0.332023,  train_metric: -0.943 test_metric: -0.896 lr: 0.00007)\n",
            "epoch: 2713, train_loss: 0.164932, val loss: 0.333016,  train_metric: -0.947 test_metric: -0.893 lr: 0.00007)\n",
            "epoch: 2714, train_loss: 0.162034, val loss: 0.340818,  train_metric: -0.955 test_metric: -0.880 lr: 0.00007)\n",
            "epoch: 2715, train_loss: 0.161823, val loss: 0.336023,  train_metric: -0.948 test_metric: -0.896 lr: 0.00007)\n",
            "epoch: 2716, train_loss: 0.161002, val loss: 0.335863,  train_metric: -0.956 test_metric: -0.890 lr: 0.00007)\n",
            "epoch: 2717, train_loss: 0.161849, val loss: 0.334258,  train_metric: -0.954 test_metric: -0.896 lr: 0.00007)\n",
            "epoch: 2718, train_loss: 0.162626, val loss: 0.332869,  train_metric: -0.951 test_metric: -0.897 lr: 0.00007)\n",
            "epoch: 2719, train_loss: 0.163078, val loss: 0.333980,  train_metric: -0.952 test_metric: -0.891 lr: 0.00007)\n",
            "epoch: 2720, train_loss: 0.162066, val loss: 0.335699,  train_metric: -0.952 test_metric: -0.891 lr: 0.00007)\n",
            "epoch: 2721, train_loss: 0.162099, val loss: 0.332511,  train_metric: -0.954 test_metric: -0.892 lr: 0.00007)\n",
            "epoch: 2722, train_loss: 0.162388, val loss: 0.334538,  train_metric: -0.949 test_metric: -0.892 lr: 0.00007)\n",
            "epoch: 2723, train_loss: 0.162922, val loss: 0.338956,  train_metric: -0.954 test_metric: -0.885 lr: 0.00007)\n",
            "epoch: 2724, train_loss: 0.162055, val loss: 0.336689,  train_metric: -0.949 test_metric: -0.886 lr: 0.00007)\n",
            "epoch: 2725, train_loss: 0.163635, val loss: 0.334449,  train_metric: -0.948 test_metric: -0.896 lr: 0.00007)\n",
            "epoch: 2726, train_loss: 0.161775, val loss: 0.332431,  train_metric: -0.953 test_metric: -0.892 lr: 0.00007)\n",
            "epoch: 2727, train_loss: 0.162173, val loss: 0.336293,  train_metric: -0.951 test_metric: -0.897 lr: 0.00007)\n",
            "epoch: 2728, train_loss: 0.161881, val loss: 0.333966,  train_metric: -0.949 test_metric: -0.895 lr: 0.00007)\n",
            "epoch: 2729, train_loss: 0.163323, val loss: 0.330397,  train_metric: -0.954 test_metric: -0.895 lr: 0.00007)\n",
            "epoch: 2730, train_loss: 0.162869, val loss: 0.335592,  train_metric: -0.949 test_metric: -0.890 lr: 0.00007)\n",
            "epoch: 2731, train_loss: 0.162666, val loss: 0.332705,  train_metric: -0.953 test_metric: -0.896 lr: 0.00007)\n",
            "epoch: 2732, train_loss: 0.163646, val loss: 0.339028,  train_metric: -0.947 test_metric: -0.887 lr: 0.00006)\n",
            "epoch: 2733, train_loss: 0.163526, val loss: 0.338283,  train_metric: -0.947 test_metric: -0.884 lr: 0.00006)\n",
            "epoch: 2734, train_loss: 0.162364, val loss: 0.336039,  train_metric: -0.950 test_metric: -0.890 lr: 0.00006)\n",
            "epoch: 2735, train_loss: 0.161555, val loss: 0.334700,  train_metric: -0.955 test_metric: -0.897 lr: 0.00006)\n",
            "epoch: 2736, train_loss: 0.161908, val loss: 0.335934,  train_metric: -0.951 test_metric: -0.888 lr: 0.00006)\n",
            "epoch: 2737, train_loss: 0.161474, val loss: 0.338147,  train_metric: -0.948 test_metric: -0.888 lr: 0.00006)\n",
            "epoch: 2738, train_loss: 0.163213, val loss: 0.343095,  train_metric: -0.954 test_metric: -0.880 lr: 0.00006)\n",
            "epoch: 2739, train_loss: 0.162848, val loss: 0.335913,  train_metric: -0.948 test_metric: -0.885 lr: 0.00006)\n",
            "epoch: 2740, train_loss: 0.164272, val loss: 0.340970,  train_metric: -0.951 test_metric: -0.887 lr: 0.00006)\n",
            "epoch: 2741, train_loss: 0.162521, val loss: 0.333551,  train_metric: -0.953 test_metric: -0.888 lr: 0.00006)\n",
            "epoch: 2742, train_loss: 0.161936, val loss: 0.335660,  train_metric: -0.951 test_metric: -0.888 lr: 0.00006)\n",
            "epoch: 2743, train_loss: 0.161669, val loss: 0.336193,  train_metric: -0.951 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2744, train_loss: 0.161107, val loss: 0.335122,  train_metric: -0.954 test_metric: -0.888 lr: 0.00006)\n",
            "epoch: 2745, train_loss: 0.161529, val loss: 0.333716,  train_metric: -0.949 test_metric: -0.892 lr: 0.00006)\n",
            "epoch: 2746, train_loss: 0.161547, val loss: 0.334371,  train_metric: -0.954 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2747, train_loss: 0.163378, val loss: 0.331828,  train_metric: -0.954 test_metric: -0.893 lr: 0.00006)\n",
            "epoch: 2748, train_loss: 0.163716, val loss: 0.336230,  train_metric: -0.950 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2749, train_loss: 0.161760, val loss: 0.333501,  train_metric: -0.953 test_metric: -0.891 lr: 0.00006)\n",
            "epoch: 2750, train_loss: 0.161912, val loss: 0.334841,  train_metric: -0.947 test_metric: -0.892 lr: 0.00006)\n",
            "epoch: 2751, train_loss: 0.160347, val loss: 0.337167,  train_metric: -0.954 test_metric: -0.890 lr: 0.00006)\n",
            "epoch: 2752, train_loss: 0.161323, val loss: 0.333103,  train_metric: -0.953 test_metric: -0.897 lr: 0.00006)\n",
            "epoch: 2753, train_loss: 0.161633, val loss: 0.337640,  train_metric: -0.955 test_metric: -0.884 lr: 0.00006)\n",
            "epoch: 2754, train_loss: 0.161511, val loss: 0.334265,  train_metric: -0.952 test_metric: -0.892 lr: 0.00006)\n",
            "epoch: 2755, train_loss: 0.160680, val loss: 0.335293,  train_metric: -0.954 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2756, train_loss: 0.162147, val loss: 0.334176,  train_metric: -0.947 test_metric: -0.896 lr: 0.00006)\n",
            "epoch: 2757, train_loss: 0.161913, val loss: 0.333709,  train_metric: -0.955 test_metric: -0.897 lr: 0.00006)\n",
            "epoch: 2758, train_loss: 0.161801, val loss: 0.339063,  train_metric: -0.952 test_metric: -0.881 lr: 0.00006)\n",
            "epoch: 2759, train_loss: 0.161846, val loss: 0.333254,  train_metric: -0.949 test_metric: -0.891 lr: 0.00006)\n",
            "epoch: 2760, train_loss: 0.160830, val loss: 0.335001,  train_metric: -0.950 test_metric: -0.890 lr: 0.00006)\n",
            "epoch: 2761, train_loss: 0.162040, val loss: 0.334115,  train_metric: -0.952 test_metric: -0.900 lr: 0.00006)\n",
            "epoch: 2762, train_loss: 0.161055, val loss: 0.339496,  train_metric: -0.953 test_metric: -0.884 lr: 0.00006)\n",
            "epoch: 2763, train_loss: 0.161496, val loss: 0.332939,  train_metric: -0.951 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2764, train_loss: 0.162325, val loss: 0.337958,  train_metric: -0.953 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2765, train_loss: 0.161872, val loss: 0.333107,  train_metric: -0.951 test_metric: -0.891 lr: 0.00006)\n",
            "epoch: 2766, train_loss: 0.161718, val loss: 0.339618,  train_metric: -0.951 test_metric: -0.879 lr: 0.00006)\n",
            "epoch: 2767, train_loss: 0.164538, val loss: 0.344200,  train_metric: -0.949 test_metric: -0.879 lr: 0.00006)\n",
            "epoch: 2768, train_loss: 0.161432, val loss: 0.332452,  train_metric: -0.950 test_metric: -0.896 lr: 0.00006)\n",
            "epoch: 2769, train_loss: 0.161686, val loss: 0.332132,  train_metric: -0.953 test_metric: -0.896 lr: 0.00006)\n",
            "epoch: 2770, train_loss: 0.161662, val loss: 0.332417,  train_metric: -0.950 test_metric: -0.893 lr: 0.00006)\n",
            "epoch: 2771, train_loss: 0.161781, val loss: 0.343016,  train_metric: -0.955 test_metric: -0.877 lr: 0.00006)\n",
            "epoch: 2772, train_loss: 0.161328, val loss: 0.335814,  train_metric: -0.949 test_metric: -0.885 lr: 0.00006)\n",
            "epoch: 2773, train_loss: 0.160481, val loss: 0.334159,  train_metric: -0.952 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2774, train_loss: 0.160582, val loss: 0.335538,  train_metric: -0.954 test_metric: -0.885 lr: 0.00006)\n",
            "epoch: 2775, train_loss: 0.162832, val loss: 0.340589,  train_metric: -0.948 test_metric: -0.885 lr: 0.00006)\n",
            "epoch: 2776, train_loss: 0.161241, val loss: 0.334302,  train_metric: -0.949 test_metric: -0.890 lr: 0.00006)\n",
            "epoch: 2777, train_loss: 0.161391, val loss: 0.333235,  train_metric: -0.951 test_metric: -0.897 lr: 0.00006)\n",
            "epoch: 2778, train_loss: 0.161113, val loss: 0.336365,  train_metric: -0.957 test_metric: -0.885 lr: 0.00006)\n",
            "epoch: 2779, train_loss: 0.161257, val loss: 0.334134,  train_metric: -0.952 test_metric: -0.898 lr: 0.00006)\n",
            "epoch: 2780, train_loss: 0.161136, val loss: 0.334522,  train_metric: -0.953 test_metric: -0.890 lr: 0.00006)\n",
            "epoch: 2781, train_loss: 0.160769, val loss: 0.334511,  train_metric: -0.949 test_metric: -0.897 lr: 0.00006)\n",
            "epoch: 2782, train_loss: 0.160415, val loss: 0.334591,  train_metric: -0.954 test_metric: -0.890 lr: 0.00006)\n",
            "epoch: 2783, train_loss: 0.161075, val loss: 0.333290,  train_metric: -0.952 test_metric: -0.886 lr: 0.00006)\n",
            "epoch: 2784, train_loss: 0.160557, val loss: 0.335129,  train_metric: -0.952 test_metric: -0.891 lr: 0.00006)\n",
            "epoch: 2785, train_loss: 0.160305, val loss: 0.333460,  train_metric: -0.953 test_metric: -0.891 lr: 0.00006)\n",
            "epoch: 2786, train_loss: 0.160368, val loss: 0.336203,  train_metric: -0.954 test_metric: -0.890 lr: 0.00006)\n",
            "epoch: 2787, train_loss: 0.159927, val loss: 0.332651,  train_metric: -0.955 test_metric: -0.896 lr: 0.00006)\n",
            "epoch: 2788, train_loss: 0.161646, val loss: 0.333618,  train_metric: -0.954 test_metric: -0.893 lr: 0.00006)\n",
            "epoch: 2789, train_loss: 0.161317, val loss: 0.336852,  train_metric: -0.951 test_metric: -0.897 lr: 0.00006)\n",
            "epoch: 2790, train_loss: 0.160850, val loss: 0.332900,  train_metric: -0.950 test_metric: -0.898 lr: 0.00006)\n",
            "epoch: 2791, train_loss: 0.161117, val loss: 0.332387,  train_metric: -0.955 test_metric: -0.892 lr: 0.00006)\n",
            "epoch: 2792, train_loss: 0.160639, val loss: 0.334920,  train_metric: -0.954 test_metric: -0.891 lr: 0.00006)\n",
            "epoch: 2793, train_loss: 0.160896, val loss: 0.333038,  train_metric: -0.949 test_metric: -0.898 lr: 0.00006)\n",
            "epoch: 2794, train_loss: 0.160091, val loss: 0.337645,  train_metric: -0.957 test_metric: -0.887 lr: 0.00006)\n",
            "epoch: 2795, train_loss: 0.160436, val loss: 0.333568,  train_metric: -0.951 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2796, train_loss: 0.160230, val loss: 0.336674,  train_metric: -0.946 test_metric: -0.880 lr: 0.00006)\n",
            "epoch: 2797, train_loss: 0.160332, val loss: 0.334277,  train_metric: -0.954 test_metric: -0.892 lr: 0.00006)\n",
            "epoch: 2798, train_loss: 0.159812, val loss: 0.331799,  train_metric: -0.954 test_metric: -0.893 lr: 0.00006)\n",
            "epoch: 2799, train_loss: 0.159445, val loss: 0.339102,  train_metric: -0.954 test_metric: -0.890 lr: 0.00006)\n",
            "epoch: 2800, train_loss: 0.161042, val loss: 0.334232,  train_metric: -0.951 test_metric: -0.891 lr: 0.00006)\n",
            "epoch: 2801, train_loss: 0.160684, val loss: 0.333095,  train_metric: -0.951 test_metric: -0.888 lr: 0.00006)\n",
            "epoch: 2802, train_loss: 0.160527, val loss: 0.337602,  train_metric: -0.952 test_metric: -0.892 lr: 0.00006)\n",
            "epoch: 2803, train_loss: 0.160415, val loss: 0.335129,  train_metric: -0.951 test_metric: -0.896 lr: 0.00006)\n",
            "epoch: 2804, train_loss: 0.160991, val loss: 0.333095,  train_metric: -0.953 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2805, train_loss: 0.160178, val loss: 0.337443,  train_metric: -0.952 test_metric: -0.892 lr: 0.00006)\n",
            "epoch: 2806, train_loss: 0.160541, val loss: 0.336055,  train_metric: -0.954 test_metric: -0.893 lr: 0.00006)\n",
            "epoch: 2807, train_loss: 0.160647, val loss: 0.336297,  train_metric: -0.954 test_metric: -0.886 lr: 0.00006)\n",
            "epoch: 2808, train_loss: 0.160402, val loss: 0.333404,  train_metric: -0.951 test_metric: -0.896 lr: 0.00006)\n",
            "epoch: 2809, train_loss: 0.159840, val loss: 0.335037,  train_metric: -0.951 test_metric: -0.887 lr: 0.00006)\n",
            "epoch: 2810, train_loss: 0.159943, val loss: 0.332322,  train_metric: -0.949 test_metric: -0.893 lr: 0.00006)\n",
            "epoch: 2811, train_loss: 0.162109, val loss: 0.334385,  train_metric: -0.954 test_metric: -0.897 lr: 0.00006)\n",
            "epoch: 2812, train_loss: 0.161534, val loss: 0.334202,  train_metric: -0.951 test_metric: -0.892 lr: 0.00006)\n",
            "epoch: 2813, train_loss: 0.159645, val loss: 0.339524,  train_metric: -0.954 test_metric: -0.891 lr: 0.00006)\n",
            "epoch: 2814, train_loss: 0.160390, val loss: 0.339168,  train_metric: -0.956 test_metric: -0.886 lr: 0.00006)\n",
            "epoch: 2815, train_loss: 0.159945, val loss: 0.332505,  train_metric: -0.952 test_metric: -0.892 lr: 0.00006)\n",
            "epoch: 2816, train_loss: 0.160568, val loss: 0.332912,  train_metric: -0.956 test_metric: -0.891 lr: 0.00006)\n",
            "epoch: 2817, train_loss: 0.159765, val loss: 0.340391,  train_metric: -0.953 test_metric: -0.886 lr: 0.00006)\n",
            "epoch: 2818, train_loss: 0.160775, val loss: 0.337138,  train_metric: -0.954 test_metric: -0.881 lr: 0.00006)\n",
            "epoch: 2819, train_loss: 0.160096, val loss: 0.333245,  train_metric: -0.952 test_metric: -0.900 lr: 0.00006)\n",
            "epoch: 2820, train_loss: 0.160062, val loss: 0.333101,  train_metric: -0.951 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2821, train_loss: 0.161654, val loss: 0.331409,  train_metric: -0.952 test_metric: -0.896 lr: 0.00006)\n",
            "epoch: 2822, train_loss: 0.159213, val loss: 0.337084,  train_metric: -0.955 test_metric: -0.891 lr: 0.00006)\n",
            "epoch: 2823, train_loss: 0.159928, val loss: 0.336172,  train_metric: -0.956 test_metric: -0.887 lr: 0.00006)\n",
            "epoch: 2824, train_loss: 0.160928, val loss: 0.331804,  train_metric: -0.952 test_metric: -0.892 lr: 0.00006)\n",
            "epoch: 2825, train_loss: 0.161140, val loss: 0.333915,  train_metric: -0.952 test_metric: -0.896 lr: 0.00006)\n",
            "epoch: 2826, train_loss: 0.160339, val loss: 0.333447,  train_metric: -0.953 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2827, train_loss: 0.159591, val loss: 0.332360,  train_metric: -0.956 test_metric: -0.900 lr: 0.00006)\n",
            "epoch: 2828, train_loss: 0.160284, val loss: 0.339762,  train_metric: -0.956 test_metric: -0.886 lr: 0.00006)\n",
            "epoch: 2829, train_loss: 0.160592, val loss: 0.335503,  train_metric: -0.948 test_metric: -0.890 lr: 0.00006)\n",
            "epoch: 2830, train_loss: 0.162099, val loss: 0.332490,  train_metric: -0.948 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2831, train_loss: 0.161079, val loss: 0.339089,  train_metric: -0.954 test_metric: -0.888 lr: 0.00006)\n",
            "epoch: 2832, train_loss: 0.161285, val loss: 0.330071,  train_metric: -0.951 test_metric: -0.888 lr: 0.00006)\n",
            "epoch: 2833, train_loss: 0.160220, val loss: 0.344916,  train_metric: -0.948 test_metric: -0.876 lr: 0.00006)\n",
            "epoch: 2834, train_loss: 0.164223, val loss: 0.341177,  train_metric: -0.948 test_metric: -0.887 lr: 0.00006)\n",
            "epoch: 2835, train_loss: 0.161816, val loss: 0.331136,  train_metric: -0.948 test_metric: -0.898 lr: 0.00006)\n",
            "epoch: 2836, train_loss: 0.161585, val loss: 0.338471,  train_metric: -0.950 test_metric: -0.893 lr: 0.00006)\n",
            "epoch: 2837, train_loss: 0.162276, val loss: 0.330352,  train_metric: -0.948 test_metric: -0.901 lr: 0.00006)\n",
            "epoch: 2838, train_loss: 0.160634, val loss: 0.337961,  train_metric: -0.950 test_metric: -0.886 lr: 0.00006)\n",
            "epoch: 2839, train_loss: 0.159440, val loss: 0.333236,  train_metric: -0.951 test_metric: -0.898 lr: 0.00006)\n",
            "epoch: 2840, train_loss: 0.159930, val loss: 0.332754,  train_metric: -0.953 test_metric: -0.891 lr: 0.00006)\n",
            "epoch: 2841, train_loss: 0.160184, val loss: 0.340453,  train_metric: -0.955 test_metric: -0.884 lr: 0.00006)\n",
            "epoch: 2842, train_loss: 0.159397, val loss: 0.333748,  train_metric: -0.948 test_metric: -0.897 lr: 0.00006)\n",
            "epoch: 2843, train_loss: 0.160197, val loss: 0.335641,  train_metric: -0.954 test_metric: -0.888 lr: 0.00006)\n",
            "epoch: 2844, train_loss: 0.159338, val loss: 0.339331,  train_metric: -0.952 test_metric: -0.887 lr: 0.00006)\n",
            "epoch: 2845, train_loss: 0.160582, val loss: 0.336202,  train_metric: -0.948 test_metric: -0.891 lr: 0.00006)\n",
            "epoch: 2846, train_loss: 0.162031, val loss: 0.331388,  train_metric: -0.951 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2847, train_loss: 0.160758, val loss: 0.344928,  train_metric: -0.950 test_metric: -0.874 lr: 0.00006)\n",
            "epoch: 2848, train_loss: 0.160486, val loss: 0.335201,  train_metric: -0.952 test_metric: -0.886 lr: 0.00006)\n",
            "epoch: 2849, train_loss: 0.161466, val loss: 0.334769,  train_metric: -0.949 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2850, train_loss: 0.159137, val loss: 0.333486,  train_metric: -0.955 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2851, train_loss: 0.160127, val loss: 0.344130,  train_metric: -0.953 test_metric: -0.877 lr: 0.00006)\n",
            "epoch: 2852, train_loss: 0.160220, val loss: 0.334100,  train_metric: -0.952 test_metric: -0.896 lr: 0.00006)\n",
            "epoch: 2853, train_loss: 0.159944, val loss: 0.332142,  train_metric: -0.950 test_metric: -0.896 lr: 0.00006)\n",
            "epoch: 2854, train_loss: 0.159264, val loss: 0.336904,  train_metric: -0.954 test_metric: -0.885 lr: 0.00006)\n",
            "epoch: 2855, train_loss: 0.160190, val loss: 0.340992,  train_metric: -0.953 test_metric: -0.887 lr: 0.00006)\n",
            "epoch: 2856, train_loss: 0.160070, val loss: 0.333919,  train_metric: -0.954 test_metric: -0.892 lr: 0.00006)\n",
            "epoch: 2857, train_loss: 0.159790, val loss: 0.331528,  train_metric: -0.954 test_metric: -0.892 lr: 0.00006)\n",
            "epoch: 2858, train_loss: 0.159299, val loss: 0.333727,  train_metric: -0.954 test_metric: -0.897 lr: 0.00006)\n",
            "epoch: 2859, train_loss: 0.158708, val loss: 0.336401,  train_metric: -0.952 test_metric: -0.884 lr: 0.00006)\n",
            "epoch: 2860, train_loss: 0.159941, val loss: 0.337784,  train_metric: -0.953 test_metric: -0.888 lr: 0.00006)\n",
            "epoch: 2861, train_loss: 0.160078, val loss: 0.334692,  train_metric: -0.952 test_metric: -0.888 lr: 0.00006)\n",
            "epoch: 2862, train_loss: 0.159052, val loss: 0.335484,  train_metric: -0.953 test_metric: -0.890 lr: 0.00006)\n",
            "epoch: 2863, train_loss: 0.159955, val loss: 0.331080,  train_metric: -0.952 test_metric: -0.896 lr: 0.00006)\n",
            "epoch: 2864, train_loss: 0.160985, val loss: 0.336913,  train_metric: -0.947 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2865, train_loss: 0.161430, val loss: 0.333787,  train_metric: -0.951 test_metric: -0.892 lr: 0.00006)\n",
            "epoch: 2866, train_loss: 0.159696, val loss: 0.333708,  train_metric: -0.955 test_metric: -0.900 lr: 0.00006)\n",
            "epoch: 2867, train_loss: 0.161480, val loss: 0.334690,  train_metric: -0.952 test_metric: -0.897 lr: 0.00006)\n",
            "epoch: 2868, train_loss: 0.159530, val loss: 0.333990,  train_metric: -0.951 test_metric: -0.893 lr: 0.00006)\n",
            "epoch: 2869, train_loss: 0.158928, val loss: 0.334953,  train_metric: -0.953 test_metric: -0.888 lr: 0.00006)\n",
            "epoch: 2870, train_loss: 0.158972, val loss: 0.339491,  train_metric: -0.956 test_metric: -0.886 lr: 0.00006)\n",
            "epoch: 2871, train_loss: 0.161638, val loss: 0.337774,  train_metric: -0.951 test_metric: -0.877 lr: 0.00006)\n",
            "epoch: 2872, train_loss: 0.160725, val loss: 0.339409,  train_metric: -0.951 test_metric: -0.888 lr: 0.00006)\n",
            "epoch: 2873, train_loss: 0.159961, val loss: 0.336291,  train_metric: -0.949 test_metric: -0.886 lr: 0.00006)\n",
            "epoch: 2874, train_loss: 0.160245, val loss: 0.335850,  train_metric: -0.952 test_metric: -0.885 lr: 0.00006)\n",
            "epoch: 2875, train_loss: 0.161758, val loss: 0.339714,  train_metric: -0.949 test_metric: -0.881 lr: 0.00006)\n",
            "epoch: 2876, train_loss: 0.160293, val loss: 0.332953,  train_metric: -0.949 test_metric: -0.897 lr: 0.00006)\n",
            "epoch: 2877, train_loss: 0.159884, val loss: 0.334429,  train_metric: -0.955 test_metric: -0.890 lr: 0.00006)\n",
            "epoch: 2878, train_loss: 0.158425, val loss: 0.333756,  train_metric: -0.955 test_metric: -0.896 lr: 0.00006)\n",
            "epoch: 2879, train_loss: 0.159508, val loss: 0.334012,  train_metric: -0.948 test_metric: -0.887 lr: 0.00006)\n",
            "epoch: 2880, train_loss: 0.159342, val loss: 0.335228,  train_metric: -0.952 test_metric: -0.888 lr: 0.00006)\n",
            "epoch: 2881, train_loss: 0.159502, val loss: 0.333531,  train_metric: -0.951 test_metric: -0.887 lr: 0.00006)\n",
            "epoch: 2882, train_loss: 0.158848, val loss: 0.335198,  train_metric: -0.954 test_metric: -0.891 lr: 0.00006)\n",
            "epoch: 2883, train_loss: 0.159200, val loss: 0.332412,  train_metric: -0.954 test_metric: -0.895 lr: 0.00006)\n",
            "epoch: 2884, train_loss: 0.160197, val loss: 0.333354,  train_metric: -0.954 test_metric: -0.888 lr: 0.00006)\n",
            "epoch: 2885, train_loss: 0.158773, val loss: 0.339098,  train_metric: -0.955 test_metric: -0.879 lr: 0.00006)\n",
            "epoch: 2886, train_loss: 0.160307, val loss: 0.338607,  train_metric: -0.955 test_metric: -0.884 lr: 0.00006)\n",
            "epoch: 2887, train_loss: 0.159637, val loss: 0.333478,  train_metric: -0.952 test_metric: -0.892 lr: 0.00006)\n",
            "epoch: 2888, train_loss: 0.159791, val loss: 0.333135,  train_metric: -0.953 test_metric: -0.891 lr: 0.00006)\n",
            "epoch: 2889, train_loss: 0.159969, val loss: 0.332458,  train_metric: -0.952 test_metric: -0.891 lr: 0.00006)\n",
            "epoch: 2890, train_loss: 0.159360, val loss: 0.334039,  train_metric: -0.950 test_metric: -0.891 lr: 0.00006)\n",
            "epoch: 2891, train_loss: 0.159374, val loss: 0.333835,  train_metric: -0.955 test_metric: -0.896 lr: 0.00006)\n",
            "epoch: 2892, train_loss: 0.158071, val loss: 0.332941,  train_metric: -0.957 test_metric: -0.897 lr: 0.00006)\n",
            "epoch: 2893, train_loss: 0.159304, val loss: 0.334437,  train_metric: -0.952 test_metric: -0.886 lr: 0.00006)\n",
            "epoch: 2894, train_loss: 0.161358, val loss: 0.333376,  train_metric: -0.953 test_metric: -0.892 lr: 0.00006)\n",
            "epoch: 2895, train_loss: 0.158337, val loss: 0.338578,  train_metric: -0.956 test_metric: -0.885 lr: 0.00006)\n",
            "epoch: 2896, train_loss: 0.159917, val loss: 0.333126,  train_metric: -0.952 test_metric: -0.890 lr: 0.00006)\n",
            "epoch: 2897, train_loss: 0.158622, val loss: 0.333638,  train_metric: -0.952 test_metric: -0.893 lr: 0.00006)\n",
            "epoch: 2898, train_loss: 0.160200, val loss: 0.338494,  train_metric: -0.948 test_metric: -0.895 lr: 0.00005)\n",
            "epoch: 2899, train_loss: 0.162354, val loss: 0.331752,  train_metric: -0.947 test_metric: -0.900 lr: 0.00005)\n",
            "epoch: 2900, train_loss: 0.159816, val loss: 0.341014,  train_metric: -0.953 test_metric: -0.886 lr: 0.00005)\n",
            "epoch: 2901, train_loss: 0.159491, val loss: 0.331552,  train_metric: -0.953 test_metric: -0.892 lr: 0.00005)\n",
            "epoch: 2902, train_loss: 0.158673, val loss: 0.333466,  train_metric: -0.953 test_metric: -0.893 lr: 0.00005)\n",
            "epoch: 2903, train_loss: 0.158438, val loss: 0.335129,  train_metric: -0.951 test_metric: -0.890 lr: 0.00005)\n",
            "epoch: 2904, train_loss: 0.159584, val loss: 0.336988,  train_metric: -0.954 test_metric: -0.888 lr: 0.00005)\n",
            "epoch: 2905, train_loss: 0.158729, val loss: 0.331239,  train_metric: -0.954 test_metric: -0.895 lr: 0.00005)\n",
            "epoch: 2906, train_loss: 0.158620, val loss: 0.333611,  train_metric: -0.953 test_metric: -0.898 lr: 0.00005)\n",
            "epoch: 2907, train_loss: 0.158763, val loss: 0.334753,  train_metric: -0.952 test_metric: -0.892 lr: 0.00005)\n",
            "epoch: 2908, train_loss: 0.159272, val loss: 0.331698,  train_metric: -0.953 test_metric: -0.895 lr: 0.00005)\n",
            "epoch: 2909, train_loss: 0.158247, val loss: 0.333947,  train_metric: -0.954 test_metric: -0.895 lr: 0.00005)\n",
            "epoch: 2910, train_loss: 0.159177, val loss: 0.333433,  train_metric: -0.953 test_metric: -0.895 lr: 0.00005)\n",
            "epoch: 2911, train_loss: 0.158584, val loss: 0.333613,  train_metric: -0.956 test_metric: -0.895 lr: 0.00005)\n",
            "epoch: 2912, train_loss: 0.158650, val loss: 0.334932,  train_metric: -0.954 test_metric: -0.897 lr: 0.00005)\n",
            "epoch: 2913, train_loss: 0.158869, val loss: 0.333143,  train_metric: -0.953 test_metric: -0.893 lr: 0.00005)\n",
            "epoch: 2914, train_loss: 0.160437, val loss: 0.341488,  train_metric: -0.951 test_metric: -0.877 lr: 0.00005)\n",
            "epoch: 2915, train_loss: 0.158505, val loss: 0.332471,  train_metric: -0.952 test_metric: -0.895 lr: 0.00005)\n",
            "epoch: 2916, train_loss: 0.158341, val loss: 0.332709,  train_metric: -0.949 test_metric: -0.898 lr: 0.00005)\n",
            "epoch: 2917, train_loss: 0.160908, val loss: 0.338502,  train_metric: -0.955 test_metric: -0.888 lr: 0.00005)\n",
            "epoch: 2918, train_loss: 0.158701, val loss: 0.332034,  train_metric: -0.953 test_metric: -0.892 lr: 0.00005)\n",
            "epoch: 2919, train_loss: 0.158800, val loss: 0.331116,  train_metric: -0.957 test_metric: -0.896 lr: 0.00005)\n",
            "epoch: 2920, train_loss: 0.159376, val loss: 0.335822,  train_metric: -0.949 test_metric: -0.887 lr: 0.00005)\n",
            "epoch: 2921, train_loss: 0.158127, val loss: 0.341336,  train_metric: -0.956 test_metric: -0.886 lr: 0.00005)\n",
            "epoch: 2922, train_loss: 0.158793, val loss: 0.331094,  train_metric: -0.953 test_metric: -0.892 lr: 0.00005)\n",
            "epoch: 2923, train_loss: 0.157661, val loss: 0.338447,  train_metric: -0.955 test_metric: -0.887 lr: 0.00005)\n",
            "epoch: 2924, train_loss: 0.158413, val loss: 0.333543,  train_metric: -0.952 test_metric: -0.892 lr: 0.00005)\n",
            "epoch: 2925, train_loss: 0.158514, val loss: 0.329891,  train_metric: -0.955 test_metric: -0.900 lr: 0.00005)\n",
            "epoch: 2926, train_loss: 0.159580, val loss: 0.338859,  train_metric: -0.953 test_metric: -0.884 lr: 0.00005)\n",
            "epoch: 2927, train_loss: 0.158077, val loss: 0.333880,  train_metric: -0.955 test_metric: -0.891 lr: 0.00005)\n",
            "epoch: 2928, train_loss: 0.158791, val loss: 0.332437,  train_metric: -0.955 test_metric: -0.895 lr: 0.00005)\n",
            "epoch: 2929, train_loss: 0.159000, val loss: 0.333737,  train_metric: -0.949 test_metric: -0.893 lr: 0.00005)\n",
            "epoch: 2930, train_loss: 0.158243, val loss: 0.337834,  train_metric: -0.957 test_metric: -0.891 lr: 0.00005)\n",
            "epoch: 2931, train_loss: 0.158799, val loss: 0.330788,  train_metric: -0.950 test_metric: -0.895 lr: 0.00005)\n",
            "epoch: 2932, train_loss: 0.158528, val loss: 0.335038,  train_metric: -0.955 test_metric: -0.886 lr: 0.00005)\n",
            "epoch: 2933, train_loss: 0.158255, val loss: 0.335066,  train_metric: -0.955 test_metric: -0.891 lr: 0.00005)\n",
            "epoch: 2934, train_loss: 0.158562, val loss: 0.338155,  train_metric: -0.953 test_metric: -0.881 lr: 0.00005)\n",
            "epoch: 2935, train_loss: 0.158093, val loss: 0.333132,  train_metric: -0.957 test_metric: -0.891 lr: 0.00005)\n",
            "epoch: 2936, train_loss: 0.158326, val loss: 0.332069,  train_metric: -0.951 test_metric: -0.892 lr: 0.00005)\n",
            "epoch: 2937, train_loss: 0.157400, val loss: 0.333425,  train_metric: -0.953 test_metric: -0.891 lr: 0.00005)\n",
            "epoch: 2938, train_loss: 0.159346, val loss: 0.333645,  train_metric: -0.953 test_metric: -0.898 lr: 0.00005)\n",
            "epoch: 2939, train_loss: 0.158234, val loss: 0.335917,  train_metric: -0.957 test_metric: -0.893 lr: 0.00005)\n",
            "epoch: 2940, train_loss: 0.158960, val loss: 0.334127,  train_metric: -0.953 test_metric: -0.892 lr: 0.00005)\n",
            "epoch: 2941, train_loss: 0.158174, val loss: 0.333837,  train_metric: -0.955 test_metric: -0.890 lr: 0.00005)\n",
            "epoch: 2942, train_loss: 0.159115, val loss: 0.337203,  train_metric: -0.951 test_metric: -0.890 lr: 0.00005)\n",
            "epoch: 2943, train_loss: 0.158557, val loss: 0.334107,  train_metric: -0.949 test_metric: -0.888 lr: 0.00005)\n",
            "epoch: 2944, train_loss: 0.157793, val loss: 0.338040,  train_metric: -0.955 test_metric: -0.890 lr: 0.00005)\n",
            "epoch: 2945, train_loss: 0.158780, val loss: 0.332130,  train_metric: -0.959 test_metric: -0.893 lr: 0.00005)\n",
            "epoch: 2946, train_loss: 0.157320, val loss: 0.336327,  train_metric: -0.954 test_metric: -0.885 lr: 0.00005)\n",
            "epoch: 2947, train_loss: 0.158732, val loss: 0.337118,  train_metric: -0.949 test_metric: -0.884 lr: 0.00005)\n",
            "epoch: 2948, train_loss: 0.158551, val loss: 0.334660,  train_metric: -0.953 test_metric: -0.895 lr: 0.00005)\n",
            "epoch: 2949, train_loss: 0.157342, val loss: 0.334934,  train_metric: -0.959 test_metric: -0.892 lr: 0.00005)\n",
            "epoch: 2950, train_loss: 0.157823, val loss: 0.339265,  train_metric: -0.954 test_metric: -0.880 lr: 0.00005)\n",
            "epoch: 2951, train_loss: 0.159887, val loss: 0.337918,  train_metric: -0.951 test_metric: -0.886 lr: 0.00005)\n",
            "epoch: 2952, train_loss: 0.158565, val loss: 0.334866,  train_metric: -0.951 test_metric: -0.886 lr: 0.00005)\n",
            "epoch: 2953, train_loss: 0.157788, val loss: 0.334441,  train_metric: -0.952 test_metric: -0.900 lr: 0.00005)\n",
            "epoch: 2954, train_loss: 0.158922, val loss: 0.335601,  train_metric: -0.954 test_metric: -0.892 lr: 0.00005)\n",
            "epoch: 2955, train_loss: 0.157926, val loss: 0.331669,  train_metric: -0.954 test_metric: -0.896 lr: 0.00005)\n",
            "epoch: 2956, train_loss: 0.158719, val loss: 0.340810,  train_metric: -0.954 test_metric: -0.888 lr: 0.00005)\n",
            "epoch: 2957, train_loss: 0.158544, val loss: 0.331083,  train_metric: -0.950 test_metric: -0.897 lr: 0.00005)\n",
            "epoch: 2958, train_loss: 0.158785, val loss: 0.336273,  train_metric: -0.956 test_metric: -0.893 lr: 0.00005)\n",
            "epoch: 2959, train_loss: 0.157570, val loss: 0.334040,  train_metric: -0.957 test_metric: -0.890 lr: 0.00005)\n",
            "epoch: 2960, train_loss: 0.158068, val loss: 0.332550,  train_metric: -0.952 test_metric: -0.897 lr: 0.00005)\n",
            "epoch: 2961, train_loss: 0.159922, val loss: 0.339109,  train_metric: -0.952 test_metric: -0.890 lr: 0.00005)\n",
            "epoch: 2962, train_loss: 0.158805, val loss: 0.331097,  train_metric: -0.953 test_metric: -0.895 lr: 0.00005)\n",
            "epoch: 2963, train_loss: 0.158457, val loss: 0.339111,  train_metric: -0.957 test_metric: -0.882 lr: 0.00005)\n",
            "epoch: 2964, train_loss: 0.159252, val loss: 0.337892,  train_metric: -0.955 test_metric: -0.891 lr: 0.00005)\n",
            "epoch: 2965, train_loss: 0.157970, val loss: 0.332915,  train_metric: -0.952 test_metric: -0.893 lr: 0.00005)\n",
            "epoch: 2966, train_loss: 0.157751, val loss: 0.339637,  train_metric: -0.953 test_metric: -0.881 lr: 0.00005)\n",
            "epoch: 2967, train_loss: 0.157404, val loss: 0.332437,  train_metric: -0.952 test_metric: -0.893 lr: 0.00005)\n",
            "epoch: 2968, train_loss: 0.157768, val loss: 0.341505,  train_metric: -0.955 test_metric: -0.880 lr: 0.00005)\n",
            "epoch: 2969, train_loss: 0.160578, val loss: 0.337955,  train_metric: -0.949 test_metric: -0.882 lr: 0.00005)\n",
            "epoch: 2970, train_loss: 0.158076, val loss: 0.333025,  train_metric: -0.952 test_metric: -0.893 lr: 0.00005)\n",
            "epoch: 2971, train_loss: 0.158994, val loss: 0.335065,  train_metric: -0.954 test_metric: -0.890 lr: 0.00005)\n",
            "epoch: 2972, train_loss: 0.157194, val loss: 0.332742,  train_metric: -0.954 test_metric: -0.898 lr: 0.00005)\n",
            "epoch: 2973, train_loss: 0.159216, val loss: 0.332310,  train_metric: -0.952 test_metric: -0.890 lr: 0.00005)\n",
            "epoch: 2974, train_loss: 0.158845, val loss: 0.339324,  train_metric: -0.955 test_metric: -0.887 lr: 0.00005)\n",
            "epoch: 2975, train_loss: 0.157762, val loss: 0.333542,  train_metric: -0.956 test_metric: -0.893 lr: 0.00005)\n",
            "epoch: 2976, train_loss: 0.157610, val loss: 0.332551,  train_metric: -0.955 test_metric: -0.895 lr: 0.00005)\n",
            "epoch: 2977, train_loss: 0.157961, val loss: 0.333247,  train_metric: -0.952 test_metric: -0.895 lr: 0.00005)\n",
            "epoch: 2978, train_loss: 0.157934, val loss: 0.331925,  train_metric: -0.955 test_metric: -0.893 lr: 0.00005)\n",
            "epoch: 2979, train_loss: 0.158417, val loss: 0.331691,  train_metric: -0.954 test_metric: -0.897 lr: 0.00005)\n",
            "epoch: 2980, train_loss: 0.158553, val loss: 0.332494,  train_metric: -0.951 test_metric: -0.898 lr: 0.00005)\n",
            "epoch: 2981, train_loss: 0.158820, val loss: 0.332399,  train_metric: -0.953 test_metric: -0.896 lr: 0.00005)\n",
            "epoch: 2982, train_loss: 0.157685, val loss: 0.335861,  train_metric: -0.956 test_metric: -0.885 lr: 0.00005)\n",
            "epoch: 2983, train_loss: 0.158240, val loss: 0.334365,  train_metric: -0.954 test_metric: -0.892 lr: 0.00005)\n",
            "epoch: 2984, train_loss: 0.157520, val loss: 0.336068,  train_metric: -0.954 test_metric: -0.888 lr: 0.00005)\n",
            "epoch: 2985, train_loss: 0.157887, val loss: 0.335247,  train_metric: -0.955 test_metric: -0.890 lr: 0.00005)\n",
            "epoch: 2986, train_loss: 0.157309, val loss: 0.333237,  train_metric: -0.953 test_metric: -0.892 lr: 0.00005)\n",
            "epoch: 2987, train_loss: 0.157336, val loss: 0.332448,  train_metric: -0.955 test_metric: -0.893 lr: 0.00005)\n",
            "epoch: 2988, train_loss: 0.157382, val loss: 0.336708,  train_metric: -0.953 test_metric: -0.892 lr: 0.00005)\n",
            "epoch: 2989, train_loss: 0.161228, val loss: 0.330698,  train_metric: -0.947 test_metric: -0.896 lr: 0.00005)\n",
            "epoch: 2990, train_loss: 0.159619, val loss: 0.341154,  train_metric: -0.952 test_metric: -0.884 lr: 0.00005)\n",
            "epoch: 2991, train_loss: 0.158481, val loss: 0.333655,  train_metric: -0.953 test_metric: -0.897 lr: 0.00005)\n",
            "epoch: 2992, train_loss: 0.158364, val loss: 0.332287,  train_metric: -0.954 test_metric: -0.893 lr: 0.00005)\n",
            "epoch: 2993, train_loss: 0.157833, val loss: 0.334183,  train_metric: -0.952 test_metric: -0.891 lr: 0.00005)\n",
            "epoch: 2994, train_loss: 0.158124, val loss: 0.332469,  train_metric: -0.953 test_metric: -0.890 lr: 0.00005)\n",
            "epoch: 2995, train_loss: 0.157508, val loss: 0.333112,  train_metric: -0.949 test_metric: -0.895 lr: 0.00005)\n",
            "epoch: 2996, train_loss: 0.157828, val loss: 0.331545,  train_metric: -0.955 test_metric: -0.898 lr: 0.00005)\n",
            "epoch: 2997, train_loss: 0.156953, val loss: 0.337462,  train_metric: -0.959 test_metric: -0.888 lr: 0.00005)\n",
            "epoch: 2998, train_loss: 0.157452, val loss: 0.334420,  train_metric: -0.953 test_metric: -0.892 lr: 0.00005)\n",
            "epoch: 2999, train_loss: 0.158120, val loss: 0.334060,  train_metric: -0.955 test_metric: -0.901 lr: 0.00005)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh5CkI36nq2S"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd8jW5pynyCK",
        "outputId": "e514c90a-64cf-444b-a726-0d7a03a8735e"
      },
      "source": [
        "# Load Classification\r\n",
        "names = list({\"bianca\":0,\"gialla\": 1, \"arancione\": 2, \"rossa\": 3})\r\n",
        "C_model = ClassificationNet(num_inputs=10,num_classes=4).to(device)\r\n",
        "C_weights = torch.load(\"weights_classification.pt\")\r\n",
        "C_model.load_state_dict(C_weights)\r\n",
        "C_model = C_model.to(device)\r\n",
        "\r\n",
        "# Predict Classication\r\n",
        "C_Xt = torch.from_numpy(C_X_test).type(torch.float32).unsqueeze(1)\r\n",
        "C_Xt = (C_Xt - C_mean) / C_std\r\n",
        "C_Y_hat = C_model.forward(C_Xt).argmax(dim=-1,keepdim=True).detach().numpy().reshape(-1)\r\n",
        "\r\n",
        "# Visualize results\r\n",
        "cm = confusion_matrix(C_Y_test,C_Y_hat)\r\n",
        "names_pred = [ \"Pred: \" + n for n in names]\r\n",
        "df = pd.DataFrame(cm, columns=names_pred, index=names)\r\n",
        "print(df)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           Pred: bianca  Pred: gialla  Pred: arancione  Pred: rossa\n",
            "bianca              242            20               15            3\n",
            "gialla                9           257                4            0\n",
            "arancione            15            10              177            2\n",
            "rossa                 2             0                0           51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzgtzyhoSjJk"
      },
      "source": [
        "## Extra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qiaz_2DYhrf"
      },
      "source": [
        "\r\n",
        "# df.head() # prime 5 righe\r\n",
        "# df.tail() # ultime 5 righe\r\n",
        "# df.to_csv(\"a.csv\") # Salva come csv\r\n",
        "# df.keys() # nomi delle colonne\r\n",
        "# df.values.shape # dimensione\r\n",
        "# df.values # dati come array \r\n",
        "\r\n",
        "# Nomi delle colonne\r\n",
        "# Numero di righe\r\n",
        "# selezionare solo la colonna con nome dimessi_guariti\r\n",
        "# voglio i dati come array\r\n",
        "# voglio i deceduti > 700\r\n",
        "# dati del 12 marzo in poi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3OuqOczPcDe",
        "outputId": "2ac2262d-7181-4924-9e2c-e886c988b46f"
      },
      "source": [
        "x = np.array( [5,3,5,6,7] , dtype=np.float32)\r\n",
        "x = x.reshape(1,-1)\r\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5., 3., 5., 6., 7.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScIeiTlAOixF",
        "outputId": "1bef673e-a10a-46c9-a6b3-3f6573b87d70"
      },
      "source": [
        "x = torch.tensor( [5,3,5,6,7] , dtype=torch.float32)\r\n",
        "#x = x.reshape(1,-1)\r\n",
        "#x = x.unsqueeze(0)\r\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zi77p6zQ9V8",
        "outputId": "e16d4c9e-a60a-452e-8e73-eda3e02e123c"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "d = {\r\n",
        "    \"c\" : [5,6,7,8,9],\r\n",
        "    \"e\" : [15,26,37,18,29],\r\n",
        "    \"t\" : [0.5,0.26,3.7,1.8,29], \r\n",
        "}\r\n",
        "\r\n",
        "df = pd.DataFrame(d)\r\n",
        "df.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5.  , 15.  ,  0.5 ],\n",
              "       [ 6.  , 26.  ,  0.26],\n",
              "       [ 7.  , 37.  ,  3.7 ],\n",
              "       [ 8.  , 18.  ,  1.8 ],\n",
              "       [ 9.  , 29.  , 29.  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gACjphVQRmfr",
        "outputId": "ab8c298d-ea14-4b5d-e55c-2b422e4eaf88"
      },
      "source": [
        "x = torch.from_numpy(df.values).type(torch.float32)\r\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5.0000, 15.0000,  0.5000],\n",
              "        [ 6.0000, 26.0000,  0.2600],\n",
              "        [ 7.0000, 37.0000,  3.7000],\n",
              "        [ 8.0000, 18.0000,  1.8000],\n",
              "        [ 9.0000, 29.0000, 29.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pK5HYRjSNaS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visiont3lab/deep-learning-course/blob/main/colab/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3ki_9emN-Nh"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrh2RbiWNw2A"
      },
      "source": [
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "#!pip install torchsummary\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset,Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "# Loss function pytorch: https://neptune.ai/blog/pytorch-loss-functions\n",
        "import copy\n",
        "import pandas as pd\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxJ3655JN8S4"
      },
      "source": [
        "## Neural Network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l18NVUALSl3M"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOpy-O6GYJXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429520cd-cc81-4171-8720-5be9a65c9907"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/visiont3lab/deep-learning-course/main/data/covid19-ita-regioni.csv\")\n",
        "df[\"data\"] = [ datetime.strptime(d, \"%Y-%m-%d\") for d in df[\"data\"]]\n",
        "#df_f = df[df[\"data\"]>datetime(2020,11,6)].copy()\n",
        "df_f = df[df[\"zona\"]!=\"unknown\"]\n",
        "inputs = [\"data\",\"denominazione_regione\",\"zona\",\"ricoverati_con_sintomi\",\"terapia_intensiva\",\n",
        "        \"totale_ospedalizzati\",\"totale_positivi\",\"isolamento_domiciliare\",\n",
        "        \"deceduti\",\"dimessi_guariti\",\"nuovi_positivi\",\"totale_casi\",\"tamponi\"]\n",
        "df_f = df_f[inputs]\n",
        "df_f = df_f.drop(columns=[\"data\",\"denominazione_regione\"])\n",
        "#display(df_f)\n",
        "\n",
        "#Y = np.array([dict_names[d] for d in df_Y],dtype=np.float) #.reshape(-1,1)\n",
        "#print(f\"X shape: {X.shape} , Y shape: {Y.shape}\")\n",
        "\n",
        "dict_names = {\"bianca\":0,\"gialla\": 1, \"arancione\": 2, \"rossa\": 3}\n",
        "Y = df_f.pop(\"zona\").tolist()\n",
        "Y = np.array ( [ dict_names[e] for e in Y] , dtype=np.float32).reshape(-1,1)\n",
        "\n",
        "X = df_f.values\n",
        "\n",
        "print( X.shape )\n",
        "print( Y.shape )"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2689, 10)\n",
            "(2689, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4yFxS_Id-LN"
      },
      "source": [
        "## Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTsVWGUOd_4O",
        "outputId": "9260217e-41b8-4dba-d212-8e44b95d0b68"
      },
      "source": [
        "class ClassificationNet(nn.Module):\n",
        "    def __init__(self,num_inputs, num_classes):\n",
        "        super(ClassificationNet,self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.fc1 = nn.Linear(num_inputs,200)\n",
        "        self.fc2 = nn.Linear(200,100)\n",
        "        self.fc3 = nn.Linear(100,50)\n",
        "        self.fc4 = nn.Linear(50,self.num_classes)\n",
        "    def forward(self,x):\n",
        "        # torch.sigmoid, torch.tanh, torch.relu\n",
        "        x = torch.tanh(self.fc1(x)) \n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        #x = torch.log_softmax(self.fc4(x),dim=-1) # sarebbe dim=1  print(self.fc3(x)) print(self.fc3(x).sum(dim=-1))\n",
        "        return x\n",
        "\n",
        "CN = ClassificationNet(num_inputs=10, num_classes=4)\n",
        "summary(CN,input_size=(1,10),batch_size=-1, device='cpu')\n",
        "#print(CN)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1               [-1, 1, 200]           2,200\n",
            "            Linear-2               [-1, 1, 100]          20,100\n",
            "            Linear-3                [-1, 1, 50]           5,050\n",
            "================================================================\n",
            "Total params: 27,350\n",
            "Trainable params: 27,350\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.10\n",
            "Estimated Total Size (MB): 0.11\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHwKMpshd37u"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0LVCoZ_cLjS",
        "outputId": "1bd63238-c078-47d6-e4eb-6bf0b91eac9d"
      },
      "source": [
        "# Training and Test Set\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3,shuffle=True,random_state=2)\n",
        "print(f\"X Train shape: {X_train.shape} , X Test shape: {X_test.shape}\")\n",
        "\n",
        "# Normalization\n",
        "C_mean = np.mean(X)\n",
        "C_std = np.std(X)\n",
        "C_min = np.min(X)\n",
        "C_max = np.max(X)\n",
        "\n",
        "# Tensor Dataset Che converte i dati da numpy a Pytorch\n",
        "class CustomTensorDataset(Dataset):\n",
        "    def __init__(self, x,y,mean,std):\n",
        "        x = (x - mean)/std           # Standard Scaler \n",
        "        #x = (x - min) / (max - min) # Min Max Scaler\n",
        "        self.x = torch.from_numpy(x).type(torch.float32)\n",
        "        self.y = torch.from_numpy(y).type(torch.LongTensor).reshape(-1)\n",
        "    def __getitem__(self, index):\n",
        "        x = self.x[index]\n",
        "        y = self.y[index]\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]\n",
        "\n",
        "# Dataset generator creation\n",
        "train_ds = CustomTensorDataset(X_train,Y_train,C_mean,C_std)\n",
        "test_ds = CustomTensorDataset(X_test,Y_test,C_mean,C_std)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X Train shape: (1882, 10) , X Test shape: (807, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPmSf8Xzfx8a"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyA8HmPQpWuS"
      },
      "source": [
        "* numero di epoche\n",
        "* learning rate\n",
        "* batch size\n",
        "* aggiornare la struttura della rete"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arHLuhtQgj9l"
      },
      "source": [
        "# validation: metric regression\n",
        "def metrics_func_regression(target, output):\n",
        "    # Comptue mean squaer error (Migliora quanto piu' ci avviciniamo a zero)\n",
        "    mse = torch.sum((output - target) ** 2)\n",
        "    return mse\n",
        "\n",
        "# validation metric classification\n",
        "def metrics_func_classification(target, output):\n",
        "    # Compute number of correct prediction\n",
        "    pred = output.argmax(dim=-1,keepdim=True)\n",
        "    corrects =pred.eq(target.reshape(pred.shape)).sum().item()\n",
        "    return -corrects # minus for coeherence with best result is the most negative one\n",
        "\n",
        "# training: loss calculation and backward step\n",
        "def loss_batch(loss_func,metric_func, xb,yb,yb_h, opt=None):\n",
        "    # obtain loss\n",
        "    loss = loss_func(yb_h, yb)\n",
        "    # obtain performance metric \n",
        "    with torch.no_grad():\n",
        "        metric_b = metric_func(yb,yb_h)\n",
        "    if opt is not None:\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    return loss.item(), metric_b\n",
        "\n",
        "# one epoch training\n",
        "def loss_epoch(model, loss_func,metric_func, dataset_dl, sanity_check,opt, device):\n",
        "    loss = 0.0\n",
        "    metric = 0.0\n",
        "    len_data = float(len(dataset_dl.dataset))\n",
        "    # get batch data\n",
        "    for xb,yb in dataset_dl:    \n",
        "        # send to cuda the data (batch size)\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        # obtain model output \n",
        "        yb_h = model.forward(xb)\n",
        "        # loss and metric Calculation\n",
        "        loss_b, metric_b = loss_batch(loss_func,metric_func, xb,yb,yb_h,opt)\n",
        "        # update loss\n",
        "        loss += loss_b\n",
        "        # update metric\n",
        "        if metric_b is not None:\n",
        "            metric+=metric_b \n",
        "        if sanity_check is True:\n",
        "            break\n",
        "    # average loss\n",
        "    loss /=len_data\n",
        "    # average metric\n",
        "    metric /=len_data\n",
        "    return loss, metric\n",
        "\n",
        "# get learning rate from optimizer\n",
        "def get_lr(opt):\n",
        "    # opt.param_groups[0]['lr']\n",
        "    for param_group in opt.param_groups:\n",
        "        return param_group[\"lr\"]\n",
        "\n",
        "# trainig - test loop\n",
        "def train_test(params):\n",
        "    # --> extract params\n",
        "    model = params[\"model\"]\n",
        "    loss_func=params[\"loss_func\"]\n",
        "    metric_func=params[\"metric_func\"]\n",
        "    num_epochs=params[\"num_epochs\"]\n",
        "    opt=params[\"optimizer\"]\n",
        "    lr_scheduler=params[\"lr_scheduler\"]\n",
        "    train_dl=params[\"train_dl\"]\n",
        "    test_dl=params[\"test_dl\"]\n",
        "    device=params[\"device\"]\n",
        "    continue_training=params[\"continue_training\"]\n",
        "    sanity_check=params[\"sanity_check\"]\n",
        "    path2weigths=params[\"path2weigths\"]\n",
        "    # --> send model to device and print device\n",
        "    model = model.to(device)\n",
        "    print(\"--> training device %s\" % (device))\n",
        "    # --> if continue_training=True load path2weigths\n",
        "    if continue_training==True and os.path.isfile(path2weigths):\n",
        "        print(\"--> continue training  from last best weights\")\n",
        "        weights = torch.load(path2weigths)\n",
        "        model.load_state_dict(weights)\n",
        "    # --> history of loss values in each epoch\n",
        "    loss_history={\"train\": [],\"test\":[]}\n",
        "    # --> history of metric values in each epoch\n",
        "    metric_history={\"train\": [],\"test\":[]}\n",
        "    # --> a deep copy of weights for the best performing model\n",
        "    best_model_weights = copy.deepcopy(model.state_dict())\n",
        "    # --> initialiaze best loss to large value\n",
        "    best_loss=float(\"inf\")\n",
        "    # --> main loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # --> get learning rate\n",
        "        lr = get_lr(opt)\n",
        "        print(\"----\\nEpoch %s/%s, lr=%.6f\" % (epoch+1,num_epochs,lr))\n",
        "        # --> train model on training dataset\n",
        "        # we tell to the model to enter in train state. it is important because\n",
        "        # there are somelayers like dropout, batchnorm that behaves \n",
        "        # differently between train and test\n",
        "        model.train()\n",
        "        train_loss,train_metric = loss_epoch(model, loss_func, metric_func,train_dl,sanity_check, opt,device)\n",
        "        # --> collect loss and metric for training dataset\n",
        "        loss_history[\"train\"].append(train_loss)\n",
        "        metric_history[\"train\"].append(train_metric)\n",
        "        # --> tell the model to be in test (validation) mode\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_loss, test_metric = loss_epoch(model, loss_func, metric_func, test_dl,sanity_check,opt=None,device=device)\n",
        "        # --> collect loss and metric for test dataset\n",
        "        loss_history[\"test\"].append(test_loss)\n",
        "        metric_history[\"test\"].append(test_metric)\n",
        "        # --> store best model\n",
        "        if test_loss < best_loss:\n",
        "            print(\"--> model improved! --> saved to %s\" %(path2weigths))\n",
        "            best_loss = test_loss\n",
        "            best_model_weights = copy.deepcopy(model.state_dict())\n",
        "            # --> store weights into local file\n",
        "            torch.save(model.state_dict(),path2weigths)\n",
        "        # --> learning rate scheduler\n",
        "        lr_scheduler.step()\n",
        "        print(\"--> train_loss: %.6f, test_loss: %.6f, train_metric: %.3f, test_metric: %.3f\" % (train_loss,test_loss,train_metric,test_metric))\n",
        "    # --> load best weights\n",
        "    model.load_state_dict(best_model_weights)\n",
        "    return model, loss_history,metric_history\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx_Ha_93hDfo",
        "outputId": "6bcc9a36-4a43-41ee-bdef-ddc3d27926db"
      },
      "source": [
        "# Setup GPU Device\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "\n",
        "# Regression\n",
        "model = ClassificationNet(num_inputs=10, num_classes=4).to(device)\n",
        "loss_func = nn.NLLLoss(reduction=\"sum\")  #nn.BCELoss  \n",
        "loss_func = nn.CrossEntropyLoss(reduction=\"sum\")  #nn.BCELoss  \n",
        "opt = optim.Adam(model.parameters(),lr=0.001)\n",
        "train_dl = DataLoader(train_ds,batch_size=124,shuffle=True)\n",
        "test_dl = DataLoader(test_ds,batch_size=124,shuffle=True)\n",
        "\n",
        "# Regression\n",
        "\n",
        "# Setup GPU Device\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.999)  #  lr = lr * gamma ** last_epoch\n",
        "params = {\n",
        "    \"model\":                 model,\n",
        "    \"loss_func\":             loss_func, \n",
        "    \"metric_func\":           metrics_func_classification,\n",
        "    \"num_epochs\":            300,\n",
        "    \"optimizer\":             opt,\n",
        "    \"lr_scheduler\":          lr_scheduler,\n",
        "    \"train_dl\":              train_dl,\n",
        "    \"test_dl\":               test_dl,\n",
        "    \"device\":                device,  \n",
        "    \"continue_training\" :    False,  # continue training from last save weights\n",
        "    \"sanity_check\":          False, # if true we only do one batch per epoch\n",
        "    \"path2weigths\":          \"./weights_classification.pt\"  \n",
        "} \n",
        "model, loss_history,metric_history = train_test(params)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> training device cuda:0\n",
            "----\n",
            "Epoch 1/300, lr=0.001000\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 3.093354, test_loss: 2.528015, train_metric: -0.357, test_metric: -0.413\n",
            "----\n",
            "Epoch 2/300, lr=0.000999\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.371011, test_loss: 2.281343, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 3/300, lr=0.000998\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.269209, test_loss: 2.260539, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 4/300, lr=0.000997\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.259485, test_loss: 2.256980, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 5/300, lr=0.000996\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.257302, test_loss: 2.255660, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 6/300, lr=0.000995\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.256295, test_loss: 2.254904, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 7/300, lr=0.000994\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.255661, test_loss: 2.254362, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 8/300, lr=0.000993\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.255187, test_loss: 2.253945, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 9/300, lr=0.000992\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.254816, test_loss: 2.253612, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 10/300, lr=0.000991\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.254520, test_loss: 2.253342, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 11/300, lr=0.000990\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.254276, test_loss: 2.253121, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 12/300, lr=0.000989\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.254076, test_loss: 2.252936, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 13/300, lr=0.000988\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.253909, test_loss: 2.252780, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 14/300, lr=0.000987\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.253768, test_loss: 2.252647, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 15/300, lr=0.000986\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.253646, test_loss: 2.252535, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 16/300, lr=0.000985\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.253542, test_loss: 2.252437, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 17/300, lr=0.000984\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.253452, test_loss: 2.252352, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 18/300, lr=0.000983\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.253373, test_loss: 2.252278, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 19/300, lr=0.000982\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.253304, test_loss: 2.252213, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 20/300, lr=0.000981\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.253243, test_loss: 2.252154, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 21/300, lr=0.000980\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.253189, test_loss: 2.252102, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 22/300, lr=0.000979\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.253140, test_loss: 2.252056, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 23/300, lr=0.000978\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.253097, test_loss: 2.252014, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 24/300, lr=0.000977\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.253058, test_loss: 2.251976, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 25/300, lr=0.000976\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.253022, test_loss: 2.251942, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 26/300, lr=0.000975\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252990, test_loss: 2.251911, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 27/300, lr=0.000974\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252961, test_loss: 2.251883, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 28/300, lr=0.000973\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252935, test_loss: 2.251857, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 29/300, lr=0.000972\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252910, test_loss: 2.251834, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 30/300, lr=0.000971\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252887, test_loss: 2.251812, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 31/300, lr=0.000970\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252867, test_loss: 2.251792, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 32/300, lr=0.000969\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252847, test_loss: 2.251773, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 33/300, lr=0.000968\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252830, test_loss: 2.251755, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 34/300, lr=0.000968\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252813, test_loss: 2.251739, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 35/300, lr=0.000967\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252798, test_loss: 2.251724, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 36/300, lr=0.000966\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252784, test_loss: 2.251711, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 37/300, lr=0.000965\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252770, test_loss: 2.251698, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 38/300, lr=0.000964\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252758, test_loss: 2.251685, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 39/300, lr=0.000963\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252746, test_loss: 2.251674, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 40/300, lr=0.000962\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252735, test_loss: 2.251663, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 41/300, lr=0.000961\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252725, test_loss: 2.251653, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 42/300, lr=0.000960\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252716, test_loss: 2.251644, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 43/300, lr=0.000959\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252706, test_loss: 2.251635, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 44/300, lr=0.000958\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252698, test_loss: 2.251626, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 45/300, lr=0.000957\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252690, test_loss: 2.251618, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 46/300, lr=0.000956\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252682, test_loss: 2.251611, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 47/300, lr=0.000955\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252675, test_loss: 2.251604, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 48/300, lr=0.000954\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252668, test_loss: 2.251597, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 49/300, lr=0.000953\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252661, test_loss: 2.251590, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 50/300, lr=0.000952\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252655, test_loss: 2.251584, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 51/300, lr=0.000951\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252649, test_loss: 2.251578, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 52/300, lr=0.000950\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252644, test_loss: 2.251573, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 53/300, lr=0.000949\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252638, test_loss: 2.251568, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 54/300, lr=0.000948\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252633, test_loss: 2.251562, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 55/300, lr=0.000947\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252628, test_loss: 2.251557, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 56/300, lr=0.000946\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252624, test_loss: 2.251553, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 57/300, lr=0.000946\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252619, test_loss: 2.251549, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 58/300, lr=0.000945\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252615, test_loss: 2.251544, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 59/300, lr=0.000944\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252611, test_loss: 2.251540, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 60/300, lr=0.000943\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252607, test_loss: 2.251537, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 61/300, lr=0.000942\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252603, test_loss: 2.251533, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 62/300, lr=0.000941\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252600, test_loss: 2.251529, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 63/300, lr=0.000940\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252596, test_loss: 2.251526, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 64/300, lr=0.000939\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252593, test_loss: 2.251522, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 65/300, lr=0.000938\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252590, test_loss: 2.251519, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 66/300, lr=0.000937\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252587, test_loss: 2.251516, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 67/300, lr=0.000936\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252584, test_loss: 2.251513, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 68/300, lr=0.000935\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252581, test_loss: 2.251511, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 69/300, lr=0.000934\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252578, test_loss: 2.251508, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 70/300, lr=0.000933\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252576, test_loss: 2.251505, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 71/300, lr=0.000932\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252573, test_loss: 2.251503, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 72/300, lr=0.000931\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252571, test_loss: 2.251500, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 73/300, lr=0.000930\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252568, test_loss: 2.251498, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 74/300, lr=0.000930\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252566, test_loss: 2.251496, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 75/300, lr=0.000929\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252564, test_loss: 2.251494, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 76/300, lr=0.000928\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252562, test_loss: 2.251491, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 77/300, lr=0.000927\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252560, test_loss: 2.251489, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 78/300, lr=0.000926\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252558, test_loss: 2.251487, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 79/300, lr=0.000925\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252556, test_loss: 2.251486, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 80/300, lr=0.000924\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252554, test_loss: 2.251484, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 81/300, lr=0.000923\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252552, test_loss: 2.251482, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 82/300, lr=0.000922\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252550, test_loss: 2.251480, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 83/300, lr=0.000921\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252549, test_loss: 2.251478, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 84/300, lr=0.000920\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252547, test_loss: 2.251477, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 85/300, lr=0.000919\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252545, test_loss: 2.251475, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 86/300, lr=0.000918\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252544, test_loss: 2.251474, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 87/300, lr=0.000918\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252542, test_loss: 2.251472, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 88/300, lr=0.000917\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252541, test_loss: 2.251471, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 89/300, lr=0.000916\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252540, test_loss: 2.251469, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 90/300, lr=0.000915\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252538, test_loss: 2.251468, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 91/300, lr=0.000914\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252537, test_loss: 2.251466, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 92/300, lr=0.000913\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252536, test_loss: 2.251465, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 93/300, lr=0.000912\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252534, test_loss: 2.251464, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 94/300, lr=0.000911\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252533, test_loss: 2.251463, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 95/300, lr=0.000910\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252532, test_loss: 2.251461, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 96/300, lr=0.000909\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252531, test_loss: 2.251460, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 97/300, lr=0.000908\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252529, test_loss: 2.251459, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 98/300, lr=0.000908\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252528, test_loss: 2.251458, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 99/300, lr=0.000907\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252527, test_loss: 2.251457, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 100/300, lr=0.000906\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252526, test_loss: 2.251456, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 101/300, lr=0.000905\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252525, test_loss: 2.251455, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 102/300, lr=0.000904\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252524, test_loss: 2.251454, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 103/300, lr=0.000903\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252523, test_loss: 2.251453, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 104/300, lr=0.000902\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252522, test_loss: 2.251452, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 105/300, lr=0.000901\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252521, test_loss: 2.251451, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 106/300, lr=0.000900\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252521, test_loss: 2.251450, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 107/300, lr=0.000899\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252520, test_loss: 2.251449, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 108/300, lr=0.000898\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252519, test_loss: 2.251448, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 109/300, lr=0.000898\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252518, test_loss: 2.251448, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 110/300, lr=0.000897\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252517, test_loss: 2.251447, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 111/300, lr=0.000896\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252516, test_loss: 2.251446, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 112/300, lr=0.000895\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252516, test_loss: 2.251445, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 113/300, lr=0.000894\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252515, test_loss: 2.251444, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 114/300, lr=0.000893\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252514, test_loss: 2.251444, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 115/300, lr=0.000892\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252513, test_loss: 2.251443, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 116/300, lr=0.000891\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252513, test_loss: 2.251442, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 117/300, lr=0.000890\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252512, test_loss: 2.251442, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 118/300, lr=0.000890\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252511, test_loss: 2.251441, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 119/300, lr=0.000889\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252511, test_loss: 2.251440, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 120/300, lr=0.000888\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252510, test_loss: 2.251440, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 121/300, lr=0.000887\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252509, test_loss: 2.251439, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 122/300, lr=0.000886\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252509, test_loss: 2.251438, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 123/300, lr=0.000885\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252508, test_loss: 2.251438, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 124/300, lr=0.000884\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252507, test_loss: 2.251437, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 125/300, lr=0.000883\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252507, test_loss: 2.251436, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 126/300, lr=0.000882\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252506, test_loss: 2.251436, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 127/300, lr=0.000882\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252506, test_loss: 2.251435, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 128/300, lr=0.000881\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252505, test_loss: 2.251435, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 129/300, lr=0.000880\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252505, test_loss: 2.251434, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 130/300, lr=0.000879\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252504, test_loss: 2.251434, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 131/300, lr=0.000878\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252504, test_loss: 2.251433, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 132/300, lr=0.000877\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252503, test_loss: 2.251433, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 133/300, lr=0.000876\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252502, test_loss: 2.251432, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 134/300, lr=0.000875\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252502, test_loss: 2.251432, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 135/300, lr=0.000875\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252502, test_loss: 2.251431, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 136/300, lr=0.000874\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252501, test_loss: 2.251431, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 137/300, lr=0.000873\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252501, test_loss: 2.251430, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 138/300, lr=0.000872\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252500, test_loss: 2.251430, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 139/300, lr=0.000871\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252500, test_loss: 2.251429, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 140/300, lr=0.000870\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252499, test_loss: 2.251429, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 141/300, lr=0.000869\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252499, test_loss: 2.251428, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 142/300, lr=0.000868\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252498, test_loss: 2.251428, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 143/300, lr=0.000868\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252498, test_loss: 2.251428, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 144/300, lr=0.000867\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252498, test_loss: 2.251427, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 145/300, lr=0.000866\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252497, test_loss: 2.251427, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 146/300, lr=0.000865\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252497, test_loss: 2.251426, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 147/300, lr=0.000864\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252497, test_loss: 2.251426, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 148/300, lr=0.000863\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252496, test_loss: 2.251426, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 149/300, lr=0.000862\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252496, test_loss: 2.251425, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 150/300, lr=0.000862\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252495, test_loss: 2.251425, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 151/300, lr=0.000861\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252495, test_loss: 2.251425, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 152/300, lr=0.000860\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252495, test_loss: 2.251424, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 153/300, lr=0.000859\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252494, test_loss: 2.251424, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 154/300, lr=0.000858\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252494, test_loss: 2.251424, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 155/300, lr=0.000857\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252494, test_loss: 2.251423, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 156/300, lr=0.000856\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252493, test_loss: 2.251423, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 157/300, lr=0.000855\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252493, test_loss: 2.251423, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 158/300, lr=0.000855\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252493, test_loss: 2.251422, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 159/300, lr=0.000854\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252492, test_loss: 2.251422, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 160/300, lr=0.000853\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252492, test_loss: 2.251422, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 161/300, lr=0.000852\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252492, test_loss: 2.251421, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 162/300, lr=0.000851\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252491, test_loss: 2.251421, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 163/300, lr=0.000850\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252491, test_loss: 2.251421, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 164/300, lr=0.000850\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252491, test_loss: 2.251420, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 165/300, lr=0.000849\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252491, test_loss: 2.251420, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 166/300, lr=0.000848\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252490, test_loss: 2.251420, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 167/300, lr=0.000847\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252490, test_loss: 2.251420, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 168/300, lr=0.000846\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252490, test_loss: 2.251419, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 169/300, lr=0.000845\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252489, test_loss: 2.251419, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 170/300, lr=0.000844\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252489, test_loss: 2.251419, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 171/300, lr=0.000844\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252489, test_loss: 2.251418, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 172/300, lr=0.000843\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252489, test_loss: 2.251418, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 173/300, lr=0.000842\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252488, test_loss: 2.251418, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 174/300, lr=0.000841\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252488, test_loss: 2.251418, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 175/300, lr=0.000840\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252488, test_loss: 2.251417, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 176/300, lr=0.000839\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252488, test_loss: 2.251417, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 177/300, lr=0.000839\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252487, test_loss: 2.251417, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 178/300, lr=0.000838\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252487, test_loss: 2.251417, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 179/300, lr=0.000837\n",
            "--> train_loss: 2.252487, test_loss: 2.251417, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 180/300, lr=0.000836\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252487, test_loss: 2.251416, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 181/300, lr=0.000835\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252486, test_loss: 2.251416, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 182/300, lr=0.000834\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252486, test_loss: 2.251416, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 183/300, lr=0.000834\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252486, test_loss: 2.251416, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 184/300, lr=0.000833\n",
            "--> train_loss: 2.252486, test_loss: 2.251416, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 185/300, lr=0.000832\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252486, test_loss: 2.251415, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 186/300, lr=0.000831\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252485, test_loss: 2.251415, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 187/300, lr=0.000830\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252485, test_loss: 2.251415, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 188/300, lr=0.000829\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252485, test_loss: 2.251415, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 189/300, lr=0.000829\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252485, test_loss: 2.251414, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 190/300, lr=0.000828\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252485, test_loss: 2.251414, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 191/300, lr=0.000827\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252484, test_loss: 2.251414, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 192/300, lr=0.000826\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252484, test_loss: 2.251414, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 193/300, lr=0.000825\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252484, test_loss: 2.251414, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 194/300, lr=0.000824\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252484, test_loss: 2.251413, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 195/300, lr=0.000824\n",
            "--> train_loss: 2.252484, test_loss: 2.251413, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 196/300, lr=0.000823\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252484, test_loss: 2.251413, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 197/300, lr=0.000822\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252483, test_loss: 2.251413, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 198/300, lr=0.000821\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252483, test_loss: 2.251413, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 199/300, lr=0.000820\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252483, test_loss: 2.251413, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 200/300, lr=0.000819\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252483, test_loss: 2.251412, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 201/300, lr=0.000819\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252483, test_loss: 2.251412, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 202/300, lr=0.000818\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252483, test_loss: 2.251412, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 203/300, lr=0.000817\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252482, test_loss: 2.251412, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 204/300, lr=0.000816\n",
            "--> train_loss: 2.252482, test_loss: 2.251412, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 205/300, lr=0.000815\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252482, test_loss: 2.251412, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 206/300, lr=0.000815\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252482, test_loss: 2.251412, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 207/300, lr=0.000814\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252482, test_loss: 2.251411, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 208/300, lr=0.000813\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252482, test_loss: 2.251411, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 209/300, lr=0.000812\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252482, test_loss: 2.251411, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 210/300, lr=0.000811\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252481, test_loss: 2.251411, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 211/300, lr=0.000810\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252481, test_loss: 2.251411, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 212/300, lr=0.000810\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252481, test_loss: 2.251411, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 213/300, lr=0.000809\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252481, test_loss: 2.251411, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 214/300, lr=0.000808\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252481, test_loss: 2.251410, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 215/300, lr=0.000807\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252481, test_loss: 2.251410, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 216/300, lr=0.000806\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252481, test_loss: 2.251410, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 217/300, lr=0.000806\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252480, test_loss: 2.251410, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 218/300, lr=0.000805\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252480, test_loss: 2.251410, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 219/300, lr=0.000804\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252480, test_loss: 2.251410, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 220/300, lr=0.000803\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252480, test_loss: 2.251410, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 221/300, lr=0.000802\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252480, test_loss: 2.251409, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 222/300, lr=0.000802\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252480, test_loss: 2.251409, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 223/300, lr=0.000801\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252480, test_loss: 2.251409, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 224/300, lr=0.000800\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252479, test_loss: 2.251409, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 225/300, lr=0.000799\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252480, test_loss: 2.251409, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 226/300, lr=0.000798\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252479, test_loss: 2.251409, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 227/300, lr=0.000798\n",
            "--> train_loss: 2.252479, test_loss: 2.251409, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 228/300, lr=0.000797\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252479, test_loss: 2.251408, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 229/300, lr=0.000796\n",
            "--> train_loss: 2.252479, test_loss: 2.251408, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 230/300, lr=0.000795\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252479, test_loss: 2.251408, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 231/300, lr=0.000794\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252479, test_loss: 2.251408, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 232/300, lr=0.000794\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252479, test_loss: 2.251408, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 233/300, lr=0.000793\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252478, test_loss: 2.251408, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 234/300, lr=0.000792\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252478, test_loss: 2.251408, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 235/300, lr=0.000791\n",
            "--> train_loss: 2.252478, test_loss: 2.251408, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 236/300, lr=0.000790\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252478, test_loss: 2.251408, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 237/300, lr=0.000790\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252478, test_loss: 2.251407, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 238/300, lr=0.000789\n",
            "--> train_loss: 2.252478, test_loss: 2.251407, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 239/300, lr=0.000788\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252478, test_loss: 2.251407, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 240/300, lr=0.000787\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252478, test_loss: 2.251407, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 241/300, lr=0.000787\n",
            "--> train_loss: 2.252477, test_loss: 2.251407, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 242/300, lr=0.000786\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252477, test_loss: 2.251407, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 243/300, lr=0.000785\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252477, test_loss: 2.251407, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 244/300, lr=0.000784\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252477, test_loss: 2.251407, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 245/300, lr=0.000783\n",
            "--> train_loss: 2.252477, test_loss: 2.251407, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 246/300, lr=0.000783\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252477, test_loss: 2.251407, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 247/300, lr=0.000782\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252477, test_loss: 2.251406, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 248/300, lr=0.000781\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252477, test_loss: 2.251406, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 249/300, lr=0.000780\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252477, test_loss: 2.251406, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 250/300, lr=0.000779\n",
            "--> train_loss: 2.252477, test_loss: 2.251406, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 251/300, lr=0.000779\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252476, test_loss: 2.251406, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 252/300, lr=0.000778\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252476, test_loss: 2.251406, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 253/300, lr=0.000777\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252476, test_loss: 2.251406, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 254/300, lr=0.000776\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252476, test_loss: 2.251406, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 255/300, lr=0.000776\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252476, test_loss: 2.251406, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 256/300, lr=0.000775\n",
            "--> train_loss: 2.252476, test_loss: 2.251406, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 257/300, lr=0.000774\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252476, test_loss: 2.251405, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 258/300, lr=0.000773\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252476, test_loss: 2.251405, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 259/300, lr=0.000772\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252476, test_loss: 2.251405, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 260/300, lr=0.000772\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252476, test_loss: 2.251405, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 261/300, lr=0.000771\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252476, test_loss: 2.251405, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 262/300, lr=0.000770\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252476, test_loss: 2.251405, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 263/300, lr=0.000769\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252476, test_loss: 2.251405, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 264/300, lr=0.000769\n",
            "--> train_loss: 2.252475, test_loss: 2.251405, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 265/300, lr=0.000768\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252475, test_loss: 2.251405, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 266/300, lr=0.000767\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252475, test_loss: 2.251405, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 267/300, lr=0.000766\n",
            "--> train_loss: 2.252475, test_loss: 2.251405, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 268/300, lr=0.000766\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252475, test_loss: 2.251405, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 269/300, lr=0.000765\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252475, test_loss: 2.251404, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 270/300, lr=0.000764\n",
            "--> train_loss: 2.252475, test_loss: 2.251404, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 271/300, lr=0.000763\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252475, test_loss: 2.251404, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 272/300, lr=0.000763\n",
            "--> train_loss: 2.252475, test_loss: 2.251404, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 273/300, lr=0.000762\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252475, test_loss: 2.251404, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 274/300, lr=0.000761\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252475, test_loss: 2.251404, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 275/300, lr=0.000760\n",
            "--> train_loss: 2.252475, test_loss: 2.251404, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 276/300, lr=0.000759\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252475, test_loss: 2.251404, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 277/300, lr=0.000759\n",
            "--> train_loss: 2.252475, test_loss: 2.251404, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 278/300, lr=0.000758\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252474, test_loss: 2.251404, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 279/300, lr=0.000757\n",
            "--> train_loss: 2.252474, test_loss: 2.251404, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 280/300, lr=0.000756\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252474, test_loss: 2.251404, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 281/300, lr=0.000756\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252474, test_loss: 2.251404, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 282/300, lr=0.000755\n",
            "--> train_loss: 2.252474, test_loss: 2.251404, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 283/300, lr=0.000754\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252474, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 284/300, lr=0.000753\n",
            "--> train_loss: 2.252474, test_loss: 2.251404, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 285/300, lr=0.000753\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252474, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 286/300, lr=0.000752\n",
            "--> train_loss: 2.252474, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 287/300, lr=0.000751\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252474, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 288/300, lr=0.000750\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252474, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 289/300, lr=0.000750\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252474, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 290/300, lr=0.000749\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252474, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 291/300, lr=0.000748\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252474, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 292/300, lr=0.000747\n",
            "--> train_loss: 2.252474, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 293/300, lr=0.000747\n",
            "--> train_loss: 2.252474, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 294/300, lr=0.000746\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252474, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 295/300, lr=0.000745\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252474, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 296/300, lr=0.000744\n",
            "--> train_loss: 2.252473, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 297/300, lr=0.000744\n",
            "--> train_loss: 2.252473, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 298/300, lr=0.000743\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252473, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 299/300, lr=0.000742\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252473, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n",
            "----\n",
            "Epoch 300/300, lr=0.000741\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 2.252473, test_loss: 2.251403, train_metric: -0.391, test_metric: -0.413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3_xmjnT8NmjV",
        "outputId": "f39e4f62-20f6-4f4e-81a2-baeae05b8203"
      },
      "source": [
        "fig_loss = go.Figure()\n",
        "fig_metric = go.Figure()\n",
        "\n",
        "x = [i+1 for i in range(params[\"num_epochs\"])]\n",
        "\n",
        "fig_loss.add_traces( go.Scatter(x=x,y=loss_history[\"train\"], name=\"train loss\", mode=\"lines+markers\" ) )\n",
        "fig_loss.add_traces( go.Scatter(x=x,y=loss_history[\"test\"], name=\"test loss\"  , mode=\"lines+markers\") )\n",
        "fig_loss.update_layout(title=\"Loss Results\", xaxis_title=\"epochs\", hovermode=\"x\")\n",
        "fig_loss.show()\n",
        "\n",
        "fig_metric.add_traces( go.Scatter(x=x,y=metric_history[\"train\"], name=\"train metric\", mode=\"lines+markers\") )\n",
        "fig_metric.add_traces( go.Scatter(x=x,y=metric_history[\"test\"], name=\"test_metric\" , mode=\"lines+markers\") )\n",
        "fig_metric.update_layout(title=\"Metric Results\", xaxis_title=\"epochs\", hovermode=\"x\")\n",
        "fig_metric.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"77782c39-eba7-4e18-a49c-65d3a89fbe6c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"77782c39-eba7-4e18-a49c-65d3a89fbe6c\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '77782c39-eba7-4e18-a49c-65d3a89fbe6c',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"train loss\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300], \"y\": [3.0933536951144114, 2.3710113975370857, 2.2692089952393877, 2.259485133269388, 2.2573019994559376, 2.256295172745059, 2.2556605607619575, 2.2551867964417216, 2.254816391769555, 2.2545195041376775, 2.2542764377898035, 2.2540763587424655, 2.2539090127164574, 2.2537677209511573, 2.253645983056485, 2.2535419869498923, 2.2534518003717112, 2.2533734105725345, 2.2533039413273777, 2.253243425067249, 2.253188900435769, 2.253140357298248, 2.2530969787986828, 2.2530580007814573, 2.2530224969359183, 2.252990175382997, 2.2529608982909117, 2.2529345622858257, 2.252909772834413, 2.2528873913853125, 2.2528666639175983, 2.252847456653364, 2.2528296804073387, 2.252813274371383, 2.252797989232127, 2.2527837459389892, 2.252770437064257, 2.252758036257737, 2.252746430010902, 2.2527354744111565, 2.2527252687784616, 2.2527155536647556, 2.2527064891981396, 2.2526978463346214, 2.2526896777745895, 2.2526820727033137, 2.2526749013967646, 2.252667999672966, 2.252661410097615, 2.2526551529400916, 2.25264934373586, 2.2526436966866665, 2.252638357732045, 2.2526333167373056, 2.2526284074935345, 2.252623777967204, 2.2526193450538563, 2.252615143211438, 2.2526109879885925, 2.2526071226178774, 2.2526034031866966, 2.2525997709138488, 2.2525963109307283, 2.252593000941018, 2.252589822702276, 2.2525867478373707, 2.252583786480992, 2.2525810663302317, 2.2525782671288908, 2.2525757375103, 2.2525731450566324, 2.2525707107041266, 2.2525684303989064, 2.252566075096982, 2.2525638839770328, 2.252561794203982, 2.252559722673374, 2.2525576714121445, 2.2525557600096358, 2.2525540634625525, 2.252552131790664, 2.2525503278158676, 2.2525486697806056, 2.252547109038366, 2.2525454854610496, 2.2525438740453607, 2.2525423862728884, 2.252540898500416, 2.2525395749099197, 2.252538200645974, 2.2525367149004394, 2.252535523060911, 2.252534173120221, 2.2525331434357305, 2.252531880653373, 2.25253063408652, 2.2525294402200533, 2.2525284308049427, 2.252527352473941, 2.252526290358443, 2.252525205946627, 2.252524332336361, 2.2525233249481884, 2.2525224047183485, 2.252521385168548, 2.2525205622317306, 2.252519696729216, 2.2525187663646866, 2.2525179778858146, 2.2525172299457026, 2.2525163137697386, 2.2525155962336965, 2.2525147530274996, 2.2525139604947517, 2.252513336197856, 2.252512649065883, 2.252512006526546, 2.2525112666941856, 2.252510636316476, 2.2525098194604727, 2.2525092417831503, 2.252508708698463, 2.2525080884554436, 2.2525074155120364, 2.2525067202723115, 2.2525063290732827, 2.252505678426193, 2.2525052020957697, 2.2525046061760055, 2.2525040872798843, 2.2525035318988795, 2.2525030798917114, 2.2525024799180713, 2.252502082638229, 2.252501543472728, 2.252501075250056, 2.252500702293469, 2.252500110427581, 2.2524997415248698, 2.252499336137275, 2.2524988395374717, 2.2524984402306907, 2.2524980044390266, 2.252497599051432, 2.252497211906279, 2.2524969261080248, 2.2524965085588025, 2.2524960626324484, 2.2524957302146205, 2.2524952903690805, 2.2524950370018337, 2.252494637695053, 2.2524942302805204, 2.2524939809671496, 2.2524936140913767, 2.2524933161314946, 2.252492995875295, 2.252492689807661, 2.252492375632275, 2.2524920067295637, 2.2524917513353793, 2.252491394594296, 2.252491098661352, 2.252490794620656, 2.252490561522789, 2.2524902959939146, 2.252490036545854, 2.2524896656162046, 2.2524894365722137, 2.2524892399592304, 2.252488954160976, 2.252488652147218, 2.2524884960729943, 2.2524882224363676, 2.25248791028792, 2.252487756240634, 2.2524874359844342, 2.252487275856334, 2.252486927223003, 2.2524868137144765, 2.252486446838703, 2.2524864103538196, 2.252486077935992, 2.252485938077272, 2.252485717141033, 2.2524854394505307, 2.2524853117534382, 2.252485175948594, 2.2524848273152625, 2.2524847381299917, 2.2524844644933655, 2.2524843246346453, 2.252484217206933, 2.252483961812748, 2.2524838097924, 2.2524835543982156, 2.2524834753476344, 2.2524832361689535, 2.2524831104987992, 2.2524828774009324, 2.2524827882156617, 2.2524825794410503, 2.2524823929627567, 2.2524822875619823, 2.2524821193261304, 2.252482058517991, 2.2524817646119852, 2.252481825420124, 2.252481523406366, 2.2524813632782665, 2.252481391655398, 2.252481077480012, 2.2524809639714856, 2.2524808828939666, 2.2524807491160606, 2.252480643715286, 2.2524804693986202, 2.252480315351334, 2.252480238327691, 2.2524799768526926, 2.2524799809065685, 2.2524796809197487, 2.252479573492036, 2.252479484306765, 2.2524795390340904, 2.252479360663549, 2.2524791701313793, 2.252479038380411, 2.252478870144559, 2.2524788012286683, 2.2524786350197545, 2.2524785417806075, 2.252478438406771, 2.2524783066558025, 2.2524781627432064, 2.2524780857195634, 2.2524779336992156, 2.2524779357261537, 2.2524777938404954, 2.25247762965852, 2.2524774776381715, 2.252477374264335, 2.252477376291273, 2.252477201974607, 2.2524771695436, 2.2524770398195693, 2.2524769749575544, 2.252476851314338, 2.2524766587552305, 2.2524765756507734, 2.2524764803846886, 2.2524763668761625, 2.2524764418728673, 2.252476164182365, 2.2524760831048463, 2.252476022296707, 2.2524760344583346, 2.252475880411049, 2.2524757182560107, 2.2524757608217083, 2.2524756169091122, 2.252475610828298, 2.2524755317777174, 2.2524754344846944, 2.2524753452994237, 2.252475245979463, 2.25247519935989, 2.252475083824425, 2.252475095986053, 2.2524749926122163, 2.252474939911829, 2.252474802080047, 2.2524747615412877, 2.252474747352722, 2.2524746804637688, 2.2524745324972963, 2.252474518308731, 2.252474421015708, 2.2524743987193903, 2.252474287237802, 2.2524742993994296, 2.252474200079469, 2.252474125082764, 2.2524740358974933, 2.252473964954664, 2.252473900092649, 2.2524739406314085, 2.2524739102273386, 2.252473744018425, 2.252473806853502, 2.252473693344976, 2.2524735595670693, 2.252473589971139, 2.252473521055248, 2.25247353929769, 2.2524734845703644, 2.2524733224153266, 2.2524733163345125, 2.252473227149242, 2.252473265661063]}, {\"mode\": \"lines+markers\", \"name\": \"test loss\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300], \"y\": [2.528015231258955, 2.281342781757038, 2.260539342036478, 2.256979941140025, 2.2556596871793198, 2.254904405482787, 2.2543617257989177, 2.2539445009668815, 2.253612437951373, 2.253341844977056, 2.253120809978121, 2.252935719165069, 2.2527800492637664, 2.2526472013679135, 2.252534585075898, 2.2524369439523313, 2.252352425009197, 2.2522780785921044, 2.252212732402513, 2.2521540607513844, 2.2521021770869636, 2.252055814570506, 2.252014235788414, 2.2519764575225567, 2.2519423663246885, 2.25191139495358, 2.251883297604699, 2.251857034335792, 2.251833512732826, 2.2518116172213833, 2.2517915557899144, 2.2517728368293546, 2.2517554414316625, 2.251739388504879, 2.251724413336431, 2.2517106293745646, 2.251697639550418, 2.251685311507706, 2.2516738343268377, 2.2516633214560584, 2.2516531867460987, 2.251643524737163, 2.2516346190498666, 2.2516261860635938, 2.251618244686387, 2.2516106436539167, 2.2516035720465943, 2.2515966517035992, 2.2515901851535878, 2.2515841156724363, 2.251578216363654, 2.2515728464800184, 2.251567514412465, 2.2515623903333624, 2.251557455334669, 2.2515529930370004, 2.2515486820036594, 2.2515443709703185, 2.2515404002817148, 2.251536561949398, 2.25153270470904, 2.2515293390777473, 2.251525784366045, 2.2515224187347527, 2.2515193178160335, 2.2515162547133967, 2.251513342875087, 2.2515105066689416, 2.251507840635165, 2.251505269141593, 2.251502659831939, 2.2515002963268183, 2.251498027361902, 2.2514957205809036, 2.25149350834011, 2.2514914851797263, 2.2514894052952195, 2.251487363226795, 2.25148551023878, 2.251483524894478, 2.251481766446668, 2.2514800836310216, 2.2514782117349657, 2.2514767369077697, 2.2514751486323283, 2.251473503632764, 2.2514720288055683, 2.251470629610537, 2.251469173691382, 2.251467944668719, 2.2514664887495646, 2.2514650328304096, 2.251463784899706, 2.2514626882333295, 2.2514614403026254, 2.2514603814523313, 2.251459171337709, 2.251457980131128, 2.25145684564867, 2.251455919154662, 2.251454860304368, 2.2514540283505653, 2.251452931684189, 2.251451948466059, 2.251451078696174, 2.251450000937839, 2.251449225708159, 2.25144846938652, 2.2514477319729225, 2.251446767662833, 2.2514458600768665, 2.2514451415713097, 2.2514442528933842, 2.251443723468237, 2.251442891514434, 2.251442305365164, 2.2514415679515665, 2.2514409628942555, 2.251440112032412, 2.2514395636992237, 2.251438731745421, 2.2514383535846014, 2.2514376161710037, 2.2514370678378155, 2.2514364816885455, 2.2514360089875214, 2.2514353093900055, 2.2514347421487764, 2.2514341559995064, 2.251433626574359, 2.251433078241171, 2.2514326433562286, 2.2514322273793272, 2.251431679046139, 2.251431092896869, 2.2514308092762545, 2.2514302231269845, 2.2514296747937963, 2.2514291453686495, 2.251428842839994, 2.2514283323228876, 2.251428162150519, 2.251427519277126, 2.2514272167484704, 2.251426857495692, 2.2514263847946676, 2.251426082266012, 2.251425552840865, 2.2514253259443735, 2.251424834335308, 2.251424569622735, 2.2514241347377926, 2.2514239645654235, 2.251423529680481, 2.2514232271518257, 2.2514228868070885, 2.251422584278433, 2.2514221115774085, 2.251421884680917, 2.2514216199683434, 2.251421317439688, 2.251421090543196, 2.2514206934743357, 2.2514204287617625, 2.25141999387682, 2.2514197669803284, 2.2514196724401234, 2.2514193131873452, 2.2514191808310584, 2.251418783762198, 2.2514183866933375, 2.2514182543370507, 2.251418178704887, 2.2514176870958216, 2.251417422383248, 2.25141727111892, 2.251416930774183, 2.251416703877691, 2.251416703877691, 2.251416306808831, 2.2514161744525443, 2.2514159097399706, 2.251415531579151, 2.2514155883032743, 2.251415153418332, 2.251415039970086, 2.2514147941655533, 2.2514146807173074, 2.251414397096693, 2.251414245832365, 2.2514140945680374, 2.2514138487635047, 2.251413697499177, 2.2514134327866033, 2.2514134895107265, 2.251413224798153, 2.2514130168097024, 2.2514128088212515, 2.251412525200637, 2.251412373936309, 2.251412279396104, 2.251412052499613, 2.251411939051367, 2.251411957959408, 2.2514116365227115, 2.2514115419825065, 2.251411258361892, 2.2514111827297283, 2.2514110881895233, 2.2514109558332365, 2.251410766752827, 2.2514105965804583, 2.251410558764376, 2.2514104075000483, 2.251410161695516, 2.25141004824727, 2.251410029339229, 2.251409972615106, 2.2514096322703687, 2.2514095377301637, 2.251409462098, 2.251409254109549, 2.2514091217532624, 2.2514089893969755, 2.2514088570406887, 2.251408705776361, 2.251408705776361, 2.2514084221557464, 2.2514084410637873, 2.251408327615542, 2.2514082708914187, 2.251408006178845, 2.2514078927305996, 2.251407741466272, 2.251407741466272, 2.25140751456978, 2.251407420029575, 2.251407495661739, 2.2514072687652473, 2.251407060776797, 2.251407060776797, 2.2514068906044282, 2.2514067582481414, 2.2514066447998955, 2.2514067960642232, 2.2514065313516496, 2.2514064179034037, 2.251406266639076, 2.25140615319083, 2.25140615319083, 2.2514059452023796, 2.2514059262943387, 2.25140575612197, 2.251405661581765, 2.251405529225478, 2.25140556704156, 2.2514054535933146, 2.2514053590531096, 2.2514053212370277, 2.2514052645129046, 2.251405113248577, 2.251405037616413, 2.251404867444044, 2.2514049241681673, 2.2514048296279623, 2.251404602731471, 2.2514047729038396, 2.2514045838234296, 2.251404432559102, 2.251404470375184, 2.2514043191108564, 2.2514043191108564, 2.2514041867545695, 2.2514040543982827, 2.2514041111224055, 2.2514039976741596, 2.2514039976741596, 2.251403884225914, 2.251403884225914, 2.251403676237463, 2.2514036195133404, 2.2514036573294223, 2.2514034871570536, 2.2514035249731355, 2.2514034115248895, 2.2514034871570536, 2.2514033926168486, 2.251403335892726, 2.2514032791686027, 2.251403203536439, 2.251403090088193, 2.251403090088193, 2.251403127904275, 2.2514029199158245, 2.2514028064675786, 2.2514028064675786, 2.2514028064675786, 2.2514026930193327, 2.25140263629521, 2.251402579571087]}],\n",
              "                        {\"hovermode\": \"x\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Loss Results\"}, \"xaxis\": {\"title\": {\"text\": \"epochs\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('77782c39-eba7-4e18-a49c-65d3a89fbe6c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"de76a610-305b-437b-9dc4-27c095e2c988\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"de76a610-305b-437b-9dc4-27c095e2c988\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'de76a610-305b-437b-9dc4-27c095e2c988',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"train metric\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300], \"y\": [-0.3565356004250797, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635, -0.39054197662061635]}, {\"mode\": \"lines+markers\", \"name\": \"test_metric\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300], \"y\": [-0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096, -0.41263940520446096]}],\n",
              "                        {\"hovermode\": \"x\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Metric Results\"}, \"xaxis\": {\"title\": {\"text\": \"epochs\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('de76a610-305b-437b-9dc4-27c095e2c988');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh5CkI36nq2S"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd8jW5pynyCK",
        "outputId": "6f63e546-0a5b-4054-f6d5-d011a07d2c3c"
      },
      "source": [
        "# Load Classification\n",
        "names = list({\"bianca\":0,\"gialla\": 1, \"arancione\": 2, \"rossa\": 3})\n",
        "model = ClassificationNet(num_inputs=10,num_classes=4).to(device)\n",
        "weights = torch.load(\"weights_classification.pt\")\n",
        "model.load_state_dict(weights)\n",
        "model = model.to(device)\n",
        "\n",
        "print(C_mean)\n",
        "# Predict Classication\n",
        "# C_mean, C_std = 186785.728337672 679353.7516865473\n",
        "Xt = torch.from_numpy(X_test).type(torch.float32).unsqueeze(1).to(device)\n",
        "Xt = (Xt - C_mean) / C_std\n",
        "Y_hat = model.forward(Xt).argmax(dim=-1,keepdim=True).cpu().numpy().reshape(-1)\n",
        "\n",
        "# Visualize results\n",
        "cm = confusion_matrix(Y_test,Y_hat)\n",
        "names_pred = [ \"Pred: \" + n for n in names]\n",
        "df = pd.DataFrame(cm, columns=names_pred, index=names)\n",
        "print(df)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "186785.728337672\n",
            "           Pred: bianca  Pred: gialla  Pred: arancione  Pred: rossa\n",
            "bianca                0             0                6            0\n",
            "gialla                0             0              300            0\n",
            "arancione             0             0              333            0\n",
            "rossa                 0             0              168            0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzgtzyhoSjJk"
      },
      "source": [
        "## Extra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qiaz_2DYhrf"
      },
      "source": [
        "\n",
        "# df.head() # prime 5 righe\n",
        "# df.tail() # ultime 5 righe\n",
        "# df.to_csv(\"a.csv\") # Salva come csv\n",
        "# df.keys() # nomi delle colonne\n",
        "# df.values.shape # dimensione\n",
        "# df.values # dati come array \n",
        "\n",
        "# Nomi delle colonne\n",
        "# Numero di righe\n",
        "# selezionare solo la colonna con nome dimessi_guariti\n",
        "# voglio i dati come array\n",
        "# voglio i deceduti > 700\n",
        "# dati del 12 marzo in poi"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3OuqOczPcDe",
        "outputId": "c9c12063-4f22-4768-a50f-9599325a0c7a"
      },
      "source": [
        "x = np.array( [5,3,5,6,7] , dtype=np.float32)\n",
        "x = x.reshape(1,-1)\n",
        "x"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5., 3., 5., 6., 7.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScIeiTlAOixF",
        "outputId": "63dcd1b7-017d-4469-e98f-01019ea69e15"
      },
      "source": [
        "x = torch.tensor( [5,3,5,6,7] , dtype=torch.float32)\n",
        "#x = x.reshape(1,-1)\n",
        "#x = x.unsqueeze(0)\n",
        "x.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zi77p6zQ9V8",
        "outputId": "e904dd97-2217-47a1-fe16-5c1649edec1d"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "d = {\n",
        "    \"c\" : [5,6,7,8,9],\n",
        "    \"e\" : [15,26,37,18,29],\n",
        "    \"t\" : [0.5,0.26,3.7,1.8,29], \n",
        "}\n",
        "\n",
        "df = pd.DataFrame(d)\n",
        "df.values"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5.  , 15.  ,  0.5 ],\n",
              "       [ 6.  , 26.  ,  0.26],\n",
              "       [ 7.  , 37.  ,  3.7 ],\n",
              "       [ 8.  , 18.  ,  1.8 ],\n",
              "       [ 9.  , 29.  , 29.  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gACjphVQRmfr",
        "outputId": "e0b5c106-515f-46eb-e5fc-200729eb082a"
      },
      "source": [
        "x = torch.from_numpy(df.values).type(torch.float32)\n",
        "x"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5.0000, 15.0000,  0.5000],\n",
              "        [ 6.0000, 26.0000,  0.2600],\n",
              "        [ 7.0000, 37.0000,  3.7000],\n",
              "        [ 8.0000, 18.0000,  1.8000],\n",
              "        [ 9.0000, 29.0000, 29.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pK5HYRjSNaS"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}
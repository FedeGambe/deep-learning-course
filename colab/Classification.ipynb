{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visiont3lab/deep-learning-course/blob/main/colab/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3ki_9emN-Nh"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrh2RbiWNw2A"
      },
      "source": [
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "#!pip install torchsummary\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset,Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "# Loss function pytorch: https://neptune.ai/blog/pytorch-loss-functions\n",
        "import copy\n",
        "import pandas as pd\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxJ3655JN8S4"
      },
      "source": [
        "## Neural Network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l18NVUALSl3M"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOpy-O6GYJXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b706711-d00c-4f37-a6f6-f94cbbb93532"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/visiont3lab/deep-learning-course/main/data/covid19-ita-regioni.csv\")\n",
        "df[\"data\"] = [ datetime.strptime(d, \"%Y-%m-%d\") for d in df[\"data\"]]\n",
        "#df_f = df[df[\"data\"]>datetime(2020,11,6)].copy()\n",
        "df_f = df[df[\"zona\"]!=\"unknown\"]\n",
        "inputs = [\"data\",\"denominazione_regione\",\"zona\",\"ricoverati_con_sintomi\",\"terapia_intensiva\",\n",
        "        \"totale_ospedalizzati\",\"totale_positivi\",\"isolamento_domiciliare\",\n",
        "        \"deceduti\",\"dimessi_guariti\",\"nuovi_positivi\",\"totale_casi\",\"tamponi\"]\n",
        "df_f = df_f[inputs]\n",
        "df_f = df_f.drop(columns=[\"data\",\"denominazione_regione\"])\n",
        "#display(df_f)\n",
        "\n",
        "#Y = np.array([dict_names[d] for d in df_Y],dtype=np.float) #.reshape(-1,1)\n",
        "#print(f\"X shape: {X.shape} , Y shape: {Y.shape}\")\n",
        "\n",
        "dict_names = {\"bianca\":0,\"gialla\": 1, \"arancione\": 2, \"rossa\": 3}\n",
        "Y = df_f.pop(\"zona\").tolist()\n",
        "Y = np.array ( [ dict_names[e] for e in Y] , dtype=np.float32).reshape(-1,1)\n",
        "\n",
        "X = df_f.values\n",
        "\n",
        "print( X.shape )\n",
        "print( Y.shape )"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2689, 10)\n",
            "(2689, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4yFxS_Id-LN"
      },
      "source": [
        "## Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTsVWGUOd_4O",
        "outputId": "9c94669a-096a-4faa-a108-19f4728c1296"
      },
      "source": [
        "class ClassificationNet(nn.Module):\n",
        "    def __init__(self,num_inputs, num_classes):\n",
        "        super(ClassificationNet,self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.fc1 = nn.Linear(num_inputs,200)\n",
        "        self.fc2 = nn.Linear(200,100)\n",
        "        self.fc3 = nn.Linear(100,50)\n",
        "        self.fc4 = nn.Linear(50,self.num_classes)\n",
        "    def forward(self,x):\n",
        "        # torch.sigmoid, torch.tanh, torch.relu\n",
        "        x = torch.tanh(self.fc1(x)) \n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        x = torch.log_softmax(self.fc4(x),dim=-1) # sarebbe dim=1  print(self.fc3(x)) print(self.fc3(x).sum(dim=-1))\n",
        "        return x\n",
        "\n",
        "CN = ClassificationNet(num_inputs=10, num_classes=4)\n",
        "summary(CN,input_size=(1,10),batch_size=-1, device='cpu')\n",
        "#print(CN)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1               [-1, 1, 200]           2,200\n",
            "            Linear-2               [-1, 1, 100]          20,100\n",
            "            Linear-3                [-1, 1, 50]           5,050\n",
            "            Linear-4                 [-1, 1, 4]             204\n",
            "================================================================\n",
            "Total params: 27,554\n",
            "Trainable params: 27,554\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.11\n",
            "Estimated Total Size (MB): 0.11\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHwKMpshd37u"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0LVCoZ_cLjS",
        "outputId": "c8a8351a-c537-49e1-c526-622769dc503e"
      },
      "source": [
        "# Training and Test Set\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3,shuffle=True,random_state=2)\n",
        "print(f\"X Train shape: {X_train.shape} , X Test shape: {X_test.shape}\")\n",
        "\n",
        "# Normalization\n",
        "C_mean = np.mean(X)\n",
        "C_std = np.std(X)\n",
        "C_min = np.min(X)\n",
        "C_max = np.max(X)\n",
        "\n",
        "# Tensor Dataset Che converte i dati da numpy a Pytorch\n",
        "class CustomTensorDataset(Dataset):\n",
        "    def __init__(self, x,y,mean,std):\n",
        "        x = (x - mean)/std           # Standard Scaler \n",
        "        #x = (x - min) / (max - min) # Min Max Scaler\n",
        "        self.x = torch.from_numpy(x).type(torch.float32)\n",
        "        self.y = torch.from_numpy(y).type(torch.LongTensor).reshape(-1)\n",
        "    def __getitem__(self, index):\n",
        "        x = self.x[index]\n",
        "        y = self.y[index]\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]\n",
        "\n",
        "# Dataset generator creation\n",
        "train_ds = CustomTensorDataset(X_train,Y_train,C_mean,C_std)\n",
        "test_ds = CustomTensorDataset(X_test,Y_test,C_mean,C_std)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X Train shape: (1882, 10) , X Test shape: (807, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPmSf8Xzfx8a"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyA8HmPQpWuS"
      },
      "source": [
        "* numero di epoche\n",
        "* learning rate\n",
        "* batch size\n",
        "* aggiornare la struttura della rete"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arHLuhtQgj9l"
      },
      "source": [
        "# validation: metric regression\n",
        "def metrics_func_regression(target, output):\n",
        "    # Comptue mean squaer error (Migliora quanto piu' ci avviciniamo a zero)\n",
        "    mse = torch.sum((output - target) ** 2)\n",
        "    return mse\n",
        "\n",
        "# validation metric classification\n",
        "def metrics_func_classification(target, output):\n",
        "    # Compute number of correct prediction\n",
        "    pred = output.argmax(dim=-1,keepdim=True)\n",
        "    corrects =pred.eq(target.reshape(pred.shape)).sum().item()\n",
        "    return -corrects # minus for coeherence with best result is the most negative one\n",
        "\n",
        "# training: loss calculation and backward step\n",
        "def loss_batch(loss_func,metric_func, xb,yb,yb_h, opt=None):\n",
        "    # obtain loss\n",
        "    loss = loss_func(yb_h, yb)\n",
        "    # obtain performance metric \n",
        "    with torch.no_grad():\n",
        "        metric_b = metric_func(yb,yb_h)\n",
        "    if opt is not None:\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    return loss.item(), metric_b\n",
        "\n",
        "# one epoch training\n",
        "def loss_epoch(model, loss_func,metric_func, dataset_dl, sanity_check,opt, device):\n",
        "    loss = 0.0\n",
        "    metric = 0.0\n",
        "    len_data = float(len(dataset_dl.dataset))\n",
        "    # get batch data\n",
        "    for xb,yb in dataset_dl:    \n",
        "        # send to cuda the data (batch size)\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        # obtain model output \n",
        "        yb_h = model.forward(xb)\n",
        "        # loss and metric Calculation\n",
        "        loss_b, metric_b = loss_batch(loss_func,metric_func, xb,yb,yb_h,opt)\n",
        "        # update loss\n",
        "        loss += loss_b\n",
        "        # update metric\n",
        "        if metric_b is not None:\n",
        "            metric+=metric_b \n",
        "        if sanity_check is True:\n",
        "            break\n",
        "    # average loss\n",
        "    loss /=len_data\n",
        "    # average metric\n",
        "    metric /=len_data\n",
        "    return loss, metric\n",
        "\n",
        "# get learning rate from optimizer\n",
        "def get_lr(opt):\n",
        "    # opt.param_groups[0]['lr']\n",
        "    for param_group in opt.param_groups:\n",
        "        return param_group[\"lr\"]\n",
        "\n",
        "# trainig - test loop\n",
        "def train_test(params):\n",
        "    # --> extract params\n",
        "    model = params[\"model\"]\n",
        "    loss_func=params[\"loss_func\"]\n",
        "    metric_func=params[\"metric_func\"]\n",
        "    num_epochs=params[\"num_epochs\"]\n",
        "    opt=params[\"optimizer\"]\n",
        "    lr_scheduler=params[\"lr_scheduler\"]\n",
        "    train_dl=params[\"train_dl\"]\n",
        "    test_dl=params[\"test_dl\"]\n",
        "    device=params[\"device\"]\n",
        "    continue_training=params[\"continue_training\"]\n",
        "    sanity_check=params[\"sanity_check\"]\n",
        "    path2weigths=params[\"path2weigths\"]\n",
        "    # --> send model to device and print device\n",
        "    model = model.to(device)\n",
        "    print(\"--> training device %s\" % (device))\n",
        "    # --> if continue_training=True load path2weigths\n",
        "    if continue_training==True and os.path.isfile(path2weigths):\n",
        "        print(\"--> continue training  from last best weights\")\n",
        "        weights = torch.load(path2weigths)\n",
        "        model.load_state_dict(weights)\n",
        "    # --> history of loss values in each epoch\n",
        "    loss_history={\"train\": [],\"test\":[]}\n",
        "    # --> history of metric values in each epoch\n",
        "    metric_history={\"train\": [],\"test\":[]}\n",
        "    # --> a deep copy of weights for the best performing model\n",
        "    best_model_weights = copy.deepcopy(model.state_dict())\n",
        "    # --> initialiaze best loss to large value\n",
        "    best_loss=float(\"inf\")\n",
        "    # --> main loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # --> get learning rate\n",
        "        lr = get_lr(opt)\n",
        "        print(\"----\\nEpoch %s/%s, lr=%.6f\" % (epoch+1,num_epochs,lr))\n",
        "        # --> train model on training dataset\n",
        "        # we tell to the model to enter in train state. it is important because\n",
        "        # there are somelayers like dropout, batchnorm that behaves \n",
        "        # differently between train and test\n",
        "        model.train()\n",
        "        train_loss,train_metric = loss_epoch(model, loss_func, metric_func,train_dl,sanity_check, opt,device)\n",
        "        # --> collect loss and metric for training dataset\n",
        "        loss_history[\"train\"].append(train_loss)\n",
        "        metric_history[\"train\"].append(train_metric)\n",
        "        # --> tell the model to be in test (validation) mode\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_loss, test_metric = loss_epoch(model, loss_func, metric_func, test_dl,sanity_check,opt=None,device=device)\n",
        "        # --> collect loss and metric for test dataset\n",
        "        loss_history[\"test\"].append(test_loss)\n",
        "        metric_history[\"test\"].append(test_metric)\n",
        "        # --> store best model\n",
        "        if test_loss < best_loss:\n",
        "            print(\"--> model improved! --> saved to %s\" %(path2weigths))\n",
        "            best_loss = test_loss\n",
        "            best_model_weights = copy.deepcopy(model.state_dict())\n",
        "            # --> store weights into local file\n",
        "            torch.save(model.state_dict(),path2weigths)\n",
        "        # --> learning rate scheduler\n",
        "        lr_scheduler.step()\n",
        "        print(\"--> train_loss: %.6f, test_loss: %.6f, train_metric: %.3f, test_metric: %.3f\" % (train_loss,test_loss,train_metric,test_metric))\n",
        "    # --> load best weights\n",
        "    model.load_state_dict(best_model_weights)\n",
        "    return model, loss_history,metric_history\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx_Ha_93hDfo",
        "outputId": "44afe5e8-7b23-4c34-dc0b-c12270682e6e"
      },
      "source": [
        "# Setup GPU Device\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "\n",
        "# Regression\n",
        "model = ClassificationNet(num_inputs=10, num_classes=4).to(device)\n",
        "loss_func = nn.NLLLoss(reduction=\"sum\")  #nn.BCELoss  \n",
        "opt = optim.Adam(model.parameters(),lr=0.001)\n",
        "train_dl = DataLoader(train_ds,batch_size=124,shuffle=True)\n",
        "test_dl = DataLoader(test_ds,batch_size=124,shuffle=True)\n",
        "\n",
        "# Regression\n",
        "\n",
        "# Setup GPU Device\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.999)  #  lr = lr * gamma ** last_epoch\n",
        "params = {\n",
        "    \"model\":                 model,\n",
        "    \"loss_func\":             loss_func, \n",
        "    \"metric_func\":           metrics_func_classification,\n",
        "    \"num_epochs\":            300,\n",
        "    \"optimizer\":             opt,\n",
        "    \"lr_scheduler\":          lr_scheduler,\n",
        "    \"train_dl\":              train_dl,\n",
        "    \"test_dl\":               test_dl,\n",
        "    \"device\":                device,  \n",
        "    \"continue_training\" :    False,  # continue training from last save weights\n",
        "    \"sanity_check\":          False, # if true we only do one batch per epoch\n",
        "    \"path2weigths\":          \"./weights_classification.pt\"  \n",
        "} \n",
        "model, loss_history,metric_history = train_test(params)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--> training device cpu\n",
            "----\n",
            "Epoch 1/300, lr=0.001000\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.170238, test_loss: 1.102329, train_metric: -0.370, test_metric: -0.372\n",
            "----\n",
            "Epoch 2/300, lr=0.000999\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.105318, test_loss: 1.095627, train_metric: -0.386, test_metric: -0.413\n",
            "----\n",
            "Epoch 3/300, lr=0.000998\n",
            "--> train_loss: 1.101353, test_loss: 1.096018, train_metric: -0.397, test_metric: -0.413\n",
            "----\n",
            "Epoch 4/300, lr=0.000997\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.106576, test_loss: 1.094528, train_metric: -0.389, test_metric: -0.372\n",
            "----\n",
            "Epoch 5/300, lr=0.000996\n",
            "--> train_loss: 1.102463, test_loss: 1.095452, train_metric: -0.410, test_metric: -0.456\n",
            "----\n",
            "Epoch 6/300, lr=0.000995\n",
            "--> train_loss: 1.100696, test_loss: 1.097028, train_metric: -0.419, test_metric: -0.372\n",
            "----\n",
            "Epoch 7/300, lr=0.000994\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.098075, test_loss: 1.087384, train_metric: -0.406, test_metric: -0.450\n",
            "----\n",
            "Epoch 8/300, lr=0.000993\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.098131, test_loss: 1.085721, train_metric: -0.427, test_metric: -0.467\n",
            "----\n",
            "Epoch 9/300, lr=0.000992\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.095183, test_loss: 1.085545, train_metric: -0.426, test_metric: -0.455\n",
            "----\n",
            "Epoch 10/300, lr=0.000991\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.093810, test_loss: 1.083300, train_metric: -0.442, test_metric: -0.467\n",
            "----\n",
            "Epoch 11/300, lr=0.000990\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.091812, test_loss: 1.079870, train_metric: -0.429, test_metric: -0.435\n",
            "----\n",
            "Epoch 12/300, lr=0.000989\n",
            "--> train_loss: 1.087754, test_loss: 1.083304, train_metric: -0.419, test_metric: -0.462\n",
            "----\n",
            "Epoch 13/300, lr=0.000988\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.088704, test_loss: 1.076489, train_metric: -0.430, test_metric: -0.450\n",
            "----\n",
            "Epoch 14/300, lr=0.000987\n",
            "--> train_loss: 1.085188, test_loss: 1.077907, train_metric: -0.444, test_metric: -0.457\n",
            "----\n",
            "Epoch 15/300, lr=0.000986\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.085367, test_loss: 1.075207, train_metric: -0.444, test_metric: -0.456\n",
            "----\n",
            "Epoch 16/300, lr=0.000985\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.084171, test_loss: 1.073627, train_metric: -0.442, test_metric: -0.461\n",
            "----\n",
            "Epoch 17/300, lr=0.000984\n",
            "--> train_loss: 1.082858, test_loss: 1.074206, train_metric: -0.451, test_metric: -0.450\n",
            "----\n",
            "Epoch 18/300, lr=0.000983\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.080634, test_loss: 1.070200, train_metric: -0.441, test_metric: -0.480\n",
            "----\n",
            "Epoch 19/300, lr=0.000982\n",
            "--> train_loss: 1.079453, test_loss: 1.075095, train_metric: -0.456, test_metric: -0.440\n",
            "----\n",
            "Epoch 20/300, lr=0.000981\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.080171, test_loss: 1.067074, train_metric: -0.451, test_metric: -0.455\n",
            "----\n",
            "Epoch 21/300, lr=0.000980\n",
            "--> train_loss: 1.078272, test_loss: 1.069919, train_metric: -0.440, test_metric: -0.451\n",
            "----\n",
            "Epoch 22/300, lr=0.000979\n",
            "--> train_loss: 1.075948, test_loss: 1.071161, train_metric: -0.445, test_metric: -0.446\n",
            "----\n",
            "Epoch 23/300, lr=0.000978\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.069657, test_loss: 1.062810, train_metric: -0.433, test_metric: -0.451\n",
            "----\n",
            "Epoch 24/300, lr=0.000977\n",
            "--> train_loss: 1.066159, test_loss: 1.065372, train_metric: -0.447, test_metric: -0.455\n",
            "----\n",
            "Epoch 25/300, lr=0.000976\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.064149, test_loss: 1.059277, train_metric: -0.449, test_metric: -0.439\n",
            "----\n",
            "Epoch 26/300, lr=0.000975\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.062575, test_loss: 1.052941, train_metric: -0.441, test_metric: -0.457\n",
            "----\n",
            "Epoch 27/300, lr=0.000974\n",
            "--> train_loss: 1.059629, test_loss: 1.066536, train_metric: -0.449, test_metric: -0.418\n",
            "----\n",
            "Epoch 28/300, lr=0.000973\n",
            "--> train_loss: 1.070465, test_loss: 1.071297, train_metric: -0.434, test_metric: -0.447\n",
            "----\n",
            "Epoch 29/300, lr=0.000972\n",
            "--> train_loss: 1.056311, test_loss: 1.059315, train_metric: -0.455, test_metric: -0.458\n",
            "----\n",
            "Epoch 30/300, lr=0.000971\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.055581, test_loss: 1.047963, train_metric: -0.440, test_metric: -0.457\n",
            "----\n",
            "Epoch 31/300, lr=0.000970\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.049969, test_loss: 1.046335, train_metric: -0.451, test_metric: -0.439\n",
            "----\n",
            "Epoch 32/300, lr=0.000969\n",
            "--> train_loss: 1.049066, test_loss: 1.060926, train_metric: -0.456, test_metric: -0.466\n",
            "----\n",
            "Epoch 33/300, lr=0.000968\n",
            "--> train_loss: 1.051162, test_loss: 1.062484, train_metric: -0.459, test_metric: -0.445\n",
            "----\n",
            "Epoch 34/300, lr=0.000968\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.041852, test_loss: 1.036429, train_metric: -0.463, test_metric: -0.442\n",
            "----\n",
            "Epoch 35/300, lr=0.000967\n",
            "--> train_loss: 1.038015, test_loss: 1.037707, train_metric: -0.456, test_metric: -0.446\n",
            "----\n",
            "Epoch 36/300, lr=0.000966\n",
            "--> train_loss: 1.033587, test_loss: 1.040487, train_metric: -0.468, test_metric: -0.447\n",
            "----\n",
            "Epoch 37/300, lr=0.000965\n",
            "--> train_loss: 1.031489, test_loss: 1.044324, train_metric: -0.478, test_metric: -0.450\n",
            "----\n",
            "Epoch 38/300, lr=0.000964\n",
            "--> train_loss: 1.035411, test_loss: 1.041098, train_metric: -0.457, test_metric: -0.455\n",
            "----\n",
            "Epoch 39/300, lr=0.000963\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.030172, test_loss: 1.031477, train_metric: -0.466, test_metric: -0.447\n",
            "----\n",
            "Epoch 40/300, lr=0.000962\n",
            "--> train_loss: 1.032366, test_loss: 1.032017, train_metric: -0.472, test_metric: -0.449\n",
            "----\n",
            "Epoch 41/300, lr=0.000961\n",
            "--> train_loss: 1.024072, test_loss: 1.032953, train_metric: -0.476, test_metric: -0.463\n",
            "----\n",
            "Epoch 42/300, lr=0.000960\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.027318, test_loss: 1.026943, train_metric: -0.480, test_metric: -0.470\n",
            "----\n",
            "Epoch 43/300, lr=0.000959\n",
            "--> train_loss: 1.024582, test_loss: 1.030519, train_metric: -0.468, test_metric: -0.451\n",
            "----\n",
            "Epoch 44/300, lr=0.000958\n",
            "--> train_loss: 1.023093, test_loss: 1.027770, train_metric: -0.478, test_metric: -0.462\n",
            "----\n",
            "Epoch 45/300, lr=0.000957\n",
            "--> train_loss: 1.026448, test_loss: 1.041832, train_metric: -0.464, test_metric: -0.462\n",
            "----\n",
            "Epoch 46/300, lr=0.000956\n",
            "--> train_loss: 1.028960, test_loss: 1.028232, train_metric: -0.467, test_metric: -0.461\n",
            "----\n",
            "Epoch 47/300, lr=0.000955\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.014776, test_loss: 1.024815, train_metric: -0.469, test_metric: -0.462\n",
            "----\n",
            "Epoch 48/300, lr=0.000954\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.023231, test_loss: 1.022389, train_metric: -0.476, test_metric: -0.465\n",
            "----\n",
            "Epoch 49/300, lr=0.000953\n",
            "--> train_loss: 1.014413, test_loss: 1.029964, train_metric: -0.474, test_metric: -0.466\n",
            "----\n",
            "Epoch 50/300, lr=0.000952\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.014400, test_loss: 1.020213, train_metric: -0.478, test_metric: -0.478\n",
            "----\n",
            "Epoch 51/300, lr=0.000951\n",
            "--> train_loss: 1.011879, test_loss: 1.023953, train_metric: -0.474, test_metric: -0.466\n",
            "----\n",
            "Epoch 52/300, lr=0.000950\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.014413, test_loss: 1.018392, train_metric: -0.477, test_metric: -0.476\n",
            "----\n",
            "Epoch 53/300, lr=0.000949\n",
            "--> train_loss: 1.012229, test_loss: 1.018715, train_metric: -0.467, test_metric: -0.442\n",
            "----\n",
            "Epoch 54/300, lr=0.000948\n",
            "--> train_loss: 1.013667, test_loss: 1.019786, train_metric: -0.472, test_metric: -0.471\n",
            "----\n",
            "Epoch 55/300, lr=0.000947\n",
            "--> train_loss: 1.007672, test_loss: 1.018657, train_metric: -0.491, test_metric: -0.472\n",
            "----\n",
            "Epoch 56/300, lr=0.000946\n",
            "--> train_loss: 1.010286, test_loss: 1.019845, train_metric: -0.481, test_metric: -0.449\n",
            "----\n",
            "Epoch 57/300, lr=0.000946\n",
            "--> train_loss: 1.008815, test_loss: 1.024646, train_metric: -0.476, test_metric: -0.458\n",
            "----\n",
            "Epoch 58/300, lr=0.000945\n",
            "--> train_loss: 1.011020, test_loss: 1.034374, train_metric: -0.494, test_metric: -0.472\n",
            "----\n",
            "Epoch 59/300, lr=0.000944\n",
            "--> train_loss: 1.008962, test_loss: 1.020216, train_metric: -0.484, test_metric: -0.467\n",
            "----\n",
            "Epoch 60/300, lr=0.000943\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.008365, test_loss: 1.015710, train_metric: -0.471, test_metric: -0.465\n",
            "----\n",
            "Epoch 61/300, lr=0.000942\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.005645, test_loss: 1.014087, train_metric: -0.490, test_metric: -0.480\n",
            "----\n",
            "Epoch 62/300, lr=0.000941\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.003916, test_loss: 1.012940, train_metric: -0.481, test_metric: -0.489\n",
            "----\n",
            "Epoch 63/300, lr=0.000940\n",
            "--> train_loss: 1.000174, test_loss: 1.018797, train_metric: -0.486, test_metric: -0.440\n",
            "----\n",
            "Epoch 64/300, lr=0.000939\n",
            "--> train_loss: 1.001082, test_loss: 1.019359, train_metric: -0.487, test_metric: -0.478\n",
            "----\n",
            "Epoch 65/300, lr=0.000938\n",
            "--> train_loss: 1.005255, test_loss: 1.019002, train_metric: -0.475, test_metric: -0.460\n",
            "----\n",
            "Epoch 66/300, lr=0.000937\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.001750, test_loss: 1.010351, train_metric: -0.483, test_metric: -0.493\n",
            "----\n",
            "Epoch 67/300, lr=0.000936\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 1.001759, test_loss: 1.009703, train_metric: -0.486, test_metric: -0.471\n",
            "----\n",
            "Epoch 68/300, lr=0.000935\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.998700, test_loss: 1.005365, train_metric: -0.482, test_metric: -0.482\n",
            "----\n",
            "Epoch 69/300, lr=0.000934\n",
            "--> train_loss: 1.001292, test_loss: 1.011102, train_metric: -0.493, test_metric: -0.477\n",
            "----\n",
            "Epoch 70/300, lr=0.000933\n",
            "--> train_loss: 1.001566, test_loss: 1.007008, train_metric: -0.490, test_metric: -0.482\n",
            "----\n",
            "Epoch 71/300, lr=0.000932\n",
            "--> train_loss: 1.004708, test_loss: 1.005956, train_metric: -0.494, test_metric: -0.483\n",
            "----\n",
            "Epoch 72/300, lr=0.000931\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.996910, test_loss: 1.000881, train_metric: -0.491, test_metric: -0.465\n",
            "----\n",
            "Epoch 73/300, lr=0.000930\n",
            "--> train_loss: 0.992896, test_loss: 1.011212, train_metric: -0.479, test_metric: -0.471\n",
            "----\n",
            "Epoch 74/300, lr=0.000930\n",
            "--> train_loss: 0.992715, test_loss: 1.002454, train_metric: -0.493, test_metric: -0.483\n",
            "----\n",
            "Epoch 75/300, lr=0.000929\n",
            "--> train_loss: 0.993800, test_loss: 1.005813, train_metric: -0.487, test_metric: -0.472\n",
            "----\n",
            "Epoch 76/300, lr=0.000928\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.998256, test_loss: 1.000839, train_metric: -0.482, test_metric: -0.491\n",
            "----\n",
            "Epoch 77/300, lr=0.000927\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.991142, test_loss: 0.999862, train_metric: -0.496, test_metric: -0.486\n",
            "----\n",
            "Epoch 78/300, lr=0.000926\n",
            "--> train_loss: 0.990986, test_loss: 1.007335, train_metric: -0.493, test_metric: -0.475\n",
            "----\n",
            "Epoch 79/300, lr=0.000925\n",
            "--> train_loss: 0.989914, test_loss: 1.000637, train_metric: -0.496, test_metric: -0.478\n",
            "----\n",
            "Epoch 80/300, lr=0.000924\n",
            "--> train_loss: 0.987817, test_loss: 1.013267, train_metric: -0.498, test_metric: -0.480\n",
            "----\n",
            "Epoch 81/300, lr=0.000923\n",
            "--> train_loss: 0.995827, test_loss: 1.017743, train_metric: -0.491, test_metric: -0.460\n",
            "----\n",
            "Epoch 82/300, lr=0.000922\n",
            "--> train_loss: 0.995425, test_loss: 1.006814, train_metric: -0.485, test_metric: -0.481\n",
            "----\n",
            "Epoch 83/300, lr=0.000921\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.987115, test_loss: 0.993549, train_metric: -0.498, test_metric: -0.497\n",
            "----\n",
            "Epoch 84/300, lr=0.000920\n",
            "--> train_loss: 0.980842, test_loss: 0.999352, train_metric: -0.495, test_metric: -0.486\n",
            "----\n",
            "Epoch 85/300, lr=0.000919\n",
            "--> train_loss: 0.982865, test_loss: 0.995375, train_metric: -0.502, test_metric: -0.485\n",
            "----\n",
            "Epoch 86/300, lr=0.000918\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.987110, test_loss: 0.993271, train_metric: -0.502, test_metric: -0.493\n",
            "----\n",
            "Epoch 87/300, lr=0.000918\n",
            "--> train_loss: 0.980330, test_loss: 1.000585, train_metric: -0.504, test_metric: -0.482\n",
            "----\n",
            "Epoch 88/300, lr=0.000917\n",
            "--> train_loss: 0.982443, test_loss: 0.994412, train_metric: -0.486, test_metric: -0.481\n",
            "----\n",
            "Epoch 89/300, lr=0.000916\n",
            "--> train_loss: 0.986105, test_loss: 1.008260, train_metric: -0.506, test_metric: -0.467\n",
            "----\n",
            "Epoch 90/300, lr=0.000915\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.981077, test_loss: 0.984144, train_metric: -0.503, test_metric: -0.485\n",
            "----\n",
            "Epoch 91/300, lr=0.000914\n",
            "--> train_loss: 0.976455, test_loss: 0.988464, train_metric: -0.498, test_metric: -0.476\n",
            "----\n",
            "Epoch 92/300, lr=0.000913\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.974344, test_loss: 0.982014, train_metric: -0.512, test_metric: -0.475\n",
            "----\n",
            "Epoch 93/300, lr=0.000912\n",
            "--> train_loss: 0.973938, test_loss: 0.991916, train_metric: -0.511, test_metric: -0.458\n",
            "----\n",
            "Epoch 94/300, lr=0.000911\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.973733, test_loss: 0.978847, train_metric: -0.511, test_metric: -0.478\n",
            "----\n",
            "Epoch 95/300, lr=0.000910\n",
            "--> train_loss: 0.971637, test_loss: 0.990403, train_metric: -0.502, test_metric: -0.468\n",
            "----\n",
            "Epoch 96/300, lr=0.000909\n",
            "--> train_loss: 0.974439, test_loss: 0.981130, train_metric: -0.505, test_metric: -0.473\n",
            "----\n",
            "Epoch 97/300, lr=0.000908\n",
            "--> train_loss: 0.971272, test_loss: 0.986117, train_metric: -0.504, test_metric: -0.466\n",
            "----\n",
            "Epoch 98/300, lr=0.000908\n",
            "--> train_loss: 0.968003, test_loss: 0.982407, train_metric: -0.515, test_metric: -0.481\n",
            "----\n",
            "Epoch 99/300, lr=0.000907\n",
            "--> train_loss: 0.968970, test_loss: 0.982806, train_metric: -0.509, test_metric: -0.499\n",
            "----\n",
            "Epoch 100/300, lr=0.000906\n",
            "--> train_loss: 0.967360, test_loss: 0.981199, train_metric: -0.521, test_metric: -0.506\n",
            "----\n",
            "Epoch 101/300, lr=0.000905\n",
            "--> train_loss: 0.966509, test_loss: 0.979780, train_metric: -0.514, test_metric: -0.493\n",
            "----\n",
            "Epoch 102/300, lr=0.000904\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.961995, test_loss: 0.971400, train_metric: -0.514, test_metric: -0.496\n",
            "----\n",
            "Epoch 103/300, lr=0.000903\n",
            "--> train_loss: 0.962725, test_loss: 0.984874, train_metric: -0.523, test_metric: -0.487\n",
            "----\n",
            "Epoch 104/300, lr=0.000902\n",
            "--> train_loss: 0.962366, test_loss: 0.983626, train_metric: -0.523, test_metric: -0.499\n",
            "----\n",
            "Epoch 105/300, lr=0.000901\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.959432, test_loss: 0.971395, train_metric: -0.518, test_metric: -0.517\n",
            "----\n",
            "Epoch 106/300, lr=0.000900\n",
            "--> train_loss: 0.956967, test_loss: 0.975403, train_metric: -0.532, test_metric: -0.492\n",
            "----\n",
            "Epoch 107/300, lr=0.000899\n",
            "--> train_loss: 0.956787, test_loss: 0.972545, train_metric: -0.523, test_metric: -0.488\n",
            "----\n",
            "Epoch 108/300, lr=0.000898\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.952112, test_loss: 0.962201, train_metric: -0.525, test_metric: -0.501\n",
            "----\n",
            "Epoch 109/300, lr=0.000898\n",
            "--> train_loss: 0.954788, test_loss: 0.963290, train_metric: -0.527, test_metric: -0.497\n",
            "----\n",
            "Epoch 110/300, lr=0.000897\n",
            "--> train_loss: 0.949916, test_loss: 0.976788, train_metric: -0.519, test_metric: -0.494\n",
            "----\n",
            "Epoch 111/300, lr=0.000896\n",
            "--> train_loss: 0.956233, test_loss: 0.969263, train_metric: -0.519, test_metric: -0.506\n",
            "----\n",
            "Epoch 112/300, lr=0.000895\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.951079, test_loss: 0.958605, train_metric: -0.534, test_metric: -0.529\n",
            "----\n",
            "Epoch 113/300, lr=0.000894\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.944502, test_loss: 0.955355, train_metric: -0.529, test_metric: -0.509\n",
            "----\n",
            "Epoch 114/300, lr=0.000893\n",
            "--> train_loss: 0.941549, test_loss: 0.958450, train_metric: -0.540, test_metric: -0.515\n",
            "----\n",
            "Epoch 115/300, lr=0.000892\n",
            "--> train_loss: 0.940667, test_loss: 0.958759, train_metric: -0.533, test_metric: -0.509\n",
            "----\n",
            "Epoch 116/300, lr=0.000891\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.937243, test_loss: 0.951252, train_metric: -0.554, test_metric: -0.507\n",
            "----\n",
            "Epoch 117/300, lr=0.000890\n",
            "--> train_loss: 0.936979, test_loss: 0.956163, train_metric: -0.548, test_metric: -0.520\n",
            "----\n",
            "Epoch 118/300, lr=0.000890\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.934018, test_loss: 0.945775, train_metric: -0.547, test_metric: -0.514\n",
            "----\n",
            "Epoch 119/300, lr=0.000889\n",
            "--> train_loss: 0.931762, test_loss: 0.957782, train_metric: -0.540, test_metric: -0.507\n",
            "----\n",
            "Epoch 120/300, lr=0.000888\n",
            "--> train_loss: 0.937478, test_loss: 0.960056, train_metric: -0.539, test_metric: -0.515\n",
            "----\n",
            "Epoch 121/300, lr=0.000887\n",
            "--> train_loss: 0.945630, test_loss: 0.947641, train_metric: -0.528, test_metric: -0.525\n",
            "----\n",
            "Epoch 122/300, lr=0.000886\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.927337, test_loss: 0.945734, train_metric: -0.550, test_metric: -0.545\n",
            "----\n",
            "Epoch 123/300, lr=0.000885\n",
            "--> train_loss: 0.928518, test_loss: 0.948243, train_metric: -0.548, test_metric: -0.524\n",
            "----\n",
            "Epoch 124/300, lr=0.000884\n",
            "--> train_loss: 0.926845, test_loss: 0.950051, train_metric: -0.561, test_metric: -0.522\n",
            "----\n",
            "Epoch 125/300, lr=0.000883\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.929695, test_loss: 0.943916, train_metric: -0.546, test_metric: -0.535\n",
            "----\n",
            "Epoch 126/300, lr=0.000882\n",
            "--> train_loss: 0.918793, test_loss: 0.948259, train_metric: -0.557, test_metric: -0.504\n",
            "----\n",
            "Epoch 127/300, lr=0.000882\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.923772, test_loss: 0.941519, train_metric: -0.555, test_metric: -0.535\n",
            "----\n",
            "Epoch 128/300, lr=0.000881\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.925141, test_loss: 0.934817, train_metric: -0.535, test_metric: -0.540\n",
            "----\n",
            "Epoch 129/300, lr=0.000880\n",
            "--> train_loss: 0.915365, test_loss: 0.952756, train_metric: -0.562, test_metric: -0.509\n",
            "----\n",
            "Epoch 130/300, lr=0.000879\n",
            "--> train_loss: 0.927317, test_loss: 0.942450, train_metric: -0.525, test_metric: -0.524\n",
            "----\n",
            "Epoch 131/300, lr=0.000878\n",
            "--> train_loss: 0.923672, test_loss: 0.950095, train_metric: -0.552, test_metric: -0.535\n",
            "----\n",
            "Epoch 132/300, lr=0.000877\n",
            "--> train_loss: 0.923500, test_loss: 0.939347, train_metric: -0.565, test_metric: -0.528\n",
            "----\n",
            "Epoch 133/300, lr=0.000876\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.914201, test_loss: 0.930838, train_metric: -0.556, test_metric: -0.522\n",
            "----\n",
            "Epoch 134/300, lr=0.000875\n",
            "--> train_loss: 0.910284, test_loss: 0.942150, train_metric: -0.562, test_metric: -0.532\n",
            "----\n",
            "Epoch 135/300, lr=0.000875\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.908087, test_loss: 0.923707, train_metric: -0.569, test_metric: -0.533\n",
            "----\n",
            "Epoch 136/300, lr=0.000874\n",
            "--> train_loss: 0.899131, test_loss: 0.927843, train_metric: -0.570, test_metric: -0.528\n",
            "----\n",
            "Epoch 137/300, lr=0.000873\n",
            "--> train_loss: 0.900839, test_loss: 0.926461, train_metric: -0.561, test_metric: -0.555\n",
            "----\n",
            "Epoch 138/300, lr=0.000872\n",
            "--> train_loss: 0.900380, test_loss: 0.932439, train_metric: -0.569, test_metric: -0.528\n",
            "----\n",
            "Epoch 139/300, lr=0.000871\n",
            "--> train_loss: 0.910662, test_loss: 0.942148, train_metric: -0.569, test_metric: -0.542\n",
            "----\n",
            "Epoch 140/300, lr=0.000870\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.909016, test_loss: 0.918088, train_metric: -0.559, test_metric: -0.532\n",
            "----\n",
            "Epoch 141/300, lr=0.000869\n",
            "--> train_loss: 0.900833, test_loss: 0.927169, train_metric: -0.569, test_metric: -0.549\n",
            "----\n",
            "Epoch 142/300, lr=0.000868\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.897694, test_loss: 0.914969, train_metric: -0.568, test_metric: -0.522\n",
            "----\n",
            "Epoch 143/300, lr=0.000868\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.898695, test_loss: 0.914004, train_metric: -0.557, test_metric: -0.537\n",
            "----\n",
            "Epoch 144/300, lr=0.000867\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.892075, test_loss: 0.913894, train_metric: -0.584, test_metric: -0.538\n",
            "----\n",
            "Epoch 145/300, lr=0.000866\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.891508, test_loss: 0.906189, train_metric: -0.571, test_metric: -0.549\n",
            "----\n",
            "Epoch 146/300, lr=0.000865\n",
            "--> train_loss: 0.884748, test_loss: 0.913435, train_metric: -0.580, test_metric: -0.538\n",
            "----\n",
            "Epoch 147/300, lr=0.000864\n",
            "--> train_loss: 0.884304, test_loss: 0.915604, train_metric: -0.567, test_metric: -0.559\n",
            "----\n",
            "Epoch 148/300, lr=0.000863\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.882362, test_loss: 0.904883, train_metric: -0.578, test_metric: -0.523\n",
            "----\n",
            "Epoch 149/300, lr=0.000862\n",
            "--> train_loss: 0.881048, test_loss: 0.906212, train_metric: -0.570, test_metric: -0.553\n",
            "----\n",
            "Epoch 150/300, lr=0.000862\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.873163, test_loss: 0.899511, train_metric: -0.578, test_metric: -0.555\n",
            "----\n",
            "Epoch 151/300, lr=0.000861\n",
            "--> train_loss: 0.876628, test_loss: 0.916481, train_metric: -0.586, test_metric: -0.551\n",
            "----\n",
            "Epoch 152/300, lr=0.000860\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.877843, test_loss: 0.890604, train_metric: -0.580, test_metric: -0.571\n",
            "----\n",
            "Epoch 153/300, lr=0.000859\n",
            "--> train_loss: 0.868558, test_loss: 0.893901, train_metric: -0.587, test_metric: -0.574\n",
            "----\n",
            "Epoch 154/300, lr=0.000858\n",
            "--> train_loss: 0.870393, test_loss: 0.893156, train_metric: -0.596, test_metric: -0.545\n",
            "----\n",
            "Epoch 155/300, lr=0.000857\n",
            "--> train_loss: 0.869326, test_loss: 0.912606, train_metric: -0.577, test_metric: -0.532\n",
            "----\n",
            "Epoch 156/300, lr=0.000856\n",
            "--> train_loss: 0.871228, test_loss: 0.896479, train_metric: -0.581, test_metric: -0.546\n",
            "----\n",
            "Epoch 157/300, lr=0.000855\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.862666, test_loss: 0.890251, train_metric: -0.588, test_metric: -0.574\n",
            "----\n",
            "Epoch 158/300, lr=0.000855\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.861890, test_loss: 0.889327, train_metric: -0.594, test_metric: -0.570\n",
            "----\n",
            "Epoch 159/300, lr=0.000854\n",
            "--> train_loss: 0.858928, test_loss: 0.890746, train_metric: -0.593, test_metric: -0.564\n",
            "----\n",
            "Epoch 160/300, lr=0.000853\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.853421, test_loss: 0.885082, train_metric: -0.610, test_metric: -0.560\n",
            "----\n",
            "Epoch 161/300, lr=0.000852\n",
            "--> train_loss: 0.853825, test_loss: 0.894695, train_metric: -0.596, test_metric: -0.544\n",
            "----\n",
            "Epoch 162/300, lr=0.000851\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.850342, test_loss: 0.875513, train_metric: -0.595, test_metric: -0.570\n",
            "----\n",
            "Epoch 163/300, lr=0.000850\n",
            "--> train_loss: 0.851953, test_loss: 0.879071, train_metric: -0.587, test_metric: -0.560\n",
            "----\n",
            "Epoch 164/300, lr=0.000850\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.841769, test_loss: 0.870085, train_metric: -0.595, test_metric: -0.586\n",
            "----\n",
            "Epoch 165/300, lr=0.000849\n",
            "--> train_loss: 0.840532, test_loss: 0.873531, train_metric: -0.616, test_metric: -0.556\n",
            "----\n",
            "Epoch 166/300, lr=0.000848\n",
            "--> train_loss: 0.835537, test_loss: 0.872734, train_metric: -0.611, test_metric: -0.586\n",
            "----\n",
            "Epoch 167/300, lr=0.000847\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.841577, test_loss: 0.867343, train_metric: -0.617, test_metric: -0.553\n",
            "----\n",
            "Epoch 168/300, lr=0.000846\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.837594, test_loss: 0.866713, train_metric: -0.607, test_metric: -0.581\n",
            "----\n",
            "Epoch 169/300, lr=0.000845\n",
            "--> train_loss: 0.834007, test_loss: 0.872887, train_metric: -0.626, test_metric: -0.546\n",
            "----\n",
            "Epoch 170/300, lr=0.000844\n",
            "--> train_loss: 0.835560, test_loss: 0.873907, train_metric: -0.614, test_metric: -0.549\n",
            "----\n",
            "Epoch 171/300, lr=0.000844\n",
            "--> train_loss: 0.829212, test_loss: 0.874031, train_metric: -0.620, test_metric: -0.559\n",
            "----\n",
            "Epoch 172/300, lr=0.000843\n",
            "--> train_loss: 0.831291, test_loss: 0.878338, train_metric: -0.616, test_metric: -0.561\n",
            "----\n",
            "Epoch 173/300, lr=0.000842\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.831369, test_loss: 0.856073, train_metric: -0.613, test_metric: -0.603\n",
            "----\n",
            "Epoch 174/300, lr=0.000841\n",
            "--> train_loss: 0.823193, test_loss: 0.863969, train_metric: -0.620, test_metric: -0.595\n",
            "----\n",
            "Epoch 175/300, lr=0.000840\n",
            "--> train_loss: 0.826753, test_loss: 0.865046, train_metric: -0.621, test_metric: -0.589\n",
            "----\n",
            "Epoch 176/300, lr=0.000839\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.826949, test_loss: 0.854102, train_metric: -0.608, test_metric: -0.591\n",
            "----\n",
            "Epoch 177/300, lr=0.000839\n",
            "--> train_loss: 0.812306, test_loss: 0.877938, train_metric: -0.623, test_metric: -0.574\n",
            "----\n",
            "Epoch 178/300, lr=0.000838\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.819335, test_loss: 0.844874, train_metric: -0.632, test_metric: -0.576\n",
            "----\n",
            "Epoch 179/300, lr=0.000837\n",
            "--> train_loss: 0.820717, test_loss: 0.855773, train_metric: -0.623, test_metric: -0.577\n",
            "----\n",
            "Epoch 180/300, lr=0.000836\n",
            "--> train_loss: 0.813366, test_loss: 0.846983, train_metric: -0.620, test_metric: -0.606\n",
            "----\n",
            "Epoch 181/300, lr=0.000835\n",
            "--> train_loss: 0.814228, test_loss: 0.856221, train_metric: -0.632, test_metric: -0.591\n",
            "----\n",
            "Epoch 182/300, lr=0.000834\n",
            "--> train_loss: 0.811850, test_loss: 0.851800, train_metric: -0.630, test_metric: -0.585\n",
            "----\n",
            "Epoch 183/300, lr=0.000834\n",
            "--> train_loss: 0.806816, test_loss: 0.859359, train_metric: -0.639, test_metric: -0.577\n",
            "----\n",
            "Epoch 184/300, lr=0.000833\n",
            "--> train_loss: 0.804531, test_loss: 0.853422, train_metric: -0.631, test_metric: -0.568\n",
            "----\n",
            "Epoch 185/300, lr=0.000832\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.799955, test_loss: 0.837696, train_metric: -0.631, test_metric: -0.601\n",
            "----\n",
            "Epoch 186/300, lr=0.000831\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.798407, test_loss: 0.835831, train_metric: -0.639, test_metric: -0.599\n",
            "----\n",
            "Epoch 187/300, lr=0.000830\n",
            "--> train_loss: 0.797576, test_loss: 0.846022, train_metric: -0.635, test_metric: -0.571\n",
            "----\n",
            "Epoch 188/300, lr=0.000829\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.794916, test_loss: 0.832075, train_metric: -0.642, test_metric: -0.581\n",
            "----\n",
            "Epoch 189/300, lr=0.000829\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.792282, test_loss: 0.825407, train_metric: -0.637, test_metric: -0.606\n",
            "----\n",
            "Epoch 190/300, lr=0.000828\n",
            "--> train_loss: 0.787983, test_loss: 0.827036, train_metric: -0.644, test_metric: -0.580\n",
            "----\n",
            "Epoch 191/300, lr=0.000827\n",
            "--> train_loss: 0.786598, test_loss: 0.841616, train_metric: -0.655, test_metric: -0.575\n",
            "----\n",
            "Epoch 192/300, lr=0.000826\n",
            "--> train_loss: 0.787772, test_loss: 0.839035, train_metric: -0.639, test_metric: -0.594\n",
            "----\n",
            "Epoch 193/300, lr=0.000825\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.787996, test_loss: 0.823457, train_metric: -0.637, test_metric: -0.590\n",
            "----\n",
            "Epoch 194/300, lr=0.000824\n",
            "--> train_loss: 0.779723, test_loss: 0.825789, train_metric: -0.647, test_metric: -0.618\n",
            "----\n",
            "Epoch 195/300, lr=0.000824\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.780605, test_loss: 0.822573, train_metric: -0.651, test_metric: -0.599\n",
            "----\n",
            "Epoch 196/300, lr=0.000823\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.775989, test_loss: 0.821546, train_metric: -0.654, test_metric: -0.589\n",
            "----\n",
            "Epoch 197/300, lr=0.000822\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.773997, test_loss: 0.813679, train_metric: -0.654, test_metric: -0.621\n",
            "----\n",
            "Epoch 198/300, lr=0.000821\n",
            "--> train_loss: 0.776108, test_loss: 0.824929, train_metric: -0.654, test_metric: -0.594\n",
            "----\n",
            "Epoch 199/300, lr=0.000820\n",
            "--> train_loss: 0.778684, test_loss: 0.835388, train_metric: -0.651, test_metric: -0.575\n",
            "----\n",
            "Epoch 200/300, lr=0.000819\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.774840, test_loss: 0.813388, train_metric: -0.650, test_metric: -0.596\n",
            "----\n",
            "Epoch 201/300, lr=0.000819\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.768438, test_loss: 0.803591, train_metric: -0.654, test_metric: -0.620\n",
            "----\n",
            "Epoch 202/300, lr=0.000818\n",
            "--> train_loss: 0.766587, test_loss: 0.804766, train_metric: -0.653, test_metric: -0.608\n",
            "----\n",
            "Epoch 203/300, lr=0.000817\n",
            "--> train_loss: 0.767682, test_loss: 0.809377, train_metric: -0.664, test_metric: -0.622\n",
            "----\n",
            "Epoch 204/300, lr=0.000816\n",
            "--> train_loss: 0.770443, test_loss: 0.804589, train_metric: -0.646, test_metric: -0.625\n",
            "----\n",
            "Epoch 205/300, lr=0.000815\n",
            "--> train_loss: 0.760766, test_loss: 0.807518, train_metric: -0.661, test_metric: -0.602\n",
            "----\n",
            "Epoch 206/300, lr=0.000815\n",
            "--> train_loss: 0.762526, test_loss: 0.806349, train_metric: -0.661, test_metric: -0.613\n",
            "----\n",
            "Epoch 207/300, lr=0.000814\n",
            "--> train_loss: 0.758962, test_loss: 0.807245, train_metric: -0.663, test_metric: -0.601\n",
            "----\n",
            "Epoch 208/300, lr=0.000813\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.759867, test_loss: 0.796267, train_metric: -0.657, test_metric: -0.616\n",
            "----\n",
            "Epoch 209/300, lr=0.000812\n",
            "--> train_loss: 0.753814, test_loss: 0.824174, train_metric: -0.660, test_metric: -0.618\n",
            "----\n",
            "Epoch 210/300, lr=0.000811\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.759459, test_loss: 0.789828, train_metric: -0.657, test_metric: -0.627\n",
            "----\n",
            "Epoch 211/300, lr=0.000810\n",
            "--> train_loss: 0.755355, test_loss: 0.811957, train_metric: -0.663, test_metric: -0.601\n",
            "----\n",
            "Epoch 212/300, lr=0.000810\n",
            "--> train_loss: 0.754365, test_loss: 0.798609, train_metric: -0.663, test_metric: -0.617\n",
            "----\n",
            "Epoch 213/300, lr=0.000809\n",
            "--> train_loss: 0.758164, test_loss: 0.794669, train_metric: -0.660, test_metric: -0.612\n",
            "----\n",
            "Epoch 214/300, lr=0.000808\n",
            "--> train_loss: 0.748611, test_loss: 0.790959, train_metric: -0.656, test_metric: -0.626\n",
            "----\n",
            "Epoch 215/300, lr=0.000807\n",
            "--> train_loss: 0.745895, test_loss: 0.794112, train_metric: -0.666, test_metric: -0.620\n",
            "----\n",
            "Epoch 216/300, lr=0.000806\n",
            "--> train_loss: 0.743572, test_loss: 0.796046, train_metric: -0.667, test_metric: -0.601\n",
            "----\n",
            "Epoch 217/300, lr=0.000806\n",
            "--> train_loss: 0.743541, test_loss: 0.794172, train_metric: -0.671, test_metric: -0.625\n",
            "----\n",
            "Epoch 218/300, lr=0.000805\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.739425, test_loss: 0.778076, train_metric: -0.675, test_metric: -0.626\n",
            "----\n",
            "Epoch 219/300, lr=0.000804\n",
            "--> train_loss: 0.736611, test_loss: 0.784819, train_metric: -0.673, test_metric: -0.613\n",
            "----\n",
            "Epoch 220/300, lr=0.000803\n",
            "--> train_loss: 0.738336, test_loss: 0.780501, train_metric: -0.674, test_metric: -0.633\n",
            "----\n",
            "Epoch 221/300, lr=0.000802\n",
            "--> train_loss: 0.735834, test_loss: 0.790861, train_metric: -0.671, test_metric: -0.608\n",
            "----\n",
            "Epoch 222/300, lr=0.000802\n",
            "--> train_loss: 0.733450, test_loss: 0.782527, train_metric: -0.671, test_metric: -0.625\n",
            "----\n",
            "Epoch 223/300, lr=0.000801\n",
            "--> train_loss: 0.744441, test_loss: 0.806434, train_metric: -0.657, test_metric: -0.617\n",
            "----\n",
            "Epoch 224/300, lr=0.000800\n",
            "--> train_loss: 0.733244, test_loss: 0.822234, train_metric: -0.672, test_metric: -0.596\n",
            "----\n",
            "Epoch 225/300, lr=0.000799\n",
            "--> train_loss: 0.748531, test_loss: 0.783426, train_metric: -0.663, test_metric: -0.636\n",
            "----\n",
            "Epoch 226/300, lr=0.000798\n",
            "--> train_loss: 0.738881, test_loss: 0.800464, train_metric: -0.667, test_metric: -0.607\n",
            "----\n",
            "Epoch 227/300, lr=0.000798\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.735747, test_loss: 0.769518, train_metric: -0.674, test_metric: -0.625\n",
            "----\n",
            "Epoch 228/300, lr=0.000797\n",
            "--> train_loss: 0.724459, test_loss: 0.769703, train_metric: -0.678, test_metric: -0.616\n",
            "----\n",
            "Epoch 229/300, lr=0.000796\n",
            "--> train_loss: 0.725624, test_loss: 0.784265, train_metric: -0.673, test_metric: -0.615\n",
            "----\n",
            "Epoch 230/300, lr=0.000795\n",
            "--> train_loss: 0.725278, test_loss: 0.779885, train_metric: -0.674, test_metric: -0.621\n",
            "----\n",
            "Epoch 231/300, lr=0.000794\n",
            "--> train_loss: 0.728224, test_loss: 0.771714, train_metric: -0.670, test_metric: -0.625\n",
            "----\n",
            "Epoch 232/300, lr=0.000794\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.720681, test_loss: 0.760179, train_metric: -0.677, test_metric: -0.627\n",
            "----\n",
            "Epoch 233/300, lr=0.000793\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.720642, test_loss: 0.759963, train_metric: -0.674, test_metric: -0.622\n",
            "----\n",
            "Epoch 234/300, lr=0.000792\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.717024, test_loss: 0.755121, train_metric: -0.682, test_metric: -0.629\n",
            "----\n",
            "Epoch 235/300, lr=0.000791\n",
            "--> train_loss: 0.715381, test_loss: 0.768795, train_metric: -0.682, test_metric: -0.631\n",
            "----\n",
            "Epoch 236/300, lr=0.000790\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.715313, test_loss: 0.755070, train_metric: -0.673, test_metric: -0.626\n",
            "----\n",
            "Epoch 237/300, lr=0.000790\n",
            "--> train_loss: 0.713147, test_loss: 0.763316, train_metric: -0.677, test_metric: -0.623\n",
            "----\n",
            "Epoch 238/300, lr=0.000789\n",
            "--> train_loss: 0.706165, test_loss: 0.761208, train_metric: -0.684, test_metric: -0.637\n",
            "----\n",
            "Epoch 239/300, lr=0.000788\n",
            "--> train_loss: 0.717573, test_loss: 0.768591, train_metric: -0.683, test_metric: -0.618\n",
            "----\n",
            "Epoch 240/300, lr=0.000787\n",
            "--> train_loss: 0.709885, test_loss: 0.757122, train_metric: -0.677, test_metric: -0.612\n",
            "----\n",
            "Epoch 241/300, lr=0.000787\n",
            "--> train_loss: 0.713929, test_loss: 0.758726, train_metric: -0.675, test_metric: -0.625\n",
            "----\n",
            "Epoch 242/300, lr=0.000786\n",
            "--> train_loss: 0.705523, test_loss: 0.755758, train_metric: -0.679, test_metric: -0.644\n",
            "----\n",
            "Epoch 243/300, lr=0.000785\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.707200, test_loss: 0.751539, train_metric: -0.682, test_metric: -0.634\n",
            "----\n",
            "Epoch 244/300, lr=0.000784\n",
            "--> train_loss: 0.699830, test_loss: 0.754914, train_metric: -0.685, test_metric: -0.633\n",
            "----\n",
            "Epoch 245/300, lr=0.000783\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.700359, test_loss: 0.746321, train_metric: -0.685, test_metric: -0.637\n",
            "----\n",
            "Epoch 246/300, lr=0.000783\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.702698, test_loss: 0.744790, train_metric: -0.672, test_metric: -0.634\n",
            "----\n",
            "Epoch 247/300, lr=0.000782\n",
            "--> train_loss: 0.699532, test_loss: 0.745384, train_metric: -0.689, test_metric: -0.644\n",
            "----\n",
            "Epoch 248/300, lr=0.000781\n",
            "--> train_loss: 0.693671, test_loss: 0.758099, train_metric: -0.695, test_metric: -0.617\n",
            "----\n",
            "Epoch 249/300, lr=0.000780\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.701343, test_loss: 0.737518, train_metric: -0.685, test_metric: -0.641\n",
            "----\n",
            "Epoch 250/300, lr=0.000779\n",
            "--> train_loss: 0.690350, test_loss: 0.749395, train_metric: -0.685, test_metric: -0.622\n",
            "----\n",
            "Epoch 251/300, lr=0.000779\n",
            "--> train_loss: 0.697487, test_loss: 0.752563, train_metric: -0.685, test_metric: -0.643\n",
            "----\n",
            "Epoch 252/300, lr=0.000778\n",
            "--> train_loss: 0.704931, test_loss: 0.740896, train_metric: -0.695, test_metric: -0.623\n",
            "----\n",
            "Epoch 253/300, lr=0.000777\n",
            "--> train_loss: 0.692645, test_loss: 0.741407, train_metric: -0.690, test_metric: -0.633\n",
            "----\n",
            "Epoch 254/300, lr=0.000776\n",
            "--> train_loss: 0.693487, test_loss: 0.757290, train_metric: -0.684, test_metric: -0.642\n",
            "----\n",
            "Epoch 255/300, lr=0.000776\n",
            "--> train_loss: 0.695995, test_loss: 0.741885, train_metric: -0.681, test_metric: -0.654\n",
            "----\n",
            "Epoch 256/300, lr=0.000775\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.688863, test_loss: 0.734906, train_metric: -0.691, test_metric: -0.633\n",
            "----\n",
            "Epoch 257/300, lr=0.000774\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.683555, test_loss: 0.731980, train_metric: -0.696, test_metric: -0.653\n",
            "----\n",
            "Epoch 258/300, lr=0.000773\n",
            "--> train_loss: 0.689050, test_loss: 0.751800, train_metric: -0.680, test_metric: -0.629\n",
            "----\n",
            "Epoch 259/300, lr=0.000772\n",
            "--> train_loss: 0.686154, test_loss: 0.753284, train_metric: -0.691, test_metric: -0.638\n",
            "----\n",
            "Epoch 260/300, lr=0.000772\n",
            "--> train_loss: 0.680424, test_loss: 0.734434, train_metric: -0.694, test_metric: -0.643\n",
            "----\n",
            "Epoch 261/300, lr=0.000771\n",
            "--> train_loss: 0.681010, test_loss: 0.749177, train_metric: -0.698, test_metric: -0.632\n",
            "----\n",
            "Epoch 262/300, lr=0.000770\n",
            "--> train_loss: 0.691336, test_loss: 0.742003, train_metric: -0.677, test_metric: -0.652\n",
            "----\n",
            "Epoch 263/300, lr=0.000769\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.680580, test_loss: 0.728948, train_metric: -0.691, test_metric: -0.643\n",
            "----\n",
            "Epoch 264/300, lr=0.000769\n",
            "--> train_loss: 0.677224, test_loss: 0.734615, train_metric: -0.705, test_metric: -0.639\n",
            "----\n",
            "Epoch 265/300, lr=0.000768\n",
            "--> train_loss: 0.675302, test_loss: 0.739882, train_metric: -0.702, test_metric: -0.627\n",
            "----\n",
            "Epoch 266/300, lr=0.000767\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.673422, test_loss: 0.721987, train_metric: -0.695, test_metric: -0.656\n",
            "----\n",
            "Epoch 267/300, lr=0.000766\n",
            "--> train_loss: 0.671867, test_loss: 0.733569, train_metric: -0.701, test_metric: -0.653\n",
            "----\n",
            "Epoch 268/300, lr=0.000766\n",
            "--> train_loss: 0.676237, test_loss: 0.728106, train_metric: -0.700, test_metric: -0.649\n",
            "----\n",
            "Epoch 269/300, lr=0.000765\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.670144, test_loss: 0.718998, train_metric: -0.700, test_metric: -0.642\n",
            "----\n",
            "Epoch 270/300, lr=0.000764\n",
            "--> train_loss: 0.665448, test_loss: 0.726786, train_metric: -0.708, test_metric: -0.646\n",
            "----\n",
            "Epoch 271/300, lr=0.000763\n",
            "--> train_loss: 0.670079, test_loss: 0.721153, train_metric: -0.702, test_metric: -0.652\n",
            "----\n",
            "Epoch 272/300, lr=0.000763\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.668154, test_loss: 0.716253, train_metric: -0.709, test_metric: -0.639\n",
            "----\n",
            "Epoch 273/300, lr=0.000762\n",
            "--> train_loss: 0.661235, test_loss: 0.725953, train_metric: -0.706, test_metric: -0.634\n",
            "----\n",
            "Epoch 274/300, lr=0.000761\n",
            "--> train_loss: 0.666952, test_loss: 0.726717, train_metric: -0.701, test_metric: -0.646\n",
            "----\n",
            "Epoch 275/300, lr=0.000760\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.665259, test_loss: 0.714390, train_metric: -0.703, test_metric: -0.644\n",
            "----\n",
            "Epoch 276/300, lr=0.000759\n",
            "--> train_loss: 0.663838, test_loss: 0.726352, train_metric: -0.706, test_metric: -0.644\n",
            "----\n",
            "Epoch 277/300, lr=0.000759\n",
            "--> train_loss: 0.664397, test_loss: 0.735406, train_metric: -0.699, test_metric: -0.639\n",
            "----\n",
            "Epoch 278/300, lr=0.000758\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.663588, test_loss: 0.713765, train_metric: -0.705, test_metric: -0.653\n",
            "----\n",
            "Epoch 279/300, lr=0.000757\n",
            "--> train_loss: 0.662267, test_loss: 0.717651, train_metric: -0.707, test_metric: -0.660\n",
            "----\n",
            "Epoch 280/300, lr=0.000756\n",
            "--> train_loss: 0.658792, test_loss: 0.729172, train_metric: -0.706, test_metric: -0.641\n",
            "----\n",
            "Epoch 281/300, lr=0.000756\n",
            "--> train_loss: 0.665349, test_loss: 0.721972, train_metric: -0.698, test_metric: -0.658\n",
            "----\n",
            "Epoch 282/300, lr=0.000755\n",
            "--> train_loss: 0.664528, test_loss: 0.718548, train_metric: -0.700, test_metric: -0.659\n",
            "----\n",
            "Epoch 283/300, lr=0.000754\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.655347, test_loss: 0.708948, train_metric: -0.709, test_metric: -0.657\n",
            "----\n",
            "Epoch 284/300, lr=0.000753\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.660141, test_loss: 0.704673, train_metric: -0.698, test_metric: -0.657\n",
            "----\n",
            "Epoch 285/300, lr=0.000753\n",
            "--> train_loss: 0.652248, test_loss: 0.710778, train_metric: -0.714, test_metric: -0.651\n",
            "----\n",
            "Epoch 286/300, lr=0.000752\n",
            "--> train_loss: 0.652065, test_loss: 0.713229, train_metric: -0.705, test_metric: -0.652\n",
            "----\n",
            "Epoch 287/300, lr=0.000751\n",
            "--> train_loss: 0.660158, test_loss: 0.711491, train_metric: -0.705, test_metric: -0.669\n",
            "----\n",
            "Epoch 288/300, lr=0.000750\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.653920, test_loss: 0.704666, train_metric: -0.704, test_metric: -0.653\n",
            "----\n",
            "Epoch 289/300, lr=0.000750\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.645113, test_loss: 0.698304, train_metric: -0.719, test_metric: -0.643\n",
            "----\n",
            "Epoch 290/300, lr=0.000749\n",
            "--> train_loss: 0.645312, test_loss: 0.701733, train_metric: -0.713, test_metric: -0.667\n",
            "----\n",
            "Epoch 291/300, lr=0.000748\n",
            "--> train_loss: 0.644317, test_loss: 0.702682, train_metric: -0.715, test_metric: -0.657\n",
            "----\n",
            "Epoch 292/300, lr=0.000747\n",
            "--> train_loss: 0.642788, test_loss: 0.702016, train_metric: -0.709, test_metric: -0.656\n",
            "----\n",
            "Epoch 293/300, lr=0.000747\n",
            "--> train_loss: 0.647552, test_loss: 0.717239, train_metric: -0.708, test_metric: -0.654\n",
            "----\n",
            "Epoch 294/300, lr=0.000746\n",
            "--> train_loss: 0.647370, test_loss: 0.709369, train_metric: -0.708, test_metric: -0.654\n",
            "----\n",
            "Epoch 295/300, lr=0.000745\n",
            "--> train_loss: 0.648026, test_loss: 0.701824, train_metric: -0.712, test_metric: -0.652\n",
            "----\n",
            "Epoch 296/300, lr=0.000744\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.641471, test_loss: 0.695145, train_metric: -0.719, test_metric: -0.660\n",
            "----\n",
            "Epoch 297/300, lr=0.000744\n",
            "--> train_loss: 0.637625, test_loss: 0.699605, train_metric: -0.714, test_metric: -0.673\n",
            "----\n",
            "Epoch 298/300, lr=0.000743\n",
            "--> model improved! --> saved to ./weights_classification.pt\n",
            "--> train_loss: 0.641112, test_loss: 0.693360, train_metric: -0.710, test_metric: -0.663\n",
            "----\n",
            "Epoch 299/300, lr=0.000742\n",
            "--> train_loss: 0.644490, test_loss: 0.696864, train_metric: -0.710, test_metric: -0.662\n",
            "----\n",
            "Epoch 300/300, lr=0.000741\n",
            "--> train_loss: 0.643866, test_loss: 0.707218, train_metric: -0.707, test_metric: -0.642\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3_xmjnT8NmjV",
        "outputId": "44fa9dbb-657a-404c-f445-a933a7b458d5"
      },
      "source": [
        "fig_loss = go.Figure()\n",
        "fig_metric = go.Figure()\n",
        "\n",
        "x = [i+1 for i in range(params[\"num_epochs\"])]\n",
        "\n",
        "fig_loss.add_traces( go.Scatter(x=x,y=loss_history[\"train\"], name=\"train loss\", mode=\"lines+markers\" ) )\n",
        "fig_loss.add_traces( go.Scatter(x=x,y=loss_history[\"test\"], name=\"test loss\"  , mode=\"lines+markers\") )\n",
        "fig_loss.update_layout(title=\"Loss Results\", xaxis_title=\"epochs\", hovermode=\"x\")\n",
        "fig_loss.show()\n",
        "\n",
        "fig_metric.add_traces( go.Scatter(x=x,y=metric_history[\"train\"], name=\"train metric\", mode=\"lines+markers\") )\n",
        "fig_metric.add_traces( go.Scatter(x=x,y=metric_history[\"test\"], name=\"test_metric\" , mode=\"lines+markers\") )\n",
        "fig_metric.update_layout(title=\"Metric Results\", xaxis_title=\"epochs\", hovermode=\"x\")\n",
        "fig_metric.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"f179ae44-49c7-4d8b-a11b-eaad5b476408\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"f179ae44-49c7-4d8b-a11b-eaad5b476408\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'f179ae44-49c7-4d8b-a11b-eaad5b476408',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"train loss\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300], \"y\": [1.170238269882932, 1.105318063377194, 1.1013533787823635, 1.106576084457219, 1.102462510120096, 1.1006960742151826, 1.0980747670853683, 1.098131121028369, 1.095182719823531, 1.093810206138619, 1.0918121348025316, 1.0877539212088023, 1.0887036906292031, 1.085188181570055, 1.0853671642469676, 1.084171267802353, 1.0828581981273817, 1.0806336174862545, 1.079453385218297, 1.0801713104329125, 1.0782715780195344, 1.07594818617915, 1.0696565555081992, 1.0661586027723062, 1.064149380236959, 1.0625746247618915, 1.0596285849397924, 1.070465013907389, 1.0563113479127793, 1.0555813563364091, 1.0499690468329046, 1.0490658635414116, 1.0511615116715305, 1.041852182840313, 1.0380147261016555, 1.033587291567536, 1.0314888888287113, 1.0354105820691295, 1.0301718321667468, 1.032365515178879, 1.0240717603093632, 1.027318149773398, 1.0245822304493564, 1.0230933330264278, 1.026447610317964, 1.0289598570366596, 1.0147763170167312, 1.0232311759567667, 1.0144133167489808, 1.0143995771499292, 1.0118791172785662, 1.0144129741964634, 1.0122292870289462, 1.0136666647559398, 1.0076715264639615, 1.0102862622608668, 1.0088150012252435, 1.0110204354102756, 1.0089623586024286, 1.0083652581469285, 1.005645217854969, 1.003916316787444, 1.0001743713930233, 1.0010817879444736, 1.0052548812882427, 1.0017498210943467, 1.0017589767731718, 0.9986996473338728, 1.0012924192308998, 1.0015664592179432, 1.00470750770204, 0.9969102077099012, 0.9928956989527509, 0.9927153582273963, 0.9937996504537358, 0.9982558330491296, 0.9911415858172459, 0.9909862625738259, 0.9899141877161203, 0.9878171683624625, 0.9958266157906317, 0.995424936479007, 0.9871149134053181, 0.9808421636615878, 0.9828652717355207, 0.9871103659699751, 0.9803302817592966, 0.9824430068418399, 0.9861053016816645, 0.9810771810387704, 0.9764554320689088, 0.9743444694089333, 0.9739379842273741, 0.9737331337680472, 0.9716372930788209, 0.9744387047957158, 0.9712721756743574, 0.9680034260440709, 0.9689696460930625, 0.9673603384198342, 0.9665093761448146, 0.9619951511670882, 0.9627250332888078, 0.962366367627913, 0.9594317982478046, 0.9569672362583983, 0.9567866948661845, 0.9521119556568888, 0.9547882363849441, 0.9499158190363398, 0.9562329303445523, 0.951078578085499, 0.9445015971136144, 0.941549215002597, 0.9406671584855991, 0.9372428859585024, 0.9369792380824479, 0.9340175695551063, 0.9317618510928342, 0.9374780269789011, 0.945630324887672, 0.927336583355408, 0.9285176426140586, 0.926844521866088, 0.9296945730000576, 0.9187931644549152, 0.9237715308648493, 0.9251405653107305, 0.9153651806043894, 0.9273167335518362, 0.923672427786524, 0.9234995745700427, 0.9142007929105688, 0.9102835285296222, 0.9080866693056858, 0.899131447551861, 0.9008388063226065, 0.9003798244610096, 0.9106617693439933, 0.9090160771206766, 0.9008334045329078, 0.8976942177913394, 0.8986945228292889, 0.8920750501432023, 0.891508493616021, 0.8847476095752939, 0.8843038601525658, 0.8823621691096728, 0.8810475934197874, 0.8731633631253217, 0.8766276423406651, 0.87784283016744, 0.8685584853736804, 0.8703932635462373, 0.869326229683271, 0.8712280011962501, 0.8626656790722189, 0.8618895624953299, 0.858928443268179, 0.8534211859059511, 0.8538249864081647, 0.850342019838177, 0.8519527585296144, 0.8417692488488938, 0.8405324893347389, 0.8355370186087175, 0.8415773586309678, 0.8375935934555264, 0.8340071121767147, 0.8355599316223015, 0.8292118512355305, 0.8312911662994612, 0.8313691324686016, 0.8231934051837982, 0.826753483061836, 0.8269489229548876, 0.8123055538132897, 0.8193349777448696, 0.8207166187314754, 0.8133657619626312, 0.8142282737302223, 0.8118501525376226, 0.8068162510676288, 0.8045309484321683, 0.799954853199747, 0.7984072105537439, 0.7975764360742031, 0.794916466622753, 0.792281683902558, 0.7879826893335194, 0.7865975782291036, 0.7877716070533939, 0.7879964856868342, 0.77972298094114, 0.7806053891318764, 0.7759891757296198, 0.7739972743927229, 0.7761082162765843, 0.7786838083540849, 0.7748398902439032, 0.7684380797853378, 0.7665868291946071, 0.7676817623385714, 0.7704427751547218, 0.7607664306916289, 0.7625263560717721, 0.7589616628559692, 0.7598672850605785, 0.753814286303951, 0.7594594033187558, 0.7553554599774125, 0.7543648801878585, 0.75816386969766, 0.7486114334730742, 0.745894972246841, 0.7435718591104279, 0.7435411606280887, 0.7394247744212622, 0.7366107318403871, 0.7383357269984376, 0.7358341065020667, 0.7334502622500997, 0.7444413030058874, 0.7332444221383073, 0.7485308824512834, 0.7388806560974446, 0.7357469831338977, 0.7244593557465215, 0.7256244239847668, 0.725277618444688, 0.7282237509991993, 0.7206814306828725, 0.7206422585863362, 0.7170242173765469, 0.7153811155799599, 0.7153126400411446, 0.7131470255593311, 0.7061652052795215, 0.7175726044317361, 0.7098850372873888, 0.7139288530339597, 0.7055232927725749, 0.7072003087885955, 0.6998297927483489, 0.7003588250795663, 0.7026984106341533, 0.6995319955280559, 0.6936711342757871, 0.701343352432332, 0.6903502183563525, 0.6974872878963458, 0.7049305608751416, 0.6926451621222826, 0.69348671079062, 0.6959950085274091, 0.6888631745682006, 0.6835554824245343, 0.6890502133100876, 0.6861544790480772, 0.6804236869122853, 0.6810095089781677, 0.6913364376955914, 0.6805798341059913, 0.6772239393179526, 0.6753024852237843, 0.673421935751385, 0.671866710330424, 0.6762369467018768, 0.6701439686206652, 0.6654482009433661, 0.670078629261629, 0.6681539239589517, 0.6612345379293282, 0.6669519107222683, 0.6652586675475685, 0.6638380044578366, 0.6643969255096728, 0.6635883485346115, 0.6622666425836707, 0.6587916824186772, 0.6653494535926552, 0.6645284555416938, 0.6553471932122356, 0.6601410398574489, 0.6522484970903548, 0.6520645088901175, 0.6601577043279958, 0.6539203864735067, 0.6451134291516101, 0.6453118116518642, 0.6443166976020642, 0.6427878892637084, 0.647551957149688, 0.6473703333725964, 0.648025618096087, 0.6414706993305721, 0.6376246169573696, 0.6411118998917712, 0.6444896957446108, 0.6438656679248709]}, {\"mode\": \"lines+markers\", \"name\": \"test loss\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300], \"y\": [1.102328502555762, 1.0956267270604623, 1.0960183125889433, 1.094528132064044, 1.0954517615033497, 1.097027548183739, 1.087384248754792, 1.0857213527296556, 1.0855453188682282, 1.0832995446641205, 1.079870136549334, 1.0833038556974617, 1.0764892275596345, 1.0779067633911283, 1.0752066573245818, 1.0736271457601214, 1.0742061808797654, 1.0702002618688042, 1.0750945137335908, 1.0670741765472525, 1.0699194490983437, 1.0711607619880006, 1.0628099216995335, 1.0653724055898826, 1.059276935336315, 1.0529413991405765, 1.0665359591610517, 1.0712970984174122, 1.059314609607947, 1.0479625621545123, 1.0463350336200095, 1.060925782687011, 1.0624842685099132, 1.0364294702087933, 1.0377072945255625, 1.040486861634284, 1.0443243241812366, 1.0410977520759517, 1.0314767357732875, 1.0320174962909987, 1.032953387594873, 1.0269430082526791, 1.0305185046190075, 1.0277695968987182, 1.0418320363929872, 1.0282322435721736, 1.024814865698454, 1.0223887607806086, 1.0299638845072656, 1.0202127903367506, 1.023953472076, 1.0183923903303371, 1.0187154531183444, 1.019785960219902, 1.0186570745418506, 1.019845473278854, 1.0246460830707385, 1.0343744166869362, 1.0202155745457835, 1.0157098025637488, 1.0140867268728944, 1.0129396138433482, 1.0187966158842066, 1.0193590166548074, 1.0190021793786181, 1.0103505830634778, 1.0097027557637759, 1.0053653929756476, 1.0111018609823348, 1.0070084024686943, 1.0059555791124535, 1.0008812092493902, 1.011212491930048, 1.0024537862898248, 1.0058128423111736, 1.0008388174215361, 0.999862349461741, 1.007335081419537, 1.0006369929922263, 1.0132670491158298, 1.017743173289624, 1.0068139474483524, 0.9935492003865106, 0.9993517756314378, 0.9953745732076993, 0.9932705242246795, 1.0005850809657442, 0.9944124989940922, 1.0082604503986117, 0.9841439059232691, 0.9884636275091727, 0.9820138442028854, 0.991915942715594, 0.9788472436765461, 0.9904031765357565, 0.9811301627271385, 0.9861171490788607, 0.9824065689408351, 0.9828064220102303, 0.9811991108985343, 0.9797802562311827, 0.9713997066833421, 0.9848742006436599, 0.9836256837903936, 0.97139537674196, 0.9754030967438235, 0.9725450990811599, 0.9622011763751137, 0.9632896839316745, 0.9767881911040239, 0.9692630696917112, 0.9586052782325792, 0.9553550521680414, 0.9584496792364298, 0.9587591045998877, 0.9512523098062848, 0.9561627800432132, 0.9457745983402673, 0.95778200322072, 0.9600563474747328, 0.947640571452428, 0.9457340831354796, 0.9482431470830792, 0.9500512190467806, 0.9439160986312999, 0.9482594978115078, 0.941519291722937, 0.9348168733542441, 0.9527560426695492, 0.9424504607435791, 0.9500947075410108, 0.9393473014217032, 0.9308376241350705, 0.9421498890703053, 0.9237069055493436, 0.9278428504310429, 0.9264611784262581, 0.9324390123029328, 0.9421480077202288, 0.9180875775834737, 0.9271694121898476, 0.9149687576766646, 0.914004116696496, 0.9138940860790834, 0.9061890688379753, 0.9134348097639309, 0.915604214390562, 0.9048825610230551, 0.9062120751968251, 0.8995108480347134, 0.9164809707963039, 0.890603997893493, 0.8939008180979903, 0.8931559263699708, 0.9126061837764773, 0.8964789058491496, 0.890250578245709, 0.8893268306311474, 0.8907464416201377, 0.8850816681777973, 0.8946950787209814, 0.8755129751515064, 0.879071104482204, 0.8700853889020993, 0.8735307459494438, 0.872733724752471, 0.8673431131771802, 0.8667133855996965, 0.8728873809474108, 0.8739068505193812, 0.8740307265497937, 0.8783378270183502, 0.8560727531877445, 0.8639689543581068, 0.8650457578373073, 0.854101542646557, 0.8779380259460676, 0.8448735063403956, 0.855773263999816, 0.8469832182816857, 0.8562208551456494, 0.8517997486440666, 0.8593590037973396, 0.8534221247374051, 0.8376958438159513, 0.8358312604448018, 0.8460217633064204, 0.8320746675890766, 0.825407257458239, 0.8270356132195341, 0.841615504344009, 0.8390348640073187, 0.8234568410351079, 0.8257887980899551, 0.8225728617577157, 0.8215462780353305, 0.8136792047020819, 0.8249292763635572, 0.8353876825306643, 0.8133875765321866, 0.803590739083556, 0.8047661385423928, 0.8093769919621161, 0.8045894901696339, 0.807518147181401, 0.8063485099480497, 0.8072450394376355, 0.7962667519570579, 0.8241742073824858, 0.7898282898403012, 0.8119568901581983, 0.7986090469833053, 0.7946690603112052, 0.7909586881616302, 0.7941119443380552, 0.7960462180212085, 0.7941723555289532, 0.7780762853232458, 0.784819327617962, 0.7805008161466804, 0.790860560156008, 0.7825271530813148, 0.8064340310173554, 0.8222343794622977, 0.7834258900846838, 0.8004641905150006, 0.7695184681643045, 0.769703298991793, 0.7842654212847666, 0.7798846787236438, 0.771713578272634, 0.7601792053071392, 0.7599628122322621, 0.7551207985576643, 0.7687946258128945, 0.7550701675509579, 0.7633164227378118, 0.7612084976063933, 0.7685908774904159, 0.7571216569070922, 0.7587263067120218, 0.7557577868228804, 0.7515385000235972, 0.754914091126774, 0.7463209821093629, 0.7447895915091702, 0.7453840177741635, 0.7580987393782189, 0.737517549498226, 0.7493948292407256, 0.752562989680445, 0.7408962178850705, 0.7414067444452713, 0.75729028827048, 0.7418851604248955, 0.7349060134225914, 0.7319796017406893, 0.7518004567502009, 0.7532839107395253, 0.7344342388923579, 0.7491771929621549, 0.7420028724339519, 0.7289478675435762, 0.7346147870485668, 0.7398822363719917, 0.7219870377059615, 0.7335688320382081, 0.7281062470049663, 0.7189982011267982, 0.7267858181448822, 0.7211532309152823, 0.7162526448567709, 0.7259529851183129, 0.726716510720708, 0.7143904675338348, 0.726351722613291, 0.7354062918245866, 0.713764753188047, 0.7176512206501825, 0.7291716536624724, 0.721971897092155, 0.7185479770362598, 0.7089478872079389, 0.7046729493170718, 0.7107777979589601, 0.7132292160166536, 0.7114909289051609, 0.704666128241292, 0.6983039695238683, 0.7017334012737061, 0.7026816111662721, 0.7020157645035263, 0.7172386093801429, 0.7093693899842443, 0.701824462399016, 0.6951445682341282, 0.6996047765644362, 0.6933604196692635, 0.6968644247326857, 0.7072176171058172]}],\n",
              "                        {\"hovermode\": \"x\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Loss Results\"}, \"xaxis\": {\"title\": {\"text\": \"epochs\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f179ae44-49c7-4d8b-a11b-eaad5b476408');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"d18519d6-e53e-4bfb-81fa-d6f209ca8fb6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"d18519d6-e53e-4bfb-81fa-d6f209ca8fb6\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'd18519d6-e53e-4bfb-81fa-d6f209ca8fb6',\n",
              "                        [{\"mode\": \"lines+markers\", \"name\": \"train metric\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300], \"y\": [-0.3703506907545165, -0.38575982996811903, -0.3969181721572795, -0.3894792773645058, -0.410201912858661, -0.41923485653560044, -0.4064824654622742, -0.4266737513283741, -0.4256110520722636, -0.44208289054197664, -0.4293304994686504, -0.41923485653560044, -0.4303931987247609, -0.4442082890541977, -0.4436769394261424, -0.44155154091392135, -0.4505844845908608, -0.4410201912858661, -0.45589798087141337, -0.4505844845908608, -0.4404888416578108, -0.4452709883103082, -0.43304994686503717, -0.44686503719447396, -0.448990435706695, -0.4410201912858661, -0.448990435706695, -0.4341126461211477, -0.45483528161530284, -0.4404888416578108, -0.45111583421891605, -0.45642933049946866, -0.4585547290116897, -0.463336875664187, -0.45642933049946866, -0.4681190223166844, -0.47768331562167904, -0.4574920297555792, -0.46599362380446335, -0.4718384697130712, -0.4760892667375133, -0.4798087141339001, -0.4675876726886291, -0.4782146652497343, -0.4638682252922423, -0.4665249734325186, -0.4691817215727949, -0.4760892667375133, -0.47449521785334753, -0.47768331562167904, -0.47396386822529224, -0.4766206163655685, -0.4670563230605739, -0.4723698193411265, -0.4909670563230606, -0.4814027630180659, -0.47555791710945805, -0.49362380446333687, -0.48352816153028694, -0.47130712008501596, -0.4904357066950053, -0.4814027630180659, -0.485653560042508, -0.48724760892667374, -0.47502656748140276, -0.48299681190223165, -0.4861849096705632, -0.4819341126461211, -0.49309245483528164, -0.4904357066950053, -0.49362380446333687, -0.4909670563230606, -0.47927736450584485, -0.49309245483528164, -0.4867162592986185, -0.4819341126461211, -0.4962805526036132, -0.49256110520722635, -0.4962805526036132, -0.49840595111583424, -0.4914984059511158, -0.4851222104144527, -0.49840595111583424, -0.4952178533475027, -0.502125398512221, -0.502125398512221, -0.5037194473963869, -0.485653560042508, -0.5063761955366631, -0.5031880977683315, -0.49840595111583424, -0.5116896918172157, -0.5106269925611052, -0.5111583421891605, -0.502125398512221, -0.5047821466524973, -0.5037194473963869, -0.5148777895855473, -0.5085015940488842, -0.5212539851222104, -0.5143464399574921, -0.5143464399574921, -0.5228480340063762, -0.5228480340063762, -0.5175345377258236, -0.5324123273113709, -0.5233793836344315, -0.5249734325185972, -0.5270988310308182, -0.5185972369819342, -0.5191285866099894, -0.5340063761955367, -0.528692879914984, -0.5398512221041445, -0.5329436769394261, -0.5536663124335813, -0.5478214665249734, -0.5472901168969182, -0.5398512221041445, -0.538788522848034, -0.5276301806588736, -0.5499468650371945, -0.5483528161530287, -0.5605738575982997, -0.5462274176408076, -0.5573857598299681, -0.5547290116896918, -0.5350690754516472, -0.5621679064824655, -0.5249734325185972, -0.5515409139213603, -0.5648246546227418, -0.5557917109458024, -0.5616365568544102, -0.5685441020191286, -0.5696068012752391, -0.5605738575982997, -0.5685441020191286, -0.5685441020191286, -0.5589798087141339, -0.5690754516471839, -0.5680127523910733, -0.5573857598299681, -0.5839532412327312, -0.5706695005313497, -0.5802337938363443, -0.5674814027630181, -0.577577045696068, -0.5701381509032943, -0.5781083953241233, -0.5860786397449522, -0.5797024442082891, -0.5871413390010627, -0.5956429330499469, -0.5770456960680127, -0.5807651434643996, -0.5876726886291179, -0.594048884165781, -0.5929861849096706, -0.6099893730074389, -0.5956429330499469, -0.5945802337938364, -0.5871413390010627, -0.5951115834218916, -0.6163655685441021, -0.6105207226354942, -0.6174282678002125, -0.6073326248671626, -0.6259298618490967, -0.614240170031881, -0.6200850159404888, -0.6163655685441021, -0.6131774707757705, -0.6195536663124336, -0.620616365568544, -0.6083953241232731, -0.6227417640807651, -0.6317747077577046, -0.6227417640807651, -0.6195536663124336, -0.6317747077577046, -0.6296493092454836, -0.6392136025504782, -0.630712008501594, -0.630712008501594, -0.6392136025504782, -0.6349628055260361, -0.6424017003188098, -0.6365568544102019, -0.6439957492029755, -0.6546227417640808, -0.6392136025504782, -0.6365568544102019, -0.6466524973432518, -0.6509032943676939, -0.6535600425079703, -0.6540913921360255, -0.6535600425079703, -0.6509032943676939, -0.6503719447396387, -0.6535600425079703, -0.6530286928799149, -0.6636556854410202, -0.6461211477151966, -0.6609989373007439, -0.6609989373007439, -0.6631243358129649, -0.657279489904357, -0.6604675876726886, -0.6567481402763018, -0.6625929861849097, -0.6625929861849097, -0.6599362380446334, -0.6562167906482466, -0.6657810839532412, -0.6668437832093518, -0.6710945802337939, -0.675345377258236, -0.6726886291179596, -0.6742826780021254, -0.6705632306057385, -0.6710945802337939, -0.6567481402763018, -0.6716259298618491, -0.6631243358129649, -0.6668437832093518, -0.6737513283740701, -0.6780021253985122, -0.6726886291179596, -0.6737513283740701, -0.6695005313496281, -0.677470775770457, -0.6737513283740701, -0.6817215727948991, -0.6822529224229543, -0.6726886291179596, -0.6769394261424017, -0.6838469713071201, -0.6833156216790648, -0.677470775770457, -0.6748140276301806, -0.6785334750265675, -0.6817215727948991, -0.6849096705632306, -0.6854410201912858, -0.6721572794899043, -0.6886291179596175, -0.6950053134962806, -0.6849096705632306, -0.6849096705632306, -0.6849096705632306, -0.6950053134962806, -0.6896918172157279, -0.6838469713071201, -0.6811902231668437, -0.6907545164718385, -0.696068012752391, -0.6801275239107333, -0.6907545164718385, -0.69394261424017, -0.6976620616365569, -0.6769394261424017, -0.6907545164718385, -0.7051009564293305, -0.701912858660999, -0.6950053134962806, -0.7013815090329437, -0.6997874601487779, -0.6997874601487779, -0.7082890541976621, -0.7024442082890542, -0.7093517534537725, -0.706163655685441, -0.7013815090329437, -0.7029755579171094, -0.706163655685441, -0.6992561105207227, -0.7045696068012752, -0.7066950053134963, -0.7056323060573858, -0.6976620616365569, -0.6997874601487779, -0.7088204038257173, -0.6981934112646121, -0.71413390010627, -0.7045696068012752, -0.7051009564293305, -0.70403825717322, -0.7189160467587673, -0.7130712008501594, -0.7146652497343252, -0.7093517534537725, -0.7077577045696068, -0.7077577045696068, -0.7120085015940489, -0.7189160467587673, -0.7136025504782146, -0.7104144527098831, -0.7104144527098831, -0.7072263549415515]}, {\"mode\": \"lines+markers\", \"name\": \"test_metric\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300], \"y\": [-0.37174721189591076, -0.41263940520446096, -0.41263940520446096, -0.37174721189591076, -0.4560099132589839, -0.37174721189591076, -0.44981412639405205, -0.46716232961586124, -0.4547707558859975, -0.46716232961586124, -0.4349442379182156, -0.46220570012391576, -0.44981412639405205, -0.45724907063197023, -0.4560099132589839, -0.46096654275092935, -0.44981412639405205, -0.4795539033457249, -0.4399008674101611, -0.4547707558859975, -0.4510532837670384, -0.44609665427509293, -0.4510532837670384, -0.4547707558859975, -0.43866171003717475, -0.45724907063197023, -0.41759603469640644, -0.4473358116480793, -0.45848822800495664, -0.45724907063197023, -0.43866171003717475, -0.46592317224287483, -0.4448574969021066, -0.4423791821561338, -0.44609665427509293, -0.4473358116480793, -0.44981412639405205, -0.4547707558859975, -0.4473358116480793, -0.4485749690210657, -0.4634448574969021, -0.46964064436183395, -0.4510532837670384, -0.46220570012391576, -0.46220570012391576, -0.46096654275092935, -0.46220570012391576, -0.4646840148698885, -0.46592317224287483, -0.47831474597273854, -0.46592317224287483, -0.4758364312267658, -0.4423791821561338, -0.4708798017348203, -0.4721189591078067, -0.4485749690210657, -0.45848822800495664, -0.4721189591078067, -0.46716232961586124, -0.4646840148698885, -0.4795539033457249, -0.48946716232961585, -0.4399008674101611, -0.47831474597273854, -0.459727385377943, -0.49318463444857497, -0.4708798017348203, -0.48203221809169766, -0.4770755885997522, -0.48203221809169766, -0.483271375464684, -0.4646840148698885, -0.4708798017348203, -0.483271375464684, -0.4721189591078067, -0.49070631970260226, -0.4857496902106567, -0.4745972738537794, -0.47831474597273854, -0.4795539033457249, -0.459727385377943, -0.48079306071871125, -0.4969021065675341, -0.4857496902106567, -0.48451053283767037, -0.49318463444857497, -0.48203221809169766, -0.48079306071871125, -0.46716232961586124, -0.48451053283767037, -0.4758364312267658, -0.4745972738537794, -0.45848822800495664, -0.47831474597273854, -0.4684014869888476, -0.47335811648079307, -0.46592317224287483, -0.48079306071871125, -0.4993804213135068, -0.5055762081784386, -0.49318463444857497, -0.49566294919454773, -0.48698884758364314, -0.4993804213135068, -0.516728624535316, -0.4919454770755886, -0.4882280049566295, -0.5006195786864932, -0.4969021065675341, -0.4944237918215613, -0.5055762081784386, -0.5291201982651796, -0.5092936802973977, -0.5154894671623296, -0.5092936802973977, -0.506815365551425, -0.5204460966542751, -0.5142503097893433, -0.506815365551425, -0.5154894671623296, -0.5254027261462205, -0.5452292441140025, -0.5241635687732342, -0.5216852540272615, -0.5353159851301115, -0.5043370508054523, -0.5353159851301115, -0.540272614622057, -0.5092936802973977, -0.5241635687732342, -0.5353159851301115, -0.5278810408921933, -0.5216852540272615, -0.5315985130111525, -0.5328376703841388, -0.5278810408921933, -0.5551425030978935, -0.5278810408921933, -0.5415117719950434, -0.5315985130111525, -0.5489467162329615, -0.5216852540272615, -0.5365551425030979, -0.5377942998760843, -0.5489467162329615, -0.5377942998760843, -0.5588599752168525, -0.5229244114002478, -0.5526641883519207, -0.5551425030978935, -0.5514250309789344, -0.5712515489467163, -0.573729863692689, -0.5452292441140025, -0.5315985130111525, -0.5464684014869888, -0.573729863692689, -0.5700123915737298, -0.563816604708798, -0.5600991325898389, -0.5439900867410161, -0.5700123915737298, -0.5600991325898389, -0.5861214374225526, -0.5563816604708798, -0.5861214374225526, -0.5526641883519207, -0.5811648079306072, -0.5464684014869888, -0.5489467162329615, -0.5588599752168525, -0.5613382899628253, -0.6034696406443618, -0.5947955390334573, -0.5885997521685254, -0.5910780669144982, -0.573729863692689, -0.5762081784386617, -0.5774473358116481, -0.6059479553903345, -0.5910780669144982, -0.5848822800495663, -0.5774473358116481, -0.5675340768277571, -0.6009913258983891, -0.5985130111524164, -0.5712515489467163, -0.5811648079306072, -0.6059479553903345, -0.5799256505576208, -0.5749690210656754, -0.5935563816604709, -0.5898389095415117, -0.6183395291201983, -0.5985130111524164, -0.5885997521685254, -0.620817843866171, -0.5935563816604709, -0.5749690210656754, -0.5960346964064436, -0.6195786864931846, -0.6084262701363073, -0.6220570012391574, -0.6245353159851301, -0.6022304832713755, -0.6133828996282528, -0.6009913258983891, -0.6158612143742255, -0.6183395291201983, -0.6270136307311028, -0.6009913258983891, -0.6171003717472119, -0.6121437422552665, -0.6257744733581165, -0.6195786864931846, -0.6009913258983891, -0.6245353159851301, -0.6257744733581165, -0.6133828996282528, -0.6332094175960347, -0.6084262701363073, -0.6245353159851301, -0.6171003717472119, -0.5960346964064436, -0.6356877323420075, -0.6071871127633209, -0.6245353159851301, -0.6158612143742255, -0.6146220570012392, -0.620817843866171, -0.6245353159851301, -0.6270136307311028, -0.6220570012391574, -0.6294919454770755, -0.630731102850062, -0.6257744733581165, -0.6232961586121437, -0.6369268897149938, -0.6183395291201983, -0.6121437422552665, -0.6245353159851301, -0.644361833952912, -0.6344485749690211, -0.6332094175960347, -0.6369268897149938, -0.6344485749690211, -0.644361833952912, -0.6171003717472119, -0.6406443618339529, -0.6220570012391574, -0.6431226765799256, -0.6232961586121437, -0.6332094175960347, -0.6418835192069393, -0.654275092936803, -0.6332094175960347, -0.6530359355638166, -0.6294919454770755, -0.6381660470879802, -0.6431226765799256, -0.6319702602230484, -0.6517967781908303, -0.6431226765799256, -0.6394052044609665, -0.6270136307311028, -0.6555142503097894, -0.6530359355638166, -0.6493184634448576, -0.6418835192069393, -0.6456009913258984, -0.6517967781908303, -0.6394052044609665, -0.6344485749690211, -0.6456009913258984, -0.644361833952912, -0.644361833952912, -0.6394052044609665, -0.6530359355638166, -0.6604708798017348, -0.6406443618339529, -0.6579925650557621, -0.6592317224287485, -0.6567534076827757, -0.6567534076827757, -0.6505576208178439, -0.6517967781908303, -0.6691449814126395, -0.6530359355638166, -0.6431226765799256, -0.6666666666666666, -0.6567534076827757, -0.6555142503097894, -0.654275092936803, -0.654275092936803, -0.6517967781908303, -0.6604708798017348, -0.6728624535315985, -0.6629491945477075, -0.6617100371747212, -0.6418835192069393]}],\n",
              "                        {\"hovermode\": \"x\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Metric Results\"}, \"xaxis\": {\"title\": {\"text\": \"epochs\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d18519d6-e53e-4bfb-81fa-d6f209ca8fb6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh5CkI36nq2S"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd8jW5pynyCK",
        "outputId": "67852cf2-901b-4929-dfe2-2fc7cb947cd4"
      },
      "source": [
        "# Load Classification\n",
        "names = list({\"bianca\":0,\"gialla\": 1, \"arancione\": 2, \"rossa\": 3})\n",
        "model = ClassificationNet(num_inputs=10,num_classes=4).to(device)\n",
        "weights = torch.load(\"weights_classification.pt\")\n",
        "model.load_state_dict(weights)\n",
        "model = model.to(device)\n",
        "\n",
        "# Predict Classication\n",
        "# C_mean, C_std = 186785.728337672 679353.7516865473\n",
        "Xt = torch.from_numpy(X_test).type(torch.float32).unsqueeze(1)\n",
        "Xt = (Xt - C_mean) / C_std\n",
        "Y_hat = model.forward(Xt).argmax(dim=-1,keepdim=True).detach().numpy().reshape(-1)\n",
        "\n",
        "# Visualize results\n",
        "cm = confusion_matrix(Y_test,Y_hat)\n",
        "names_pred = [ \"Pred: \" + n for n in names]\n",
        "df = pd.DataFrame(cm, columns=names_pred, index=names)\n",
        "print(df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           Pred: bianca  Pred: gialla  Pred: arancione  Pred: rossa\n",
            "bianca                3             0                3            0\n",
            "gialla                8           212               53           27\n",
            "arancione             0            60              240           33\n",
            "rossa                 0            51               37           80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzgtzyhoSjJk"
      },
      "source": [
        "## Extra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qiaz_2DYhrf"
      },
      "source": [
        "\n",
        "# df.head() # prime 5 righe\n",
        "# df.tail() # ultime 5 righe\n",
        "# df.to_csv(\"a.csv\") # Salva come csv\n",
        "# df.keys() # nomi delle colonne\n",
        "# df.values.shape # dimensione\n",
        "# df.values # dati come array \n",
        "\n",
        "# Nomi delle colonne\n",
        "# Numero di righe\n",
        "# selezionare solo la colonna con nome dimessi_guariti\n",
        "# voglio i dati come array\n",
        "# voglio i deceduti > 700\n",
        "# dati del 12 marzo in poi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3OuqOczPcDe",
        "outputId": "2ac2262d-7181-4924-9e2c-e886c988b46f"
      },
      "source": [
        "x = np.array( [5,3,5,6,7] , dtype=np.float32)\n",
        "x = x.reshape(1,-1)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5., 3., 5., 6., 7.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScIeiTlAOixF",
        "outputId": "1bef673e-a10a-46c9-a6b3-3f6573b87d70"
      },
      "source": [
        "x = torch.tensor( [5,3,5,6,7] , dtype=torch.float32)\n",
        "#x = x.reshape(1,-1)\n",
        "#x = x.unsqueeze(0)\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zi77p6zQ9V8",
        "outputId": "e16d4c9e-a60a-452e-8e73-eda3e02e123c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "d = {\n",
        "    \"c\" : [5,6,7,8,9],\n",
        "    \"e\" : [15,26,37,18,29],\n",
        "    \"t\" : [0.5,0.26,3.7,1.8,29], \n",
        "}\n",
        "\n",
        "df = pd.DataFrame(d)\n",
        "df.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5.  , 15.  ,  0.5 ],\n",
              "       [ 6.  , 26.  ,  0.26],\n",
              "       [ 7.  , 37.  ,  3.7 ],\n",
              "       [ 8.  , 18.  ,  1.8 ],\n",
              "       [ 9.  , 29.  , 29.  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gACjphVQRmfr",
        "outputId": "ab8c298d-ea14-4b5d-e55c-2b422e4eaf88"
      },
      "source": [
        "x = torch.from_numpy(df.values).type(torch.float32)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5.0000, 15.0000,  0.5000],\n",
              "        [ 6.0000, 26.0000,  0.2600],\n",
              "        [ 7.0000, 37.0000,  3.7000],\n",
              "        [ 8.0000, 18.0000,  1.8000],\n",
              "        [ 9.0000, 29.0000, 29.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pK5HYRjSNaS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}